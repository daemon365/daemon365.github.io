[{"content":"简介 介绍及简单使用：https://www.cnblogs.com/daemon365/p/17690167.html 源码地址：https://github.com/etcd-io/bbolt\npage 因为 boltdb 是要落盘的，所以就要操作文件。为了提高效率，boltdb 会和其他数据库一样，会按 页（page）来操作文件。而且 boltdb 使用了 linux 的 mmap 来内存映射操作文件，这样可以提高效率。\n在 linux 中，每个 page 的大小是 4KB。\ngetconf PAGESIZE 4096 对应的每页在我们的代理里也应该有一个数据结构，来存储数据。这个数据结构就是 page。\ntype Pgid uint64 type Page struct { id Pgid flags uint16 // page 类型 count uint16 // page 中的元素数量 overflow uint32 // 是否有后序页，如果有，overflow 表示后续页的数量 } const ( BranchPageFlag = 0x01 LeafPageFlag = 0x02 MetaPageFlag = 0x04 FreelistPageFlag = 0x10 ) Page 里面有一个 flags 字段，用来标识这个 page 是什么类型的。boltdb 里面有四种类型的 page, 分别是 分支页（BranchPageFlag）、叶子页（LeafPageFlag）、元数据页（MetaPageFlag）、空闲列表页（FreelistPageFlag）。\n分支页：由于 boltdb 使用的是 B+ 树，所以分支页用来存储 key 和子节点的指针。 叶子页：叶子页用来存储 key 和 value。 元数据页：元数据页用来存储 boltdb 的元数据，比如 boltdb 的版本号、boltdb 的根节点等。 空闲列表页：由于 boltdb 使用 copy on write，所以当一个 page 被删除的时候，boltdb 并不会立即释放这个 page，而是把这个 page 加入到空闲列表页中，等到需要新的 page 的时候，再从空闲列表页中取出一个 page。 在 page 之后会存储对用的结构，比如 meta 或者 freelist。先读取 page 判断自己的结构（定长的：8 + 2 + 2 +4），然后再根据不同的数据类型读取其他的结构（比如BranchPage）。\nBranchPage \u0026amp;\u0026amp; LeafPage 这两个分别存储 B+ tree 的分支页和叶子页。对应结构为：\n// branchPageElement represents a node on a branch page. type branchPageElement struct { pos uint32 // 真实数据对应的偏移量 ksize uint32 // key 的大小 pgid Pgid // 指向 page 的 id } // leafPageElement represents a node on a leaf page. type leafPageElement struct { flags uint32 // 是否是一个 bucket pos uint32 // 真实数据对应的偏移量 ksize uint32 // key 的大小 vsize uint32 // value 的大小 } 对应的存储方式为:\n从 page 中拿取数据：\nfunc (p *Page) LeafPageElements() []leafPageElement { if p.count == 0 { return nil } // 从 page 的指针 加上 page 的大小，就是第一个元素的地址 data := UnsafeAdd(unsafe.Pointer(p), unsafe.Sizeof(*p)) // 转换为 slice elems := unsafe.Slice((*leafPageElement)(data), int(p.count)) return elems } func (p *Page) BranchPageElements() []branchPageElement { if p.count == 0 { return nil } data := UnsafeAdd(unsafe.Pointer(p), unsafe.Sizeof(*p)) elems := unsafe.Slice((*branchPageElement)(data), int(p.count)) return elems } MetaPage type Meta struct { magic uint32 // boltdb 的魔数 version uint32 // boltdb 的版本 pageSize uint32 // boltdb 的 page 大小 ，该值和操作系统默认的页大小保持一致 flags uint32 root InBucket // boltdb 的根节点 freelist Pgid // 空闲页的 id pgid Pgid // 当前 page 的 id txid Txid // 当前事务的 id checksum uint64 // 用作校验的校验和 } 它是如何写到 page 中的和从 page 中读取的呢？\n// 把 meta 写到 page 中 func (m *Meta) Write(p *Page) { // 检查 root bucket 的 pgid 是否有效。 // 如果 root.root 的 pgid 大于或等于 m.pgid，这是不合理的，因为这意味着它引用了一个尚未分配的 pgid。 if m.root.root \u0026gt;= m.pgid { panic(fmt.Sprintf(\u0026quot;root bucket pgid (%d) above high water mark (%d)\u0026quot;, m.root.root, m.pgid)) // 检查 freelist 的 pgid 是否有效。 // 如果 freelist 的 pgid 大于或等于 m.pgid 且 freelist 不是 PgidNoFreelist， // 这同样表示它引用了一个尚未分配的 pgid，这是不合理的。 } else if m.freelist \u0026gt;= m.pgid \u0026amp;\u0026amp; m.freelist != PgidNoFreelist { panic(fmt.Sprintf(\u0026quot;freelist pgid (%d) above high water mark (%d)\u0026quot;, m.freelist, m.pgid)) } // 指定 pageId 和 page 类型 p.id = Pgid(m.txid % 2) p.SetFlags(MetaPageFlag) // 计算校验和 m.checksum = m.Sum64() m.Copy(p.Meta()) } // 从 page 中读取 meta func (p *Page) Meta() *Meta { return (*Meta)(UnsafeAdd(unsafe.Pointer(p), unsafe.Sizeof(*p))) } // 把 meta 的数据拷贝到 page 中 func (m *Meta) Copy(dest *Meta) { *dest = *m } // 计算校验和 func (m *Meta) Sum64() uint64 { var h = fnv.New64a() _, _ = h.Write((*[unsafe.Offsetof(Meta{}.checksum)]byte)(unsafe.Pointer(m))[:]) return h.Sum64() } FreelistPage type freelist struct { // 表示 freelist 的类型，可能会有不同的策略或实现。 freelistType FreelistType // 存储所有已释放且可供分配的页面ID。 ids []common.Pgid // 记录哪个事务ID分配了特定的页面ID。 allocs map[common.Pgid]common.Txid // 记录即将被释放的页面ID及其所属的事务ID。 pending map[common.Txid]*txPending // 快速查找所有空闲和待处理页面ID的缓存。 cache map[common.Pgid]struct{} // 按连续页面大小分类的空闲页面，键是连续页面的大小，值是具有相同大小的起始页面ID的集合。 freemaps map[uint64]pidSet // 正向映射，键是起始页面ID，值是其span大小。 forwardMap map[common.Pgid]uint64 // 反向映射，键是结束页面ID，值是其span大小。 backwardMap map[common.Pgid]uint64 // 空闲页面的计数（基于哈希图的版本）。 freePagesCount uint64 // 分配函数，根据提供的事务ID和需求的页面数量分配页面。 allocate func(txid common.Txid, n int) common.Pgid // 返回当前空闲页面数量的函数。 free_count func() int // 合并连续空闲页面的函数。 mergeSpans func(ids common.Pgids) // 获取所有空闲页面ID的函数。 getFreePageIDs func() []common.Pgid // 读取一系列页面ID并初始化 freelist 的函数。 readIDs func(pgids []common.Pgid) } // FreelistType 定义了 freelist 后端的类型，用字符串表示不同的实现策略。 type FreelistType string // 未来的开发计划： // 1. 默认改为使用 `FreelistMapType`； // 2. 移除 `FreelistArrayType`，不再公开 `FreelistMapType`， // 并从 `DB` 和 `Options` 结构体中移除 `FreelistType` 字段。 const ( // FreelistArrayType 表示 freelist 的后端类型为数组。 // 这种类型可能适用于需要按顺序访问空闲页的场景。 FreelistArrayType = FreelistType(\u0026quot;array\u0026quot;) // FreelistMapType 表示 freelist 的后端类型为哈希映射。 // 这种类型提供了更快的查找速度，适合于频繁、随机地访问空闲页的情况。 FreelistMapType = FreelistType(\u0026quot;hashmap\u0026quot;) ) 把 freelist 写到 page 中：\n// write 将空闲和待处理的页面ID写入到 freelist 页面。 // 在程序崩溃的事件中，所有待处理的ID都将变成空闲的，因此这些ID都需要被保存到磁盘上。 func (f *freelist) write(p *common.Page) error { // 设置页头中的页类型标识 p.SetFlags(common.FreelistPageFlag) // 获取需要保存的 PageID 数量。 l := f.count() if l == 0 { // 没有 id 需要保存，直接返回。 p.SetCount(uint16(l)) } else if l \u0026lt; 0xFFFF { // 如果数量小于 0xFFFF // 将 id 数量写入到页头中 p.SetCount(uint16(l)) // 计算指针头 data := common.UnsafeAdd(unsafe.Pointer(p), unsafe.Sizeof(*p)) // 开辟一个 slice 用来存储 id ids := unsafe.Slice((*common.Pgid)(data), l) // 将 id 拷贝到 page 中 f.copyall(ids) } else { // 如果数量大于 0xFFFF ，则需要分多个 page 来保存 // 先设置本页的数量为 0xFFFF p.SetCount(0xFFFF) // 计算指针头 data := common.UnsafeAdd(unsafe.Pointer(p), unsafe.Sizeof(*p)) ids := unsafe.Slice((*common.Pgid)(data), l+1) // 将ID数量存储在第一个元素中 ids[0] = common.Pgid(l) // 将剩余的 id 拷贝到 page 中 f.copyall(ids[1:]) } return nil } // copyall 将所有空闲的ID和所有待处理的ID复制到一个排序后的列表中。 // f.count 返回目标数组 dst 需要的最小长度。 func (f *freelist) copyall(dst []common.Pgid) { // 创建一个切片用于存放待处理的ID，容量预设为待处理ID的数量。 m := make(common.Pgids, 0, f.pending_count()) // 遍历所有待处理事务，并将它们的ID加入到切片 m 中。 for _, txp := range f.pending { m = append(m, txp.ids...) } // 对切片 m 进行排序。 sort.Sort(m) // 将已经空闲的ID和刚排序的待处理ID合并到目标切片 dst 中。 common.Mergepgids(dst, f.getFreePageIDs(), m) } // Mergepgids 将两个已排序列表 a 和 b 的并集复制到 dst 中。 // 如果 dst 的长度不足以容纳结果，会触发 panic。 func Mergepgids(dst, a, b Pgids) { // 检查目标切片 dst 是否足够大以容纳 a 和 b 的所有元素。 if len(dst) \u0026lt; len(a)+len(b) { panic(fmt.Errorf(\u0026quot;mergepgids bad len %d \u0026lt; %d + %d\u0026quot;, len(dst), len(a), len(b))) } // 如果其中一个列表为空，则直接将另一个列表复制到 dst 中。 if len(a) == 0 { copy(dst, b) return } if len(b) == 0 { copy(dst, a) return } // 初始化一个切片 merged 来存储最终合并的结果。 merged := dst[:0] // 确定哪个列表的起始值更小，并将其设为 lead，另一个设为 follow。 lead, follow := a, b if b[0] \u0026lt; a[0] { lead, follow = b, a } // 循环合并，直到 lead 为空。 for len(lead) \u0026gt; 0 { // 合并 lead 中所有小于 follow[0] 的元素。 n := sort.Search(len(lead), func(i int) bool { return lead[i] \u0026gt; follow[0] }) merged = append(merged, lead[:n]...) if n \u0026gt;= len(lead) { break } // 交换 lead 和 follow，继续合并过程。 lead, follow = follow, lead[n:] } // 将剩余的 follow 元素加入到 merged 中。 _ = append(merged, follow...) } 从 page 中读取 freelist：\n// read 从 freelist 页面读取页面ID。 func (f *freelist) read(p *common.Page) { // 首先检查是否为 freelist 页面，如果不是则抛出错误。 if !p.IsFreelistPage() { panic(fmt.Sprintf(\u0026quot;invalid freelist page: %d, page type is %s\u0026quot;, p.Id(), p.Typ())) } // 从页面获取 freelist 页面的ID列表。 ids := p.FreelistPageIds() // 如果获取的ID列表为空，将 f.ids 设置为 nil，表示没有空闲页面。 if len(ids) == 0 { f.ids = nil } else { // 如果ID列表不为空，则创建一个新切片并复制这些ID，以避免直接修改原始页面数据。 idsCopy := make([]common.Pgid, len(ids)) copy(idsCopy, ids) // 确保复制的ID列表是排序的。 sort.Sort(common.Pgids(idsCopy)) // 将排序后的ID列表读入 freelist 结构。 f.readIDs(idsCopy) } } // hashmapReadIDs 读取输入的 pgids 并初始化 freelist（基于哈希映射的版本）。 func (f *freelist) hashmapReadIDs(pgids []common.Pgid) { // 初始化 freelist。 f.init(pgids) // 重建页面缓存。 f.reindex() } // reindex 基于可用和待处理的空闲列表重建自由缓存。 func (f *freelist) reindex() { // 获取所有空闲页面ID。 ids := f.getFreePageIDs() // 创建一个新的缓存映射。 f.cache = make(map[common.Pgid]struct{}, len(ids)) // 将所有空闲ID添加到缓存中。 for _, id := range ids { f.cache[id] = struct{}{} } // 将所有待处理的空闲ID也添加到缓存中。 for _, txp := range f.pending { for _, pendingID := range txp.ids { f.cache[pendingID] = struct{}{} } } } 分配页：\n// hashmapAllocate 根据传入的事务ID和请求的页面数量，分配页面。 func (f *freelist) hashmapAllocate(txid common.Txid, n int) common.Pgid { if n == 0 { // 如果请求的页面数量为0，则直接返回0，表示没有分配任何页面。 return 0 } // 检查是否存在完全匹配的空闲span。 if bm, ok := f.freemaps[uint64(n)]; ok { for pid := range bm { // 移除这个span。 f.delSpan(pid, uint64(n)) // 记录这个页面ID被哪个事务分配。 f.allocs[pid] = txid // 从缓存中移除已分配的页面。 for i := common.Pgid(0); i \u0026lt; common.Pgid(n); i++ { delete(f.cache, pid+i) } return pid } } // 在映射中查找大于请求大小的更大span。 for size, bm := range f.freemaps { if size \u0026lt; uint64(n) { continue } for pid := range bm { // 移除找到的大span。 f.delSpan(pid, size) // 记录页面分配。 f.allocs[pid] = txid // 计算剩余的span大小，并添加回 freelist。 remain := size - uint64(n) f.addSpan(pid+common.Pgid(n), remain) // 从缓存中移除已分配的页面。 for i := common.Pgid(0); i \u0026lt; common.Pgid(n); i++ { delete(f.cache, pid+i) } return pid } } return 0 } // delSpan 从 freelist 中删除一个span。 func (f *freelist) delSpan(start common.Pgid, size uint64) { // 更新前向和后向映射，移除对应的条目。 delete(f.forwardMap, start) delete(f.backwardMap, start+common.Pgid(size-1)) // 从 freemaps 中移除span。 delete(f.freemaps[size], start) if len(f.freemaps[size]) == 0 { // 如果某个大小的span已经没有其他项，从 freemaps 中完全移除这个大小。 delete(f.freemaps, size) } // 更新空闲页面计数。 f.freePagesCount -= size } // addSpan 向 freelist 中添加一个新的span。 func (f *freelist) addSpan(start common.Pgid, size uint64) { // 更新前向和后向映射。 f.backwardMap[start-1+common.Pgid(size)] = size f.forwardMap[start] = size // 确保 freemaps 中存在对应大小的映射。 if _, ok := f.freemaps[size]; !ok { f.freemaps[size] = make(map[common.Pgid]struct{}) } // 添加新的span到 freemaps。 f.freemaps[size][start] = struct{}{} // 更新空闲页面计数。 f.freePagesCount += size } Node page 的操作跟多都是基于磁盘设计的，在内存中使用这些数据结构并不是很方便。所以 boltdb 会把 page 的数据结构转换为 node 的数据结构，这样在内存中操作就会方便很多。\ntype node struct { bucket *Bucket // bucket 的指针 isLeaf bool // 是否是叶子节点 unbalanced bool // 是否平衡 spilled bool // 是否溢出 key []byte // 该 node 的起始 key pgid common.Pgid // 该 node 对应的 page id parent *node // 父节点 children nodes // 子节点 inodes common.Inodes // 存储键值对的结构体数组 } type Inode struct { flags uint32 // 用于 leaf node 是否是一个 bucket （subbucket） pgid Pgid\t// 用于 branch node, 子节点的 page id key []byte // key value []byte // value } type Inodes []Inode page to node func (n *node) read(p *common.Page) { n.pgid = p.Id() n.isLeaf = p.IsLeafPage() // 读取 inodes n.inodes = common.ReadInodeFromPage(p) // 保存第一个键，以便在将节点写入到父节点时能够找到这个节点。 if len(n.inodes) \u0026gt; 0 { n.key = n.inodes[0].Key() common.Assert(len(n.key) \u0026gt; 0, \u0026quot;read: zero-length node key\u0026quot;) } else { n.key = nil } } func ReadInodeFromPage(p *Page) Inodes { inodes := make(Inodes, int(p.Count())) isLeaf := p.IsLeafPage() for i := 0; i \u0026lt; int(p.Count()); i++ { inode := \u0026amp;inodes[i] if isLeaf { // 转换为 leafPageElement 结构 elem := p.LeafPageElement(uint16(i)) inode.SetFlags(elem.Flags()) inode.SetKey(elem.Key()) inode.SetValue(elem.Value()) } else { // 转换为 branchPageElement 结构 elem := p.BranchPageElement(uint16(i)) inode.SetPgid(elem.Pgid()) inode.SetKey(elem.Key()) } Assert(len(inode.Key()) \u0026gt; 0, \u0026quot;read: zero-length inode key\u0026quot;) } return inodes } node to page // write writes the items onto one or more pages. // The page should have p.id (might be 0 for meta or bucket-inline page) and p.overflow set // and the rest should be zeroed. func (n *node) write(p *common.Page) { common.Assert(p.Count() == 0 \u0026amp;\u0026amp; p.Flags() == 0, \u0026quot;node cannot be written into a not empty page\u0026quot;) // Initialize page. if n.isLeaf { p.SetFlags(common.LeafPageFlag) } else { p.SetFlags(common.BranchPageFlag) } if len(n.inodes) \u0026gt;= 0xFFFF { panic(fmt.Sprintf(\u0026quot;inode overflow: %d (pgid=%d)\u0026quot;, len(n.inodes), p.Id())) } p.SetCount(uint16(len(n.inodes))) // Stop here if there are no items to write. if p.Count() == 0 { return } // 将node的inodes写入page common.WriteInodeToPage(n.inodes, p) // DEBUG ONLY: n.dump() } func WriteInodeToPage(inodes Inodes, p *Page) uint32 { // 计算写入的初始偏移量。 off := unsafe.Sizeof(*p) + p.PageElementSize()*uintptr(len(inodes)) isLeaf := p.IsLeafPage() for i, item := range inodes { Assert(len(item.Key()) \u0026gt; 0, \u0026quot;write: zero-length inode key\u0026quot;) // 创建一个足够大小的切片来存放键和值。 sz := len(item.Key()) + len(item.Value()) b := UnsafeByteSlice(unsafe.Pointer(p), off, 0, sz) off += uintptr(sz) // Write the page element. if isLeaf { elem := p.LeafPageElement(uint16(i)) elem.SetPos(uint32(uintptr(unsafe.Pointer(\u0026amp;b[0])) - uintptr(unsafe.Pointer(elem)))) elem.SetFlags(item.Flags()) elem.SetKsize(uint32(len(item.Key()))) elem.SetVsize(uint32(len(item.Value()))) } else { elem := p.BranchPageElement(uint16(i)) elem.SetPos(uint32(uintptr(unsafe.Pointer(\u0026amp;b[0])) - uintptr(unsafe.Pointer(elem)))) elem.SetKsize(uint32(len(item.Key()))) elem.SetPgid(item.Pgid()) Assert(elem.Pgid() != p.Id(), \u0026quot;write: circular dependency occurred\u0026quot;) } // 将键和值数据写入到页面的末尾。 l := copy(b, item.Key()) copy(b[l:], item.Value()) } return uint32(off) } Bucket Bucket 是 boltdb 的上层的数据结构，每个 bucket 都有一个完成的 B+ 树。将多个 page 联合起来。\ntype Bucket struct { *common.InBucket tx *Tx // 指向关联事务的指针，将 bucket 与其事务上下文连接。 buckets map[string]*Bucket // 子 bucket 缓存；允许通过名字快速访问子 bucket。 page *common.Page // 内联页面的引用，用于直接存储少量数据或作为数据节点的入口点。 rootNode *node // 根页面的已实例化节点，如果 bucket 直接存储在内存中，则此节点将被激活。 nodes map[common.Pgid]*node // 节点缓存，用于快速访问已加载的页面节点，避免重复读取磁盘。 // 设置节点分裂时的填充阈值。默认情况下，bucket 将填充至 50%， // 但如果你知道你的写入工作负载主要是追加操作，提高这个比例可能会有用。 // // 这个设置不会跨事务持久化，因此每个事务都必须设置它。 FillPercent float64 } type InBucket struct { root Pgid // bucket 根级页面的页面ID。如果 bucket 是内联的，则此值为 0。 sequence uint64 // 单调递增的序列号，用于 NextSequence() 函数。 } Bucket 有可能是 node，也可能是 page。查找某页面的键值对时，首先检查 Bucket.nodes 缓存是否有对应的 node，如果没有，再从 page 中查找。 Bucket.FillPercent 记录 node 的填充百分比。当 node 的已用空间超过其容量的某个百分比后，节点必须分裂，以减少在 B+ Tree 中插入键值对时触发再平衡的概率。默认值是 50%，仅当大量写入操作在尾部添加时，增大该值才有帮助。\nbucket 存储方式：\n遍历 cursor type Cursor struct { bucket *Bucket stack []elemRef } type elemRef struct { page *common.Page node *node index int } cursor 分为三类，定位到某一个元素的位置、在当前位置从前往后找、在当前位置从后往前找。方法为：First、Last、Next、Prev 等。\nSeek 如果该键存在，它会返回该键及其对应的值；如果键不存在，它则返回最近的后续键。\n// Seek 方法使用B树搜索将光标移动到给定的键并返回它。 // 如果键不存在，则使用下一个键。如果没有更多的键，返回nil。 // 返回的键和值只在事务的生命周期内有效。 func (c *Cursor) Seek(seek []byte) (key []byte, value []byte) { // 确保数据库事务没有关闭 common.Assert(c.bucket.tx.db != nil, \u0026quot;tx closed\u0026quot;) // 调用内部的seek方法，获取键和值 k, v, flags := c.seek(seek) // 检查是否位于页面的最后一个元素之后，如果是，则移动到下一个元素。 if ref := \u0026amp;c.stack[len(c.stack)-1]; ref.index \u0026gt;= ref.count() { k, v, flags = c.next() } // 如果k为nil，表示未找到键，返回nil。 if k == nil { return nil, nil } else if (flags \u0026amp; uint32(common.BucketLeafFlag)) != 0 { // 如果是叶子节点，返回键和nil值。 return k, nil } // 返回找到的键和值。 return k, v } // seek 方法将光标移动到给定的键，并返回该键。 // 如果键不存在，则使用下一个键。 func (c *Cursor) seek(seek []byte) (key []byte, value []byte, flags uint32) { // 从根页面/节点开始，遍历到正确的页面。 c.stack = c.stack[:0] c.search(seek, c.bucket.RootPage()) // 如果是桶，则返回nil值。 return c.keyValue() } search\n// search 方法递归地对给定的页面/节点进行二分搜索，直到找到给定的键。 func (c *Cursor) search(key []byte, pgId common.Pgid) { p, n := c.bucket.pageNode(pgId) if p != nil \u0026amp;\u0026amp; !p.IsBranchPage() \u0026amp;\u0026amp; !p.IsLeafPage() { panic(fmt.Sprintf(\u0026quot;invalid page type: %d: %x\u0026quot;, p.Id(), p.Flags())) } e := elemRef{page: p, node: n} c.stack = append(c.stack, e) // 如果我们位于叶节点页面上，则在该页面内部继续查找特定节点。 if e.isLeaf() { c.nsearch(key) return } // 如果是节点，继续在节点内部搜索。 if n != nil { c.searchNode(key, n) return } // 如果是页面，继续在页面内部搜索。 c.searchPage(key, p) } func (c *Cursor) searchNode(key []byte, n *node) { var exact bool // 使用二分搜索确定键的位置。 index := sort.Search(len(n.inodes), func(i int) bool { ret := bytes.Compare(n.inodes[i].Key(), key) if ret == 0 { exact = true } return ret != -1 }) if !exact \u0026amp;\u0026amp; index \u0026gt; 0 { index-- } c.stack[len(c.stack)-1].index = index // 递归搜索到下一页。 c.search(key, n.inodes[index].Pgid()) } func (c *Cursor) searchPage(key []byte, p *common.Page) { // 对页面进行二分搜索以确定正确的范围。 inodes := p.BranchPageElements() var exact bool index := sort.Search(int(p.Count()), func(i int) bool { ret := bytes.Compare(inodes[i].Key(), key) if ret == 0 { exact = true } return ret != -1 }) if !exact \u0026amp;\u0026amp; index \u0026gt; 0 { index-- } c.stack[len(c.stack)-1].index = index // 递归搜索到下一页。 c.search(key, inodes[index].Pgid()) } func (c *Cursor) nsearch(key []byte) { e := \u0026amp;c.stack[len(c.stack)-1] p, n := e.page, e.node // If we have a node then search its inodes. if n != nil { index := sort.Search(len(n.inodes), func(i int) bool { return bytes.Compare(n.inodes[i].Key(), key) != -1 }) e.index = index return } // If we have a page then search its leaf elements. inodes := p.LeafPageElements() index := sort.Search(int(p.Count()), func(i int) bool { return bytes.Compare(inodes[i].Key(), key) != -1 }) e.index = index } keyValue\nfunc (c *Cursor) keyValue() ([]byte, []byte, uint32) { ref := \u0026amp;c.stack[len(c.stack)-1] // 如果索引超出范围，则返回nil。 if ref.count() == 0 || ref.index \u0026gt;= ref.count() { return nil, nil, 0 } // 从node中获取键值对。 if ref.node != nil { inode := \u0026amp;ref.node.inodes[ref.index] return inode.Key(), inode.Value(), inode.Flags() } // 从 page 中获取键值对。 elem := ref.page.LeafPageElement(uint16(ref.index)) return elem.Key(), elem.Value(), elem.Flags() } 创建 bucket 如果不存在 // CreateBucketIfNotExists 如果指定的存储桶不存在，则创建它，并返回一个对它的引用。 // 如果存储桶名为空或太长，则返回错误。 // 存储桶实例仅在事务的生命周期内有效。 func (b *Bucket) CreateBucketIfNotExists(key []byte) (rb *Bucket, err error) { // 如果日志不是被丢弃，记录创建存储桶的尝试。 if lg := b.tx.db.Logger(); lg != discardLogger { lg.Debugf(\u0026quot;Creating bucket if not exist %q\u0026quot;, key) defer func() { if err != nil { lg.Errorf(\u0026quot;Creating bucket if not exist %q failed: %v\u0026quot;, key, err) } else { lg.Debugf(\u0026quot;Creating bucket if not exist %q successfully\u0026quot;, key) } }() } // 检查数据库是否关闭，检查事务是否可写，检查键名是否为空。 if b.tx.db == nil { return nil, errors.ErrTxClosed } else if !b.tx.writable { return nil, errors.ErrTxNotWritable } else if len(key) == 0 { return nil, errors.ErrBucketNameRequired } // 使用克隆的键而不是原始键，以避免内存泄漏。 newKey := cloneBytes(key) // 检查键是否已存在。 if b.buckets != nil { if child := b.buckets[string(newKey)]; child != nil { return child, nil } } // 使用光标寻找正确的位置。 c := b.Cursor() k, v, flags := c.seek(newKey) // 如果找到的键相同，检查是否已有相同名字的非存储桶键。 if bytes.Equal(newKey, k) { if (flags \u0026amp; common.BucketLeafFlag) != 0 { var child = b.openBucket(v) if b.buckets != nil { b.buckets[string(newKey)] = child } return child, nil } return nil, errors.ErrIncompatibleValue } // 创建空的内联存储桶。 var bucket = Bucket{ InBucket: \u0026amp;common.InBucket{}, rootNode: \u0026amp;node{isLeaf: true}, FillPercent: DefaultFillPercent, } var value = bucket.write() // 在当前节点上插入键、值、标志。 c.node().put(newKey, newKey, value, 0, common.BucketLeafFlag) // 如果存在内联页面，取消引用它，使得存储桶被视为常规非内联存储桶。 b.page = nil // 返回新创建的存储桶。 return b.Bucket(newKey), nil } // node方法返回光标当前定位的节点。 func (c *Cursor) node() *node { // 确保光标栈长度大于0，否则抛出异常。 common.Assert(len(c.stack) \u0026gt; 0, \u0026quot;accessing a node with a zero-length cursor stack\u0026quot;) // 如果栈顶是叶子节点，直接返回该节点。 if ref := \u0026amp;c.stack[len(c.stack)-1]; ref.node != nil \u0026amp;\u0026amp; ref.isLeaf() { return ref.node } // 从根开始，向下遍历层级结构。 var n = c.stack[0].node if n == nil { n = c.bucket.node(c.stack[0].page.Id(), nil) } for _, ref := range c.stack[:len(c.stack)-1] { common.Assert(!n.isLeaf, \u0026quot;expected branch node\u0026quot;) n = n.childAt(ref.index) } common.Assert(n.isLeaf, \u0026quot;expected leaf node\u0026quot;) return n } // put方法在节点中插入键值对。 func (n *node) put(oldKey, newKey, value []byte, pgId common.Pgid, flags uint32) { // 检查pgId是否超出限制。 if pgId \u0026gt;= n.bucket.tx.meta.Pgid() { panic(fmt.Sprintf(\u0026quot;pgId (%d) above high water mark (%d)\u0026quot;, pgId, n.bucket.tx.meta.Pgid())) } else if len(oldKey) \u0026lt;= 0 { panic(\u0026quot;put: zero-length old key\u0026quot;) } else if len(newKey) \u0026lt;= 0 { panic(\u0026quot;put: zero-length new key\u0026quot;) } // 寻找插入的位置。 index := sort.Search(len(n.inodes), func(i int) bool { return bytes.Compare(n.inodes[i].Key(), oldKey) != -1 }) // 如果没有找到确切匹配，增加容量并移动节点。 exact := len(n.inodes) \u0026gt; 0 \u0026amp;\u0026amp; index \u0026lt; len(n.inodes) \u0026amp;\u0026amp; bytes.Equal(n.inodes[index].Key(), oldKey) if !exact { n.inodes = append(n.inodes, common.Inode{}) copy(n.inodes[index+1:], n.inodes[index:]) } // 设置inode的属性。 inode := \u0026amp;n.inodes[index] inode.SetFlags(flags) inode.SetKey(newKey) inode.SetValue(value) inode.SetPgid(pgId) common.Assert(len(inode.Key()) \u0026gt; 0, \u0026quot;put: zero-length inode key\u0026quot;) } // Bucket方法通过名称检索嵌套桶。 // 如果桶不存在，返回nil。 // 返回的桶实例只在事务生命周期内有效。 func (b *Bucket) Bucket(name []byte) *Bucket { // 如果已有桶缓存，则直接返回对应桶。 if b.buckets != nil { if child := b.buckets[string(name)]; child != nil { return child } } // 移动光标到键位置。 c := b.Cursor() k, v, flags := c.seek(name) // 如果键不存在或者不是桶标志，则返回nil。 if !bytes.Equal(name, k) || (flags \u0026amp; common.BucketLeafFlag) == 0 { return nil } // 否则创建并缓存桶。 var child = b.openBucket(v) if b.buckets != nil { b.buckets[string(name)] = child } return child } 插入 key/value func (b *Bucket) Put(key []byte, value []byte) (err error) { if lg := b.tx.db.Logger(); lg != discardLogger { lg.Debugf(\u0026quot;Putting key %q\u0026quot;, key) defer func() { if err != nil { lg.Errorf(\u0026quot;Putting key %q failed: %v\u0026quot;, key, err) } else { lg.Debugf(\u0026quot;Putting key %q successfully\u0026quot;, key) } }() } if b.tx.db == nil { return errors.ErrTxClosed } else if !b.Writable() { return errors.ErrTxNotWritable } else if len(key) == 0 { return errors.ErrKeyRequired } else if len(key) \u0026gt; MaxKeySize { return errors.ErrKeyTooLarge } else if int64(len(value)) \u0026gt; MaxValueSize { return errors.ErrValueTooLarge } newKey := cloneBytes(key) // 移动光标到键位置。 c := b.Cursor() k, _, flags := c.seek(newKey) // Return an error if there is an existing key with a bucket value. if bytes.Equal(newKey, k) \u0026amp;\u0026amp; (flags\u0026amp;common.BucketLeafFlag) != 0 { return errors.ErrIncompatibleValue } // gofail: var beforeBucketPut struct{} c.node().put(newKey, newKey, value, 0, 0) return nil } 事务 BoltDB 支持 ACID 事务，并采用了使用读写锁机制，支持多个读操作与一个写操作并发执行，让应用程序可以更简单的处理复杂操作。每个事务都有一个 txid，其中db.meta.txid 保存了最大的已提交的写事务 id。BoltDB 对写事务和读事务执行不同的 id 分配策略：\n读事务：txid == db.meta.txid； 写事务：txid == db.meta.txid + 1； 当写事务成功提交时，会更新了db.meta.txid为当前写事务 id。 数据库初始化时会将页号为 0 和 1 的两个页面设置为meta页，每个事务会获得一个txid，并选取txid % 2的meta页做为该事务的读取对象，每次写数据后会交替更新meta页。当其中一个出现数据校验不一致时会使用另一个meta页。 BoltDB 的写操作都是在内存中进行，若事务未 commit 时出错，不会对数据库造成影响；若是在 commit 的过程中出错，BoltDB 写入文件的顺序也保证了不会造成影响：因为数据会写在新的 page 中不会覆盖原来的数据，且此时 meta中的信息不发生变化。\n开始一份写事务时，会拷贝一份 meta数据； 从 rootBucket 开始，遍历 B+ Tree 查找数据位置并修改； 修改操作完成后会进行事务 commit，此时会将数据写入新的 page； 最后更新meta的信息。 // Tx 代表数据库上的一个只读或读写事务。 // 只读事务可用于检索键值和创建光标。 // 读写事务可以创建和删除桶以及创建和删除键。 // // 重要：必须在使用完事务后提交或回滚事务。 // 只有当没有事务在使用页面时，写入者才能回收这些页面。 // 长时间运行的读事务可能会导致数据库迅速增长。 type Tx struct { writable bool // 是否为可写事务 managed bool // 是否为管理事务 db *DB // 关联的数据库实例 meta *common.Meta // 元数据指针 root Bucket // 根桶 pages map[common.Pgid]*common.Page // 页面映射 stats TxStats // 事务统计 commitHandlers []func() // 提交处理程序列表 // WriteFlag 指定写相关方法（如 WriteTo()）的标志。 // Tx 使用指定的标志打开数据库文件以复制数据。 // // 默认情况下，此标志未设置，适合主要在内存中的工作负载。 // 对于大于可用 RAM 的数据库，可以设置为 syscall.O_DIRECT 来避免淘汰页面缓存。 WriteFlag int } Begin // Begin 开始一个新事务。 // 多个只读事务可以并发使用，但一次只能使用一个写事务。 // 启动多个写事务会导致调用阻塞，并序列化直到当前写事务完成。 // // 事务不应该彼此依赖。在同一个goroutine中打开一个读事务和一个写事务可能会导致写入者死锁， // 因为数据库需要定期重新映射自身以应对增长，并且在读事务打开的时候无法进行。 // // 如果需要长时间运行的读事务（例如，快照事务），你可能想要将DB.InitialMmapSize设置为足够大的值 // 以避免写事务的潜在阻塞。 // // 重要：你必须在完成后关闭只读事务，否则数据库将无法回收旧页面。 func (db *DB) Begin(writable bool) (t *Tx, err error) { if lg := db.Logger(); lg != discardLogger { lg.Debugf(\u0026quot;Starting a new transaction [writable: %t]\u0026quot;, writable) defer func() { if err != nil { lg.Errorf(\u0026quot;Starting a new transaction [writable: %t] failed: %v\u0026quot;, writable, err) } else { lg.Debugf(\u0026quot;Starting a new transaction [writable: %t] successfully\u0026quot;, writable) } }() } if writable { return db.beginRWTx() } return db.beginTx() } func (db *DB) beginRWTx() (*Tx, error) { // If the database was opened with Options.ReadOnly, return an error. if db.readOnly { return nil, berrors.ErrDatabaseReadOnly } // Obtain writer lock. This is released by the transaction when it closes. // This enforces only one writer transaction at a time. db.rwlock.Lock() // Once we have the writer lock then we can lock the meta pages so that // we can set up the transaction. db.metalock.Lock() defer db.metalock.Unlock() // Exit if the database is not open yet. if !db.opened { db.rwlock.Unlock() return nil, berrors.ErrDatabaseNotOpen } // Exit if the database is not correctly mapped. if db.data == nil { db.rwlock.Unlock() return nil, berrors.ErrInvalidMapping } // Create a transaction associated with the database. t := \u0026amp;Tx{writable: true} t.init(db) db.rwtx = t db.freePages() return t, nil } // freePages 释放与已关闭的只读事务关联的任何页面。 func (db *DB) freePages() { sort.Sort(txsById(db.txs)) minid := common.Txid(0xFFFFFFFFFFFFFFFF) if len(db.txs) \u0026gt; 0 { minid = db.txs[0].meta.Txid() } if minid \u0026gt; 0 { db.freelist.release(minid - 1) } for _, t := range db.txs { db.freelist.releaseRange(minid, t.meta.Txid()-1) minid = t.meta.Txid() + 1 } db.freelist.releaseRange(minid, common.Txid(0xFFFFFFFFFFFFFFFF)) } func (db *DB) beginTx() (*Tx, error) { // Lock the meta pages while we initialize the transaction. We obtain // the meta lock before the mmap lock because that's the order that the // write transaction will obtain them. db.metalock.Lock() // Obtain a read-only lock on the mmap. When the mmap is remapped it will // obtain a write lock so all transactions must finish before it can be // remapped. db.mmaplock.RLock() // Exit if the database is not open yet. if !db.opened { db.mmaplock.RUnlock() db.metalock.Unlock() return nil, berrors.ErrDatabaseNotOpen } // Exit if the database is not correctly mapped. if db.data == nil { db.mmaplock.RUnlock() db.metalock.Unlock() return nil, berrors.ErrInvalidMapping } // Create a transaction associated with the database. t := \u0026amp;Tx{} t.init(db) // Keep track of transaction until it closes. db.txs = append(db.txs, t) n := len(db.txs) // Unlock the meta pages. db.metalock.Unlock() // Update the transaction stats. db.statlock.Lock() db.stats.TxN++ db.stats.OpenTxN = n db.statlock.Unlock() return t, nil } Commit // Commit 将所有更改写入磁盘，更新元数据页，并关闭事务。 // 如果磁盘写入发生错误，或者在只读事务上调用Commit，将返回错误。 func (tx *Tx) Commit() (err error) { txId := tx.ID() // 获取事务ID lg := tx.db.Logger() // 获取日志记录器 if lg != discardLogger { lg.Debugf(\u0026quot;Committing transaction %d\u0026quot;, txId) defer func() { if err != nil { lg.Errorf(\u0026quot;Committing transaction failed: %v\u0026quot;, err) } else { lg.Debugf(\u0026quot;Committing transaction %d successfully\u0026quot;, txId) } }() } // 检查是否为管理事务，不允许提交。 common.Assert(!tx.managed, \u0026quot;managed tx commit not allowed\u0026quot;) if tx.db == nil { return berrors.ErrTxClosed // 事务已关闭错误 } else if !tx.writable { return berrors.ErrTxNotWritable // 非写事务错误 } // TODO: 使用向量化I/O写出脏页 // 重新平衡删除后的节点 var startTime = time.Now() tx.root.rebalance() if tx.stats.GetRebalance() \u0026gt; 0 { tx.stats.IncRebalanceTime(time.Since(startTime)) } opgid := tx.meta.Pgid() // 获取旧的页面ID // 将数据溢出到脏页 startTime = time.Now() if err = tx.root.spill(); err != nil { lg.Errorf(\u0026quot;spilling data onto dirty pages failed: %v\u0026quot;, err) tx.rollback() return err } tx.stats.IncSpillTime(time.Since(startTime)) // 释放旧的根桶 tx.meta.RootBucket().SetRootPage(tx.root.RootPage()) // 释放旧的自由列表，因为提交会写出一个新的自由列表 if tx.meta.Freelist() != common.PgidNoFreelist { tx.db.freelist.free(tx.meta.Txid(), tx.db.page(tx.meta.Freelist())) } if !tx.db.NoFreelistSync { err = tx.commitFreelist() if err != nil { lg.Errorf(\u0026quot;committing freelist failed: %v\u0026quot;, err) return err } } else { tx.meta.SetFreelist(common.PgidNoFreelist) } // 如果高水位标记已上移，则尝试扩大数据库 if tx.meta.Pgid() \u0026gt; opgid { if err = tx.db.grow(int(tx.meta.Pgid()+1) * tx.db.pageSize); err != nil { lg.Errorf(\u0026quot;growing db size failed, pgid: %d, pagesize: %d, error: %v\u0026quot;, tx.meta.Pgid(), tx.db.pageSize, err) tx.rollback() return err } } // 将脏页写入磁盘 startTime = time.Now() if err = tx.write(); err != nil { lg.Errorf(\u0026quot;writing data failed: %v\u0026quot;, err) tx.rollback() return err } // 如果启用了严格模式，则执行一致性检查 if tx.db.StrictMode { ch := tx.Check() var errs []string for { chkErr, ok := \u0026lt;-ch if !ok { break } errs = append(errs, chkErr.Error()) } if len(errs) \u0026gt; 0 { panic(\u0026quot;check fail: \u0026quot; + strings.Join(errs, \u0026quot;\\n\u0026quot;)) } } // 将元数据写入磁盘 if err = tx.writeMeta(); err != nil { lg.Errorf(\u0026quot;writeMeta failed: %v\u0026quot;, err) tx.rollback() return err } tx.stats.IncWriteTime(time.Since(startTime)) // 结束事务 tx.close() // 执行提交处理程序，锁已经移除 for _, fn := range tx.commitHandlers { fn() } return nil } Rollback // Rollback 关闭事务并忽略所有之前的更新。 // 只读事务必须回滚而不是提交。 func (tx *Tx) Rollback() error { common.Assert(!tx.managed, \u0026quot;managed tx rollback not allowed\u0026quot;) // 断言此事务不是管理型事务 if tx.db == nil { return berrors.ErrTxClosed // 如果数据库已关闭，返回错误 } tx.nonPhysicalRollback() // 执行非物理性回滚 return nil } // nonPhysicalRollback 在用户直接调用Rollback时被调用，在这种情况下，我们不需要从磁盘重新加载自由页面。 func (tx *Tx) nonPhysicalRollback() { if tx.db == nil { return // 如果数据库已关闭，直接返回 } if tx.writable { tx.db.freelist.rollback(tx.meta.Txid()) // 如果事务是可写的，回滚自由列表 } tx.close() // 关闭事务 } // rollback 从给定的挂起事务中移除页面。 func (f *freelist) rollback(txid common.Txid) { // 从缓存中移除页面ID。 txp := f.pending[txid] if txp == nil { return // 如果没有挂起的事务，直接返回 } var m common.Pgids for i, pgid := range txp.ids { delete(f.cache, pgid) // 从缓存中删除页面ID tx := txp.alloctx[i] if tx == 0 { continue // 如果未分配事务ID，继续下一个 } if tx != txid { // 如果待释放页面被中断，恢复页面回分配列表。 f.allocs[pgid] = tx } else { // 如果释放的页面由此事务分配，可以安全地丢弃。 m = append(m, pgid) } } // 从挂起列表中移除页面，并将其标记为由txid分配的自由页面。 delete(f.pending, txid) f.mergeSpans(m) } View \u0026amp;\u0026amp; Update // View 在管理的只读事务上下文中执行一个函数。 // 从函数返回的任何错误都会从View()方法返回。 // // 尝试在函数内手动回滚会导致panic。 func (db *DB) View(fn func(*Tx) error) error { t, err := db.Begin(false) // 开始一个只读事务 if err != nil { return err // 如果无法开始事务，返回错误 } // 确保在发生panic的情况下事务能够回滚。 defer func() { if t.db != nil { t.rollback() // 执行回滚操作 } }() // 标记为管理的事务，以便内部函数不能手动回滚。 t.managed = true // 如果函数返回错误，则传递该错误。 err = fn(t) t.managed = false if err != nil { _ = t.Rollback() // 执行回滚 return err } return t.Rollback() // 完成后回滚事务 } // Update 在读写管理事务的上下文中执行一个函数。 // 如果函数没有返回错误，则提交事务。 // 如果返回了错误，则整个事务被回滚。 // 从函数返回的任何错误或从提交返回的错误都会从Update()方法返回。 // // 尝试在函数内手动提交或回滚将导致panic。 func (db *DB) Update(fn func(*Tx) error) error { t, err := db.Begin(true) // 开始一个读写事务 if err != nil { return err // 如果无法开始事务，返回错误 } // 确保在发生panic的情况下事务能够回滚。 defer func() { if t.db != nil { t.rollback() // 执行回滚操作 } }() // 标记为管理的事务，以便内部函数不能手动提交。 t.managed = true // 如果函数返回错误，则回滚并返回错误。 err = fn(t) t.managed = false if err != nil { _ = t.Rollback() // 执行回滚 return err } return t.Commit() // 无错误时提交事务 } Reference https://wingsxdu.com/posts/database/boltdb/ https://jaydenwen123.github.io/boltdb/ https://youjiali1995.github.io/storage/boltdb/ https://www.cnblogs.com/huxiao-tee/p/4660352.html ","date":"2024-06-15","permalink":"https://daemon365.dev/2024/06/15/boltdb-%E5%8E%9F%E7%90%86/","tags":["boltdb","etcd","golang","数据库","kubernetes","源码分析","事务"],"title":"boltdb 原理"},{"content":"介绍 在 etcd 中，watch 是一个非常重要的特性，它可以让客户端监控 etcd 中的 key 或者一组 key，当 key 发生变化时，etcd 会通知客户端。本文将介绍 etcd watch 的实现原理。\netcdctl watch /test # 当 /test 的值发生变化时，会输出如下信息 PUT /test a PUT /test b DELETE /test watch 的 api etcd watch api 是由 grpc stream 实现的，客户端通过 grpc stream 发送 watch 请求，etcd 会将 key 的变化通过 stream 返回给客户端。\nrpc Watch(stream WatchRequest) returns (stream WatchResponse) { option (google.api.http) = { post: \u0026quot;/v3/watch\u0026quot; body: \u0026quot;*\u0026quot; }; } api 实现 func (ws *watchServer) Watch(stream pb.Watch_WatchServer) (err error) { sws := serverWatchStream{ lg: ws.lg, clusterID: ws.clusterID, memberID: ws.memberID, maxRequestBytes: ws.maxRequestBytes, sg: ws.sg, watchable: ws.watchable, ag: ws.ag, gRPCStream: stream, watchStream: ws.watchable.NewWatchStream(), // chan for sending control response like watcher created and canceled. ctrlStream: make(chan *pb.WatchResponse, ctrlStreamBufLen), progress: make(map[mvcc.WatchID]bool), prevKV: make(map[mvcc.WatchID]bool), fragment: make(map[mvcc.WatchID]bool), closec: make(chan struct{}), } sws.wg.Add(1) go func() { // 开启一个 goroutine 处理新的 event 然后发送给客户端 sws.sendLoop() sws.wg.Done() }() errc := make(chan error, 1) go func() { // 开启一个 goroutine 处理客户端发送的 watch 请求 if rerr := sws.recvLoop(); rerr != nil { if isClientCtxErr(stream.Context().Err(), rerr) { sws.lg.Debug(\u0026quot;failed to receive watch request from gRPC stream\u0026quot;, zap.Error(rerr)) } else { sws.lg.Warn(\u0026quot;failed to receive watch request from gRPC stream\u0026quot;, zap.Error(rerr)) streamFailures.WithLabelValues(\u0026quot;receive\u0026quot;, \u0026quot;watch\u0026quot;).Inc() } errc \u0026lt;- rerr } }() // 处理结束 select { case err = \u0026lt;-errc: if err == context.Canceled { err = rpctypes.ErrGRPCWatchCanceled } close(sws.ctrlStream) case \u0026lt;-stream.Context().Done(): err = stream.Context().Err() if err == context.Canceled { err = rpctypes.ErrGRPCWatchCanceled } } sws.close() return err } 这里 主要的逻辑是开启两个 goroutine，一个用于处理客户端发送的 watch 请求，另一个用于处理新的 event 然后发送给客户端。\nsendLoop func (sws *serverWatchStream) sendLoop() { // watch ids that are currently active ids := make(map[mvcc.WatchID]struct{}) // watch responses pending on a watch id creation message pending := make(map[mvcc.WatchID][]*pb.WatchResponse) interval := GetProgressReportInterval() progressTicker := time.NewTicker(interval) defer func() { progressTicker.Stop() // 清空chan ，清理待处理 event for ws := range sws.watchStream.Chan() { mvcc.ReportEventReceived(len(ws.Events)) } for _, wrs := range pending { for _, ws := range wrs { mvcc.ReportEventReceived(len(ws.Events)) } } }() for { select { case wresp, ok := \u0026lt;-sws.watchStream.Chan(): // 从 watchStream.Chan() 中获取 event // 然后发送给客户端 if !ok { return } evs := wresp.Events events := make([]*mvccpb.Event, len(evs)) sws.mu.RLock() needPrevKV := sws.prevKV[wresp.WatchID] sws.mu.RUnlock() for i := range evs { events[i] = \u0026amp;evs[i] if needPrevKV \u0026amp;\u0026amp; !IsCreateEvent(evs[i]) { opt := mvcc.RangeOptions{Rev: evs[i].Kv.ModRevision - 1} r, err := sws.watchable.Range(context.TODO(), evs[i].Kv.Key, nil, opt) if err == nil \u0026amp;\u0026amp; len(r.KVs) != 0 { events[i].PrevKv = \u0026amp;(r.KVs[0]) } } } canceled := wresp.CompactRevision != 0 wr := \u0026amp;pb.WatchResponse{ Header: sws.newResponseHeader(wresp.Revision), WatchId: int64(wresp.WatchID), Events: events, CompactRevision: wresp.CompactRevision, Canceled: canceled, } // Progress notifications can have WatchID -1 // if they announce on behalf of multiple watchers if wresp.WatchID != clientv3.InvalidWatchID { if _, okID := ids[wresp.WatchID]; !okID { // buffer if id not yet announced wrs := append(pending[wresp.WatchID], wr) pending[wresp.WatchID] = wrs continue } } mvcc.ReportEventReceived(len(evs)) sws.mu.RLock() fragmented, ok := sws.fragment[wresp.WatchID] sws.mu.RUnlock() var serr error // gofail: var beforeSendWatchResponse struct{} if !fragmented \u0026amp;\u0026amp; !ok { serr = sws.gRPCStream.Send(wr) } else { serr = sendFragments(wr, sws.maxRequestBytes, sws.gRPCStream.Send) } if serr != nil { if isClientCtxErr(sws.gRPCStream.Context().Err(), serr) { sws.lg.Debug(\u0026quot;failed to send watch response to gRPC stream\u0026quot;, zap.Error(serr)) } else { sws.lg.Warn(\u0026quot;failed to send watch response to gRPC stream\u0026quot;, zap.Error(serr)) streamFailures.WithLabelValues(\u0026quot;send\u0026quot;, \u0026quot;watch\u0026quot;).Inc() } return } sws.mu.Lock() if len(evs) \u0026gt; 0 \u0026amp;\u0026amp; sws.progress[wresp.WatchID] { // elide next progress update if sent a key update sws.progress[wresp.WatchID] = false } sws.mu.Unlock() case c, ok := \u0026lt;-sws.ctrlStream: // 处理客户端发送的 watch 请求 if !ok { return } if err := sws.gRPCStream.Send(c); err != nil { if isClientCtxErr(sws.gRPCStream.Context().Err(), err) { sws.lg.Debug(\u0026quot;failed to send watch control response to gRPC stream\u0026quot;, zap.Error(err)) } else { sws.lg.Warn(\u0026quot;failed to send watch control response to gRPC stream\u0026quot;, zap.Error(err)) streamFailures.WithLabelValues(\u0026quot;send\u0026quot;, \u0026quot;watch\u0026quot;).Inc() } return } // track id creation wid := mvcc.WatchID(c.WatchId) verify.Assert(!(c.Canceled \u0026amp;\u0026amp; c.Created) || wid == clientv3.InvalidWatchID, \u0026quot;unexpected watchId: %d, wanted: %d, since both 'Canceled' and 'Created' are true\u0026quot;, wid, clientv3.InvalidWatchID) if c.Canceled \u0026amp;\u0026amp; wid != clientv3.InvalidWatchID { delete(ids, wid) continue } if c.Created { // flush buffered events ids[wid] = struct{}{} for _, v := range pending[wid] { mvcc.ReportEventReceived(len(v.Events)) if err := sws.gRPCStream.Send(v); err != nil { if isClientCtxErr(sws.gRPCStream.Context().Err(), err) { sws.lg.Debug(\u0026quot;failed to send pending watch response to gRPC stream\u0026quot;, zap.Error(err)) } else { sws.lg.Warn(\u0026quot;failed to send pending watch response to gRPC stream\u0026quot;, zap.Error(err)) streamFailures.WithLabelValues(\u0026quot;send\u0026quot;, \u0026quot;watch\u0026quot;).Inc() } return } } delete(pending, wid) } case \u0026lt;-progressTicker.C: sws.mu.Lock() for id, ok := range sws.progress { if ok { sws.watchStream.RequestProgress(id) } sws.progress[id] = true } sws.mu.Unlock() case \u0026lt;-sws.closec: return } } } 这里使用了 for select 循环：\n从 watchStream.Chan() 中获取 event 然后发送给客户端。 处理客户端发送的 watch 请求。 dispatch progress 事件。 处理结束。 recvLoop func (sws *serverWatchStream) recvLoop() error { for { req, err := sws.gRPCStream.Recv() if err == io.EOF { return nil } if err != nil { return err } switch uv := req.RequestUnion.(type) { case *pb.WatchRequest_CreateRequest: if uv.CreateRequest == nil { break } creq := uv.CreateRequest if len(creq.Key) == 0 { // \\x00 is the smallest key creq.Key = []byte{0} } if len(creq.RangeEnd) == 0 { // force nil since watchstream.Watch distinguishes // between nil and []byte{} for single key / \u0026gt;= creq.RangeEnd = nil } if len(creq.RangeEnd) == 1 \u0026amp;\u0026amp; creq.RangeEnd[0] == 0 { // support \u0026gt;= key queries creq.RangeEnd = []byte{} } err := sws.isWatchPermitted(creq) if err != nil { var cancelReason string switch err { case auth.ErrInvalidAuthToken: cancelReason = rpctypes.ErrGRPCInvalidAuthToken.Error() case auth.ErrAuthOldRevision: cancelReason = rpctypes.ErrGRPCAuthOldRevision.Error() case auth.ErrUserEmpty: cancelReason = rpctypes.ErrGRPCUserEmpty.Error() default: if err != auth.ErrPermissionDenied { sws.lg.Error(\u0026quot;unexpected error code\u0026quot;, zap.Error(err)) } cancelReason = rpctypes.ErrGRPCPermissionDenied.Error() } wr := \u0026amp;pb.WatchResponse{ Header: sws.newResponseHeader(sws.watchStream.Rev()), WatchId: clientv3.InvalidWatchID, Canceled: true, Created: true, CancelReason: cancelReason, } select { case sws.ctrlStream \u0026lt;- wr: continue case \u0026lt;-sws.closec: return nil } } filters := FiltersFromRequest(creq) wsrev := sws.watchStream.Rev() rev := creq.StartRevision if rev == 0 { rev = wsrev + 1 } id, err := sws.watchStream.Watch(mvcc.WatchID(creq.WatchId), creq.Key, creq.RangeEnd, rev, filters...) if err == nil { sws.mu.Lock() if creq.ProgressNotify { sws.progress[id] = true } if creq.PrevKv { sws.prevKV[id] = true } if creq.Fragment { sws.fragment[id] = true } sws.mu.Unlock() } else { id = clientv3.InvalidWatchID } wr := \u0026amp;pb.WatchResponse{ Header: sws.newResponseHeader(wsrev), WatchId: int64(id), Created: true, Canceled: err != nil, } if err != nil { wr.CancelReason = err.Error() } select { case sws.ctrlStream \u0026lt;- wr: case \u0026lt;-sws.closec: return nil } case *pb.WatchRequest_CancelRequest: if uv.CancelRequest != nil { id := uv.CancelRequest.WatchId err := sws.watchStream.Cancel(mvcc.WatchID(id)) if err == nil { sws.ctrlStream \u0026lt;- \u0026amp;pb.WatchResponse{ Header: sws.newResponseHeader(sws.watchStream.Rev()), WatchId: id, Canceled: true, } sws.mu.Lock() delete(sws.progress, mvcc.WatchID(id)) delete(sws.prevKV, mvcc.WatchID(id)) delete(sws.fragment, mvcc.WatchID(id)) sws.mu.Unlock() } } case *pb.WatchRequest_ProgressRequest: if uv.ProgressRequest != nil { sws.mu.Lock() sws.watchStream.RequestProgressAll() sws.mu.Unlock() } default: // we probably should not shutdown the entire stream when // receive an invalid command. // so just do nothing instead. sws.lg.Sugar().Infof(\u0026quot;invalid watch request type %T received in gRPC stream\u0026quot;, uv) continue } } } 这里主要处理客户端发送的 watch 请求，然后发送给 ctrlStream。sendLoop 会从 ctrlStream 中获取 event 然后发送给客户端。\nWatchStream 这个 inferface 才是处理 watch 的主要逻辑\n// WatchStream 是一个接口，定义了一个流式处理watch请求的机制 type WatchStream interface { // Watch 创建一个观察者。观察者会监听在给定的键或范围 [key, end) 上发生的事件或已发生的事件。 // // 整个事件历史都可以被观察到，除非被压缩。 // 如果 \u0026quot;startRev\u0026quot; \u0026lt;= 0，watch 将观察在当前修订版本之后的事件。 // // 返回的 \u0026quot;id\u0026quot; 是这个观察者的ID。它作为 WatchID 出现在通过 stream 通道发送到创建的观察者的事件中。 // 当 WatchID 不等于 AutoWatchID 时，使用指定的 WatchID，否则返回自动生成的 WatchID。 Watch(id WatchID, key, end []byte, startRev int64, fcs ...FilterFunc) (WatchID, error) // Chan 返回一个通道。所有的watch响应将被发送到这个返回的通道。 Chan() \u0026lt;-chan WatchResponse // RequestProgress 请求给定ID的观察者的进度。响应只有在观察者当前同步时才会被发送。 // 响应将通过与此流关联的 WatchResponse 通道发送，以确保正确的顺序。 // 响应不包含事件。响应中的修订版本是观察者自同步以来的进度。 RequestProgress(id WatchID) // RequestProgressAll 请求所有共享此流的观察者的进度通知。 // 如果所有观察者都已同步，将向此流的任意观察者发送带有watch ID -1的进度通知，并返回 true。 RequestProgressAll() bool // Cancel 通过给定ID取消观察者。如果观察者不存在，将返回错误。 Cancel(id WatchID) error // Close 关闭通道并释放所有相关资源。 Close() // Rev 返回流上观察到的KV的当前修订版本。 Rev() int64 } // WatchResponse 表示一个watch操作的响应。 type WatchResponse struct { // WatchID 是发送此响应的观察者的ID。 WatchID WatchID // Events 包含所有需要发送的事件。 Events []mvccpb.Event // Revision 是创建watch响应时KV的修订版本。 // 对于正常响应，修订版本应该与Events中最后一个修改的修订版本相同。 // 对于延迟响应的未同步观察者，修订版本大于Events中最后一个修改的修订版本。 Revision int64 // CompactRevision 在观察者由于压缩而被取消时设置。 CompactRevision int64 } // 实现了 WatchStream // watchStream 包含共享一个流通道发送被观察事件和其他控制事件的观察者集合。 type watchStream struct { // 可观察对象（例如KV存储） watchable watchable // 用于发送watch响应的通道 ch chan WatchResponse // 互斥锁，保护以下字段 mu sync.Mutex // nextID 是为此流中下一个新观察者预分配的ID nextID WatchID // 标志流是否已关闭 closed bool // 取消函数的映射，用于取消特定的观察者 cancels map[WatchID]cancelFunc // 观察者的映射，根据观察者ID索引 watchers map[WatchID]*watcher } // Watch 在流中创建一个新的观察者并返回其 WatchID。 func (ws *watchStream) Watch(id WatchID, key, end []byte, startRev int64, fcs ...FilterFunc) (WatchID, error) { // 防止键 \u0026gt;= 结束键（按字典顺序）的错误范围 // 带有 'WithFromKey' 的watch请求具有空字节范围结束 if len(end) != 0 \u0026amp;\u0026amp; bytes.Compare(key, end) != -1 { return -1, ErrEmptyWatcherRange } // 获取互斥锁 ws.mu.Lock() defer ws.mu.Unlock() // 如果流已关闭，返回错误 if ws.closed { return -1, ErrEmptyWatcherRange } // 自动生成 WatchID if id == clientv3.AutoWatchID { for ws.watchers[ws.nextID] != nil { ws.nextID++ } id = ws.nextID ws.nextID++ } else if _, ok := ws.watchers[id]; ok { return -1, ErrWatcherDuplicateID } // 创建新的观察者 w, c := ws.watchable.watch(key, end, startRev, id, ws.ch, fcs...) // 保存取消函数和观察者 ws.cancels[id] = c ws.watchers[id] = w return id, nil } // Chan 返回用于接收watch响应的通道。 func (ws *watchStream) Chan() \u0026lt;-chan WatchResponse { return ws.ch } // Cancel 取消具有给定ID的观察者。 func (ws *watchStream) Cancel(id WatchID) error { // 获取互斥锁 ws.mu.Lock() cancel, ok := ws.cancels[id] w := ws.watchers[id] ok = ok \u0026amp;\u0026amp; !ws.closed ws.mu.Unlock() // 如果观察者不存在或流已关闭，返回错误 if !ok { return ErrWatcherNotExist } cancel() // 获取互斥锁 ws.mu.Lock() // 在取消之前不删除观察者，以确保 Close() 调用时等待取消 if ww := ws.watchers[id]; ww == w { delete(ws.cancels, id) delete(ws.watchers, id) } ws.mu.Unlock() return nil } // Close 关闭通道并释放所有相关资源。 func (ws *watchStream) Close() { // 获取互斥锁 ws.mu.Lock() defer ws.mu.Unlock() // 取消所有观察者 for _, cancel := range ws.cancels { cancel() } // 标记流已关闭并关闭通道 ws.closed = true close(ws.ch) watchStreamGauge.Dec() } // Rev 返回流上观察到的KV的当前修订版本。 func (ws *watchStream) Rev() int64 { // 获取互斥锁 ws.mu.Lock() defer ws.mu.Unlock() return ws.watchable.rev() } // RequestProgress 请求给定ID的观察者的进度。 func (ws *watchStream) RequestProgress(id WatchID) { // 获取互斥锁 ws.mu.Lock() w, ok := ws.watchers[id] ws.mu.Unlock() // 如果观察者不存在，直接返回 if !ok { return } // 请求进度 ws.watchable.progress(w) } // RequestProgressAll 请求所有观察者的进度通知。 func (ws *watchStream) RequestProgressAll() bool { // 获取互斥锁 ws.mu.Lock() defer ws.mu.Unlock() return ws.watchable.progressAll(ws.watchers) } Watch 方法：创建一个新的观察者，如果指定的范围不正确或观察者ID重复，则返回错误。否则，创建观察者并保存取消函数和观察者实例。 Chan 方法：返回用于接收watch响应的通道。 Cancel 方法：取消给定ID的观察者，删除相关的取消函数和观察者实例。 Close 方法：关闭所有观察者并释放资源。 Rev 方法：返回当前观察到的KV修订版本。 RequestProgress 方法：请求特定观察者的进度。 RequestProgressAll 方法：请求所有观察者的进度通知。 可以可到 当调用 Watch 的时候 每个 watchId 都会调用 watchable.watch 并把自己 ch 放入进去\nwatchable // watchable 接口定义了可观察对象的行为 type watchable interface { // watch 创建一个新的观察者，用于监听指定键或范围[startRev, end)上的事件。 // 返回观察者指针和取消函数。 watch(key, end []byte, startRev int64, id WatchID, ch chan\u0026lt;- WatchResponse, fcs ...FilterFunc) (*watcher, cancelFunc) // progress 通知特定观察者当前的进度。 progress(w *watcher) // progressAll 通知所有观察者当前的进度。 // 如果所有观察者都已同步，则返回 true。 progressAll(watchers map[WatchID]*watcher) bool // rev 返回当前观察到的修订版本。 rev() int64 } // watchableStore 是一个实现了 watchable 接口的结构体，代表一个可观察的存储 type watchableStore struct { // store 是一个指向基础存储的指针 *store // mu 保护观察者组和批次。为了避免死锁，在锁定 store.mu 之前不应锁定 mu。 mu sync.RWMutex // victims 是在 watch 通道上被阻塞的观察者批次 victims []watcherBatch victimc chan struct{} // unsynced 包含所有需要同步已经发生的事件的未同步观察者 unsynced watcherGroup // synced 包含所有与存储进度同步的观察者 // 映射的键是观察者监听的键 synced watcherGroup // stopc 是一个用于停止操作的通道 stopc chan struct{} // wg 用于等待所有 goroutine 完成 wg sync.WaitGroup } func (s *watchableStore) watch(key, end []byte, startRev int64, id WatchID, ch chan\u0026lt;- WatchResponse, fcs ...FilterFunc) (*watcher, cancelFunc) { // 创建一个新的观察者 wa := \u0026amp;watcher{ key: key, end: end, minRev: startRev, id: id, ch: ch, fcs: fcs, } // 锁定 watchableStore 的互斥锁 s.mu.Lock() // 锁定 store 的读写锁用于获取当前修订版本 s.revMu.RLock() // 判断观察者是否与当前存储修订版本同步 synced := startRev \u0026gt; s.store.currentRev || startRev == 0 if synced { // 如果同步，设置最小修订版本为当前修订版本的下一个版本 wa.minRev = s.store.currentRev + 1 if startRev \u0026gt; wa.minRev { wa.minRev = startRev } // 将观察者添加到同步观察者组中 s.synced.add(wa) } else { // 如果未同步，增加慢速观察者计数器 slowWatcherGauge.Inc() // 将观察者添加到未同步观察者组中 s.unsynced.add(wa) } // 解锁 store 的读写锁 s.revMu.RUnlock() // 解锁 watchableStore 的互斥锁 s.mu.Unlock() // 增加观察者计数器 watcherGauge.Inc() // 返回观察者和取消函数 return wa, func() { s.cancelWatcher(wa) } } newWatchableStore func newWatchableStore(lg *zap.Logger, b backend.Backend, le lease.Lessor, cfg StoreConfig) *watchableStore { if lg == nil { lg = zap.NewNop() } s := \u0026amp;watchableStore{ store: NewStore(lg, b, le, cfg), victimc: make(chan struct{}, 1), unsynced: newWatcherGroup(), synced: newWatcherGroup(), stopc: make(chan struct{}), } s.store.ReadView = \u0026amp;readView{s} s.store.WriteView = \u0026amp;writeView{s} if s.le != nil { // use this store as the deleter so revokes trigger watch events s.le.SetRangeDeleter(func() lease.TxnDelete { return s.Write(traceutil.TODO()) }) } s.wg.Add(2) go s.syncWatchersLoop() go s.syncVictimsLoop() return s } syncWatchersLoop // syncWatchersLoop 每100毫秒同步一次unsynced集合中的观察者。 func (s *watchableStore) syncWatchersLoop() { defer s.wg.Done() // 设置等待时间为100毫秒 waitDuration := 100 * time.Millisecond delayTicker := time.NewTicker(waitDuration) defer delayTicker.Stop() for { // 锁定以获取未同步观察者的数量 s.mu.RLock() st := time.Now() lastUnsyncedWatchers := s.unsynced.size() s.mu.RUnlock() unsyncedWatchers := 0 // 如果有未同步观察者，同步这些观察者 if lastUnsyncedWatchers \u0026gt; 0 { unsyncedWatchers = s.syncWatchers() } syncDuration := time.Since(st) // 重置定时器 delayTicker.Reset(waitDuration) // 检查是否有更多待处理的工作 if unsyncedWatchers != 0 \u0026amp;\u0026amp; lastUnsyncedWatchers \u0026gt; unsyncedWatchers { // 公平对待其他存储操作，通过延长时间来避免占用太多资源 delayTicker.Reset(syncDuration) } // 等待定时器或停止信号 select { case \u0026lt;-delayTicker.C: case \u0026lt;-s.stopc: return } } } // syncWatchers 通过以下步骤同步未同步的观察者： // 1. 从未同步观察者组中选择一组观察者 // 2. 迭代该组以获取最小修订版本并移除压缩的观察者 // 3. 使用最小修订版本获取所有键值对，并将这些事件发送给观察者 // 4. 从未同步组中移除已同步的观察者，并移动到同步组中 func (s *watchableStore) syncWatchers() int { // 锁定 s.mu.Lock() defer s.mu.Unlock() // 如果没有未同步观察者，返回0 if s.unsynced.size() == 0 { return 0 } // 锁定存储的读写锁 s.store.revMu.RLock() defer s.store.revMu.RUnlock() // 为了从未同步观察者中找到键值对，我们需要找到最小修订版本 curRev := s.store.currentRev compactionRev := s.store.compactMainRev // 选择一组观察者 wg, minRev := s.unsynced.choose(maxWatchersPerSync, curRev, compactionRev) minBytes, maxBytes := NewRevBytes(), NewRevBytes() minBytes = RevToBytes(Revision{Main: minRev}, minBytes) maxBytes = RevToBytes(Revision{Main: curRev + 1}, maxBytes) // UnsafeRange 返回键和值。在boltdb中，键是修订版本，值是实际的键值对。 tx := s.store.b.ReadTx() tx.RLock() revs, vs := tx.UnsafeRange(schema.Key, minBytes, maxBytes, 0) evs := kvsToEvents(s.store.lg, wg, revs, vs) // 必须在kvsToEvents之后解锁，因为vs（来自boltdb内存）不是深拷贝。 // 我们只能在Unmarshal之后解锁，这将进行深拷贝。 // 否则我们将在boltdb重新mmap期间触发SIGSEGV。 tx.RUnlock() // 创建一个新的观察者批次 victims := make(watcherBatch) wb := newWatcherBatch(wg, evs) for w := range wg.watchers { if w.minRev \u0026lt; compactionRev { // 跳过因压缩而无法发送响应的观察者 continue } w.minRev = curRev + 1 eb, ok := wb[w] if !ok { // 将未通知的观察者移至同步 s.synced.add(w) s.unsynced.delete(w) continue } if eb.moreRev != 0 { w.minRev = eb.moreRev } // 发送响应 if w.send(WatchResponse{WatchID: w.id, Events: eb.evs, Revision: curRev}) { pendingEventsGauge.Add(float64(len(eb.evs))) } else { w.victim = true } // 处理受害者观察者 if w.victim { victims[w] = eb } else { if eb.moreRev != 0 { // 保持未同步状态；还有更多要读取 continue } s.synced.add(w) } s.unsynced.delete(w) } s.addVictim(victims) // 更新慢速观察者计数器 vsz := 0 for _, v := range s.victims { vsz += len(v) } slowWatcherGauge.Set(float64(s.unsynced.size() + vsz)) return s.unsynced.size() } watcher \u0026amp; send type watcher struct { // the watcher key key []byte // end indicates the end of the range to watch. // If end is set, the watcher is on a range. end []byte // victim is set when ch is blocked and undergoing victim processing victim bool // compacted is set when the watcher is removed because of compaction compacted bool // restore is true when the watcher is being restored from leader snapshot // which means that this watcher has just been moved from \u0026quot;synced\u0026quot; to \u0026quot;unsynced\u0026quot; // watcher group, possibly with a future revision when it was first added // to the synced watcher // \u0026quot;unsynced\u0026quot; watcher revision must always be \u0026lt;= current revision, // except when the watcher were to be moved from \u0026quot;synced\u0026quot; watcher group restore bool // minRev is the minimum revision update the watcher will accept minRev int64 id WatchID fcs []FilterFunc // a chan to send out the watch response. // The chan might be shared with other watchers. ch chan\u0026lt;- WatchResponse } func (w *watcher) send(wr WatchResponse) bool { progressEvent := len(wr.Events) == 0 if len(w.fcs) != 0 { ne := make([]mvccpb.Event, 0, len(wr.Events)) for i := range wr.Events { filtered := false for _, filter := range w.fcs { if filter(wr.Events[i]) { filtered = true break } } if !filtered { ne = append(ne, wr.Events[i]) } } wr.Events = ne } // if all events are filtered out, we should send nothing. if !progressEvent \u0026amp;\u0026amp; len(wr.Events) == 0 { return true } select { case w.ch \u0026lt;- wr: return true default: return false } } syncVictimsLoop // syncVictimsLoop 尝试将预先计算的观察者响应写入被阻塞的观察者通道 func (s *watchableStore) syncVictimsLoop() { defer s.wg.Done() for { // 尝试更新所有受害者观察者 for s.moveVictims() != 0 { // 持续更新，直到所有受害者观察者都处理完毕 } // 检查是否有受害者观察者 s.mu.RLock() isEmpty := len(s.victims) == 0 s.mu.RUnlock() var tickc \u0026lt;-chan time.Time if !isEmpty { tickc = time.After(10 * time.Millisecond) } // 等待10毫秒或收到新的受害者通知或停止信号 select { case \u0026lt;-tickc: case \u0026lt;-s.victimc: case \u0026lt;-s.stopc: return } } } // moveVictims 尝试使用已存在的事件数据更新观察者 func (s *watchableStore) moveVictims() (moved int) { s.mu.Lock() victims := s.victims s.victims = nil s.mu.Unlock() var newVictim watcherBatch for _, wb := range victims { // 再次尝试发送响应 for w, eb := range wb { // 观察者已观察到存储，直到但不包括 w.minRev rev := w.minRev - 1 if w.send(WatchResponse{WatchID: w.id, Events: eb.evs, Revision: rev}) { pendingEventsGauge.Add(float64(len(eb.evs))) } else { if newVictim == nil { newVictim = make(watcherBatch) } newVictim[w] = eb continue } moved++ } // 将完成的受害者观察者分配到未同步/同步组 s.mu.Lock() s.store.revMu.RLock() curRev := s.store.currentRev for w, eb := range wb { if newVictim != nil \u0026amp;\u0026amp; newVictim[w] != nil { // 无法发送watch响应，仍然是受害者 continue } w.victim = false if eb.moreRev != 0 { w.minRev = eb.moreRev } if w.minRev \u0026lt;= curRev { s.unsynced.add(w) } else { slowWatcherGauge.Dec() s.synced.add(w) } } s.store.revMu.RUnlock() s.mu.Unlock() } // 如果仍然有未处理的受害者，重新添加到受害者列表中 if len(newVictim) \u0026gt; 0 { s.mu.Lock() s.victims = append(s.victims, newVictim) s.mu.Unlock() } return moved } Reference https://blog.csdn.net/qq_24433609/article/details/120653747 ","date":"2024-06-10","permalink":"https://daemon365.dev/2024/06/10/etcd-watch-%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","tags":["boltdb","etcd","golang","数据库","kubernetes"],"title":"etcd watch 实现原理"},{"content":"什么是 MVCC MVCC 是 Multi-Version Concurrency Control 的缩写，即多版本并发控制。它是一种并发控制的方法，用于在数据库系统中实现事务的隔离性。MVCC 是一种乐观锁机制，它通过保存数据的多个版本来实现事务的隔禽性。在 etcd 中，MVCC 是用于实现数据的版本控制的。而且可以查看历史版本的数据。\n测试 # 添加数据 etcdctl put /test t1 OK etcdctl put /test t2 OK # 查看数据 etcdctl get /test /test t2 # 查看 json 格式数据 etcdctl get /test --write-out=json # {\u0026quot;header\u0026quot;:{\u0026quot;cluster_id\u0026quot;:8735285696067307020,\u0026quot;member_id\u0026quot;:7131777314758672153,\u0026quot;revision\u0026quot;:15,\u0026quot;raft_term\u0026quot;:4},\u0026quot;kvs\u0026quot;:[{\u0026quot;key\u0026quot;:\u0026quot;L3Rlc3Q=\u0026quot;,\u0026quot;create_revision\u0026quot;:14,\u0026quot;mod_revision\u0026quot;:15,\u0026quot;version\u0026quot;:2,\u0026quot;value\u0026quot;:\u0026quot;dDI=\u0026quot;}],\u0026quot;count\u0026quot;:1} # 查看历史版本 etcdctl get /test --rev=14 /test t1 可以看到，通过 --rev 参数可以查看历史版本的数据。也就是我第一次添加的数据。那么 json 中 revision 是什么意思呢？\nrevision reversion 中是 etcd 中的一个概念，它是一个递增的整数，用于标识 etcd 中的数据版本。他是一个 int64 类型。没操作一次 etcd 数据（增，删，改），reversion 就会递增。\n# 删除数据 etcdctl del /test 1 # 查看 revision etcdctl get / -wjson # {\u0026quot;header\u0026quot;:{\u0026quot;cluster_id\u0026quot;:8735285696067307020,\u0026quot;member_id\u0026quot;:7131777314758672153,\u0026quot;revision\u0026quot;:16,\u0026quot;raft_term\u0026quot;:4}} # 刚才是 15 现在是 16 # 添加 /test2 数据 etcdctl put /test2 t3 OK # 查看 revision etcdctl get / -wjson # {\u0026quot;header\u0026quot;:{\u0026quot;cluster_id\u0026quot;:8735285696067307020,\u0026quot;member_id\u0026quot;:7131777314758672153,\u0026quot;revision\u0026quot;:17,\u0026quot;raft_term\u0026quot;:4}} 存储结构 etcd mvcc 中，维护了两个数据结构，分别是 treeindex 和 boltDB。treeindex 是一个 B 树，用于存储 key 和 revision 之间的映射关系，它主要维护在内存中。而 boltDB 是一个 key-value 数据库，用于存储 key 和 value 之间的映射关系, 它主要维护在磁盘中, 用于持久化数据，虽然 boltdb 使用了 mmap 机制，但是它还是一个磁盘数据库。\ntreeindex 为什么 etcd 的 treeindex 使用 B-tree 而不使用哈希表、平衡二叉树？\n因为 etcd 需要范围查询，所以哈希表不适合。而且etcd 中的 key 过多，平衡二叉树的查询效率不高，所以使用 B tree。\nb-tree:\n在 treeindex 中，数据的每个 key 是一个 keyIndex 结构，它保存了 key 和 revision 之间的映射关系。keyIndex 结构如下：\ntype keyIndex struct { key []byte // key 的值 modified Revision // 最后一次修改的 main revision generations []generation // 保存了 key 的历史版本 没删除一次然后添加一次就是一个 generation } type Revision struct { // 就是 revision 的值，比如上边的 15 等 Main int64 // 子 revision 的值 主要是在事务中使用 比如事务中多个操作 那么就是 0 1 2 3 等 Sub int64 } // generation 保存了 key 的历史版本 type generation struct { ver int64 // 版本号 created Revision // 最后一次被创建的 revision revs []Revision // 保存了 key 的历史 revision } 在 treeindex 中，每个 keyIndex 保存了 key 的历史版本，而且每个 keyIndex 中的 generations 保存了 key 的历史版本。而且每个 generation 中的 revs 保存了 key 的历史 revision。这样就可以实现历史版本的查询。\n获取 resersion 的值\nfunc (ti *treeIndex) Get(key []byte, atRev int64) (modified, created Revision, ver int64, err error) { ti.RLock() defer ti.RUnlock() return ti.unsafeGet(key, atRev) } func (ti *treeIndex) unsafeGet(key []byte, atRev int64) (modified, created Revision, ver int64, err error) { keyi := \u0026amp;keyIndex{key: key} // 从 B 树中获取 keyIndex if keyi = ti.keyIndex(keyi); keyi == nil { return Revision{}, Revision{}, 0, ErrRevisionNotFound } // 从 keyIndex 中获取 revision return keyi.get(ti.lg, atRev) } func (ti *treeIndex) keyIndex(keyi *keyIndex) *keyIndex { if ki, ok := ti.tree.Get(keyi); ok { return ki } return nil } func (ki *keyIndex) get(lg *zap.Logger, atRev int64) (modified, created Revision, ver int64, err error) { if ki.isEmpty() { lg.Panic( \u0026quot;'get' got an unexpected empty keyIndex\u0026quot;, zap.String(\u0026quot;key\u0026quot;, string(ki.key)), ) } // 找到 key 的 generation g := ki.findGeneration(atRev) if g.isEmpty() { return Revision{}, Revision{}, 0, ErrRevisionNotFound } // 从 generation 中获取 revision 找到第一次小于 atRev 的 revision n := g.walk(func(rev Revision) bool { return rev.Main \u0026gt; atRev }) if n != -1 { return g.revs[n], g.created, g.ver - int64(len(g.revs)-n-1), nil } return Revision{}, Revision{}, 0, ErrRevisionNotFound } // 基本的意思就是从后往前找到第一个 revision 小于 atRev 的 generation func (ki *keyIndex) findGeneration(rev int64) *generation { lastg := len(ki.generations) - 1 cg := lastg for cg \u0026gt;= 0 { if len(ki.generations[cg].revs) == 0 { cg-- continue } g := ki.generations[cg] if cg != lastg { // 如果当前 generation 的最后一个 revision 小于等于 rev 那么就返回 nil if tomb := g.revs[len(g.revs)-1].Main; tomb \u0026lt;= rev { return nil } } if g.revs[0].Main \u0026lt;= rev { return \u0026amp;ki.generations[cg] } cg-- } return nil } // walk 从后往前遍历 generation func (g *generation) walk(f func(rev Revision) bool) int { l := len(g.revs) for i := range g.revs { ok := f(g.revs[l-i-1]) if !ok { return l - i - 1 } } return -1 } boltdb 上边的 treeindex 拿到 revision 之后，并没有拿到 value，那么如何拿到 value 呢？这就需要用到 boltdb 了。boltdb 是一个 key-value 数据库，用于存储 key 和 value 之间的映射关系。在 etcd 中，boltdb 主要用于持久化数据。 在 etcd 中，boltdb 报错的不是 etcd key-value 数据，而他的 ket 是 revision，value 是元数据。\nfunc (tr *storeTxnCommon) rangeKeys(ctx context.Context, key, end []byte, curRev int64, ro RangeOptions) (*RangeResult, error) { rev := ro.Rev // 如果 rev 大于当前的 revision 那么就返回 ErrFutureRev if rev \u0026gt; curRev { return \u0026amp;RangeResult{KVs: nil, Count: -1, Rev: curRev}, ErrFutureRev } // 如果 rev 小于等于 0 那么就是当前的 revision if rev \u0026lt;= 0 { rev = curRev } // 如果 rev 小于 compactMainRev 那么就返回 ErrCompacted if rev \u0026lt; tr.s.compactMainRev { return \u0026amp;RangeResult{KVs: nil, Count: -1, Rev: 0}, ErrCompacted } // 如果 re.Count 代表 count 操作 查出来直接返回数量就可以了 不需要在查 value if ro.Count { total := tr.s.kvindex.CountRevisions(key, end, rev) tr.trace.Step(\u0026quot;count revisions from in-memory index tree\u0026quot;) return \u0026amp;RangeResult{KVs: nil, Count: total, Rev: curRev}, nil } // 查好需要的 revision 之后，从 boltdb 中查出 value ，revpairs 是从 treeindex 中查出来的 revisions revpairs, total := tr.s.kvindex.Revisions(key, end, rev, int(ro.Limit)) tr.trace.Step(\u0026quot;range keys from in-memory index tree\u0026quot;) if len(revpairs) == 0 { return \u0026amp;RangeResult{KVs: nil, Count: total, Rev: curRev}, nil } limit := int(ro.Limit) if limit \u0026lt;= 0 || limit \u0026gt; len(revpairs) { limit = len(revpairs) } kvs := make([]mvccpb.KeyValue, limit) revBytes := NewRevBytes() // 对于每个 revision 从 boltdb 中查出 value for i, revpair := range revpairs[:len(kvs)] { select { case \u0026lt;-ctx.Done(): return nil, fmt.Errorf(\u0026quot;rangeKeys: context cancelled: %w\u0026quot;, ctx.Err()) default: } // 把 revision 转换成 bytes revBytes = RevToBytes(revpair, revBytes) // 从 boltdb 中查出 value _, vs := tr.tx.UnsafeRange(schema.Key, revBytes, nil, 0) if len(vs) != 1 { tr.s.lg.Fatal( \u0026quot;range failed to find revision pair\u0026quot;, zap.Int64(\u0026quot;revision-main\u0026quot;, revpair.Main), zap.Int64(\u0026quot;revision-sub\u0026quot;, revpair.Sub), zap.Int64(\u0026quot;revision-current\u0026quot;, curRev), zap.Int64(\u0026quot;range-option-rev\u0026quot;, ro.Rev), zap.Int64(\u0026quot;range-option-limit\u0026quot;, ro.Limit), zap.Binary(\u0026quot;key\u0026quot;, key), zap.Binary(\u0026quot;end\u0026quot;, end), zap.Int(\u0026quot;len-revpairs\u0026quot;, len(revpairs)), zap.Int(\u0026quot;len-values\u0026quot;, len(vs)), ) } // 把 value 转换成 mvccpb.KeyValue if err := kvs[i].Unmarshal(vs[0]); err != nil { tr.s.lg.Fatal( \u0026quot;failed to unmarshal mvccpb.KeyValue\u0026quot;, zap.Error(err), ) } } tr.trace.Step(\u0026quot;range keys from bolt db\u0026quot;) return \u0026amp;RangeResult{KVs: kvs, Count: total, Rev: curRev}, nil } // boltdb 的 key 结构 type BucketKey struct { Revision // 墓碑标志 当删除的时候 先标记一下 tombstone bool } func (baseReadTx *baseReadTx) UnsafeRange(bucketType Bucket, key, endKey []byte, limit int64) ([][]byte, [][]byte) { if endKey == nil { // forbid duplicates for single keys limit = 1 } if limit \u0026lt;= 0 { limit = math.MaxInt64 } if limit \u0026gt; 1 \u0026amp;\u0026amp; !bucketType.IsSafeRangeBucket() { panic(\u0026quot;do not use unsafeRange on non-keys bucket\u0026quot;) } // 从缓存中拿出数据 keys, vals := baseReadTx.buf.Range(bucketType, key, endKey, limit) if int64(len(keys)) == limit { return keys, vals } // find/cache bucket bn := bucketType.ID() baseReadTx.txMu.RLock() bucket, ok := baseReadTx.buckets[bn] baseReadTx.txMu.RUnlock() lockHeld := false if !ok { baseReadTx.txMu.Lock() lockHeld = true bucket = baseReadTx.tx.Bucket(bucketType.Name()) baseReadTx.buckets[bn] = bucket } // ignore missing bucket since may have been created in this batch if bucket == nil { if lockHeld { baseReadTx.txMu.Unlock() } return keys, vals } if !lockHeld { baseReadTx.txMu.Lock() } c := bucket.Cursor() baseReadTx.txMu.Unlock() // 从 boltdb 中查出数据 k2, v2 := unsafeRange(c, key, endKey, limit-int64(len(keys))) return append(k2, keys...), append(v2, vals...) } 流程 用户通过 etcdctl get /b 命令获取数据 etcd 通过 treeindex 获取 key 的 revision 信息 {man: 19, sub: 0} etcd 通过 key = {man: 19, sub: 0, tombstone: false} 从 boltdb 中获取 value 值 他是一个protobuf 序列化的数据 etcd 将 value 值反序列化成 mvccpb.KeyValue etcd 将 mvccpb.KeyValue 返回给用户 ","date":"2024-05-26","permalink":"https://daemon365.dev/2024/05/26/etcd-mvcc-%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84%E5%8F%8A%E6%B5%81%E7%A8%8B/","tags":["boltdb","etcd","golang","数据库","kubernetes"],"title":"etcd MVCC 存储结构及流程"},{"content":"istio 是什么 Istio 是一个开放源代码的服务网格，它为基于微服务的应用程序提供了一种统一的方式来连接、保护、监控和管理服务。Istio 主要解决的是在微服务架构中的服务间通信的复杂性问题，它通过提供服务间的负载均衡、服务到服务的认证、监控以及服务的弹性（例如重试、熔断等）来实现。\nsidecar 是什么 sidecar 是一种设计模式，它将挂在业务容器旁边作为辅助，当业务接受流量和传出流量的时候，都先经过 sidecar 然后在到达业务容器或者发出。sidecar 可以看作是一个代理，或者是一个专门为我一个服务而工作的 gateway。这样，服务的熔断、限流、监控、日志等功能都可以在 sidecar 中实现，而不需要在业务容器中实现。从而实现了业务容器的轻量化，只需要关注业务逻辑。\n在 istio 中，sidecar 使用的是 envoy，envoy 是一个高性能的代理，它支持 http1.1, http2, grpc, tcp 等协议，支持负载均衡，熔断，限流，监控等功能。envoy 是一个 c++ 项目，它的性能非常好。通过 istiod 控制平面，使用 grpc stream 的方式更新 envoy 的配置，从而实现了动态配置。\n如果在 pod test 中访问 test namespace 下的 nginx service，那么流量会经过自己的 sidecar，然后到达 nginx 的 sidecar，最后到达 nginx 的容器。nginx 回复同样如此，先到达 sidecar，然后到达 test 的 sidecar，最后到达 test 的容器。\n启动方式 在 kubernetes 中，当 namespace 存在 istio-injection=enabled label，那么在该 namespace 中的 pod 在启动的时候，istio 就会利用 mutating addmission webhook 的方式，自动修改 pod spec ，把 containers 中加入 sidecar 容器。当然，它也加入了一个 initcontainer，目的是做一些网络配置，能做到这个的原因是，kubernetes 的 pod 的多个 container 是使用同一个 linux network namespace, 所以 initcontainer 修改的网络配置对所有的 container 都生效。\n流量劫持方式 在 test 访问 nginx 的例子中，我们可以看到 nginx 的容器是启动在 80 端口上的，流量也是访问的 80, sidecar 是怎么劫持到的呢？而且，在 test pod 中，我们访问的是 nginx 的 service，流量又是怎么劫持到 sidecar 的呢？\n刚刚说过，pod 除了被插入了一个 sidecar 容器，还被插入了一个 initcontainer，我们启动一个 nginx pod 和 service 看一下。\napiVersion: apps/v1 kind: Deployment metadata: name: nginx namespace: test spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx namespace: test spec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80 type: ClusterIP kubectl create ns test kubectl label namespace test istio-injection=enabled # 开启 istio 注入 kubectl -n test apply -f nginx.yaml 现在我们看一下 nginx pod 被插入的 sidecar 和 initcontainer\n## sidecar - args: - proxy - sidecar - --domain - $(POD_NAMESPACE).svc.cluster.local - --proxyLogLevel=warning - --proxyComponentLogLevel=misc:error - --log_output_level=default:info # ... ## intcontainer - args: - istio-iptables - -p - \u0026quot;15001\u0026quot; - -z - \u0026quot;15006\u0026quot; - -u - \u0026quot;1337\u0026quot; - -m - REDIRECT - -i - '*' - -x - \u0026quot;\u0026quot; - -b - '*' - -d - 15090,15021,15020 - --log_output_level=default:info 可以看到，initcontainer 是自动执行了一系列的脚本，这个脚本我们并不知道是做啥的，但是从名字可以看出来是 iptables 的操作，那么我们看一下 iptables 的规则\n这里我们容器内没有 iptables，所以我们在宿主机上查看 在nginx pod 所在的主机上查看，先确定 nginx 这个container 的 pid。\n# 从 pod 的 status 中的容器的 uid kubectl get pod -n test nginx-7c79c4bf97-7985c -o jsonpath='{.status.containerStatuses[0].container ID}' docker://b0d6c9060661e0912a35d33ff220e67a628c9fc5300ca3c67dc30f9e40c9e0ce% # 我这里是用 docker 作为 container runtime 的 # 如果你是 containerd 的话 需要执行 ctr -n k8s.io info [container_id] |grep -i pod docker inspect b0d6c9060661e0912a35d33ff220e67a628c9fc5300ca3c67dc30f9e40c9e0ce | grep -i pid # 在 nginx 所在的主机上执行 \u0026quot;Pid\u0026quot;: 4652 # 在主机上使用 nseneter 进入容器的 network namespace 查看 iptables 规则 nsenter -t 4652 -n -- iptables-save # Warning: iptables-legacy tables present, use iptables-legacy-save to see them # 说明这个容器使用的是 iptables-legacy 我们按提示执行 nsenter -t 4652 -n -- iptables-legacy-save # Generated by iptables-save v1.8.7 on Sun May 12 05:53:29 2024 *nat :PREROUTING ACCEPT [54:3240] :INPUT ACCEPT [54:3240] :OUTPUT ACCEPT [49:4198] :POSTROUTING ACCEPT [49:4198] :ISTIO_INBOUND - [0:0] :ISTIO_IN_REDIRECT - [0:0] :ISTIO_OUTPUT - [0:0] :ISTIO_REDIRECT - [0:0] # 所有入栈流量都走 ISTIO_INBOUND 链 -A PREROUTING -p tcp -j ISTIO_INBOUND # 所有出栈流量都走 ISTIO_OUTPUT 链 -A OUTPUT -p tcp -j ISTIO_OUTPUT # 忽略（也就是不劫持）目标端口是 15008 15090 15021 15020 的流量 # 15008:隧道端口 15090:prometheus 端口 15021:健康检查 15020:管理端口 # 官方文档端口介绍 https://istio.io/latest/docs/ops/deployment/requirements/#ports-used-by-istio -A ISTIO_INBOUND -p tcp -m tcp --dport 15008 -j RETURN -A ISTIO_INBOUND -p tcp -m tcp --dport 15090 -j RETURN -A ISTIO_INBOUND -p tcp -m tcp --dport 15021 -j RETURN -A ISTIO_INBOUND -p tcp -m tcp --dport 15020 -j RETURN # 如果是 tcp 协议就走 ISTIO_IN_REDIRECT 链 -A ISTIO_INBOUND -p tcp -j ISTIO_IN_REDIRECT # 如果是 tcp 就使用 dnat 把流量转发到15006端口（也就是修改tcp的目标端口）也就是交给envoy处理 -A ISTIO_IN_REDIRECT -p tcp -j REDIRECT --to-ports 15006 # 回环地址 不处理 -A ISTIO_OUTPUT -s 127.0.0.6/32 -o lo -j RETURN # 从 lo（回环） 口出来的 目标不是本机 owner是 envoy（--uid-owner 1337） 目标端口是 15008 的 转到 ISTIO_IN_REDIRECT 链处理 -A ISTIO_OUTPUT ! -d 127.0.0.1/32 -o lo -p tcp -m tcp ! --dport 15008 -m owner --uid-owner 1337 -j ISTIO_IN_REDIRECT # 从 lo 口出来的 owner 不是 envoy（不是 envoy 发出的） 不处理 -A ISTIO_OUTPUT -o lo -m owner ! --uid-owner 1337 -j RETURN # owner 是 envoy 的不处理 （不能劫持自己发出去的流量） -A ISTIO_OUTPUT -m owner --uid-owner 1337 -j RETURN # 从 lo（回环） 口出来的 目标不是本机 owner是 envoy（--gid-owner 1337） 目标端口是 15008 的 转到 ISTIO_IN_REDIRECT 链处理 -A ISTIO_OUTPUT ! -d 127.0.0.1/32 -o lo -p tcp -m tcp ! --dport 15008 -m owner --gid-owner 1337 -j ISTIO_IN_REDIRECT # 从 lo（回环） 口出来的 owner 不是 envoy 不处理 -A ISTIO_OUTPUT -o lo -m owner ! --gid-owner 1337 -j RETURN # owner 是 envoy 不处理 -A ISTIO_OUTPUT -m owner --gid-owner 1337 -j RETURN # 目标地址是 127.0.0.1 不处理 -A ISTIO_OUTPUT -d 127.0.0.1/32 -j RETURN # 剩下转到 ISTIO_REDIRECT 链 -A ISTIO_OUTPUT -j ISTIO_REDIRECT # 如果是 tcp 就使用 dnat 把流量转发到 15001 端口（也就是修改tcp的目标端口）也就是交给envoy处理 -A ISTIO_REDIRECT -p tcp -j REDIRECT --to-ports 15001 COMMIT # Completed on Sun May 12 05:53:29 2024 为什么 --uid-owner 1337 和 --gid-owner 1337 就代表 owner 是 envoy 呢？\n因为 envot (istio proxy) 是以 uid 和 gid 起来的。\nkubectl get pod -n test nginx-7c79c4bf97-7985c -o jsonpath='{.spec.containers[1].name}' istio-proxy kubectl get pod -n test nginx-7c79c4bf97-7985c -o jsonpath='{.spec.containers[1].securityContext.ru nAsGroup}' 1337 kubectl get pod -n test nginx-7c79c4bf97-7985c -o jsonpath='{.spec.containers[1].securityContext.runAsUser}' 1337 每个 pod 中的 iptables 都是一样的，这样我们就知道了 test pod 是怎么劫持的，比如现在在 test pod container 中 curl nginx.test 首先查询 dns nginx.test 对应的 ip 是 10.102.168.134, 访问也就是向 http://10.102.168.134:80 发请求。\ntest pod 是发出去的包 所以 iptables 走 OUTPUT 链 http 是 tcp 协议的 所以走 ISTIO_OUTPUT 链 这个流量不是 lo 口的，目标地址不是 127.0.0.1, owner 也不是 envoy ，所以给 ISTIO_REDIRECT 链处理 ISTIO_REDIRECT 中 我们是 tcp 所以 dnat 到端口 15001 （也就是 envoy）做处理 test envoy gateway 会转发到 http://10.244.0.73:80(pod ip) （具体是怎么转发的后续会介绍到） 流量是进入到 nginx pod 中 所以 iptables 的 PREROUTING 链 http 是 tcp 协议的，所以走 ISTIO_INBOUND 链 目标端口是 80 不是15008 15090 15021 15020 所以走 ISTIO_IN_REDIRECT 链 ISTIO_IN_REDIRECT 会 dnat 到端口 15006 也就是 envoy 做处理 envoy 会转发到自己的 nginx container 容器中 （具体是怎么转发的后续会介绍到） 所以可以看到，流量从 test pod 的 test container 出来之后，会被自己的 envoy(istio-proxy sidecar) 劫持，然后7层转发到 10.244.0.73 (nigix pod IP地址)。然后 nginx pod 中的 envoy container 会劫持这个流量交给自己，然后7层代理到 nginx container。流量回复回去是一个道理。\nxDS 在上方的流程中 test envoy 劫持到 http://10.102.168.134:80 会转发到 http://10.244.0.73:80，那么他是怎么知道要转发到这里去的呢？如果是 nginx 我们会配置 upstream，然后执行 nginx -s reload，这是 nginx 不支持动态加载方式。envoy 是支持动态加载的，对外提供 API，我们调用 API 就可以动态的修改配置。这个 API 就是 xDS API。它包括了以下API：\nCDS(Cluster Discovery Service): 用于发现集群信息，比如集群的名字，集群的地址等。 EDS(Endpoint Discovery Service): 用于发现集群中的 endpoint 信息，比如集群中的服务的地址，端口等。 LDS(Listener Discovery Service): 用于发现监听器信息，比如监听器的名字，监听器的地址等。 RDS(Route Discovery Service): 用于发现路由信息，比如路由的名字，路由的规则等。 SDS(Secret Discovery Service): 用于发现证书信息，比如证书的名字，证书的内容等。 HDS(Health Discovery Service): 用于发现健康检查信息，比如健康检查的名字，健康检查的规则等。 ADS(Aggregated Discovery Service): 用于聚合以上所有的服务，提供一个统一的服务。 listener + route 是用来表达监听哪个端口和哪些路由的，是 envoy 进入流量的入口。cluster + endpoint 是用来表达集群和集群中的服务的，是 envoy 转发流量的目的地。\n数据来源 那这些 xDS 的配置来自哪呢？\n这些 envoy 配置都来自于 istiod，istiod 是 istio 的控制平面，它会使用 grpc stream 的方式，动态的更新 envoy 的配置。istiod 会监听 kubernetes 的资源变化，比如 service, endpoint 和 istio 自己 CRD 等，组合成一个配置，然后定时的推送给 envoy。这样就实现了动态配置。\n比如我们这个 service 的配置，查看 service 和 endpoints 就知道 service clusterIp 对应的 pod ip。\nkubectl -n test get svc nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx ClusterIP 10.102.168.134 \u0026lt;none\u0026gt; 80/TCP 112m kubectl -n test get endpoints nginx NAME ENDPOINTS AGE nginx 10.244.0.73:80 112m 这样我们就知道了 nginx service 对应的 clusterIp 是 10.102.168.134, 而对应的 pod ip 是 10.244.0.73。然后 istiod 把这个配置推送给所有 sidecar，这样 test pod 中的 envoy 就知道了要把流量转发到 10.244.0.73 了。\n流量流程图 在 test pod 的 test 容器中 执行 curl http://nginx.test 流量的流程图如下\n配置查看 我们可以通过调用 pod 中的 envoy 的 admin api 来查看配置\nkubectl port-forward pod/nginx-7c79c4bf97-7985c 15000:15000 curl http://localhost:15000/config_dump 打印出的 json 格式的配置文件，它而很大看起来很不方便，幸好 istio 给我们提供了一个工具 istioctl，我们可以使用 istioctl 来查看配置。\n因为我们流量是从 test pod 到 nginx pod，所以所限我们查看 test pod 中的 envoy 的配置。\n1.首先我们看下 listerner 15001 的配置 因为从 iptables 我们知道 出去的流量会被劫持到 15001\nistioctl -n test proxy-config listener test-f5b5d48b5-qdzft --port 15001 ADDRESSES PORT MATCH DESTINATION 0.0.0.0 15001 ALL PassthroughCluster 0.0.0.0 15001 Addr: *:15001 Non-HTTP/Non-TCP 首先会监听 15001 端口，这个是真是监听的端口。它会转发到虚拟端口（Virtual Port），虚拟端口不会真是监听操作系统端口，它只是envoy的逻辑端口。那我们怎么知道流量转发到哪个虚拟端口了？请求的端口是什么，就转发到哪个了。比如我们请求的是 nginx 80 端口，就会转发到 80 虚拟端口\n2.查看 listener 的虚拟端口，nginx service ip 10.102.168.134 prot 80\nistioctl -n test proxy-config listener test-f5b5d48b5-qdzft --port 80 --address 10.102.168.134 ADDRESSES PORT MATCH DESTINATION 10.102.168.134 80 Trans: raw_buffer; App: http/1.1,h2c Route: nginx.test.svc.cluster.local:80 10.102.168.134 80 ALL Cluster: outbound|80||nginx.test.svc.cluster.local 第一条是明文而且是 http/1.1 或者 h2c 无加密的 http/2）协议的流量会被转发到 nginx.test.svc.cluster.local:80\n3.查看 route 配置\nistioctl -n test proxy-config route test-f5b5d48b5-qdzft |grep nginx.test.svc.cluster.local nginx.test.svc.cluster.local:80 nginx.test.svc.cluster.local:80 * /* 80 nginx.test.svc.cluster.local:80 nginx, nginx.test + 1 more... /* 这里我们没有指定 host 所以走第一条\n# istioctl -n test proxy-config route test-f5b5d48b5-qdzft --name nginx.test.svc.cluster.local:80 -oyaml - ignorePortInHostMatching: true maxDirectResponseBodySizeBytes: 1048576 name: nginx.test.svc.cluster.local:80 validateClusters: false virtualHosts: - domains: - '*' includeRequestAttemptCount: true name: nginx.test.svc.cluster.local:80 routes: - decorator: operation: nginx.test.svc.cluster.local:80/* match: prefix: / name: default route: cluster: outbound|80||nginx.test.svc.cluster.local maxGrpcTimeout: 0s retryPolicy: hostSelectionRetryMaxAttempts: \u0026quot;5\u0026quot; numRetries: 2 retriableStatusCodes: - 503 retryHostPredicate: - name: envoy.retry_host_predicates.previous_hosts typedConfig: '@type': type.googleapis.com/envoy.extensions.retry.host.previous_hosts.v3.PreviousHostsPredicate retryOn: connect-failure,refused-stream,unavailable,cancelled,retriable-status-codes timeout: 0s 可以看到，match 的路由是 prefix 是 /，也就是所有的请求都会走这个路由，这个路由会把流量转发到 outbound|80||nginx.test.svc.cluster.local 这个 cluster 中。\n4.我们查看 cluster\n# istioctl -n test proxy-config cluster test-f5b5d48b5-qdzft --fqdn \u0026quot;outbound|80||nginx.test.svc.cluster.local\u0026quot; -oyaml - circuitBreakers: thresholds: - maxConnections: 4294967295 maxPendingRequests: 4294967295 maxRequests: 4294967295 maxRetries: 4294967295 trackRemaining: true commonLbConfig: localityWeightedLbConfig: {} connectTimeout: 10s edsClusterConfig: edsConfig: ads: {} initialFetchTimeout: 0s resourceApiVersion: V3 serviceName: outbound|80||nginx.test.svc.cluster.local filters: - name: istio.metadata_exchange typedConfig: '@type': type.googleapis.com/udpa.type.v1.TypedStruct typeUrl: type.googleapis.com/envoy.tcp.metadataexchange.config.MetadataExchange value: enable_discovery: true protocol: istio-peer-exchange lbPolicy: LEAST_REQUEST metadata: filterMetadata: istio: services: - host: nginx.test.svc.cluster.local name: nginx namespace: test name: outbound|80||nginx.test.svc.cluster.local transportSocketMatches: - match: tlsMode: istio name: tlsMode-istio transportSocket: name: envoy.transport_sockets.tls typedConfig: '@type': type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext commonTlsContext: alpnProtocols: - istio-peer-exchange - istio combinedValidationContext: defaultValidationContext: matchSubjectAltNames: - exact: spiffe://cluster.local/ns/test/sa/default validationContextSdsSecretConfig: name: ROOTCA sdsConfig: apiConfigSource: apiType: GRPC grpcServices: - envoyGrpc: clusterName: sds-grpc setNodeOnFirstMessageOnly: true transportApiVersion: V3 initialFetchTimeout: 0s resourceApiVersion: V3 tlsCertificateSdsSecretConfigs: - name: default sdsConfig: apiConfigSource: apiType: GRPC grpcServices: - envoyGrpc: clusterName: sds-grpc setNodeOnFirstMessageOnly: true transportApiVersion: V3 initialFetchTimeout: 0s resourceApiVersion: V3 tlsParams: tlsMaximumProtocolVersion: TLSv1_3 tlsMinimumProtocolVersion: TLSv1_2 sni: outbound_.80_._.nginx.test.svc.cluster.local - match: {} name: tlsMode-disabled transportSocket: name: envoy.transport_sockets.raw_buffer typedConfig: '@type': type.googleapis.com/envoy.extensions.transport_sockets.raw_buffer.v3.RawBuffer type: EDS typedExtensionProtocolOptions: envoy.extensions.upstreams.http.v3.HttpProtocolOptions: '@type': type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions useDownstreamProtocolConfig: http2ProtocolOptions: {} httpProtocolOptions: {} 可以看到，这个 cluster 是一个 EDS 类型的 cluster，它会从 edsConfig 中获取配置。\n5.查看 endpoint\n# istioctl -n test proxy-config endpoint test-f5b5d48b5-qdzft --cluster \u0026quot;outbound|80||nginx.test.svc.cluster.local\u0026quot; -oyaml - addedViaApi: true circuitBreakers: thresholds: - maxConnections: 4294967295 maxPendingRequests: 4294967295 maxRequests: 4294967295 maxRetries: 4294967295 - maxConnections: 1024 maxPendingRequests: 1024 maxRequests: 1024 maxRetries: 3 priority: HIGH edsServiceName: outbound|80||nginx.test.svc.cluster.local hostStatuses: - address: socketAddress: address: 10.244.0.73 portValue: 80 healthStatus: edsHealthStatus: HEALTHY locality: {} stats: - name: cx_connect_fail - name: cx_total - name: rq_error - name: rq_success - name: rq_timeout - name: rq_total - name: cx_active type: GAUGE - name: rq_active type: GAUGE weight: 1 name: outbound|80||nginx.test.svc.cluster.local observabilityName: outbound|80||nginx.test.svc.cluster.local 从 endpoint 中我们可以看到，流量会转发到 10.244.0.73:80 这个地址。因为我们这个 service 只有一个 pod，如果多个 pod 的话，会有多个 address。\nenvoy 会向访问 http://10.244.0.73:80，那么接下来就是 nginx pod 中的 envoy 会把流量转发到 nginx container 中。\n6.server 劫持到15006\nistioctl -n test proxy-config listener nginx-7c79c4bf97-7985c --port 15006 ADDRESSES PORT MATCH DESTINATION 0.0.0.0 15006 Addr: *:15006 Non-HTTP/Non-TCP 0.0.0.0 15006 Trans: tls; App: istio-http/1.0,istio-http/1.1,istio-h2; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: raw_buffer; App: http/1.1,h2c; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: tls; App: TCP TLS; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: raw_buffer; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: tls; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: tls; App: istio-http/1.0,istio-http/1.1,istio-h2; Addr: *:80 Cluster: inbound|80|| 0.0.0.0 15006 Trans: raw_buffer; App: http/1.1,h2c; Addr: *:80 Cluster: inbound|80|| 0.0.0.0 15006 Trans: tls; App: TCP TLS; Addr: *:80 Cluster: inbound|80|| 0.0.0.0 15006 Trans: raw_buffer; Addr: *:80 Cluster: inbound|80|| 0.0.0.0 15006 Trans: tls; Addr: *:80 Cluster: inbound|80|| listener 的虚拟端口\nistioctl -n test proxy-config listener nginx-7c79c4bf97-7985c --port 80 --address 0.0.0.0 ADDRESSES PORT MATCH DESTINATION 0.0.0.0 80 Trans: raw_buffer; App: http/1.1,h2c Route: 80 0.0.0.0 80 ALL PassthroughCluster 所以 DESTINATION 是 Route: 80 ,所以 route 会匹配 inbound|80||\n7.查看 server route\n# istioctl -n test proxy-config route nginx-7c79c4bf97-7985c --name \u0026quot;inbound|80||\u0026quot; -oyaml - name: inbound|80|| validateClusters: false virtualHosts: - domains: - '*' name: inbound|http|80 routes: - decorator: operation: nginx.test.svc.cluster.local:80/* match: prefix: / name: default route: cluster: inbound|80|| maxStreamDuration: grpcTimeoutHeaderMax: 0s maxStreamDuration: 0s timeout: 0s - name: inbound|80|| validateClusters: false virtualHosts: - domains: - '*' name: inbound|http|80 routes: - decorator: operation: nginx.test.svc.cluster.local:80/* match: prefix: / name: default route: cluster: inbound|80|| maxStreamDuration: grpcTimeoutHeaderMax: 0s maxStreamDuration: 0s timeout: 0s 会匹配到 cluster inbound|80||，它是 inbound 类型的 route, 所以它会直接把流量转发到 80 端口。也就是 pod 中 nginx container 中的 nigix 进程。\n7.回复\n当 nginx 把静态资源回复给 test 的时候，流量的方式和请求的时候是一样的，只是方向相反。\n","date":"2024-05-12","permalink":"https://daemon365.dev/2024/05/12/istio-sidecar-%E5%B7%A5%E4%BD%9C%E6%96%B9%E5%BC%8F/","tags":["istio","sidecar","service mesh","kubernetes","iptables"],"title":"istio sidecar 工作方式"},{"content":"早期 kubelet 创建容器工作原理 因为 docker 出生的比 k8s 早，所以 k8s 早期的容器运行时都是基于 docker 的，kubelet 通过 docker 的 api 创建容器。后来，k8s 官方不想绑死在 docker 这架马车上，就把容器运行时抽象出来，定义了一个接口，叫 CRI (container runtime interface)，容器运行时接口, 通过这个接口，kubelet 可以和任何容器运行时交互。但是，docker 并没有实现这个接口，k8s 也不想直接失去 docker 的用户，所以 k8s 官方在 kubelet 中实现了一个叫 docker-shim 的组件，这个组件简单来说就是把 cri 接口转换成 docker 的 api，这样 kubelet 就可以和 docker 交互了, 这个组件在 kuberbetes 1.24 版本中已经被移除了。至于实现了 cri 接口的容器运行时，比如 containerd，cri-o 等，kubelet 可以直接和它们交互。\n调用架构图如下：\n目前 dockershim 组件已经删除，不能使用了，所以 k8s 1.24 版本之后，kubelet 只能和实现了 cri 接口的容器运行时交互，比如 containerd，cri-o 等。\n这里建议使用 containerd 因为 containerd 是 docker 官方出品的，而且 containerd 也是 docker 的核心组件，docker 的容器运行时就是基于 containerd 的，所以 containerd 的稳定性和可靠性都是有保障的。\ndocker containerd runc 的关系 因为 podman 等新兴 container runtime 的崛起，docker 不想失去定义标准的机会，所以 docker 官方把 containerd 从 docker 中分离出来，独立成一个项目，实现了 cri 接口，这种 kubelet 就可以通过 cri 直接调用 containerd 了。然后，docker 官方又把 runc 从 containerd 中分离出来，独立成一个项目，定义了一个叫 OCI (Open Container Initiative) 的标准，这个标准定义了容器的格式和运行时，runc 就是这个标准的实现，目前实现 oci 的还有 crun youki keta 等。\n因为 containerd 和 runc 脱胎于 docker，docker 又不能维护两份代码，所以 docker 就通过调用 containerd ，containerd 再 通过配置实现 oci 标准的 runc 来创建容器。 当然，你也可以手动配置其他实现了 oci 标准的容器运行时。\n调用架构图如下：\n在上图中可以看到 containerd 不是直接调用 runc 的，而是通过 containerd-shim 来调用 runc 的，这个是为什么？\nrunc runc 是一款设计精巧的命令行工具，专注于创建和运行符合 Open Container Initiative（OCI）规范的容器。执行 runc start 时，它首先通过 fork 创建一个子进程，在这个新进程中进行一系列容器运行的准备工作，包括准备文件系统、配置 namespaces 和 cgroups 。接着，通过 execve 系统调用，这个子进程变身为容器的首个进程——通常被称作“init”进程——并执行用户指定的首个命令（例如，bash）。\n如果首个命令是一个shell（比如 bash），当执行一个shell命令（例如 ls）时，bash 会 fork 并执行相应的子进程。这个新的子进程执行 ls 命令并在完成任务后退出。此后，bash 可能继续接受新的命令，或在结束会话后终止。\n当容器的“init”进程终止时，整个容器也会按照规定的生命周期走向结束。不同的命令和应用会在这个基本框架下有不同的具体行为，但总体流程大致一致。\n如果这些容器的进的父进程是 containerd ，那么当 containerd 进程挂掉或者重启时，容器的进程也会挂掉，这样就不符合容器的定义了，所以 containerd 通过 containerd-shim 来调用 runc，这样当 containerd 挂掉时，容器的进程还是会继续运行的。\ncontainerd-shim containerd-shim 是一个轻量级的代理进程，它的主要作用是：\n通过runC命令可以启动、执行容器、进程； 监控容器进程状态，当容器执行完成后，通过exit fifo文件报告容器进程结束状态； 当此容器SHIM的第一个实例进程被杀死后，reaper掉所有其子进程； 当 containerd 通过 containerd-shim 来调用 runc 后, 会把 containerd-shim 的挂到 system （pid=1）的进程下，这样当 containerd 挂掉或者重启时，containerd-shim 还是会继续运行的，这样就保证了容器的进程不会挂掉。\n验证，这里我随便启动了一下 docker 容器看下效果：\n# 启动的nginx 容器 root 19455 19435 0 22:20 ? 00:00:00 nginx: master process nginx -g daemon off; # nginx 进程的父进程是 containerd-shim root 19435 1 0 22:20 ? 00:00:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 0af95b326dfc8fee31bd28abb61e5d23a9cee98fada2b32c5ade852a0782f559 -address /run/containerd/containerd.sock # containerd-shim 的父进程是 systemd ","date":"2024-05-09","permalink":"https://daemon365.dev/2024/05/09/docker-containerd-runc-containerd-shim%E7%AD%89%E7%BB%84%E4%BB%B6%E7%9A%84%E5%85%B3%E7%B3%BB/","tags":["docker","containerd","runc","containerd-shim","kubernetes","cri"],"title":"docker containerd runc containerd-shim等组件的关系"},{"content":"介绍 BoltDB 是一个用 Go 语言编写的嵌入式键/值数据库。以下是关于 BoltDB 的一些基本介绍：\n键/值存储: BoltDB 为应用程序提供了简单的键/值存储接口。 事务: BoltDB 支持完整的 ACID 事务。 嵌入式: 与像 MySQL 或 PostgreSQL 这样的数据库系统不同，BoltDB 不运行在单独的服务器进程中。它作为一个库被直接嵌入到你的应用程序中。 单文件存储: 所有的数据都存储在一个文件中，这使得备份和迁移变得简单。 高效的二进制存储: 数据在磁盘上使用 B+ 树结构存储，这为随机读取提供了高性能。 前缀扫描: 可以很容易地按键的前缀进行扫描，这使得它适用于范围查询。 没有外部依赖: BoltDB 不依赖于任何外部系统或库。 线程安全: BoltDB 是线程安全的，可以在多个 goroutines 中并发地使用。 BoltDB 特别适用于需要一个轻量级、高性能、易于部署和维护的数据库解决方案的场景。\n虽然 BoltDB 非常有用，但它也有其局限性。例如，它不支持分布式存储，也不适用于需要多节点复制或分片的场景。但对于许多应用程序，它提供了一个简单且高性能的存储解决方案。\n源码地址：\nhttps://github.com/boltdb/bolt 这个项目已经不维护了, etcd 官方维护了一个 fork 库，api 是互通的: https://github.com/etcd-io/bbolt\n谁在使用 etcd、tidb、influxdb、consul \u0026hellip;\u0026hellip;\n架构图 引入 go get go.etcd.io/bbolt 使用 open a database Bolt 中的顶级对象是 DB。它在你的硬盘上表示为一个单独的文件，并代表了你数据的一个一致性快照。\n要打开你的数据库，只需使用 bolt.Open() 函数：\npackage main import ( \u0026quot;log\u0026quot; bolt \u0026quot;go.etcd.io/bbolt\u0026quot; ) func main() { db, err := bolt.Open(\u0026quot;my.db\u0026quot;, 0600, nil) if err != nil { log.Fatal(err) } defer db.Close() } 请注意，Bolt 在数据文件上获得一个文件锁，因此多个进程不能同时打开同一个数据库。打开一个已经打开的 Bolt 数据库会导致它挂起，直到另一个进程关闭它。为了防止无限期的等待，你可以向 Open() 函数传递一个超时选项：\ndb, err := bolt.Open(\u0026quot;my.db\u0026quot;, 0600, \u0026amp;bolt.Options{Timeout: 1 * time.Second}) Transactions Bolt一次只允许一个读写 transactions ，但允许您一次进行多个只读 transactions 。每一个 transactions 在开始时都能看到数据的一致视图。\n单独的 transactions 和从它们创建的所有对象（例如桶、键）都不是线程安全的。要在多个goroutines中处理数据，您必须为每一个goroutine启动一个 transactions ，或使用锁来确保一次只有一个goroutine访问一个 transactions 。从数据库创建 transactions 是线程安全的。\ntransactions 不应相互依赖，并且通常不应在同一goroutine中同时打开。这可能会导致死锁，因为读写 transactions 需要定期重新映射数据文件，但在任何只读 transactions 打开时都不能这样做。即使是嵌套的只读 transactions 也可能导致死锁，因为子 transactions 可能阻止父 transactions 释放其资源。\nRead-write transactions 要开始一个读写 transactions ，可以使用DB.Update()函数：\nerr := db.Update(func(tx *bolt.Tx) error { ... return nil }) 在闭包中，您可以看到数据库的一致视图。通过在最后返回nil来提交 transactions 。您还可以通过返回错误在任何时候回滚 transactions 。所有的数据库操作都允许在一个读写 transactions 中进行。\n始终检查返回错误，因为它会报告任何可能导致您的 transactions 未完成的磁盘故障。如果您在闭包内返回错误，它将被传递。\nRead-only transactions 要开始一个只读 transactions ，您可以使用DB.View()函数：\nerr := db.View(func(tx *bolt.Tx) error { ... return nil }) 在这个闭包内，您也可以看到数据库的一致视图，但在只读 transactions 中不允许进行变动操作。在一个只读 transactions 中，您只能检索桶、检索值和复制数据库。\nBatch read-write transactions 每个DB.Update()都会等待磁盘提交写入。这种开销可以通过使用DB.Batch()函数结合多个更新来最小化：\nerr := db.Batch(func(tx *bolt.Tx) error { ... return nil }) 并发的Batch调用会被机会性地组合成更大的 transactions 。只有当有多个goroutines调用它时，Batch才是有用的。\n权衡之处在于，Batch在 transactions 的部分失败时可以多次调用给定的函数。该函数必须是幂等的，且只有在成功从DB.Batch()返回后，副作用才会生效。\n例如：不要从函数内部显示消息，而是在封闭范围内设置变量：\nvar id uint64 err := db.Batch(func(tx *bolt.Tx) error { // 在桶中查找最后一个键，解码为bigendian uint64，增加 // 一个，编码回[]byte，并添加新键。 ... id = newValue return nil }) if err != nil { return ... } fmt.Println(\u0026quot;Allocated ID %d\u0026quot;, id) Managing transactions manually DB.View()和DB.Update()函数是DB.Begin()函数的包装。这些助手函数将启动 transactions 、执行函数，然后在返回错误时安全地关闭您的 transactions 。这是使用Bolt transactions 的推荐方法。\n然而，有时您可能希望手动开始和结束您的 transactions 。您可以直接使用DB.Begin()函数，但请确保关闭 transactions 。\n// 开始一个可写的 transactions 。 tx, err := db.Begin(true) if err != nil { return err } defer tx.Rollback() // 使用 transactions ... _, err := tx.CreateBucket([]byte(\u0026quot;MyBucket\u0026quot;)) if err != nil { return err } // 提交 transactions 并检查错误。 if err := tx.Commit(); err != nil { return err } DB.Begin()的第一个参数是一个布尔值，表示 transactions 是否应该是可写的。\nUsing buckets 桶是数据库中的键/值对集合。一个桶中的所有键都必须是唯一的。您可以使用Tx.CreateBucket()函数创建一个桶：\ndb.Update(func(tx *bolt.Tx) error { b, err := tx.CreateBucket([]byte(\u0026quot;MyBucket\u0026quot;)) if err != nil { return fmt.Errorf(\u0026quot;create bucket: %s\u0026quot;, err) } return nil }) 您可以使用Tx.Bucket()函数检索现有的桶：\ndb.Update(func(tx *bolt.Tx) error { b := tx.Bucket([]byte(\u0026quot;MyBucket\u0026quot;)) if b == nil { return fmt.Errorf(\u0026quot;bucket does not exist\u0026quot;) } return nil }) 您还可以使用Tx.CreateBucketIfNotExists()函数只在桶不存在时创建一个桶。打开数据库后，为所有的顶级桶调用此函数是一种常见的模式，这样您可以保证它们存在于未来的 transactions 中。\n要删除一个桶，只需调用Tx.DeleteBucket()函数。\nUsing key/value pairs 要将键/值对保存到存储桶，使用Bucket.Put()函数：\ndb.Update(func(tx *bolt.Tx) error { b := tx.Bucket([]byte(\u0026quot;MyBucket\u0026quot;)) err := b.Put([]byte(\u0026quot;answer\u0026quot;), []byte(\u0026quot;42\u0026quot;)) return err }) 这会在MyBucket存储桶中将\u0026quot;answer\u0026quot;键的值设置为\u0026quot;42\u0026quot;。要检索此值，我们可以使用Bucket.Get()函数：\ndb.View(func(tx *bolt.Tx) error { b := tx.Bucket([]byte(\u0026quot;MyBucket\u0026quot;)) v := b.Get([]byte(\u0026quot;answer\u0026quot;)) fmt.Printf(\u0026quot;The answer is: %s\\n\u0026quot;, v) return nil }) Get()函数不返回错误，因为其操作保证可以工作（除非有某种系统故障）。如果键存在，它将返回其字节切片值。如果不存在，则返回nil。重要的是要注意，您可以将零长度的值设置为一个键，这与键不存在是不同的。\n使用Bucket.Delete()函数从存储桶中删除键：\ndb.Update(func (tx *bolt.Tx) error { b := tx.Bucket([]byte(\u0026quot;MyBucket\u0026quot;)) err := b.Delete([]byte(\u0026quot;answer\u0026quot;)) return err }) 这将从MyBucket存储桶中删除答案键。\n请注意，从Get()返回的值只在事务打开时有效。如果您需要在事务外部使用值，则必须使用copy()将其复制到另一个字节切片。\nAutoincrementing integer for the bucket 通过使用NextSequence()函数，您可以让Bolt确定一个序列，该序列可以用作键/值对的唯一标识符。请参阅下面的示例。\n// CreateUser saves u to the store. The new user ID is set on u once the data is persisted. func (s *Store) CreateUser(u *User) error { return s.db.Update(func(tx *bolt.Tx) error { // Retrieve the users bucket. // This should be created when the DB is first opened. b := tx.Bucket([]byte(\u0026quot;users\u0026quot;)) // Generate ID for the user. // This returns an error only if the Tx is closed or not writeable. // That can't happen in an Update() call so I ignore the error check. id, _ := b.NextSequence() u.ID = int(id) // Marshal user data into bytes. buf, err := json.Marshal(u) if err != nil { return err } // Persist bytes to users bucket. return b.Put(itob(u.ID), buf) }) } // itob returns an 8-byte big endian representation of v. func itob(v int) []byte { b := make([]byte, 8) binary.BigEndian.PutUint64(b, uint64(v)) return b } type User struct { ID int ... } Iterating over keys Bolt在存储桶内按字节排序存储其键。这使得对这些键进行顺序迭代非常快。要迭代键，我们将使用一个Cursor：\ndb.View(func(tx *bolt.Tx) error { // Assume bucket exists and has keys b := tx.Bucket([]byte(\u0026quot;MyBucket\u0026quot;)) c := b.Cursor() for k, v := c.First(); k != nil; k, v = c.Next() { fmt.Printf(\u0026quot;key=%s, value=%s\\n\u0026quot;, k, v) } return nil }) 游标允许您移动到键列表中的特定点，并一次向前或向后移动一个键。\n以下函数在游标上可用：\nFirst() 移动到第一个键。 Last() 移动到最后一个键。 Seek() 移动到特定键。 Next() 移动到下一个键。 Prev() 移动到上一个键。 每个函数都有一个返回签名（key []byte, value []byte）。当您迭代到游标的末尾时，Next()将返回一个nil键。您必须使用First()、Last()或Seek()移动到某个位置，然后再调用Next()或Prev()。如果您没有移动到某个位置，那么这些函数将返回一个nil键。 在迭代过程中，如果键不是nil，但值是nil，那么意味着键引用的是一个存储桶而不是一个值。使用Bucket.Bucket()来访问子存储桶。\nPrefix scans 要迭代键前缀，您可以结合使用Seek()和bytes.HasPrefix()：\ndb.View(func(tx *bolt.Tx) error { // Assume bucket exists and has keys c := tx.Bucket([]byte(\u0026quot;MyBucket\u0026quot;)).Cursor() prefix := []byte(\u0026quot;1234\u0026quot;) for k, v := c.Seek(prefix); k != nil \u0026amp;\u0026amp; bytes.HasPrefix(k, prefix); k, v = c.Next() { fmt.Printf(\u0026quot;key=%s, value=%s\\n\u0026quot;, k, v) } return nil }) Range scans 另一个常见的用例是扫描范围，例如时间范围。如果您使用可排序的时间编码，例如RFC3339，那么您可以像这样查询特定的日期范围：\ndb.View(func(tx *bolt.Tx) error { // Assume our events bucket exists and has RFC3339 encoded time keys. c := tx.Bucket([]byte(\u0026quot;Events\u0026quot;)).Cursor() // Our time range spans the 90's decade. min := []byte(\u0026quot;1990-01-01T00:00:00Z\u0026quot;) max := []byte(\u0026quot;2000-01-01T00:00:00Z\u0026quot;) // Iterate over the 90's. for k, v := c.Seek(min); k != nil \u0026amp;\u0026amp; bytes.Compare(k, max) \u0026lt;= 0; k, v = c.Next() { fmt.Printf(\u0026quot;%s: %s\\n\u0026quot;, k, v) } return nil }) 请注意，虽然RFC3339是可排序的，但Golang的RFC3339Nano实现不使用小数点后固定数量的数字，因此不可排序。\nForEach() 如果您知道要在存储桶中迭代所有键，也可以使用ForEach()函数：\ndb.View(func(tx *bolt.Tx) error { // Assume bucket exists and has keys b := tx.Bucket([]byte(\u0026quot;MyBucket\u0026quot;)) b.ForEach(func(k, v []byte) error { fmt.Printf(\u0026quot;key=%s, value=%s\\n\u0026quot;, k, v) return nil }) return nil }) 请注意，ForEach()中的键和值仅在事务打开时有效。如果您需要在事务之外使用键或值，您必须使用copy()将其复制到另一个字节切片。\nNested buckets 以下是给定文档的中文翻译：\n请注意，ForEach()中的键和值仅在事务打开时有效。如果您需要在事务之外使用键或值，您必须使用copy()将其复制到另一个字节切片。\n嵌套的桶 您还可以在键中存储一个桶，以创建嵌套的桶。该API与DB对象上的桶管理API相同：\nfunc (*Bucket) CreateBucket(key []byte) (*Bucket, error) func (*Bucket) CreateBucketIfNotExists(key []byte) (*Bucket, error) func (*Bucket) DeleteBucket(key []byte) error 假设您有一个多租户应用程序，其中根级别的桶是帐户桶。在此桶内是一系列自身为桶的帐户。在这个序列桶里，你可以拥有许多与账户自身相关的桶（如用户、笔记等），将信息隔离到逻辑分组中。\n// createUser 在给定账户中创建一个新用户。 func createUser(accountID int, u *User) error { // 开始事务。 tx, err := db.Begin(true) if err != nil { return err } defer tx.Rollback() // 检索帐户的根桶。 // 假设在设置帐户时已经创建了此桶。 root := tx.Bucket([]byte(strconv.FormatUint(accountID, 10))) // 设置用户桶。 bkt, err := root.CreateBucketIfNotExists([]byte(\u0026quot;USERS\u0026quot;)) if err != nil { return err } // 为新用户生成ID。 userID, err := bkt.NextSequence() if err != nil { return err } u.ID = userID // 序列化并保存编码的用户。 if buf, err := json.Marshal(u); err != nil { return err } else if err := bkt.Put([]byte(strconv.FormatUint(u.ID, 10)), buf); err != nil { return err } // 提交事务。 if err := tx.Commit(); err != nil { return err } return nil } 数据库备份 Bolt是一个单一的文件，所以备份很容易。您可以使用Tx.WriteTo()函数将数据库的一致视图写入一个写入器。如果您从只读事务中调用此函数，它将执行一个热备份，并且不会阻止您的其他数据库读取和写入。\n默认情况下，它将使用一个常规的文件句柄，该句柄将利用操作系统的页面缓存。请查看Tx文档，了解有关优化大于RAM数据集的信息。\n一个常见的用例是通过HTTP进行备份，这样您就可以使用像cURL这样的工具进行数据库备份：\nfunc BackupHandleFunc(w http.ResponseWriter, req *http.Request) { err := db.View(func(tx *bolt.Tx) error { w.Header().Set(\u0026quot;Content-Type\u0026quot;, \u0026quot;application/octet-stream\u0026quot;) w.Header().Set(\u0026quot;Content-Disposition\u0026quot;, `attachment; filename=\u0026quot;my.db\u0026quot;`) w.Header().Set(\u0026quot;Content-Length\u0026quot;, strconv.Itoa(int(tx.Size()))) _, err := tx.WriteTo(w) return err }) if err != nil { http.Error(w, err.Error(), http.StatusInternalServerError) } } 然后你可以使用这个命令备份：\n$ curl http://localhost/backup \u0026gt; my.db 或者您可以打开您的浏览器访问http://localhost/backup，它将自动下载。\n如果您想备份到另一个文件，您可以使用Tx.CopyFile()辅助函数。\n统计信息 数据库保持对其执行的许多内部操作的持续计数，这样您可以更好地了解正在发生的事情。通过在两个时间点获取这些统计的快照，我们可以看到在这个时间范围内执行了哪些操作。\n例如，我们可以启动一个goroutine，每10秒记录一次统计信息：\ngo func() { // 获取初始统计。 prev := db.Stats() for { // 等待10秒。 time.Sleep(10 * time.Second) // 获取当前统计并对其进行差异处理。 stats := db.Stats() diff := stats.Sub(\u0026amp;prev) // 将统计信息编码为JSON并打印到STDERR。 json.NewEncoder(os.Stderr).Encode(diff) // 为下一个循环保存统计。 prev = stats } }() 将这些统计信息管道到像statsd这样的服务进行监控，或者提供一个HTTP端点来执行固定长度的样本，也很有用。\n只读模式 有时创建一个共享的、只读的Bolt数据库是很有用的。要做到这一点，打开数据库时设置Options.ReadOnly标志。只读模式使用共享锁，允许多个进程从数据库中读取，但它会阻止任何进程以读写模式打开数据库。\ndb, err := bolt.Open(\u0026quot;my.db\u0026quot;, 0600, \u0026amp;bolt.Options{ReadOnly: true}) if err != nil { log.Fatal(err) } 希望这可以帮助您理解给定的文档！如果您有任何问题或需要进一步的澄清，请告诉我。\nreferance https://github.com/etcd-io/bbolt https://zhuanlan.zhihu.com/p/377572049 ","date":"2024-05-08","permalink":"https://daemon365.dev/2024/05/08/boltdb-%E4%BB%8B%E7%BB%8D/","tags":["boltdb","etcd","golang","数据库","kubernetes"],"title":"boltdb 介绍"},{"content":"简介 kube-proxy 是 Kubernetes 集群中负责服务发现和负载均衡的组件之一。它是一个网络代理，运行在每个节点上, 用于 service 资源的负载均衡。它有两种模式：iptables 和 ipvs。\niptables iptables 是 Linux 系统中的一个用户空间实用程序，用于配置内核的网络包过滤和网络地址转换（NAT）规则。它是 Linux 内核中的 netfilter 框架的一部分，并负责在网络包进入、转发或离开计算机时进行筛选和处理。其主要功能和用途包括：\n防火墙：iptables 提供了强大的防火墙功能，可以根据不同的规则来过滤和拒绝不需要的网络包。管理员可以创建自定义的规则集，允许或拒绝从特定 IP 地址、端口或协议的数据包。 NAT（网络地址转换）：iptables 支持 NAT 功能，可以用来将私有网络中的计算机与外部网络连接。例如，它可以在一个 NAT 路由器上将内部网络的多个设备映射到单个外部 IP 地址。 端口转发：iptables 可以将特定的端口流量从一个网络接口转发到另一个接口或目标 IP 地址，通常用于内部网络的服务公开。 负载均衡：它也可以通过 DNAT（目标网络地址转换）功能将流量转发给多个内部服务器，实现简单的负载均衡。 iptables 是通过链（chains）和表（tables）来组织规则的。每个链由一组规则组成，当网络数据包经过时，这些规则会逐一执行。常用的表包括：\nfilter 表：用于包过滤，是最常用的表。 nat 表：用于网络地址转换。 mangle 表：用于修改数据包的 IP 层字段。 raw 表：用于绕过连接跟踪。 链的流向为：\n所以，根据上图，我们能够想象出某些常用场景中，报文的流向：\n到本机某进程的报文：PREROUTING –\u0026gt; INPUT\n由本机转发的报文：PREROUTING –\u0026gt; FORWARD –\u0026gt; POSTROUTING\n由本机的某进程发出报文（通常为响应报文）：OUTPUT –\u0026gt; POSTROUTING\n尽管在某些情况下配置 iptables 规则可能复杂，但它提供了高度的灵活性和强大的功能，使其成为 Linux 网络安全的重要组成部分。\nservice 负载均衡 我启动了一个 3 个 nginx pod，和一个对应的 service，service 的类型是 ClusterIP，这样 service 就会有一个虚拟 IP，这个 IP 会被 kube-proxy 代理到后端的 pod 上。\n~ » k get pod -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-deployment-59f546cb79-2k9ng 1/1 Running 2 (31m ago) 50m 10.244.0.28 minikube \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-59f546cb79-wfw84 1/1 Running 2 (31m ago) 50m 10.244.0.30 minikube \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-59f546cb79-zr9xm 1/1 Running 2 (31m ago) 50m 10.244.0.27 minikube \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ---------------------------------------------------------------------------------------------------------------------------------------------------- ~ » k get svc nginx-service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-service ClusterIP 10.101.57.97 \u0026lt;none\u0026gt; 80/TCP 29m 当我们在 master 使用 curl 10.101.57.97 访问 service 的时候，首先会进入 PREROUTING 链：\nroot@minikube:/# iptables-save |grep PREROUTING :PREROUTING ACCEPT [0:0] :PREROUTING ACCEPT [34:2040] -A PREROUTING -m comment --comment \u0026quot;kubernetes service portals\u0026quot; -j KUBE-SERVICES -A PREROUTING -d 192.168.49.1/32 -j DOCKER_OUTPUT -A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER 首先会尝试匹配 KUBE-SERVICES 链，这个链是 kube-proxy 生成的，用于处理 service 的请求。后两个是 docker 的链，用于处理 docker 的请求。\nroot@minikube:/# iptables-save |grep \u0026quot;\\-A KUBE-SERVICES\u0026quot; -A KUBE-SERVICES -d 10.96.0.1/32 -p tcp -m comment --comment \u0026quot;default/kubernetes:https cluster IP\u0026quot; -j KUBE-SVC-NPX46M4PTMTKRN6Y -A KUBE-SERVICES -d 10.101.57.97/32 -p tcp -m comment --comment \u0026quot;default/nginx-service cluster IP\u0026quot; -j KUBE-SVC-V2OKYYMBY3REGZOG -A KUBE-SERVICES -d 10.96.0.10/32 -p udp -m comment --comment \u0026quot;kube-system/kube-dns:dns cluster IP\u0026quot; -j KUBE-SVC-TCOU7JCQXEZGVUNU -A KUBE-SERVICES -d 10.96.0.10/32 -p tcp -m comment --comment \u0026quot;kube-system/kube-dns:dns-tcp cluster IP\u0026quot; -j KUBE-SVC-ERIFXISQEP7F7OF4 -A KUBE-SERVICES -d 10.96.0.10/32 -p tcp -m comment --comment \u0026quot;kube-system/kube-dns:metrics cluster IP\u0026quot; -j KUBE-SVC-JD5MR3NA4I4DYORP -A KUBE-SERVICES -m comment --comment \u0026quot;kubernetes service nodeports; NOTE: this must be the last rule in this chain\u0026quot; -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS 我们这个 nginx-service 的 cluster IP 是 10.101.57.97, 所以会走 KUBE-SVC-V2OKYYMBY3REGZOG 链：\nroot@minikube:/# iptables-save |grep \u0026quot;\\-A KUBE-SVC-V2OKYYMBY3REGZOG\u0026quot; -A KUBE-SVC-V2OKYYMBY3REGZOG ! -s 10.244.0.0/16 -d 10.101.57.97/32 -p tcp -m comment --comment \u0026quot;default/nginx-service cluster IP\u0026quot; -j KUBE-MARK-MASQ -A KUBE-SVC-V2OKYYMBY3REGZOG -m comment --comment \u0026quot;default/nginx-service -\u0026gt; 10.244.0.27:80\u0026quot; -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-POZMZY2HDLRATSJV -A KUBE-SVC-V2OKYYMBY3REGZOG -m comment --comment \u0026quot;default/nginx-service -\u0026gt; 10.244.0.28:80\u0026quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-Z3HXRORN5VDCFRJU -A KUBE-SVC-V2OKYYMBY3REGZOG -m comment --comment \u0026quot;default/nginx-service -\u0026gt; 10.244.0.30:80\u0026quot; -j KUBE-SEP-S46ZL6MIFVWDY42O -A KUBE-SVC-V2OKYYMBY3REGZOG ! -s 10.244.0.0/16 -d 10.101.57.97/32 -p tcp -m comment --comment \u0026quot;default/nginx-service cluster IP\u0026quot; -j KUBE-MARK-MASQ 这条规则的如果源ip 不是 10.244.0.0/16 的 ip（也就是不是 pod发出来的请求），目的ip 是 service ip，jump 跳转到 这个链 KUBE-MARK-MASQ\nroot@minikube:/# iptables-save |grep \u0026quot;\\-A KUBE-MARK-MASQ\u0026quot; -A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000 这条规则的作用是给数据包打上 0x4000 这个标记。这个标记会被后续的 NAT 规则识别到，从而让这些数据包通过特定的 NAT 规则进行 IP 地址转换。\n接下来看主要的三条规则：\n-A KUBE-SVC-V2OKYYMBY3REGZOG -m comment --comment \u0026quot;default/nginx-service -\u0026gt; 10.244.0.27:80\u0026quot; -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-POZMZY2HDLRATSJV -A KUBE-SVC-V2OKYYMBY3REGZOG -m comment --comment \u0026quot;default/nginx-service -\u0026gt; 10.244.0.28:80\u0026quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-Z3HXRORN5VDCFRJU -A KUBE-SVC-V2OKYYMBY3REGZOG -m comment --comment \u0026quot;default/nginx-service -\u0026gt; 10.244.0.30:80\u0026quot; -j KUBE-SEP-S46ZL6MIFVWDY42O 第一条是说有 33.33% 的几率会被转发到 KUBE-SEP-POZMZY2HDLRATSJV, 第二条是 50% 的几率会被转发到 KUBE-SEP-Z3HXRORN5VDCFRJU, 第三条是一定会被转发到 KUBE-SEP-S46ZL6MIFVWDY42O。 转到第一条的概率是 33.33%，转到第二条的概率是 66.67%（没有到第一条的概率） * 50% = 33.33%，第三条就是 66.67%（没有到第一条的概率） * 50%（没有到第二条的概率） = 33.33%。所以这三条规则的概率是一样的。都是 33.33%。 那么 KUBE-SEP-POZMZY2HDLRATSJV 又是什么呢？\nroot@minikube:/# iptables-save |grep \u0026quot;\\-A KUBE-SEP-POZMZY2HDLRATSJV\u0026quot; -A KUBE-SEP-POZMZY2HDLRATSJV -s 10.244.0.27/32 -m comment --comment \u0026quot;default/nginx-service\u0026quot; -j KUBE-MARK-MASQ -A KUBE-SEP-POZMZY2HDLRATSJV -p tcp -m comment --comment \u0026quot;default/nginx-service\u0026quot; -m tcp -j DNAT --to-destination 10.244.0.27:80 root@minikube:/# iptables-save |grep \u0026quot;\\-A KUBE-SEP-POZMZY2HDLRATSJV\u0026quot; -A KUBE-SEP-Z3HXRORN5VDCFRJU -s 10.244.0.28/32 -m comment --comment \u0026quot;default/nginx-service\u0026quot; -j KUBE-MARK-MASQ -A KUBE-SEP-Z3HXRORN5VDCFRJU -p tcp -m comment --comment \u0026quot;default/nginx-service\u0026quot; -m tcp -j DNAT --to-destination 10.244.0.28:80 root@minikube:/# iptables-save |grep \u0026quot;\\-A KUBE-SEP-S46ZL6MIFVWDY42O\u0026quot; -A KUBE-SEP-S46ZL6MIFVWDY42O -s 10.244.0.30/32 -m comment --comment \u0026quot;default/nginx-service\u0026quot; -j KUBE-MARK-MASQ -A KUBE-SEP-S46ZL6MIFVWDY42O -p tcp -m comment --comment \u0026quot;default/nginx-service\u0026quot; -m tcp -j DNAT --to-destination 10.244.0.30:80 第一条规则确保流量从 10.244.0.20 发出时被打上 MASQUERADE 标记，以便通过 NAT 机制进行 IP 伪装。 第二条是将流量转发到 10.244.0.20:80 并使用 DNAT 机制进行目标地址转换, 转换的 ip 10.244.0.27:80 就是 pod 的 ip 和端口。 KUBE-SEP-Z3HXRORN5VDCFRJU 和 KUBE-SEP-S46ZL6MIFVWDY42O 的规则和这条同理。\nipvs IPVS（IP Virtual Server）是 Linux 内核中实现负载均衡功能的模块。是一种高效的负载均衡技术，可以在第 4 层（传输层）进行流量转发和调度。IPVS 通常被用于构建高性能、高可用性的负载均衡集群。\n查看 ipvs 的规则：\nroot@minikube:/etc/apt# sudo ipvsadm -Ln|grep -A 4 10.101.57.97 Prot LocalAddress:Port Scheduler Flags -\u0026gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.101.57.97:80 rr -\u0026gt; 10.244.0.27:80 Masq 1 0 0 -\u0026gt; 10.244.0.28:80 Masq 1 0 0 -\u0026gt; 10.244.0.30:80 Masq 1 0 0 UDP 10.96.0.10:53 rr 这个的意思是到10.101.57.97:80 的 tcp 浏览会使用 rr（轮询）的方式转发到 10.244.0.27:80，10.244.0.28:80，10.244.0.30:80 这三个 pod 上。Masq：指示 \u0026ldquo;Masquerading\u0026rdquo;，表示通过 NAT 来处理网络流量。 所以 ipvs 的模式比 iptables 性能高的多，第一因为 ipvs 是轮询选，iptables 是逐条百分比匹配的，这个还是可以接受的。更要命的是第二条，当 pod 频繁变更的时候 service 对应的 endpoint 的 ENDPOINTS 就会增加或者是减少。那么 iptables 对应 service 的所有规则的百分比都会变化，就会导致一个 service 的规则全部要重刷，当 pod 变化太频繁时，会吃掉大量的 CPU。\n不是说开启了 ipvs 就不会有 iptables 了，还需要 iptables 的 SNAT 规则来处理返回的数据包。\n-A POSTROUTING -m comment --comment \u0026quot;kubernetes postrouting rules\u0026quot; -j KUBE-POSTROUTING -A KUBE-POSTROUTING -m comment --comment \u0026quot;Kubernetes endpoints dst ip:port, source ip for solving hairpin purpose\u0026quot; -m set --match-set KUBE-LOOP-BACK dst,dst,src -j MASQUERADE -m set --match-set KUBE-LOOP-BACK dst,dst,src 的意思是匹配 KUBE-LOOP-BACK 这个 set，这个 set 里面存放的是 pod 的 ip，这个规则的作用是将数据包的源地址替换为本机的地址，以便数据包能够正确返回到客户端。 为什么是 pod ip 而不是 service cluster ip 呢？因为已经通过 ipvs DNAT 过了，所以这里是 pod ip。\nroot@minikube:/etc/apt# ipset -L|grep -A 15 KUBE-LOOP-BACK Name: KUBE-LOOP-BACK Type: hash:ip,port,ip Revision: 6 Header: family inet hashsize 1024 maxelem 65536 bucketsize 12 initval 0xe4e21451 Size in memory: 544 References: 1 Number of entries: 6 Members: 10.244.0.28,tcp:80,10.244.0.28 10.244.0.30,tcp:80,10.244.0.30 10.244.0.29,tcp:9153,10.244.0.29 10.244.0.29,tcp:53,10.244.0.29 10.244.0.27,tcp:80,10.244.0.27 10.244.0.29,udp:53,10.244.0.29 很明显我们的三个 pod 都在这个 set 里面。\n-A KUBE-POSTROUTING -m comment --comment \u0026quot;Kubernetes endpoints dst ip:port, source ip for solving hairpin purpose\u0026quot; -m set --match-set KUBE-LOOP-BACK dst,dst,src -j MASQUERADE 这条规则的作用是将数据包的源地址替换为本机的地址，以便数据包能够正确返回到客户端。现在我们使用 curl 发出的包目标 ip 是 pod ip了，源 ip 是 node ip。等到数据包返回的时候，kernel 会根据 链路追踪 中的数据记录，会把包的源ip 的pod ip 替换为 serice cluster ip, 目标 ip 从 node ip 替换为我发起请求的 ip。这样包就能正确返回到客户端了。\nReferences https://www.zsythink.net/archives/1199 ","date":"2024-05-08","permalink":"https://daemon365.dev/2024/05/08/kube-proxy-%E6%B5%81%E9%87%8F%E6%B5%81%E8%BD%AC%E6%96%B9%E5%BC%8F/","tags":["kube-proxy","iptables","ipvs","kubernetes"],"title":"kube-proxy 流量流转方式"},{"content":"PV 与 PVC PVC (PersistentVolumeClaim)，命名空间（namespace）级别的资源，由 用户 or StatefulSet 控制器（根据VolumeClaimTemplate） 创建。PVC 类似于 Pod，Pod 消耗 Node 资源，PVC 消耗 PV 资源。Pod 可以请求特定级别的资源（CPU 和内存），而 PVC 可以请求特定存储卷的大小及访问模式（Access Mode PV（PersistentVolume）是集群中的一块存储资源，可以是 NFS、iSCSI、Ceph、GlusterFS 等存储卷，PV 由集群管理员创建，然后由开发者使用 PVC 来申请 PV，PVC 是对 PV 的申请，类似于 Pod 对 Node 的申请。\n静态创建存储卷 也就是我们手动创建一个pv和pvc，然后将pv和pvc绑定，然后pod使用pvc，这样就可以使用pv了。\n创建一个 nfs 的 pv 以及 对应的 pvc\napiVersion: v1 kind: PersistentVolume metadata: name: nfs-pv spec: capacity: storage: 10Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Delete nfs: server: 192.168.203.110 path: /data/nfs apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi 查看 pvc\n$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nfs-pvc Bound nfs-pv 10Gi RWO 101s 创建一个 pod 使用 pvc\napiVersion: v1 kind: Pod metadata: name: test-nfs spec: containers: - image: ubuntu:22.04 name: ubuntu command: - /bin/sh - -c - sleep 10000 volumeMounts: - mountPath: /data name: nfs-volume volumes: - name: nfs-volume persistentVolumeClaim: claimName: nfs-pvc ❯ kubectl exec -it test-nfs -- cat /data/nfs 192.168.203.110:/data/nfs pvc pv 绑定流程 func (ctrl *PersistentVolumeController) syncUnboundClaim(ctx context.Context, claim *v1.PersistentVolumeClaim) error { logger := klog.FromContext(ctx) if claim.Spec.VolumeName == \u0026quot;\u0026quot; { // 是不是延迟绑定 也就是 VolumeBindingMode 为 WaitForFirstConsumer delayBinding, err := storagehelpers.IsDelayBindingMode(claim, ctrl.classLister) if err != nil { return err } // 通过 pvc 找到最合适的 pv volume, err := ctrl.volumes.findBestMatchForClaim(claim, delayBinding) if err != nil { logger.V(2).Info(\u0026quot;Synchronizing unbound PersistentVolumeClaim, Error finding PV for claim\u0026quot;, \u0026quot;PVC\u0026quot;, klog.KObj(claim), \u0026quot;err\u0026quot;, err) return fmt.Errorf(\u0026quot;error finding PV for claim %q: %w\u0026quot;, claimToClaimKey(claim), err) } if volume == nil { //// No PV found for this claim } else /* pv != nil */ { claimKey := claimToClaimKey(claim) logger.V(4).Info(\u0026quot;Synchronizing unbound PersistentVolumeClaim, volume found\u0026quot;, \u0026quot;PVC\u0026quot;, klog.KObj(claim), \u0026quot;volumeName\u0026quot;, volume.Name, \u0026quot;volumeStatus\u0026quot;, getVolumeStatusForLogging(volume)) // 绑定 pv 和 pvc // 这里会处理 pvc 的 spec.volumeName status 和 pv 的 status if err = ctrl.bind(ctx, volume, claim); err != nil { return err } return nil } } else /* pvc.Spec.VolumeName != nil */ { /* ...... */ } } // 选择 func FindMatchingVolume( claim *v1.PersistentVolumeClaim, volumes []*v1.PersistentVolume, node *v1.Node, excludedVolumes map[string]*v1.PersistentVolume, delayBinding bool) (*v1.PersistentVolume, error) { var smallestVolume *v1.PersistentVolume var smallestVolumeQty resource.Quantity requestedQty := claim.Spec.Resources.Requests[v1.ResourceName(v1.ResourceStorage)] requestedClass := GetPersistentVolumeClaimClass(claim) var selector labels.Selector if claim.Spec.Selector != nil { internalSelector, err := metav1.LabelSelectorAsSelector(claim.Spec.Selector) if err != nil { return nil, fmt.Errorf(\u0026quot;error creating internal label selector for claim: %v: %v\u0026quot;, claimToClaimKey(claim), err) } selector = internalSelector } // Go through all available volumes with two goals: // - find a volume that is either pre-bound by user or dynamically // provisioned for this claim. Because of this we need to loop through // all volumes. // - find the smallest matching one if there is no volume pre-bound to // the claim. for _, volume := range volumes { if _, ok := excludedVolumes[volume.Name]; ok { // Skip volumes in the excluded list continue } if volume.Spec.ClaimRef != nil \u0026amp;\u0026amp; !IsVolumeBoundToClaim(volume, claim) { continue } volumeQty := volume.Spec.Capacity[v1.ResourceStorage] if volumeQty.Cmp(requestedQty) \u0026lt; 0 { continue } // filter out mismatching volumeModes if CheckVolumeModeMismatches(\u0026amp;claim.Spec, \u0026amp;volume.Spec) { continue } // check if PV's DeletionTimeStamp is set, if so, skip this volume. if volume.ObjectMeta.DeletionTimestamp != nil { continue } nodeAffinityValid := true if node != nil { // Scheduler path, check that the PV NodeAffinity // is satisfied by the node // CheckNodeAffinity is the most expensive call in this loop. // We should check cheaper conditions first or consider optimizing this function. err := CheckNodeAffinity(volume, node.Labels) if err != nil { nodeAffinityValid = false } } if IsVolumeBoundToClaim(volume, claim) { // If PV node affinity is invalid, return no match. // This means the prebound PV (and therefore PVC) // is not suitable for this node. if !nodeAffinityValid { return nil, nil } return volume, nil } if node == nil \u0026amp;\u0026amp; delayBinding { // PV controller does not bind this claim. // Scheduler will handle binding unbound volumes // Scheduler path will have node != nil continue } // filter out: // - volumes in non-available phase // - volumes whose labels don't match the claim's selector, if specified // - volumes in Class that is not requested // - volumes whose NodeAffinity does not match the node if volume.Status.Phase != v1.VolumeAvailable { // We ignore volumes in non-available phase, because volumes that // satisfies matching criteria will be updated to available, binding // them now has high chance of encountering unnecessary failures // due to API conflicts. continue } else if selector != nil \u0026amp;\u0026amp; !selector.Matches(labels.Set(volume.Labels)) { continue } if GetPersistentVolumeClass(volume) != requestedClass { continue } if !nodeAffinityValid { continue } if node != nil { // Scheduler path // Check that the access modes match if !CheckAccessModes(claim, volume) { continue } } if smallestVolume == nil || smallestVolumeQty.Cmp(volumeQty) \u0026gt; 0 { smallestVolume = volume smallestVolumeQty = volumeQty } } if smallestVolume != nil { // Found a matching volume return smallestVolume, nil } return nil, nil } kubelet 绑定 if err := os.MkdirAll(dir, 0750); err != nil { return err } source := fmt.Sprintf(\u0026quot;%s:%s\u0026quot;, nfsMounter.server, nfsMounter.exportPath) options := []string{} if nfsMounter.readOnly { options = append(options, \u0026quot;ro\u0026quot;) } mountOptions := util.JoinMountOptions(nfsMounter.mountOptions, options) err = nfsMounter.mounter.MountSensitiveWithoutSystemd(source, dir, \u0026quot;nfs\u0026quot;, mountOptions, nil) kubelet 就会在调用 sudo mount -t nfs ... 命令把 nfs 绑定到主机上 绑定的目录大概为 /var/lib/kubelet/pods/[POD-ID]/volumes/\nStorageClass StorageClass 是 Kubernetes 中用来定义存储卷的类型的资源对象，StorageClass 用来定义存储卷的类型，比如 NFS、iSCSI、Ceph、GlusterFS 等存储卷。StorageClass 是集群级别的资源，由集群管理员创建，用户可以使用 StorageClass 来动态创建 PV。\n动态创建存储卷 动态创建存储卷相比静态创建存储卷，少了集群管理员的干预，流程如下图所示：\n创建一个 StorageClass pvc pod\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: local-storage provisioner: rancher.io/local-path reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-local-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 128Mi storageClassName: local-storage --- apiVersion: v1 kind: Pod metadata: name: test spec: containers: - image: ubuntu:22.04 name: ubuntu command: - /bin/sh - -c - sleep 10000 volumeMounts: - mountPath: /data name: my-local-pvc volumes: - name: my-local-pvc persistentVolumeClaim: claimName: my-local-pvc 查看 pv\n❯ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-9d257d8a-29a8-4abf-a1e2-c7e4953fc0ca 128Mi RWO Delete Bound default/my-local-pvc local-storage 85s StorageClass 创建 pv 流程 // 还是 syncUnboundClaim 中 // volume 为空，说明没有找到合适的 pv 那么去检查 如果 pvc 的 storageClassName 不为空，那么就会去找到对应的 storageClass if volume == nil { switch { case delayBinding \u0026amp;\u0026amp; !storagehelpers.IsDelayBindingProvisioning(claim): // ...... case storagehelpers.GetPersistentVolumeClaimClass(claim) != \u0026quot;\u0026quot;: // 如果 pvc 的 storageClassName 不为空，那么就会去找到对应的 storageClass if err = ctrl.provisionClaim(ctx, claim); err != nil { return err } return nil default: } return nil } func (ctrl *PersistentVolumeController) provisionClaim(ctx context.Context, claim *v1.PersistentVolumeClaim) error { plugin, storageClass, err := ctrl.findProvisionablePlugin(claim) ctrl.scheduleOperation(logger, opName, func() error { var err error if plugin == nil { // 如果是外部的 provisioner 这里我们就安装了 rancher.io/local-path 这个插件 // 所以这里会调用 provisionClaimOperationExternal _, err = ctrl.provisionClaimOperationExternal(ctx, claim, storageClass) } else { // 内部的 provisioner 直接处理 _, err = ctrl.provisionClaimOperation(ctx, claim, plugin, storageClass) } return err }) return nil } // 如果是外部的 provisioner 会在 pvc 的 annotations 加入 volume.beta.kubernetes.io/storage-provisioner: rancher.io/local-path 和 volume.kubernetes.io/storage-provisioner: rancher.io/local-path func (ctrl *PersistentVolumeController) setClaimProvisioner(ctx context.Context, claim *v1.PersistentVolumeClaim, provisionerName string) (*v1.PersistentVolumeClaim, error) { if val, ok := claim.Annotations[storagehelpers.AnnStorageProvisioner]; ok \u0026amp;\u0026amp; val == provisionerName { // annotation is already set, nothing to do return claim, nil } // The volume from method args can be pointing to watcher cache. We must not // modify these, therefore create a copy. claimClone := claim.DeepCopy() // TODO: remove the beta storage provisioner anno after the deprecation period logger := klog.FromContext(ctx) metav1.SetMetaDataAnnotation(\u0026amp;claimClone.ObjectMeta, storagehelpers.AnnBetaStorageProvisioner, provisionerName) metav1.SetMetaDataAnnotation(\u0026amp;claimClone.ObjectMeta, storagehelpers.AnnStorageProvisioner, provisionerName) updateMigrationAnnotations(logger, ctrl.csiMigratedPluginManager, ctrl.translator, claimClone.Annotations, true) newClaim, err := ctrl.kubeClient.CoreV1().PersistentVolumeClaims(claim.Namespace).Update(ctx, claimClone, metav1.UpdateOptions{}) if err != nil { return newClaim, err } _, err = ctrl.storeClaimUpdate(logger, newClaim) if err != nil { return newClaim, err } return newClaim, nil } kubernetes external provisioner kubernetes external provisioner 是一个独立的进程，用来动态创建 PV，它通过监听 StorageClass 的事件，当 StorageClass 的 ReclaimPolicy 为 Retain 时，会创建 PV。\n在这里我新建一个 关于 nfs 的 external provisioner\npackage main import ( \u0026quot;context\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;path/filepath\u0026quot; \u0026quot;github.com/golang/glog\u0026quot; v1 \u0026quot;k8s.io/api/core/v1\u0026quot; metav1 \u0026quot;k8s.io/apimachinery/pkg/apis/meta/v1\u0026quot; \u0026quot;k8s.io/client-go/kubernetes\u0026quot; \u0026quot;k8s.io/client-go/tools/clientcmd\u0026quot; \u0026quot;k8s.io/client-go/util/homedir\u0026quot; \u0026quot;sigs.k8s.io/controller-runtime/pkg/log\u0026quot; \u0026quot;sigs.k8s.io/sig-storage-lib-external-provisioner/v10/controller\u0026quot; ) const provisionerName = \u0026quot;provisioner.test.com/nfs\u0026quot; var _ controller.Provisioner = \u0026amp;nfsProvisioner{} type nfsProvisioner struct { client kubernetes.Interface } func (p *nfsProvisioner) Provision(ctx context.Context, options controller.ProvisionOptions) (*v1.PersistentVolume, controller.ProvisioningState, error) { if options.PVC.Spec.Selector != nil { return nil, controller.ProvisioningFinished, fmt.Errorf(\u0026quot;claim Selector is not supported\u0026quot;) } glog.V(4).Infof(\u0026quot;nfs provisioner: VolumeOptions %v\u0026quot;, options) pv := \u0026amp;v1.PersistentVolume{ ObjectMeta: metav1.ObjectMeta{ Name: options.PVName, }, Spec: v1.PersistentVolumeSpec{ PersistentVolumeReclaimPolicy: *options.StorageClass.ReclaimPolicy, AccessModes: options.PVC.Spec.AccessModes, MountOptions: options.StorageClass.MountOptions, Capacity: v1.ResourceList{ v1.ResourceName(v1.ResourceStorage): options.PVC.Spec.Resources.Requests[v1.ResourceName(v1.ResourceStorage)], }, PersistentVolumeSource: v1.PersistentVolumeSource{ NFS: \u0026amp;v1.NFSVolumeSource{ Server: options.StorageClass.Parameters[\u0026quot;server\u0026quot;], Path: options.StorageClass.Parameters[\u0026quot;path\u0026quot;], ReadOnly: options.StorageClass.Parameters[\u0026quot;readOnly\u0026quot;] == \u0026quot;true\u0026quot;, }, }, }, } return pv, controller.ProvisioningFinished, nil } func (p *nfsProvisioner) Delete(ctx context.Context, volume *v1.PersistentVolume) error { // 因为是 nfs 没有产生实际的资源，所以这里不需要删除 // 如果在 provisioner 中创建了资源，那么这里需要删除 // 一般是调用 csi 创建/删除资源 return nil } func main() { l := log.FromContext(context.Background()) config, err := clientcmd.BuildConfigFromFlags(\u0026quot;\u0026quot;, filepath.Join(homedir.HomeDir(), \u0026quot;.kube\u0026quot;, \u0026quot;config\u0026quot;)) if err != nil { glog.Fatalf(\u0026quot;Failed to create kubeconfig: %v\u0026quot;, err) } clientset, err := kubernetes.NewForConfig(config) if err != nil { glog.Fatalf(\u0026quot;Failed to create client: %v\u0026quot;, err) } clientNFSProvisioner := \u0026amp;nfsProvisioner{ client: clientset, } pc := controller.NewProvisionController(l, clientset, provisionerName, clientNFSProvisioner, controller.LeaderElection(true), ) glog.Info(\u0026quot;Starting provision controller\u0026quot;) pc.Run(context.Background()) } 创建一个 nfs 的 storageClass\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: my-nfs provisioner: provisioner.test.com/nfs reclaimPolicy: Delete volumeBindingMode: Immediate parameters: server: \u0026quot;192.168.203.110\u0026quot; path: /data/nfs readOnly: \u0026quot;false\u0026quot; apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-nfs-pvc spec: storageClassName: my-nfs accessModes: - ReadWriteOnce resources: requests: storage: 1Gi apiVersion: v1 kind: Pod metadata: name: test-nfs spec: containers: - image: ubuntu:22.04 name: ubuntu command: - /bin/sh - -c - sleep 10000 volumeMounts: - mountPath: /data name: my-nfs-pvc volumes: - name: my-nfs-pvc persistentVolumeClaim: claimName: my-nfs-pvc ❯ kubectl exec -it test-nfs -- cat /data/nfs 192.168.203.110:/data/nfs CSI 流程 持久化存储流程图如下：\nProvisioner 当部署 csi-controller 时 ，会启动一个伴生容器，项目地址为 https://github.com/kubernetes-csi/external-provisioner 这个项目是一个 csi 的 provisioner 它会监控属于自己的pvc，当有新的pvc创建时，会调用 csi 的 createVolume 方法，创建一个 volume，然后创建一个 pv。当 pvc 删除时，会调用 csi 的 deleteVolume 方法，然后删除 volume 和 pv。\nAttacher external-attacher 也是 csi-controller 的伴生容器，项目地址为 https://github.com/kubernetes-csi/external-attacher 这个项目是一个 csi 的 attacher, 它会监控 AttachDetachController 资源，当有新的资源创建时，会调用 csi 的 controllerPublishVolume 方法，挂载 volume 到 node 上。当资源删除时，会调用 csi 的 controllerUnpublishVolume 方法，卸载 volume。\nSnapshotter external-snapshotter 也是 csi-controller 的伴生容器，项目地址为 https://github.com/kubernetes-csi/external-snapshotter 这个项目是一个 csi 的 snapshotter, 它会监控 VolumeSnapshot 资源，当有新的资源创建时，会调用 csi 的 createSnapshot 方法，创建一个快照。当资源删除时，会调用 csi 的 deleteSnapshot 方法，删除快照。\ncsi-node csi-node 是一个 kubelet 的插件，所以它需要每个节点上都运行，当 pod 创建时，并且 VolumeAttachment 的 .spec.Attached 时，kubelet 会调用 csi 的 NodeStageVolume 函数，之后插件（csiAttacher）调用内部 in-tree CSI 插件（csiMountMgr）的 SetUp 函数，该函数内部会调用 csi 的 NodePublishVolume 函数，挂载 volume 到 pod 上。当 pod 删除时，kubelet 观察到包含 CSI 存储卷的 Pod 被删除，于是调用内部 in-tree CSI 插件（csiMountMgr）的 TearDown 函数，该函数内部会通过 unix domain socket 调用外部 CSI 插件的 NodeUnpublishVolume 函数。kubelet 调用内部 in-tree CSI 插件（csiAttacher）的 UnmountDevice 函数，该函数内部会通过 unix domain socket 调用外部 CSI 插件的 NodeUnstageVolume 函数。\ncsi-node-driver-registrar 这个是 csi-node 的伴生容器，项目地址为 https://github.com/kubernetes-csi/node-driver-registrar, 它的主要作用是向 kubelet 注册 csi 插件，kubelet 会调用 csi 插件的 Probe 方法，如果返回成功，kubelet 会调用 csi 插件的 NodeGetInfo 方法，获取节点信息。\ncsi-livenessprobe 这个是 csi-node 的伴生容器，项目地址为 https://github.com/kubernetes-csi/livenessprobe, 它的主要作用是给 kubernetes 的 livenessprobe 提供一个接口，用来检查 csi 插件是否正常运行。它在 /healthz 时，会调用 csi 的 Probe 方法，如果返回成功，返回 200，否则返回 500。\nReference https://developer.aliyun.com/article/754434 https://developer.aliyun.com/article/783464 ","date":"2024-05-03","permalink":"https://daemon365.dev/2024/05/03/kubernetes-%E5%AD%98%E5%82%A8%E6%B5%81%E7%A8%8B/","tags":["csi","kubernetes"],"title":"kubernetes 存储流程"},{"content":"Reference https://atbug.com/kubelet-source-code-analysis/ kubelet 简介 kubernetes 分为控制面和数据面，kubelet 就是数据面最主要的组件，在每个节点上启动，主要负责容器的创建、启停、监控、日志收集等工作。它是一个在每个集群节点上运行的代理，负责确保节点上的容器根据PodSpec（Pod定义文件）正确运行。\nKubelet执行以下几项重要功能：\nPod生命周期管理：Kubelet根据从API服务器接收到的PodSpecs创建、启动、终止容器。它负责启动Pod中的容器，并确保它们按预期运行。 节点状态监控：Kubelet定期监控节点和容器的状态，并将状态报告回集群的控制平面。这使得集群中的其他组件能够做出相应的调度决策。 资源管理：Kubelet负责管理分配给每个Pod的资源。这包括CPU、内存和磁盘存储资源。 健康检查：Kubelet可以执行容器健康检查，并根据检查结果决定是否需要重启容器。 与容器运行时的通信：Kubelet与容器运行时（如Docker、containerd等）通信，以管理容器的生命周期。 秘密和配置管理：Kubelet负责将秘密、配置映射等挂载到Pod的容器中，以便应用程序可以访问这些配置。 服务发现和负载均衡：尽管Kubelet本身不直接处理服务发现，但它通过设置网络规则和环境变量来支持容器内的服务发现机制。 kubelet 架构 kubelet 的架构由 N 多的组件组成，下面简单介绍下比较重要的几个：\nSync Loop: 这是Kubelet活动的核心，负责同步Pod的状态。同步循环会定期从API服务器获取PodSpecs，并确保容器的当前状态与这些规格相匹配。 PodConfig: 负责将各个配置源转换成 PodSpecs，可以选择的配置源包括：Kube-apiserver、本地文件、HTTP。 PLEG(Pod Lifecycle Event Generator)： 负责监测和缓存Pod生命周期事件，如创建、启动或停止容器，然后将这些事件通知 Sync Loop。 PodWorkers: 负责管理 Pod 的生命周期事件处理。当 Pod 生命周期事件 PLEG 检测到新的事件时，PodWorkers 会被调用来处理这些事件，包括启动新的 Pod、更新现有的 Pod、或者停止和清理不再需要的 Pod。 PodManager: 存储 Pod 的期望状态，kubelet 服务的不同渠道的 Pod。 ContainerRuntime: 顾名思义，容器运行时。与遵循 CRI 规范的高级容器运行时进行交互。 StatsProvider: 提供节点和容器的统计信息，有 cAdvisor 和 CRI 两种实现。 ProbeManager: 负责执行容器的健康检查，包括 Liveness，Startup 和 Readiness 检查。 VolumeManager: 负责管理 Pod 的卷，包括挂载和卸载卷。 ImageManager: 负责管理镜像，包括拉取、删除、镜像 GC 等。 DeviceManager: 负责管理设备，包括 GPU、RDMA 等。 PluginManager: PluginManager 运行一组异步循环，根据此节点确定哪些插件需要注册/取消注册并执行。如 CSI 驱动和设备管理器插件（Device Plugin）。 CertificateManager: 处理证书轮换。 OOMWatcher: 从系统日志中获取容器的 OOM 日志，将其封装成事件并记录。 流程 首先在 cmd/kubelet 中使用传入命令行参数的方式初始化配置，然后创建 pkg/kubelet 中的 Bootstrap inferface, kubelet struct 实现了这个接口， 然后调用 Run 方法启动 kubelet。\nfunc startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *kubelet.Dependencies, enableServer bool) { // start the kubelet go k.Run(podCfg.Updates()) // start the kubelet server if enableServer { go k.ListenAndServe(kubeCfg, kubeDeps.TLSOptions, kubeDeps.Auth, kubeDeps.TracerProvider) } if kubeCfg.ReadOnlyPort \u0026gt; 0 { go k.ListenAndServeReadOnly(netutils.ParseIPSloppy(kubeCfg.Address), uint(kubeCfg.ReadOnlyPort)) } go k.ListenAndServePodResources() } Bootstrap // Bootstrap is a bootstrapping interface for kubelet, targets the initialization protocol type Bootstrap interface { GetConfiguration() kubeletconfiginternal.KubeletConfiguration BirthCry() StartGarbageCollection() ListenAndServe(kubeCfg *kubeletconfiginternal.KubeletConfiguration, tlsOptions *server.TLSOptions, auth server.AuthInterface, tp trace.TracerProvider) ListenAndServeReadOnly(address net.IP, port uint) ListenAndServePodResources() Run(\u0026lt;-chan kubetypes.PodUpdate) RunOnce(\u0026lt;-chan kubetypes.PodUpdate) ([]RunPodResult, error) } type Kubelet struct { // ... } 方法：\nGetConfiguration: 获取 kubelet 的配置 BirthCry: 打印 kubelet 启动信息 StartGarbageCollection: 启动垃圾回收 ListenAndServe: 启动 kubelet 服务 ListenAndServeReadOnly: 启动只读服务 ListenAndServePodResources: 启动 pod 资源服务 Run: 启动 kubelet 的同步循环 RunOnce: 启动一次同步循环 func (kl *Kubelet) StartGarbageCollection() { loggedContainerGCFailure := false go wait.Until(func() { ctx := context.Background() if err := kl.containerGC.GarbageCollect(ctx); err != nil { klog.ErrorS(err, \u0026quot;Container garbage collection failed\u0026quot;) kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.ContainerGCFailed, err.Error()) loggedContainerGCFailure = true } else { var vLevel klog.Level = 4 if loggedContainerGCFailure { vLevel = 1 loggedContainerGCFailure = false } klog.V(vLevel).InfoS(\u0026quot;Container garbage collection succeeded\u0026quot;) } }, ContainerGCPeriod, wait.NeverStop) // when the high threshold is set to 100, stub the image GC manager if kl.kubeletConfiguration.ImageGCHighThresholdPercent == 100 { klog.V(2).InfoS(\u0026quot;ImageGCHighThresholdPercent is set 100, Disable image GC\u0026quot;) return } prevImageGCFailed := false go wait.Until(func() { ctx := context.Background() if err := kl.imageManager.GarbageCollect(ctx); err != nil { if prevImageGCFailed { klog.ErrorS(err, \u0026quot;Image garbage collection failed multiple times in a row\u0026quot;) // Only create an event for repeated failures kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.ImageGCFailed, err.Error()) } else { klog.ErrorS(err, \u0026quot;Image garbage collection failed once. Stats initialization may not have completed yet\u0026quot;) } prevImageGCFailed = true } else { var vLevel klog.Level = 4 if prevImageGCFailed { vLevel = 1 prevImageGCFailed = false } klog.V(vLevel).InfoS(\u0026quot;Image garbage collection succeeded\u0026quot;) } }, ImageGCPeriod, wait.NeverStop) } 大致的流程为使用 containerGC 启动容器垃圾回收，当ImageGCHighThresholdPercent 为100时，不启动镜像垃圾回收，否则使用 imageManager 启动镜像垃圾回收。\n// RunOnce polls from one configuration update and run the associated pods. func (kl *Kubelet) RunOnce(updates \u0026lt;-chan kubetypes.PodUpdate) ([]RunPodResult, error) { ctx := context.Background() // Setup filesystem directories. if err := kl.setupDataDirs(); err != nil { return nil, err } // If the container logs directory does not exist, create it. if _, err := os.Stat(ContainerLogsDir); err != nil { if err := kl.os.MkdirAll(ContainerLogsDir, 0755); err != nil { klog.ErrorS(err, \u0026quot;Failed to create directory\u0026quot;, \u0026quot;path\u0026quot;, ContainerLogsDir) } } select { case u := \u0026lt;-updates: klog.InfoS(\u0026quot;Processing manifest with pods\u0026quot;, \u0026quot;numPods\u0026quot;, len(u.Pods)) result, err := kl.runOnce(ctx, u.Pods, runOnceRetryDelay) klog.InfoS(\u0026quot;Finished processing pods\u0026quot;, \u0026quot;numPods\u0026quot;, len(u.Pods)) return result, err case \u0026lt;-time.After(runOnceManifestDelay): return nil, fmt.Errorf(\u0026quot;no pod manifest update after %v\u0026quot;, runOnceManifestDelay) } } // runOnce runs a given set of pods and returns their status. func (kl *Kubelet) runOnce(ctx context.Context, pods []*v1.Pod, retryDelay time.Duration) (results []RunPodResult, err error) { ch := make(chan RunPodResult) admitted := []*v1.Pod{} for _, pod := range pods { // Check if we can admit the pod. if ok, reason, message := kl.canAdmitPod(admitted, pod); !ok { kl.rejectPod(pod, reason, message) results = append(results, RunPodResult{pod, nil}) continue } admitted = append(admitted, pod) go func(pod *v1.Pod) { err := kl.runPod(ctx, pod, retryDelay) ch \u0026lt;- RunPodResult{pod, err} }(pod) } klog.InfoS(\u0026quot;Waiting for pods\u0026quot;, \u0026quot;numPods\u0026quot;, len(admitted)) failedPods := []string{} for i := 0; i \u0026lt; len(admitted); i++ { res := \u0026lt;-ch results = append(results, res) if res.Err != nil { failedContainerName, err := kl.getFailedContainers(ctx, res.Pod) if err != nil { klog.InfoS(\u0026quot;Unable to get failed containers' names for pod\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(res.Pod), \u0026quot;err\u0026quot;, err) } else { klog.InfoS(\u0026quot;Unable to start pod because container failed\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(res.Pod), \u0026quot;containerName\u0026quot;, failedContainerName) } failedPods = append(failedPods, format.Pod(res.Pod)) } else { klog.InfoS(\u0026quot;Started pod\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(res.Pod)) } } if len(failedPods) \u0026gt; 0 { return results, fmt.Errorf(\u0026quot;error running pods: %v\u0026quot;, failedPods) } klog.InfoS(\u0026quot;Pods started\u0026quot;, \u0026quot;numPods\u0026quot;, len(pods)) return results, err } 大致作用为从 updates 中获取 PodUpdate，然后调用 runOnce 方法，该方法会调用 runPod 方法启动 Pod。\nRun func (kl *Kubelet) Run(updates \u0026lt;-chan kubetypes.PodUpdate) { ctx := context.Background() if kl.logServer == nil { file := http.FileServer(http.Dir(nodeLogDir)) if utilfeature.DefaultFeatureGate.Enabled(features.NodeLogQuery) \u0026amp;\u0026amp; kl.kubeletConfiguration.EnableSystemLogQuery { kl.logServer = http.StripPrefix(\u0026quot;/logs/\u0026quot;, http.HandlerFunc(func(w http.ResponseWriter, req *http.Request) { if nlq, errs := newNodeLogQuery(req.URL.Query()); len(errs) \u0026gt; 0 { http.Error(w, errs.ToAggregate().Error(), http.StatusBadRequest) return } else if nlq != nil { if req.URL.Path != \u0026quot;/\u0026quot; \u0026amp;\u0026amp; req.URL.Path != \u0026quot;\u0026quot; { http.Error(w, \u0026quot;path not allowed in query mode\u0026quot;, http.StatusNotAcceptable) return } if errs := nlq.validate(); len(errs) \u0026gt; 0 { http.Error(w, errs.ToAggregate().Error(), http.StatusNotAcceptable) return } // Validation ensures that the request does not query services and files at the same time if len(nlq.Services) \u0026gt; 0 { journal.ServeHTTP(w, req) return } // Validation ensures that the request does not explicitly query multiple files at the same time if len(nlq.Files) == 1 { // Account for the \\ being used on Windows clients req.URL.Path = filepath.ToSlash(nlq.Files[0]) } } // Fall back in case the caller is directly trying to query a file // Example: kubectl get --raw /api/v1/nodes/$name/proxy/logs/foo.log file.ServeHTTP(w, req) })) } else { kl.logServer = http.StripPrefix(\u0026quot;/logs/\u0026quot;, file) } } if kl.kubeClient == nil { klog.InfoS(\u0026quot;No API server defined - no node status update will be sent\u0026quot;) } // Start the cloud provider sync manager if kl.cloudResourceSyncManager != nil { go kl.cloudResourceSyncManager.Run(wait.NeverStop) } if err := kl.initializeModules(); err != nil { kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.KubeletSetupFailed, err.Error()) klog.ErrorS(err, \u0026quot;Failed to initialize internal modules\u0026quot;) os.Exit(1) } // Start volume manager go kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop) if kl.kubeClient != nil { // Start two go-routines to update the status. // // The first will report to the apiserver every nodeStatusUpdateFrequency and is aimed to provide regular status intervals, // while the second is used to provide a more timely status update during initialization and runs an one-shot update to the apiserver // once the node becomes ready, then exits afterwards. // // Introduce some small jittering to ensure that over time the requests won't start // accumulating at approximately the same time from the set of nodes due to priority and // fairness effect. go wait.JitterUntil(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, 0.04, true, wait.NeverStop) go kl.fastStatusUpdateOnce() // start syncing lease go kl.nodeLeaseController.Run(context.Background()) } go wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop) // Set up iptables util rules if kl.makeIPTablesUtilChains { kl.initNetworkUtil() } // Start component sync loops. kl.statusManager.Start() // Start syncing RuntimeClasses if enabled. if kl.runtimeClassManager != nil { kl.runtimeClassManager.Start(wait.NeverStop) } // Start the pod lifecycle event generator. kl.pleg.Start() // Start eventedPLEG only if EventedPLEG feature gate is enabled. if utilfeature.DefaultFeatureGate.Enabled(features.EventedPLEG) { kl.eventedPleg.Start() } kl.syncLoop(ctx, updates, kl) } 代码的流程为：\n检查是否需要创建日志服务器 如果需要则创建 启动云提供商同步管理器 初始化模块，如果出错则打印日志并退出 启动卷管理器 启动两个 goroutine 来更新状态，一个是定时更新，一个是在初始化时更新 启动同步租约的goroutine 定期更新RuntimeUp状态的goroutine 设置iptables规则 启动组件同步循环 如果启用了RuntimeClasses，则启动RuntimeClasses同步循环 启动Pod Lifecycle Event Generator 如果启用了EventedPLEG特性，则启动EventedPLEG 启动 syncloop syncLoop // syncLoop is the main loop for processing changes. It watches for changes from // three channels (file, apiserver, and http) and creates a union of them. For // any new change seen, will run a sync against desired state and running state. If // no changes are seen to the configuration, will synchronize the last known desired // state every sync-frequency seconds. Never returns. func (kl *Kubelet) syncLoop(ctx context.Context, updates \u0026lt;-chan kubetypes.PodUpdate, handler SyncHandler) { klog.InfoS(\u0026quot;Starting kubelet main sync loop\u0026quot;) // The syncTicker wakes up kubelet to checks if there are any pod workers // that need to be sync'd. A one-second period is sufficient because the // sync interval is defaulted to 10s. syncTicker := time.NewTicker(time.Second) defer syncTicker.Stop() housekeepingTicker := time.NewTicker(housekeepingPeriod) defer housekeepingTicker.Stop() plegCh := kl.pleg.Watch() const ( base = 100 * time.Millisecond max = 5 * time.Second factor = 2 ) duration := base // Responsible for checking limits in resolv.conf // The limits do not have anything to do with individual pods // Since this is called in syncLoop, we don't need to call it anywhere else if kl.dnsConfigurer != nil \u0026amp;\u0026amp; kl.dnsConfigurer.ResolverConfig != \u0026quot;\u0026quot; { kl.dnsConfigurer.CheckLimitsForResolvConf() } for { if err := kl.runtimeState.runtimeErrors(); err != nil { klog.ErrorS(err, \u0026quot;Skipping pod synchronization\u0026quot;) // exponential backoff time.Sleep(duration) duration = time.Duration(math.Min(float64(max), factor*float64(duration))) continue } // reset backoff if we have a success duration = base kl.syncLoopMonitor.Store(kl.clock.Now()) if !kl.syncLoopIteration(ctx, updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) { break } kl.syncLoopMonitor.Store(kl.clock.Now()) } } 代码流程为：\n从 pleg 获取 update 事件 他是一个 channel 进入循环 如果 runtimeState 有错误 sleep 一会儿然后跳过 执行 syncLoopIteration syncLoopIteration func (kl *Kubelet) syncLoopIteration(ctx context.Context, configCh \u0026lt;-chan kubetypes.PodUpdate, handler SyncHandler, syncCh \u0026lt;-chan time.Time, housekeepingCh \u0026lt;-chan time.Time, plegCh \u0026lt;-chan *pleg.PodLifecycleEvent) bool { select { case u, open := \u0026lt;-configCh: // Update from a config source; dispatch it to the right handler // callback. if !open { klog.ErrorS(nil, \u0026quot;Update channel is closed, exiting the sync loop\u0026quot;) return false } switch u.Op { case kubetypes.ADD: klog.V(2).InfoS(\u0026quot;SyncLoop ADD\u0026quot;, \u0026quot;source\u0026quot;, u.Source, \u0026quot;pods\u0026quot;, klog.KObjSlice(u.Pods)) // After restarting, kubelet will get all existing pods through // ADD as if they are new pods. These pods will then go through the // admission process and *may* be rejected. This can be resolved // once we have checkpointing. handler.HandlePodAdditions(u.Pods) case kubetypes.UPDATE: klog.V(2).InfoS(\u0026quot;SyncLoop UPDATE\u0026quot;, \u0026quot;source\u0026quot;, u.Source, \u0026quot;pods\u0026quot;, klog.KObjSlice(u.Pods)) handler.HandlePodUpdates(u.Pods) case kubetypes.REMOVE: klog.V(2).InfoS(\u0026quot;SyncLoop REMOVE\u0026quot;, \u0026quot;source\u0026quot;, u.Source, \u0026quot;pods\u0026quot;, klog.KObjSlice(u.Pods)) handler.HandlePodRemoves(u.Pods) case kubetypes.RECONCILE: klog.V(4).InfoS(\u0026quot;SyncLoop RECONCILE\u0026quot;, \u0026quot;source\u0026quot;, u.Source, \u0026quot;pods\u0026quot;, klog.KObjSlice(u.Pods)) handler.HandlePodReconcile(u.Pods) case kubetypes.DELETE: klog.V(2).InfoS(\u0026quot;SyncLoop DELETE\u0026quot;, \u0026quot;source\u0026quot;, u.Source, \u0026quot;pods\u0026quot;, klog.KObjSlice(u.Pods)) // DELETE is treated as a UPDATE because of graceful deletion. handler.HandlePodUpdates(u.Pods) case kubetypes.SET: // TODO: Do we want to support this? klog.ErrorS(nil, \u0026quot;Kubelet does not support snapshot update\u0026quot;) default: klog.ErrorS(nil, \u0026quot;Invalid operation type received\u0026quot;, \u0026quot;operation\u0026quot;, u.Op) } kl.sourcesReady.AddSource(u.Source) case e := \u0026lt;-plegCh: if isSyncPodWorthy(e) { // PLEG event for a pod; sync it. if pod, ok := kl.podManager.GetPodByUID(e.ID); ok { klog.V(2).InfoS(\u0026quot;SyncLoop (PLEG): event for pod\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(pod), \u0026quot;event\u0026quot;, e) handler.HandlePodSyncs([]*v1.Pod{pod}) } else { // If the pod no longer exists, ignore the event. klog.V(4).InfoS(\u0026quot;SyncLoop (PLEG): pod does not exist, ignore irrelevant event\u0026quot;, \u0026quot;event\u0026quot;, e) } } if e.Type == pleg.ContainerDied { if containerID, ok := e.Data.(string); ok { kl.cleanUpContainersInPod(e.ID, containerID) } } case \u0026lt;-syncCh: // Sync pods waiting for sync podsToSync := kl.getPodsToSync() if len(podsToSync) == 0 { break } klog.V(4).InfoS(\u0026quot;SyncLoop (SYNC) pods\u0026quot;, \u0026quot;total\u0026quot;, len(podsToSync), \u0026quot;pods\u0026quot;, klog.KObjSlice(podsToSync)) handler.HandlePodSyncs(podsToSync) case update := \u0026lt;-kl.livenessManager.Updates(): if update.Result == proberesults.Failure { handleProbeSync(kl, update, handler, \u0026quot;liveness\u0026quot;, \u0026quot;unhealthy\u0026quot;) } case update := \u0026lt;-kl.readinessManager.Updates(): ready := update.Result == proberesults.Success kl.statusManager.SetContainerReadiness(update.PodUID, update.ContainerID, ready) status := \u0026quot;\u0026quot; if ready { status = \u0026quot;ready\u0026quot; } handleProbeSync(kl, update, handler, \u0026quot;readiness\u0026quot;, status) case update := \u0026lt;-kl.startupManager.Updates(): started := update.Result == proberesults.Success kl.statusManager.SetContainerStartup(update.PodUID, update.ContainerID, started) status := \u0026quot;unhealthy\u0026quot; if started { status = \u0026quot;started\u0026quot; } handleProbeSync(kl, update, handler, \u0026quot;startup\u0026quot;, status) case \u0026lt;-housekeepingCh: if !kl.sourcesReady.AllReady() { // If the sources aren't ready or volume manager has not yet synced the states, // skip housekeeping, as we may accidentally delete pods from unready sources. klog.V(4).InfoS(\u0026quot;SyncLoop (housekeeping, skipped): sources aren't ready yet\u0026quot;) } else { start := time.Now() klog.V(4).InfoS(\u0026quot;SyncLoop (housekeeping)\u0026quot;) if err := handler.HandlePodCleanups(ctx); err != nil { klog.ErrorS(err, \u0026quot;Failed cleaning pods\u0026quot;) } duration := time.Since(start) if duration \u0026gt; housekeepingWarningDuration { klog.ErrorS(fmt.Errorf(\u0026quot;housekeeping took too long\u0026quot;), \u0026quot;Housekeeping took longer than expected\u0026quot;, \u0026quot;expected\u0026quot;, housekeepingWarningDuration, \u0026quot;actual\u0026quot;, duration.Round(time.Millisecond)) } klog.V(4).InfoS(\u0026quot;SyncLoop (housekeeping) end\u0026quot;, \u0026quot;duration\u0026quot;, duration.Round(time.Millisecond)) } } return true } 首先解释一下这个函数的参数：\nconfigCh: 将配置更改的 Pod 分派给适当的处理程序回调函数 plegCh: 更新运行时缓存；同步 Pod syncCh: 同步所有等待同步的 Pod housekeepingCh: 触发 Pod 的清理 health manager: 同步失败的 Pod 或其中一个或多个容器的健康检查失败的 Pod 代码流程为：\n如果 updates channel 有消息 则使用 handler 调用对应方法做处理 如果 plegCh 有消息 则使用 handler 的 HandlePodSyncs 做同步 如果 syncCh 有消息 代表到了同步时间 做同步操作 如果是 三种 probe 的更新 则使用 handleProbeSync 做同步 如果 housekeepingCh 有消息 则使用 handler 的 HandlePodCleanups 做清理 func handleProbeSync(kl *Kubelet, update proberesults.Update, handler SyncHandler, probe, status string) { // We should not use the pod from manager, because it is never updated after initialization. pod, ok := kl.podManager.GetPodByUID(update.PodUID) if !ok { // If the pod no longer exists, ignore the update. klog.V(4).InfoS(\u0026quot;SyncLoop (probe): ignore irrelevant update\u0026quot;, \u0026quot;probe\u0026quot;, probe, \u0026quot;status\u0026quot;, status, \u0026quot;update\u0026quot;, update) return } klog.V(1).InfoS(\u0026quot;SyncLoop (probe)\u0026quot;, \u0026quot;probe\u0026quot;, probe, \u0026quot;status\u0026quot;, status, \u0026quot;pod\u0026quot;, klog.KObj(pod)) handler.HandlePodSyncs([]*v1.Pod{pod}) } handleProbeSync也是使用 handler 的 HandlePodSyncs 做同步\nhandle (SyncHandler) // SyncHandler is an interface implemented by Kubelet, for testability type SyncHandler interface { HandlePodAdditions(pods []*v1.Pod) HandlePodUpdates(pods []*v1.Pod) HandlePodRemoves(pods []*v1.Pod) HandlePodReconcile(pods []*v1.Pod) HandlePodSyncs(pods []*v1.Pod) HandlePodCleanups(ctx context.Context) error } 也是 kubelet struct 实现了这个接口\n// HandlePodAdditions is the callback in SyncHandler for pods being added from // a config source. func (kl *Kubelet) HandlePodAdditions(pods []*v1.Pod) { start := kl.clock.Now() sort.Sort(sliceutils.PodsByCreationTime(pods)) if utilfeature.DefaultFeatureGate.Enabled(features.InPlacePodVerticalScaling) { kl.podResizeMutex.Lock() defer kl.podResizeMutex.Unlock() } for _, pod := range pods { existingPods := kl.podManager.GetPods() kl.podManager.AddPod(pod) pod, mirrorPod, wasMirror := kl.podManager.GetPodAndMirrorPod(pod) if wasMirror { if pod == nil { klog.V(2).InfoS(\u0026quot;Unable to find pod for mirror pod, skipping\u0026quot;, \u0026quot;mirrorPod\u0026quot;, klog.KObj(mirrorPod), \u0026quot;mirrorPodUID\u0026quot;, mirrorPod.UID) continue } kl.podWorkers.UpdatePod(UpdatePodOptions{ Pod: pod, MirrorPod: mirrorPod, UpdateType: kubetypes.SyncPodUpdate, StartTime: start, }) continue } if !kl.podWorkers.IsPodTerminationRequested(pod.UID) { // We failed pods that we rejected, so activePods include all admitted // pods that are alive. activePods := kl.filterOutInactivePods(existingPods) if utilfeature.DefaultFeatureGate.Enabled(features.InPlacePodVerticalScaling) { // To handle kubelet restarts, test pod admissibility using AllocatedResources values // (for cpu \u0026amp; memory) from checkpoint store. If found, that is the source of truth. podCopy := pod.DeepCopy() kl.updateContainerResourceAllocation(podCopy) // Check if we can admit the pod; if not, reject it. if ok, reason, message := kl.canAdmitPod(activePods, podCopy); !ok { kl.rejectPod(pod, reason, message) continue } // For new pod, checkpoint the resource values at which the Pod has been admitted if err := kl.statusManager.SetPodAllocation(podCopy); err != nil { //TODO(vinaykul,InPlacePodVerticalScaling): Can we recover from this in some way? Investigate klog.ErrorS(err, \u0026quot;SetPodAllocation failed\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(pod)) } } else { // Check if we can admit the pod; if not, reject it. if ok, reason, message := kl.canAdmitPod(activePods, pod); !ok { kl.rejectPod(pod, reason, message) continue } } } kl.podWorkers.UpdatePod(UpdatePodOptions{ Pod: pod, MirrorPod: mirrorPod, UpdateType: kubetypes.SyncPodCreate, StartTime: start, }) } } func (kl *Kubelet) HandlePodUpdates(pods []*v1.Pod) { start := kl.clock.Now() for _, pod := range pods { kl.podManager.UpdatePod(pod) pod, mirrorPod, wasMirror := kl.podManager.GetPodAndMirrorPod(pod) if wasMirror { if pod == nil { klog.V(2).InfoS(\u0026quot;Unable to find pod for mirror pod, skipping\u0026quot;, \u0026quot;mirrorPod\u0026quot;, klog.KObj(mirrorPod), \u0026quot;mirrorPodUID\u0026quot;, mirrorPod.UID) continue } } kl.podWorkers.UpdatePod(UpdatePodOptions{ Pod: pod, MirrorPod: mirrorPod, UpdateType: kubetypes.SyncPodUpdate, StartTime: start, }) } } func (kl *Kubelet) HandlePodRemoves(pods []*v1.Pod) { start := kl.clock.Now() for _, pod := range pods { kl.podManager.RemovePod(pod) pod, mirrorPod, wasMirror := kl.podManager.GetPodAndMirrorPod(pod) if wasMirror { if pod == nil { klog.V(2).InfoS(\u0026quot;Unable to find pod for mirror pod, skipping\u0026quot;, \u0026quot;mirrorPod\u0026quot;, klog.KObj(mirrorPod), \u0026quot;mirrorPodUID\u0026quot;, mirrorPod.UID) continue } kl.podWorkers.UpdatePod(UpdatePodOptions{ Pod: pod, MirrorPod: mirrorPod, UpdateType: kubetypes.SyncPodUpdate, StartTime: start, }) continue } // Deletion is allowed to fail because the periodic cleanup routine // will trigger deletion again. if err := kl.deletePod(pod); err != nil { klog.V(2).InfoS(\u0026quot;Failed to delete pod\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(pod), \u0026quot;err\u0026quot;, err) } } } func (kl *Kubelet) HandlePodReconcile(pods []*v1.Pod) { start := kl.clock.Now() for _, pod := range pods { // Update the pod in pod manager, status manager will do periodically reconcile according // to the pod manager. kl.podManager.UpdatePod(pod) pod, mirrorPod, wasMirror := kl.podManager.GetPodAndMirrorPod(pod) if wasMirror { if pod == nil { klog.V(2).InfoS(\u0026quot;Unable to find pod for mirror pod, skipping\u0026quot;, \u0026quot;mirrorPod\u0026quot;, klog.KObj(mirrorPod), \u0026quot;mirrorPodUID\u0026quot;, mirrorPod.UID) continue } // Static pods should be reconciled the same way as regular pods } // TODO: reconcile being calculated in the config manager is questionable, and avoiding // extra syncs may no longer be necessary. Reevaluate whether Reconcile and Sync can be // merged (after resolving the next two TODOs). // Reconcile Pod \u0026quot;Ready\u0026quot; condition if necessary. Trigger sync pod for reconciliation. // TODO: this should be unnecessary today - determine what is the cause for this to // be different than Sync, or if there is a better place for it. For instance, we have // needsReconcile in kubelet/config, here, and in status_manager. if status.NeedToReconcilePodReadiness(pod) { kl.podWorkers.UpdatePod(UpdatePodOptions{ Pod: pod, MirrorPod: mirrorPod, UpdateType: kubetypes.SyncPodSync, StartTime: start, }) } // After an evicted pod is synced, all dead containers in the pod can be removed. // TODO: this is questionable - status read is async and during eviction we already // expect to not have some container info. The pod worker knows whether a pod has // been evicted, so if this is about minimizing the time to react to an eviction we // can do better. If it's about preserving pod status info we can also do better. if eviction.PodIsEvicted(pod.Status) { if podStatus, err := kl.podCache.Get(pod.UID); err == nil { kl.containerDeletor.deleteContainersInPod(\u0026quot;\u0026quot;, podStatus, true) } } } } func (kl *Kubelet) HandlePodSyncs(pods []*v1.Pod) { start := kl.clock.Now() for _, pod := range pods { pod, mirrorPod, wasMirror := kl.podManager.GetPodAndMirrorPod(pod) if wasMirror { if pod == nil { klog.V(2).InfoS(\u0026quot;Unable to find pod for mirror pod, skipping\u0026quot;, \u0026quot;mirrorPod\u0026quot;, klog.KObj(mirrorPod), \u0026quot;mirrorPodUID\u0026quot;, mirrorPod.UID) continue } // Syncing a mirror pod is a programmer error since the intent of sync is to // batch notify all pending work. We should make it impossible to double sync, // but for now log a programmer error to prevent accidental introduction. klog.V(3).InfoS(\u0026quot;Programmer error, HandlePodSyncs does not expect to receive mirror pods\u0026quot;, \u0026quot;podUID\u0026quot;, pod.UID, \u0026quot;mirrorPodUID\u0026quot;, mirrorPod.UID) continue } kl.podWorkers.UpdatePod(UpdatePodOptions{ Pod: pod, MirrorPod: mirrorPod, UpdateType: kubetypes.SyncPodSync, StartTime: start, }) } } func (kl *Kubelet) HandlePodCleanups(ctx context.Context) error { // The kubelet lacks checkpointing, so we need to introspect the set of pods // in the cgroup tree prior to inspecting the set of pods in our pod manager. // this ensures our view of the cgroup tree does not mistakenly observe pods // that are added after the fact... var ( cgroupPods map[types.UID]cm.CgroupName err error ) if kl.cgroupsPerQOS { pcm := kl.containerManager.NewPodContainerManager() cgroupPods, err = pcm.GetAllPodsFromCgroups() if err != nil { return fmt.Errorf(\u0026quot;failed to get list of pods that still exist on cgroup mounts: %v\u0026quot;, err) } } allPods, mirrorPods, orphanedMirrorPodFullnames := kl.podManager.GetPodsAndMirrorPods() // Pod phase progresses monotonically. Once a pod has reached a final state, // it should never leave regardless of the restart policy. The statuses // of such pods should not be changed, and there is no need to sync them. // TODO: the logic here does not handle two cases: // 1. If the containers were removed immediately after they died, kubelet // may fail to generate correct statuses, let alone filtering correctly. // 2. If kubelet restarted before writing the terminated status for a pod // to the apiserver, it could still restart the terminated pod (even // though the pod was not considered terminated by the apiserver). // These two conditions could be alleviated by checkpointing kubelet. // Stop the workers for terminated pods not in the config source klog.V(3).InfoS(\u0026quot;Clean up pod workers for terminated pods\u0026quot;) workingPods := kl.podWorkers.SyncKnownPods(allPods) // Reconcile: At this point the pod workers have been pruned to the set of // desired pods. Pods that must be restarted due to UID reuse, or leftover // pods from previous runs, are not known to the pod worker. allPodsByUID := make(map[types.UID]*v1.Pod) for _, pod := range allPods { allPodsByUID[pod.UID] = pod } // Identify the set of pods that have workers, which should be all pods // from config that are not terminated, as well as any terminating pods // that have already been removed from config. Pods that are terminating // will be added to possiblyRunningPods, to prevent overly aggressive // cleanup of pod cgroups. stringIfTrue := func(t bool) string { if t { return \u0026quot;true\u0026quot; } return \u0026quot;\u0026quot; } runningPods := make(map[types.UID]sets.Empty) possiblyRunningPods := make(map[types.UID]sets.Empty) for uid, sync := range workingPods { switch sync.State { case SyncPod: runningPods[uid] = struct{}{} possiblyRunningPods[uid] = struct{}{} case TerminatingPod: possiblyRunningPods[uid] = struct{}{} default: } } // Retrieve the list of running containers from the runtime to perform cleanup. // We need the latest state to avoid delaying restarts of static pods that reuse // a UID. if err := kl.runtimeCache.ForceUpdateIfOlder(ctx, kl.clock.Now()); err != nil { klog.ErrorS(err, \u0026quot;Error listing containers\u0026quot;) return err } runningRuntimePods, err := kl.runtimeCache.GetPods(ctx) if err != nil { klog.ErrorS(err, \u0026quot;Error listing containers\u0026quot;) return err } // Stop probing pods that are not running klog.V(3).InfoS(\u0026quot;Clean up probes for terminated pods\u0026quot;) kl.probeManager.CleanupPods(possiblyRunningPods) // Remove orphaned pod statuses not in the total list of known config pods klog.V(3).InfoS(\u0026quot;Clean up orphaned pod statuses\u0026quot;) kl.removeOrphanedPodStatuses(allPods, mirrorPods) // Remove orphaned pod user namespace allocations (if any). klog.V(3).InfoS(\u0026quot;Clean up orphaned pod user namespace allocations\u0026quot;) if err = kl.usernsManager.CleanupOrphanedPodUsernsAllocations(allPods, runningRuntimePods); err != nil { klog.ErrorS(err, \u0026quot;Failed cleaning up orphaned pod user namespaces allocations\u0026quot;) } // Remove orphaned volumes from pods that are known not to have any // containers. Note that we pass all pods (including terminated pods) to // the function, so that we don't remove volumes associated with terminated // but not yet deleted pods. // TODO: this method could more aggressively cleanup terminated pods // in the future (volumes, mount dirs, logs, and containers could all be // better separated) klog.V(3).InfoS(\u0026quot;Clean up orphaned pod directories\u0026quot;) err = kl.cleanupOrphanedPodDirs(allPods, runningRuntimePods) if err != nil { // We want all cleanup tasks to be run even if one of them failed. So // we just log an error here and continue other cleanup tasks. // This also applies to the other clean up tasks. klog.ErrorS(err, \u0026quot;Failed cleaning up orphaned pod directories\u0026quot;) } // Remove any orphaned mirror pods (mirror pods are tracked by name via the // pod worker) klog.V(3).InfoS(\u0026quot;Clean up orphaned mirror pods\u0026quot;) for _, podFullname := range orphanedMirrorPodFullnames { if !kl.podWorkers.IsPodForMirrorPodTerminatingByFullName(podFullname) { _, err := kl.mirrorPodClient.DeleteMirrorPod(podFullname, nil) if err != nil { klog.ErrorS(err, \u0026quot;Encountered error when deleting mirror pod\u0026quot;, \u0026quot;podName\u0026quot;, podFullname) } else { klog.V(3).InfoS(\u0026quot;Deleted mirror pod\u0026quot;, \u0026quot;podName\u0026quot;, podFullname) } } } // After pruning pod workers for terminated pods get the list of active pods for // metrics and to determine restarts. activePods := kl.filterOutInactivePods(allPods) allRegularPods, allStaticPods := splitPodsByStatic(allPods) activeRegularPods, activeStaticPods := splitPodsByStatic(activePods) metrics.DesiredPodCount.WithLabelValues(\u0026quot;\u0026quot;).Set(float64(len(allRegularPods))) metrics.DesiredPodCount.WithLabelValues(\u0026quot;true\u0026quot;).Set(float64(len(allStaticPods))) metrics.ActivePodCount.WithLabelValues(\u0026quot;\u0026quot;).Set(float64(len(activeRegularPods))) metrics.ActivePodCount.WithLabelValues(\u0026quot;true\u0026quot;).Set(float64(len(activeStaticPods))) metrics.MirrorPodCount.Set(float64(len(mirrorPods))) // At this point, the pod worker is aware of which pods are not desired (SyncKnownPods). // We now look through the set of active pods for those that the pod worker is not aware of // and deliver an update. The most common reason a pod is not known is because the pod was // deleted and recreated with the same UID while the pod worker was driving its lifecycle (very // very rare for API pods, common for static pods with fixed UIDs). Containers that may still // be running from a previous execution must be reconciled by the pod worker's sync method. // We must use active pods because that is the set of admitted pods (podManager includes pods // that will never be run, and statusManager tracks already rejected pods). var restartCount, restartCountStatic int for _, desiredPod := range activePods { if _, knownPod := workingPods[desiredPod.UID]; knownPod { continue } klog.V(3).InfoS(\u0026quot;Pod will be restarted because it is in the desired set and not known to the pod workers (likely due to UID reuse)\u0026quot;, \u0026quot;podUID\u0026quot;, desiredPod.UID) isStatic := kubetypes.IsStaticPod(desiredPod) pod, mirrorPod, wasMirror := kl.podManager.GetPodAndMirrorPod(desiredPod) if pod == nil || wasMirror { klog.V(2).InfoS(\u0026quot;Programmer error, restartable pod was a mirror pod but activePods should never contain a mirror pod\u0026quot;, \u0026quot;podUID\u0026quot;, desiredPod.UID) continue } kl.podWorkers.UpdatePod(UpdatePodOptions{ UpdateType: kubetypes.SyncPodCreate, Pod: pod, MirrorPod: mirrorPod, }) // the desired pod is now known as well workingPods[desiredPod.UID] = PodWorkerSync{State: SyncPod, HasConfig: true, Static: isStatic} if isStatic { // restartable static pods are the normal case restartCountStatic++ } else { // almost certainly means shenanigans, as API pods should never have the same UID after being deleted and recreated // unless there is a major API violation restartCount++ } } metrics.RestartedPodTotal.WithLabelValues(\u0026quot;true\u0026quot;).Add(float64(restartCountStatic)) metrics.RestartedPodTotal.WithLabelValues(\u0026quot;\u0026quot;).Add(float64(restartCount)) // Complete termination of deleted pods that are not runtime pods (don't have // running containers), are terminal, and are not known to pod workers. // An example is pods rejected during kubelet admission that have never // started before (i.e. does not have an orphaned pod). // Adding the pods with SyncPodKill to pod workers allows to proceed with // force-deletion of such pods, yet preventing re-entry of the routine in the // next invocation of HandlePodCleanups. for _, pod := range kl.filterTerminalPodsToDelete(allPods, runningRuntimePods, workingPods) { klog.V(3).InfoS(\u0026quot;Handling termination and deletion of the pod to pod workers\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(pod), \u0026quot;podUID\u0026quot;, pod.UID) kl.podWorkers.UpdatePod(UpdatePodOptions{ UpdateType: kubetypes.SyncPodKill, Pod: pod, }) } // Finally, terminate any pods that are observed in the runtime but not present in the list of // known running pods from config. If we do terminate running runtime pods that will happen // asynchronously in the background and those will be processed in the next invocation of // HandlePodCleanups. var orphanCount int for _, runningPod := range runningRuntimePods { // If there are orphaned pod resources in CRI that are unknown to the pod worker, terminate them // now. Since housekeeping is exclusive to other pod worker updates, we know that no pods have // been added to the pod worker in the meantime. Note that pods that are not visible in the runtime // but which were previously known are terminated by SyncKnownPods(). _, knownPod := workingPods[runningPod.ID] if !knownPod { one := int64(1) killPodOptions := \u0026amp;KillPodOptions{ PodTerminationGracePeriodSecondsOverride: \u0026amp;one, } klog.V(2).InfoS(\u0026quot;Clean up containers for orphaned pod we had not seen before\u0026quot;, \u0026quot;podUID\u0026quot;, runningPod.ID, \u0026quot;killPodOptions\u0026quot;, killPodOptions) kl.podWorkers.UpdatePod(UpdatePodOptions{ UpdateType: kubetypes.SyncPodKill, RunningPod: runningPod, KillPodOptions: killPodOptions, }) // the running pod is now known as well workingPods[runningPod.ID] = PodWorkerSync{State: TerminatingPod, Orphan: true} orphanCount++ } } metrics.OrphanedRuntimePodTotal.Add(float64(orphanCount)) // Now that we have recorded any terminating pods, and added new pods that should be running, // record a summary here. Not all possible combinations of PodWorkerSync values are valid. counts := make(map[PodWorkerSync]int) for _, sync := range workingPods { counts[sync]++ } for validSync, configState := range map[PodWorkerSync]string{ {HasConfig: true, Static: true}: \u0026quot;desired\u0026quot;, {HasConfig: true, Static: false}: \u0026quot;desired\u0026quot;, {Orphan: true, HasConfig: true, Static: true}: \u0026quot;orphan\u0026quot;, {Orphan: true, HasConfig: true, Static: false}: \u0026quot;orphan\u0026quot;, {Orphan: true, HasConfig: false}: \u0026quot;runtime_only\u0026quot;, } { for _, state := range []PodWorkerState{SyncPod, TerminatingPod, TerminatedPod} { validSync.State = state count := counts[validSync] delete(counts, validSync) staticString := stringIfTrue(validSync.Static) if !validSync.HasConfig { staticString = \u0026quot;unknown\u0026quot; } metrics.WorkingPodCount.WithLabelValues(state.String(), configState, staticString).Set(float64(count)) } } if len(counts) \u0026gt; 0 { // in case a combination is lost klog.V(3).InfoS(\u0026quot;Programmer error, did not report a kubelet_working_pods metric for a value returned by SyncKnownPods\u0026quot;, \u0026quot;counts\u0026quot;, counts) } // Remove any cgroups in the hierarchy for pods that are definitely no longer // running (not in the container runtime). if kl.cgroupsPerQOS { pcm := kl.containerManager.NewPodContainerManager() klog.V(3).InfoS(\u0026quot;Clean up orphaned pod cgroups\u0026quot;) kl.cleanupOrphanedPodCgroups(pcm, cgroupPods, possiblyRunningPods) } // Cleanup any backoff entries. kl.backOff.GC() return nil } 可以看到这些函数基本都是把pod交给 podWorkers 去处理\npodconfig 上文中的 updates channel 是从 podconfig 中获取的 那就来看看 podconfig 是怎么工作的\ntype SourcesReady interface { // AddSource adds the specified source to the set of sources managed. AddSource(source string) // AllReady returns true if the currently configured sources have all been seen. AllReady() bool } // sourcesImpl implements SourcesReady. It is thread-safe. type sourcesImpl struct { // lock protects access to sources seen. lock sync.RWMutex // set of sources seen. sourcesSeen sets.String // sourcesReady is a function that evaluates if the sources are ready. sourcesReadyFn SourcesReadyFn } 这里定义了接口 接口提中可以存储多个 source 那么有什么 source 呢\nkube-apiserver func NewSourceApiserver(c clientset.Interface, nodeName types.NodeName, nodeHasSynced func() bool, updates chan\u0026lt;- interface{}) { lw := cache.NewListWatchFromClient(c.CoreV1().RESTClient(), \u0026quot;pods\u0026quot;, metav1.NamespaceAll, fields.OneTermEqualSelector(\u0026quot;spec.nodeName\u0026quot;, string(nodeName))) // The Reflector responsible for watching pods at the apiserver should be run only after // the node sync with the apiserver has completed. klog.InfoS(\u0026quot;Waiting for node sync before watching apiserver pods\u0026quot;) go func() { for { if nodeHasSynced() { klog.V(4).InfoS(\u0026quot;node sync completed\u0026quot;) break } time.Sleep(WaitForAPIServerSyncPeriod) klog.V(4).InfoS(\u0026quot;node sync has not completed yet\u0026quot;) } klog.InfoS(\u0026quot;Watching apiserver\u0026quot;) newSourceApiserverFromLW(lw, updates) }() } // newSourceApiserverFromLW holds creates a config source that watches and pulls from the apiserver. func newSourceApiserverFromLW(lw cache.ListerWatcher, updates chan\u0026lt;- interface{}) { send := func(objs []interface{}) { var pods []*v1.Pod for _, o := range objs { pods = append(pods, o.(*v1.Pod)) } updates \u0026lt;- kubetypes.PodUpdate{Pods: pods, Op: kubetypes.SET, Source: kubetypes.ApiserverSource} } r := cache.NewReflector(lw, \u0026amp;v1.Pod{}, cache.NewUndeltaStore(send, cache.MetaNamespaceKeyFunc), 0) go r.Run(wait.NeverStop) } 可以看到代码简单 就是 watrch apiserver 的 pod 然后把 pods 传给 updates channel\nfile func (s *sourceFile) doWatch() error { _, err := os.Stat(s.path) if err != nil { if !os.IsNotExist(err) { return err } // Emit an update with an empty PodList to allow FileSource to be marked as seen s.updates \u0026lt;- kubetypes.PodUpdate{Pods: []*v1.Pod{}, Op: kubetypes.SET, Source: kubetypes.FileSource} return \u0026amp;retryableError{\u0026quot;path does not exist, ignoring\u0026quot;} } w, err := fsnotify.NewWatcher() if err != nil { return fmt.Errorf(\u0026quot;unable to create inotify: %v\u0026quot;, err) } defer w.Close() err = w.Add(s.path) if err != nil { return fmt.Errorf(\u0026quot;unable to create inotify for path %q: %v\u0026quot;, s.path, err) } for { select { case event := \u0026lt;-w.Events: if err = s.produceWatchEvent(\u0026amp;event); err != nil { return fmt.Errorf(\u0026quot;error while processing inotify event (%+v): %v\u0026quot;, event, err) } case err = \u0026lt;-w.Errors: return fmt.Errorf(\u0026quot;error while watching %q: %v\u0026quot;, s.path, err) } } } func (s *sourceFile) produceWatchEvent(e *fsnotify.Event) error { // Ignore file start with dots if strings.HasPrefix(filepath.Base(e.Name), \u0026quot;.\u0026quot;) { klog.V(4).InfoS(\u0026quot;Ignored pod manifest, because it starts with dots\u0026quot;, \u0026quot;eventName\u0026quot;, e.Name) return nil } var eventType podEventType switch { case (e.Op \u0026amp; fsnotify.Create) \u0026gt; 0: eventType = podAdd case (e.Op \u0026amp; fsnotify.Write) \u0026gt; 0: eventType = podModify case (e.Op \u0026amp; fsnotify.Chmod) \u0026gt; 0: eventType = podModify case (e.Op \u0026amp; fsnotify.Remove) \u0026gt; 0: eventType = podDelete case (e.Op \u0026amp; fsnotify.Rename) \u0026gt; 0: eventType = podDelete default: // Ignore rest events return nil } s.watchEvents \u0026lt;- \u0026amp;watchEvent{e.Name, eventType} return nil } func (s *sourceFile) run() { listTicker := time.NewTicker(s.period) go func() { // Read path immediately to speed up startup. if err := s.listConfig(); err != nil { klog.ErrorS(err, \u0026quot;Unable to read config path\u0026quot;, \u0026quot;path\u0026quot;, s.path) } for { select { case \u0026lt;-listTicker.C: if err := s.listConfig(); err != nil { klog.ErrorS(err, \u0026quot;Unable to read config path\u0026quot;, \u0026quot;path\u0026quot;, s.path) } case e := \u0026lt;-s.watchEvents: if err := s.consumeWatchEvent(e); err != nil { klog.ErrorS(err, \u0026quot;Unable to process watch event\u0026quot;) } } } }() s.startWatch() } 上面是 file source 的代码 也是 watch 文件变化 然后把变化的 pod 传给 updates channel 这个主要的作用就是 kubeadm 等部署 k8s 集群 使用文件拉起 kube-apiserver etcd kube-controller-manager kube-scheduler 等组件\nhttp func (s *sourceURL) run() { if err := s.extractFromURL(); err != nil { // Don't log this multiple times per minute. The first few entries should be // enough to get the point across. if s.failureLogs \u0026lt; 3 { klog.InfoS(\u0026quot;Failed to read pods from URL\u0026quot;, \u0026quot;err\u0026quot;, err) } else if s.failureLogs == 3 { klog.InfoS(\u0026quot;Failed to read pods from URL. Dropping verbosity of this message to V(4)\u0026quot;, \u0026quot;err\u0026quot;, err) } else { klog.V(4).InfoS(\u0026quot;Failed to read pods from URL\u0026quot;, \u0026quot;err\u0026quot;, err) } s.failureLogs++ } else { if s.failureLogs \u0026gt; 0 { klog.InfoS(\u0026quot;Successfully read pods from URL\u0026quot;) s.failureLogs = 0 } } } func (s *sourceURL) applyDefaults(pod *api.Pod) error { return applyDefaults(pod, s.url, false, s.nodeName) } func (s *sourceURL) extractFromURL() error { req, err := http.NewRequest(\u0026quot;GET\u0026quot;, s.url, nil) if err != nil { return err } req.Header = s.header resp, err := s.client.Do(req) if err != nil { return err } defer resp.Body.Close() data, err := utilio.ReadAtMost(resp.Body, maxConfigLength) if err != nil { return err } if resp.StatusCode != http.StatusOK { return fmt.Errorf(\u0026quot;%v: %v\u0026quot;, s.url, resp.Status) } if len(data) == 0 { // Emit an update with an empty PodList to allow HTTPSource to be marked as seen s.updates \u0026lt;- kubetypes.PodUpdate{Pods: []*v1.Pod{}, Op: kubetypes.SET, Source: kubetypes.HTTPSource} return fmt.Errorf(\u0026quot;zero-length data received from %v\u0026quot;, s.url) } // Short circuit if the data has not changed since the last time it was read. if bytes.Equal(data, s.data) { return nil } s.data = data // First try as it is a single pod. parsed, pod, singlePodErr := tryDecodeSinglePod(data, s.applyDefaults) if parsed { if singlePodErr != nil { // It parsed but could not be used. return singlePodErr } s.updates \u0026lt;- kubetypes.PodUpdate{Pods: []*v1.Pod{pod}, Op: kubetypes.SET, Source: kubetypes.HTTPSource} return nil } // That didn't work, so try a list of pods. parsed, podList, multiPodErr := tryDecodePodList(data, s.applyDefaults) if parsed { if multiPodErr != nil { // It parsed but could not be used. return multiPodErr } pods := make([]*v1.Pod, 0, len(podList.Items)) for i := range podList.Items { pods = append(pods, \u0026amp;podList.Items[i]) } s.updates \u0026lt;- kubetypes.PodUpdate{Pods: pods, Op: kubetypes.SET, Source: kubetypes.HTTPSource} return nil } return fmt.Errorf(\u0026quot;%v: received '%v', but couldn't parse as \u0026quot;+ \u0026quot;single (%v) or multiple pods (%v)\u0026quot;, s.url, string(data), singlePodErr, multiPodErr) } http source 是 kubelet 本身开启 http 服务 通过调用 kubelet 的 http 接口来管理 pod 这个主要的作用是 给那些不想部署集群 只想使用 kubelet 的需求提供的\nPLEG PLEG（Pod Lifecycle Event Generator）是 Kubernetes 中的一个关键组件，它负责监视和处理 Pod 的生命周期事件。PLEG 运行在每个节点上，并与 kubelet 组件紧密配合工作。\nPLEG 的主要功能包括：\n监控容器状态：PLEG 监控每个节点上正在运行的容器的状态，并根据其状态变化生成相应的事件。 生成事件：当容器的状态发生变化时，PLEG 会生成相应的事件，例如容器的创建、启动、停止、退出等事件。 同步状态：PLEG 通过与 kubelet 进行交互，将容器的状态信息同步给 kubelet，使 kubelet 能够了解容器的当前状态。 故障处理：PLEG 检测容器的状态变化，并在发现容器失败或异常时生成相应的事件，以便 kubelet 采取适当的故障处理措施。 PLEG 的设计目标是提供高效可靠的容器生命周期事件处理。它使用操作系统的文件系统事件和容器运行时的状态查询机制来监视容器的状态变化，从而及时地生成相应的事件。这些事件对于监控、日志记录、故障排除和自动恢复等方面非常重要。\ntype PodLifecycleEventGenerator interface { Start() Watch() chan *PodLifecycleEvent Healthy() (bool, error) UpdateCache(*kubecontainer.Pod, types.UID) (error, bool) } kubelet 中实现了两种 PLEG，分别是：\n通用 PLEG：用于处理普通容器的生命周期事件。使用轮询机制监控容器的状态变化，因此可能会有一定的延迟。而且耗费资源较多，不适合大规模部署。 Event PLEG：用于处理事件容器的生命周期事件。使用事件机制监控容器的状态变化，因此响应速度较快。但是，他需要 container runtime 支持事件机制。 GenericPLEG func (g *GenericPLEG) Watch() chan *PodLifecycleEvent { return g.eventChannel } func (g *GenericPLEG) Start() { g.runningMu.Lock() defer g.runningMu.Unlock() if !g.isRunning { g.isRunning = true g.stopCh = make(chan struct{}) go wait.Until(g.Relist, g.relistDuration.RelistPeriod, g.stopCh) } } func (g *GenericPLEG) Relist() { g.relistLock.Lock() defer g.relistLock.Unlock() ctx := context.Background() klog.V(5).InfoS(\u0026quot;GenericPLEG: Relisting\u0026quot;) if lastRelistTime := g.getRelistTime(); !lastRelistTime.IsZero() { metrics.PLEGRelistInterval.Observe(metrics.SinceInSeconds(lastRelistTime)) } timestamp := g.clock.Now() defer func() { metrics.PLEGRelistDuration.Observe(metrics.SinceInSeconds(timestamp)) }() // Get all the pods. podList, err := g.runtime.GetPods(ctx, true) if err != nil { klog.ErrorS(err, \u0026quot;GenericPLEG: Unable to retrieve pods\u0026quot;) return } g.updateRelistTime(timestamp) pods := kubecontainer.Pods(podList) // update running pod and container count updateRunningPodAndContainerMetrics(pods) g.podRecords.setCurrent(pods) // Compare the old and the current pods, and generate events. eventsByPodID := map[types.UID][]*PodLifecycleEvent{} for pid := range g.podRecords { oldPod := g.podRecords.getOld(pid) pod := g.podRecords.getCurrent(pid) // Get all containers in the old and the new pod. allContainers := getContainersFromPods(oldPod, pod) for _, container := range allContainers { events := computeEvents(oldPod, pod, \u0026amp;container.ID) for _, e := range events { updateEvents(eventsByPodID, e) } } } var needsReinspection map[types.UID]*kubecontainer.Pod if g.cacheEnabled() { needsReinspection = make(map[types.UID]*kubecontainer.Pod) } // If there are events associated with a pod, we should update the // podCache. for pid, events := range eventsByPodID { pod := g.podRecords.getCurrent(pid) if g.cacheEnabled() { // updateCache() will inspect the pod and update the cache. If an // error occurs during the inspection, we want PLEG to retry again // in the next relist. To achieve this, we do not update the // associated podRecord of the pod, so that the change will be // detect again in the next relist. // TODO: If many pods changed during the same relist period, // inspecting the pod and getting the PodStatus to update the cache // serially may take a while. We should be aware of this and // parallelize if needed. if err, updated := g.updateCache(ctx, pod, pid); err != nil { // Rely on updateCache calling GetPodStatus to log the actual error. klog.V(4).ErrorS(err, \u0026quot;PLEG: Ignoring events for pod\u0026quot;, \u0026quot;pod\u0026quot;, klog.KRef(pod.Namespace, pod.Name)) // make sure we try to reinspect the pod during the next relisting needsReinspection[pid] = pod continue } else { // this pod was in the list to reinspect and we did so because it had events, so remove it // from the list (we don't want the reinspection code below to inspect it a second time in // this relist execution) delete(g.podsToReinspect, pid) if utilfeature.DefaultFeatureGate.Enabled(features.EventedPLEG) { if !updated { continue } } } } // Update the internal storage and send out the events. g.podRecords.update(pid) // Map from containerId to exit code; used as a temporary cache for lookup containerExitCode := make(map[string]int) for i := range events { // Filter out events that are not reliable and no other components use yet. if events[i].Type == ContainerChanged { continue } select { case g.eventChannel \u0026lt;- events[i]: default: metrics.PLEGDiscardEvents.Inc() klog.ErrorS(nil, \u0026quot;Event channel is full, discard this relist() cycle event\u0026quot;) } // Log exit code of containers when they finished in a particular event if events[i].Type == ContainerDied { // Fill up containerExitCode map for ContainerDied event when first time appeared if len(containerExitCode) == 0 \u0026amp;\u0026amp; pod != nil \u0026amp;\u0026amp; g.cache != nil { // Get updated podStatus status, err := g.cache.Get(pod.ID) if err == nil { for _, containerStatus := range status.ContainerStatuses { containerExitCode[containerStatus.ID.ID] = containerStatus.ExitCode } } } if containerID, ok := events[i].Data.(string); ok { if exitCode, ok := containerExitCode[containerID]; ok \u0026amp;\u0026amp; pod != nil { klog.V(2).InfoS(\u0026quot;Generic (PLEG): container finished\u0026quot;, \u0026quot;podID\u0026quot;, pod.ID, \u0026quot;containerID\u0026quot;, containerID, \u0026quot;exitCode\u0026quot;, exitCode) } } } } } if g.cacheEnabled() { // reinspect any pods that failed inspection during the previous relist if len(g.podsToReinspect) \u0026gt; 0 { klog.V(5).InfoS(\u0026quot;GenericPLEG: Reinspecting pods that previously failed inspection\u0026quot;) for pid, pod := range g.podsToReinspect { if err, _ := g.updateCache(ctx, pod, pid); err != nil { // Rely on updateCache calling GetPodStatus to log the actual error. klog.V(5).ErrorS(err, \u0026quot;PLEG: pod failed reinspection\u0026quot;, \u0026quot;pod\u0026quot;, klog.KRef(pod.Namespace, pod.Name)) needsReinspection[pid] = pod } } } // Update the cache timestamp. This needs to happen *after* // all pods have been properly updated in the cache. g.cache.UpdateTime(timestamp) } // make sure we retain the list of pods that need reinspecting the next time relist is called g.podsToReinspect = needsReinspection } 可以看到 GenericPLEG 会定时从 runtime 获取 pods 然后和缓存中的旧的 pod 进行对比 然后生成事件发送给 eventChannel\nEventPLEG func (e *EventedPLEG) Start() { e.runningMu.Lock() defer e.runningMu.Unlock() if isEventedPLEGInUse() { return } setEventedPLEGUsage(true) e.stopCh = make(chan struct{}) e.stopCacheUpdateCh = make(chan struct{}) go wait.Until(e.watchEventsChannel, 0, e.stopCh) go wait.Until(e.updateGlobalCache, globalCacheUpdatePeriod, e.stopCacheUpdateCh) } func (e *EventedPLEG) watchEventsChannel() { containerEventsResponseCh := make(chan *runtimeapi.ContainerEventResponse, cap(e.eventChannel)) defer close(containerEventsResponseCh) // Get the container events from the runtime. go func() { numAttempts := 0 for { if numAttempts \u0026gt;= e.eventedPlegMaxStreamRetries { if isEventedPLEGInUse() { // Fall back to Generic PLEG relisting since Evented PLEG is not working. klog.V(4).InfoS(\u0026quot;Fall back to Generic PLEG relisting since Evented PLEG is not working\u0026quot;) e.Stop() e.genericPleg.Stop() // Stop the existing Generic PLEG which runs with longer relisting period when Evented PLEG is in use. e.Update(e.relistDuration) // Update the relisting period to the default value for the Generic PLEG. e.genericPleg.Start() break } } err := e.runtimeService.GetContainerEvents(containerEventsResponseCh) if err != nil { metrics.EventedPLEGConnErr.Inc() numAttempts++ e.Relist() // Force a relist to get the latest container and pods running metric. klog.V(4).InfoS(\u0026quot;Evented PLEG: Failed to get container events, retrying: \u0026quot;, \u0026quot;err\u0026quot;, err) } } }() if isEventedPLEGInUse() { e.processCRIEvents(containerEventsResponseCh) } } // 转换 runtimeapi.ContainerEventResponse 为 PodLifecycleEvent func (e *EventedPLEG) processCRIEvents(containerEventsResponseCh chan *runtimeapi.ContainerEventResponse) { for event := range containerEventsResponseCh { // Ignore the event if PodSandboxStatus is nil. // This might happen under some race condition where the podSandbox has // been deleted, and therefore container runtime couldn't find the // podSandbox for the container when generating the event. // It is safe to ignore because // a) a event would have been received for the sandbox deletion, // b) in worst case, a relist will eventually sync the pod status. // TODO(#114371): Figure out a way to handle this case instead of ignoring. if event.PodSandboxStatus == nil || event.PodSandboxStatus.Metadata == nil { klog.ErrorS(nil, \u0026quot;Evented PLEG: received ContainerEventResponse with nil PodSandboxStatus or PodSandboxStatus.Metadata\u0026quot;, \u0026quot;containerEventResponse\u0026quot;, event) continue } podID := types.UID(event.PodSandboxStatus.Metadata.Uid) shouldSendPLEGEvent := false status, err := e.runtime.GeneratePodStatus(event) if err != nil { // nolint:logcheck // Not using the result of klog.V inside the // if branch is okay, we just use it to determine whether the // additional \u0026quot;podStatus\u0026quot; key and its value should be added. if klog.V(6).Enabled() { klog.ErrorS(err, \u0026quot;Evented PLEG: error generating pod status from the received event\u0026quot;, \u0026quot;podUID\u0026quot;, podID, \u0026quot;podStatus\u0026quot;, status) } else { klog.ErrorS(err, \u0026quot;Evented PLEG: error generating pod status from the received event\u0026quot;, \u0026quot;podUID\u0026quot;, podID) } } else { if klogV := klog.V(6); klogV.Enabled() { klogV.InfoS(\u0026quot;Evented PLEG: Generated pod status from the received event\u0026quot;, \u0026quot;podUID\u0026quot;, podID, \u0026quot;podStatus\u0026quot;, status) } else { klog.V(4).InfoS(\u0026quot;Evented PLEG: Generated pod status from the received event\u0026quot;, \u0026quot;podUID\u0026quot;, podID) } // Preserve the pod IP across cache updates if the new IP is empty. // When a pod is torn down, kubelet may race with PLEG and retrieve // a pod status after network teardown, but the kubernetes API expects // the completed pod's IP to be available after the pod is dead. status.IPs = e.getPodIPs(podID, status) } e.updateRunningPodMetric(status) e.updateRunningContainerMetric(status) e.updateLatencyMetric(event) if event.ContainerEventType == runtimeapi.ContainerEventType_CONTAINER_DELETED_EVENT { for _, sandbox := range status.SandboxStatuses { if sandbox.Id == event.ContainerId { // When the CONTAINER_DELETED_EVENT is received by the kubelet, // the runtime has indicated that the container has been removed // by the runtime and hence, it must be removed from the cache // of kubelet too. e.cache.Delete(podID) } } shouldSendPLEGEvent = true } else { if e.cache.Set(podID, status, err, time.Unix(event.GetCreatedAt(), 0)) { shouldSendPLEGEvent = true } } if shouldSendPLEGEvent { e.processCRIEvent(event) } } } 从代码中可以看到 EventPLEG 可以从 runtime 获取容器事件。\n","date":"2024-05-01","permalink":"https://daemon365.dev/2024/05/01/kubelet-%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/","tags":["kubelet","kubernetes","源码分析","golang"],"title":"kubelet 原理分析"},{"content":"为什么需要 CNI 在 kubernetes 中，pod 的网络是使用 network namespace 隔离的，但是我们有时又需要互相访问网络，这就需要一个网络插件来实现 pod 之间的网络通信。CNI 就是为了解决这个问题而诞生的。CNI 是 container network interface 的缩写，它是一个规范，定义了容器运行时如何配置网络。CNI 插件是实现了 CNI 规范的二进制文件，它可以被容器运行时调用，来配置容器的网络。\nDocker 网络 基础 计算机五层网络如下：\n如果我们想把 pod 中的网络对外，首先想到的就是七层代理，比如nginx，但是我们并不知道 pod 里的网络一定是 http，甚至他可能不是tcp。所以我们像做一些网络操作，就不能在五层做了，只能在二三四层做。\nDocker 实验 当我们在物理机上启动 docker daemon 不需要启动任何容器的时候，使用 ip a 命令查看网卡，发现多了一个 docker0\n4: docker0: \u0026lt;NO-CARRIER,BROADCAST,MULTICAST,UP\u0026gt; mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:9b:65:e1:01 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever docker0 是一个 linux Bridge 设备，这个可以理解成一个虚拟的交换机，用来做二层网络的转发。当我们启动一个容器的时候，docker 会为这个容器创建一个 veth pair 设备，一个端口挂载在容器的 network namespace 中，另一个端口挂载在 docker0 上。这样容器就可以和 docker0 上的其他容器通信了。\ndocker run -d --rm -it ubuntu:22.04 sleep 3000 在物理机上查看 ip a\n8: veth6bc75d9@if7: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue master docker0 state UP group default link/ether d6:87:ca:5c:54:51 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::d487:caff:fe5c:5451/64 scope link valid_lft forever preferred_lft forever docker 容器里面 ip a\n7: eth0@if8: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever 再启动一个 docker\ndocker run --name test -d --rm -it ubuntu:22.04 sleep 3000 # ip a 9: eth0@if10: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.17.0.3/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever 这样两个容器就可以通过 docker0 通信了。\nroot@b19a3dc4b32d:/# ping 172.17.0.2 PING 172.17.0.2 (172.17.0.2) 56(84) bytes of data. 64 bytes from 172.17.0.2: icmp_seq=1 ttl=64 time=0.055 ms 通信方式 CNI 网络 当两个 pod 在同一 node 上的时候，我们可以使用像上述 docker 的 bridge 的方式通信是没问题的。但是 kubernetes 是这个多节点的集群，当 pod 在不同的 node 上的时候，直接通信肯定不行了，这时候我们需要一些办法来解决这个问题。\nUDP 封包 当 pod 在不同节点上的时候，两个 pod 不可以直接通信，那最简单的方式就是通过 udp 封包，把整个网络包使用 udp 封包起来，然后第二个节点再解包，然后发给网桥。\n整个过程就是 node1 上的 pod 把网络包封装，然后由于 process 再封装发给 node2，node2 再解包，然后发给 pod2。\nprocess 是 cni 实现的进程，很多 cni 都实现 udp 封包的方式，比如 flannel,cailco 等。\n至于我们怎么知道目标 ip （pod 的 ip） 是在哪台主机上，这个就有很多中方式了，比如把每台机器发配 ip 分配不同的网段，甚至于把这些对应关系写到 etcd 中。\nVXLAN 上述的 udp 封包方式，是可以满足基本需求但是。cni 创建的 process 进程是一个用户态的进程，每个包要在 node1 上从内核态 copy 到用户态，然后再封包，再 copy 到内核态，再发给 node2，再从内核态 copy 到用户态，再解包，再 copy 到内核态，再发给 pod2。这样的方式效率很低。所以我们使用一种更加高效的方式，就是 vxlan。\nVXLAN 是什么?\nVXLAN（Virtual Extensible LAN）是一种网络虚拟化技术，用于解决大规模云计算环境中的网络隔离、扩展性和灵活性问题。VXLAN 允许网络工程师在现有的网络架构上创建一个逻辑网络层，这可以使得数据中心的网络设计变得更加灵活和可扩展。\n为什么性能会高？\nVXLAN 是在内核态实现的，原理和 udp 封包一样，只不过是在内核态实现的，数据包不会在内核态和用户态之间 copy，所以效率会高很多。\nip 路由 就算是 vxlan，也是需要封包和解包的，这样的方式效率还是不够高，所以我们可以使用 ip 路由的方式。\nip 路由故名思意，就是使用路由表来实现 pod 之间的通信。这样的方式效率最高，但是配置比较复杂，需要配置路由表。\n而且路由表跳转是二层网络实现的，所以又要要求所有 node 在同一个二层网络中。\n查看 node1 上的 container 的是设备\nip a 2: eth0@if10: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 66:5e:d8:8d:86:ba brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.10.184.69/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::645e:d8ff:fe8d:86ba/64 scope link valid_lft forever preferred_lft forever 这个和主机上是对应的是一个 veth pair 设备，一个端口挂载在容器的 network namespace 中，一边挂载在主机上。\n# 主机 ip a 10: calia78b8700057@if2: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netns cni-0da431c8-dd8b-ca68-55e6-40b04acf78d6 inet6 fe80::ecee:eeff:feee:eeee/64 scope link valid_lft forever preferred_lft forever 当 pod 中的数据包来到主机 查看 node1 上的路由表 会命中一下这条路由 这条的意思是跳到192.168.229.102节点使用 ens33 设备\nip r 172.10.190.0/26 via 192.168.229.102 dev ens33 proto bird 当 数据包来到 node2 上的时候 我们看下 node2 的路由表\nip r 172.10.190.2 dev calie28ee63d6b0 scope link ip a 7: calie28ee63d6b0@if2: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netns cni-dd892c92-1826-f648-2b8c-d22618311ca9 inet6 fe80::ecee:eeff:feee:eeee/64 scope link valid_lft forever preferred_lft forever 这个设备是 veth pair 设备，对应的容器内的\nip a 2: eth0@if7: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether fa:a6:2f:97:58:28 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.10.190.2/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::f8a6:2fff:fe97:5828/64 scope link valid_lft forever preferred_lft forever 这样node2上的 172.10.190.2 pod 就可以收到数据包了。\n路由跳转 路由跳转是怎么实现的？\n路由跳转是通过路由表来实现的，它作用在二层上，所以当跳转的时候，直接修改数据包的目标 mac 地址（如果知道的话是使用 ARP 协议获得）。\n所以当我们访问百度的时候，获得百度的ip的时候，数据包会经过很多路由器，每个路由器都会修改数据包的目标 mac 地址，这样数据包就可以到达百度的服务器了。\nFelix 那么主机上的路由表是怎么来的呢？\n这个就是 cni 的实现了，cni 会调用 felix 这个进程，felix 会根据 cni 的配置来配置路由表。\nBGP 那么 node1 怎么知道对应的 pod ip 在哪个 node 上呢？\n这个就是 BGP 协议了，BGP 是一个路由协议，用来告诉 node1 对应的 pod ip 在哪个 node 上。\n这个协议很重，之前都是用到互联网上，比如我们刚才距离的百度的时候，经过那么多路由器，每个路由器怎么知道要跳到哪，他们之间就是通过 BGP 协议来告诉对方自己的路由表，再经过一系列的学习优化。\nip in ip 刚才也说过了，ip 路由是最高效的，是因为它作用在二层网络上，这就需要保证所有的 node 在同一个二层网络上。但是有时候我们的 node 不在同一个二层网络上，这时候我们可以使用 ip in ip。\n简单来说就是如果 node 之间在一个二层网络上，那么就直接使用 ip 路由，如果不在，那么就使用 ip in ip，把数据包封装起来，然后再发给对应的 node。\nip-in-ip 是一种隧道技术，它将一个 IP 数据包封装在另一个 IP 数据包中，这样就可以在一个 IP 网络上传输另一个 IP 网络的数据包。\n172.10.180.0/24 via 192.168.228.28 tunl0 这个只是在源数据包的三层上再封装一层，三层数据包和二层数据包。这样性能方面稍微比使用了 udp 的 vxlan 要好一点。但是最好还是避免使用ip in ip, 尽量保证 node 在同一个二层网络上。\n","date":"2024-04-20","permalink":"https://daemon365.dev/2024/04/20/kubernetes-cnicontainer-network-inferface/","tags":["cni","kubernetes","network"],"title":"kubernetes CNI(Container Network Inferface)"},{"content":"原文地址 https://haiyux.cc/2023/02/26/k8s-client-go/\nclient-go是什么？ client-go是Kubernetes官方提供的Go语言客户端库，用于与Kubernetes API服务器交互。使用client-go，您可以编写Go语言程序来创建、修改和删除Kubernetes对象，如Pod、Deployment、Service等。\n作用 client-go的主要功能包括：\n连接Kubernetes API服务器：client-go提供了一个API客户端，用于连接Kubernetes API服务器。 对象管理：client-go提供了一组API，用于创建、读取、更新和删除Kubernetes对象，如Pod、Deployment、Service等。 Watch API：client-go提供了一个Watch API，可以用于监视Kubernetes对象的变化。 命名空间支持：client-go支持多个命名空间，并提供了一组API，用于管理命名空间。 认证和授权：client-go提供了一组API，用于执行身份验证和授权，以确保只有授权的用户才能对Kubernetes对象进行操作。 client-go是使用Kubernetes API的标准方式，是Kubernetes生态系统中的重要组成部分。\napi client client-go 中包含四种client，RestClient, ClientSet，DynamicClient和DiscoveryClient。\nClientSet，DynamicClient，DiscoveryClient都是RestClient上的封装\nRestClient RestClient是最基础的客户端，它基于HTTP请求进行了封装，实现了RESTful API。使用RESTClient提供的RESTful方法，如Get()、Put()、Post()和Delete()，可以直接与API进行交互。同时，它支持JSON和Protocol Buffers，并支持所有原生资源和自定义资源定义（CRDs）。然而，为了更加优雅地处理API交互，一般需要进一步封装，通过Clientset对RESTClient进行封装，然后再对外提供接口和服务。\npackage main import ( \u0026quot;context\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;path/filepath\u0026quot; corev1 \u0026quot;k8s.io/api/core/v1\u0026quot; metav1 \u0026quot;k8s.io/apimachinery/pkg/apis/meta/v1\u0026quot; \u0026quot;k8s.io/client-go/kubernetes/scheme\u0026quot; \u0026quot;k8s.io/client-go/rest\u0026quot; \u0026quot;k8s.io/client-go/tools/clientcmd\u0026quot; \u0026quot;k8s.io/client-go/util/homedir\u0026quot; ) func main() { // 使用kubeconfig生成配置 config, err := clientcmd.BuildConfigFromFlags(\u0026quot;\u0026quot;, filepath.Join(homedir.HomeDir(), \u0026quot;.kube\u0026quot;, \u0026quot;config\u0026quot;)) if err != nil { panic(err) } config.APIPath = \u0026quot;api\u0026quot; config.GroupVersion = \u0026amp;corev1.SchemeGroupVersion config.NegotiatedSerializer = scheme.Codecs // 生成restClient restClient, err := rest.RESTClientFor(config) if err != nil { panic(err) } rest := \u0026amp;corev1.PodList{} if err = restClient.Get().Namespace(\u0026quot;default\u0026quot;).Resource(\u0026quot;pods\u0026quot;).VersionedParams(\u0026amp;metav1.ListOptions{}, scheme.ParameterCodec).Do(context.TODO()).Into(rest); err != nil { panic(err) } for _, v := range rest.Items { fmt.Printf(\u0026quot;NameSpace: %v Name: %v Status: %v \\n\u0026quot;, v.Namespace, v.Name, v.Status.Phase) } } /* 结果 NameSpace: default Name: nginx-76d6c9b8c-8ljkt Status: Running NameSpace: default Name: nginx-76d6c9b8c-jqv9h Status: Running NameSpace: default Name: nginx-76d6c9b8c-kr9d2 Status: Running NameSpace: default Name: nginx-76d6c9b8c-m4g5l Status: Running NameSpace: default Name: nginx-76d6c9b8c-n8st9 Status: Running */ ClientSet ClientSet是在RestClient的基础上封装了对资源和版本的管理方法。资源可以理解为一个客户端，而ClientSet是多个客户端的集合。在操作资源对象时，需要指定Group和Version，然后根据资源获取。然而，ClientSet不支持自定义资源定义（CRDs），但使用kubebuilder生成代码时，会生成相应的ClientSet。\npackage main import ( \u0026quot;context\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;path/filepath\u0026quot; metav1 \u0026quot;k8s.io/apimachinery/pkg/apis/meta/v1\u0026quot; \u0026quot;k8s.io/client-go/kubernetes\u0026quot; \u0026quot;k8s.io/client-go/tools/clientcmd\u0026quot; \u0026quot;k8s.io/client-go/util/homedir\u0026quot; ) func main() { ctx := context.Background() // 使用kubeconfig生成配置 ~/.kube/config config, err := clientcmd.BuildConfigFromFlags(\u0026quot;\u0026quot;, filepath.Join(homedir.HomeDir(), \u0026quot;.kube\u0026quot;, \u0026quot;config\u0026quot;)) if err != nil { panic(err) } // 生成clientSet clientSet, err := kubernetes.NewForConfig(config) if err != nil { panic(err) } nodeList, err := clientSet.CoreV1().Nodes().List(ctx, metav1.ListOptions{}) if err != nil { panic(err) } for _, node := range nodeList.Items { fmt.Printf(\u0026quot;nodeName: %v, status: %v \\n\u0026quot;, node.GetName(), node.GetCreationTimestamp()) } // pod 是有namespace资源所以指定namespace 而node没有 pods, err := clientSet.CoreV1().Pods(\u0026quot;default\u0026quot;).List(ctx, metav1.ListOptions{}) if err != nil { panic(err) } for _, v := range pods.Items { fmt.Printf(\u0026quot;namespace: %v podName: %v status: %v \\n\u0026quot;, v.Namespace, v.Name, v.Status.Phase) } } /* 结果: nodeName: minikube, status: 2023-01-27 18:45:35 +0800 CST nodeName: minikube-m02, status: 2023-02-26 21:19:30 +0800 CST nodeName: minikube-m03, status: 2023-02-26 21:19:38 +0800 CST namespace: default podName: nginx-76d6c9b8c-8ljkt status: Running namespace: default podName: nginx-76d6c9b8c-jqv9h status: Running namespace: default podName: nginx-76d6c9b8c-kr9d2 status: Running namespace: default podName: nginx-76d6c9b8c-m4g5l status: Running namespace: default podName: nginx-76d6c9b8c-n8st9 status: Running */ DynamicClient DynamicClient是一种动态客户端，它可以对任何资源进行RESTful操作，包括自定义资源定义（CRD）。与ClientSet不同，DynamicClient返回的对象是一个map[string]interface{}。如果一个控制器需要控制所有的API，可以使用DynamicClient。目前，DynamicClient在垃圾回收器和命名空间控制器中被广泛使用。\nDynamicClient的处理过程将Resource（例如PodList）转换为unstructured类型。Kubernetes的所有资源都可以转换为这个结构类型。处理完毕后，再将其转换回PodList。整个转换过程类似于接口转换，即通过interface{}的断言实现。\nDynamicClient是一种动态的客户端，它能处理Kubernetes所有的资源，但仅支持JSON\npackage main import ( \u0026quot;context\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;path/filepath\u0026quot; apiv1 \u0026quot;k8s.io/api/core/v1\u0026quot; metav1 \u0026quot;k8s.io/apimachinery/pkg/apis/meta/v1\u0026quot; \u0026quot;k8s.io/apimachinery/pkg/runtime\u0026quot; \u0026quot;k8s.io/apimachinery/pkg/runtime/schema\u0026quot; \u0026quot;k8s.io/client-go/dynamic\u0026quot; \u0026quot;k8s.io/client-go/tools/clientcmd\u0026quot; \u0026quot;k8s.io/client-go/util/homedir\u0026quot; ) func main() { ctx := context.Background() // 使用kubeconfig生成配置 ~/.kube/config config, err := clientcmd.BuildConfigFromFlags(\u0026quot;\u0026quot;, filepath.Join(homedir.HomeDir(), \u0026quot;.kube\u0026quot;, \u0026quot;config\u0026quot;)) if err != nil { panic(err) } // dynamicClient dynamicClient, err := dynamic.NewForConfig(config) if err != nil { panic(err) } // 定义组版本资源 gvr := schema.GroupVersionResource{Version: \u0026quot;v1\u0026quot;, Resource: \u0026quot;pods\u0026quot;} unStructObj, err := dynamicClient.Resource(gvr).Namespace(\u0026quot;default\u0026quot;).List(ctx, metav1.ListOptions{}) if err != nil { panic(err) } podList := \u0026amp;apiv1.PodList{} if err = runtime.DefaultUnstructuredConverter.FromUnstructured(unStructObj.UnstructuredContent(), podList); err != nil { panic(err) } for _, v := range podList.Items { fmt.Printf(\u0026quot;namespaces:%v podName:%v status:%v \\n\u0026quot;, v.Namespace, v.Name, v.Status.Phase) } } /* namespaces:default podName:nginx-76d6c9b8c-8ljkt status:Running namespaces:default podName:nginx-76d6c9b8c-jqv9h status:Running namespaces:default podName:nginx-76d6c9b8c-kr9d2 status:Running namespaces:default podName:nginx-76d6c9b8c-m4g5l status:Running namespaces:default podName:nginx-76d6c9b8c-n8st9 status:Running */ 其中，GVR（group,version,resource） 用于标识 Kubernetes API 中的资源类型，其中 Group 表示 API 群组，Version 表示 API 版本，Resource 表示资源类型。例如，Deployment 的 GVR 为 \u0026ldquo;apps/v1/deployments\u0026rdquo;，其中 \u0026ldquo;apps\u0026rdquo; 是 API 群组，\u0026ldquo;v1\u0026rdquo; 是 API 版本，\u0026ldquo;deployments\u0026rdquo; 是资源类型。\nDiscoveryClient DiscoveryClient 是一个发现客户端，它的主要作用是用于发现 API Server 支持的资源组、资源版本和资源信息。在 Kubernetes 中，API Server 支持很多资源组、资源版本和资源信息，我们可以通过使用 DiscoveryClient 来查看这些信息。此外，kubectl 的 API 版本和 API 资源也是通过 DiscoveryClient 来实现的。我们还可以将这些信息缓存到本地，以减轻 API 访问的压力。缓存文件默认存储在 ./kube/cache 和 ./kube/http-cache 目录下。\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;path/filepath\u0026quot; \u0026quot;k8s.io/apimachinery/pkg/runtime/schema\u0026quot; \u0026quot;k8s.io/client-go/discovery\u0026quot; \u0026quot;k8s.io/client-go/tools/clientcmd\u0026quot; \u0026quot;k8s.io/client-go/util/homedir\u0026quot; ) func main() { // 使用kubeconfig生成配置 ~/.kube/config config, err := clientcmd.BuildConfigFromFlags(\u0026quot;\u0026quot;, filepath.Join(homedir.HomeDir(), \u0026quot;.kube\u0026quot;, \u0026quot;config\u0026quot;)) if err != nil { panic(err) } // 生成discoverClient discoverClient, err := discovery.NewDiscoveryClientForConfig(config) if err != nil { panic(err) } _, apiResourceList, err := discoverClient.ServerGroupsAndResources() for _, v := range apiResourceList { gv, err := schema.ParseGroupVersion(v.GroupVersion) if err != nil { panic(err) } for _, resource := range v.APIResources { fmt.Printf(\u0026quot;name:%v group:%v version:%v\\n\u0026quot;, resource.Name, gv.Group, gv.Version) } } } /* name:bindings group: version:v1 name:componentstatuses group: version:v1 name:configmaps group: version:v1 name:endpoints group: version:v1 name:events group: version:v1 name:limitranges group: version:v1 name:namespaces group: version:v1 name:namespaces/finalize group: version:v1 name:namespaces/status group: version:v1 name:nodes group: version:v1 name:nodes/proxy group: version:v1 name:nodes/status group: version:v1 name:persistentvolumeclaims group: version:v1 name:persistentvolumeclaims/status group: version:v1 name:persistentvolumes group: version:v1 name:persistentvolumes/status group: version:v1 name:pods group: version:v1 name:pods/attach group: version:v1 name:pods/binding group: version:v1 name:pods/ephemeralcontainers group: version:v1 name:pods/eviction group: version:v1 name:pods/exec group: version:v1 name:pods/log group: version:v1 name:pods/portforward group: version:v1 name:pods/proxy group: version:v1 name:pods/status group: version:v1 name:podtemplates group: version:v1 name:replicationcontrollers group: version:v1 name:replicationcontrollers/scale group: version:v1 name:replicationcontrollers/status group: version:v1 name:resourcequotas group: version:v1 name:resourcequotas/status group: version:v1 name:secrets group: version:v1 name:serviceaccounts group: version:v1 name:serviceaccounts/token group: version:v1 name:services group: version:v1 name:services/proxy group: version:v1 name:services/status group: version:v1 name:apiservices group:apiregistration.k8s.io version:v1 name:apiservices/status group:apiregistration.k8s.io version:v1 name:controllerrevisions group:apps version:v1 name:daemonsets group:apps version:v1 name:daemonsets/status group:apps version:v1 name:deployments group:apps version:v1 name:deployments/scale group:apps version:v1 name:deployments/status group:apps version:v1 name:replicasets group:apps version:v1 name:replicasets/scale group:apps version:v1 name:replicasets/status group:apps version:v1 name:statefulsets group:apps version:v1 name:statefulsets/scale group:apps version:v1 name:statefulsets/status group:apps version:v1 name:events group:events.k8s.io version:v1 name:tokenreviews group:authentication.k8s.io version:v1 name:localsubjectaccessreviews group:authorization.k8s.io version:v1 name:selfsubjectaccessreviews group:authorization.k8s.io version:v1 name:selfsubjectrulesreviews group:authorization.k8s.io version:v1 name:subjectaccessreviews group:authorization.k8s.io version:v1 name:horizontalpodautoscalers group:autoscaling version:v2 name:horizontalpodautoscalers/status group:autoscaling version:v2 name:horizontalpodautoscalers group:autoscaling version:v1 name:horizontalpodautoscalers/status group:autoscaling version:v1 name:horizontalpodautoscalers group:autoscaling version:v2beta2 name:horizontalpodautoscalers/status group:autoscaling version:v2beta2 name:cronjobs group:batch version:v1 name:cronjobs/status group:batch version:v1 name:jobs group:batch version:v1 name:jobs/status group:batch version:v1 name:certificatesigningrequests group:certificates.k8s.io version:v1 name:certificatesigningrequests/approval group:certificates.k8s.io version:v1 name:certificatesigningrequests/status group:certificates.k8s.io version:v1 name:ingressclasses group:networking.k8s.io version:v1 name:ingresses group:networking.k8s.io version:v1 name:ingresses/status group:networking.k8s.io version:v1 name:networkpolicies group:networking.k8s.io version:v1 name:networkpolicies/status group:networking.k8s.io version:v1 name:poddisruptionbudgets group:policy version:v1 name:poddisruptionbudgets/status group:policy version:v1 name:clusterrolebindings group:rbac.authorization.k8s.io version:v1 name:clusterroles group:rbac.authorization.k8s.io version:v1 name:rolebindings group:rbac.authorization.k8s.io version:v1 name:roles group:rbac.authorization.k8s.io version:v1 name:csidrivers group:storage.k8s.io version:v1 name:csinodes group:storage.k8s.io version:v1 name:csistoragecapacities group:storage.k8s.io version:v1 name:storageclasses group:storage.k8s.io version:v1 name:volumeattachments group:storage.k8s.io version:v1 name:volumeattachments/status group:storage.k8s.io version:v1 name:csistoragecapacities group:storage.k8s.io version:v1beta1 name:mutatingwebhookconfigurations group:admissionregistration.k8s.io version:v1 name:validatingwebhookconfigurations group:admissionregistration.k8s.io version:v1 name:customresourcedefinitions group:apiextensions.k8s.io version:v1 name:customresourcedefinitions/status group:apiextensions.k8s.io version:v1 name:priorityclasses group:scheduling.k8s.io version:v1 name:leases group:coordination.k8s.io version:v1 name:runtimeclasses group:node.k8s.io version:v1 name:endpointslices group:discovery.k8s.io version:v1 name:flowschemas group:flowcontrol.apiserver.k8s.io version:v1beta2 name:flowschemas/status group:flowcontrol.apiserver.k8s.io version:v1beta2 name:prioritylevelconfigurations group:flowcontrol.apiserver.k8s.io version:v1beta2 name:prioritylevelconfigurations/status group:flowcontrol.apiserver.k8s.io version:v1beta2 name:flowschemas group:flowcontrol.apiserver.k8s.io version:v1beta1 name:flowschemas/status group:flowcontrol.apiserver.k8s.io version:v1beta1 name:prioritylevelconfigurations group:flowcontrol.apiserver.k8s.io version:v1beta1 name:prioritylevelconfigurations/status group:flowcontrol.apiserver.k8s.io version:v1beta1 name:nodes group:metrics.k8s.io version:v1beta1 name:pods group:metrics.k8s.io version:v1beta1 */ informer indexer lister机制 上图展示了自定义控制器的工作方式。在虚线上方，是client-go包的informer和indexer工作方式。informer负责监听Kubernetes API资源对象的变化，如创建、更新、删除等操作，并将这些变化通知给indexer进行索引和缓存。而indexer则是将API对象进行索引，以便在需要时快速地访问它们。lister则是对indexer的封装，提供了一种简单的方式来获取已经索引的对象列表，以供代码中的其他部分使用。这种分层结构的设计使得client-go可以高效地处理Kubernetes资源对象的变化，并在应用程序中方便地使用这些资源对象。\ninformer Informer是Kubernetes API客户端中一种重要的机制，它可以实现对资源对象的监视和事件通知。当Kubernetes集群中的资源对象发生变化时，Informer可以及时地获取到这些变化，并将这些变化以事件的形式通知给相关的监听器。Informer通过调用API Server提供的REST接口，以及Kubernetes中定义的watch机制，实现了对集群资源对象的全面监视。\n下面是一个简单的pod informer示例，用于监控所有pod的变化并将其放入队列中，worker从队列中取出pod并打印相关信息。\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;os\u0026quot; \u0026quot;os/signal\u0026quot; \u0026quot;syscall\u0026quot; \u0026quot;time\u0026quot; v1 \u0026quot;k8s.io/api/core/v1\u0026quot; \u0026quot;k8s.io/apimachinery/pkg/util/wait\u0026quot; \u0026quot;k8s.io/client-go/informers\u0026quot; \u0026quot;k8s.io/client-go/kubernetes\u0026quot; \u0026quot;k8s.io/client-go/tools/cache\u0026quot; \u0026quot;k8s.io/client-go/tools/clientcmd\u0026quot; \u0026quot;k8s.io/client-go/util/workqueue\u0026quot; ) func main() { // 获取 kubeconfig 文件路径 kubeconfigPath := os.Getenv(\u0026quot;KUBECONFIG\u0026quot;) if kubeconfigPath == \u0026quot;\u0026quot; { kubeconfigPath = os.Getenv(\u0026quot;HOME\u0026quot;) + \u0026quot;/.kube/config\u0026quot; } // 使用 kubeconfig 文件创建 kubernetes 客户端 config, err := clientcmd.BuildConfigFromFlags(\u0026quot;\u0026quot;, kubeconfigPath) if err != nil { panic(err) } clientset, err := kubernetes.NewForConfig(config) if err != nil { panic(err) } // 创建 informer 工厂 informerFactory := informers.NewSharedInformerFactory(clientset, time.Minute) // 创建 informer 对象 podInformer := informerFactory.Core().V1().Pods() // 创建工作队列 queue := workqueue.NewRateLimitingQueue(workqueue.DefaultControllerRateLimiter()) // 定义处理新增、更新和删除事件的回调函数 podHandler := cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { key, err := cache.MetaNamespaceKeyFunc(obj) if err == nil { queue.Add(key) } }, UpdateFunc: func(oldObj, newObj interface{}) { key, err := cache.MetaNamespaceKeyFunc(newObj) if err == nil { queue.Add(key) } }, DeleteFunc: func(obj interface{}) { key, err := cache.MetaNamespaceKeyFunc(obj) if err == nil { queue.Add(key) } }, } // 将回调函数注册到 informer 上 podInformer.Informer().AddEventHandler(podHandler) // 启动 informer stopCh := make(chan struct{}) defer close(stopCh) informerFactory.Start(stopCh) // 等待 informer 同步完成 if !cache.WaitForCacheSync(stopCh) { panic(\u0026quot;同步 informer 缓存失败\u0026quot;) } // 创建信号处理程序，用于捕捉 SIGTERM 和 SIGINT 信号 signalCh := make(chan os.Signal, 1) signal.Notify(signalCh, syscall.SIGTERM, syscall.SIGINT) // 创建 worker 函数，用于处理队列中的事件 processNextItem := func() { obj, shutdown := queue.Get() if shutdown { return } // 转换对象为 Pod key := obj.(string) podObj, exists, err := podInformer.Informer().GetIndexer().GetByKey(key) if err != nil { queue.Forget(obj) panic(fmt.Sprintf(\u0026quot;获取 Pod 失败：%v\u0026quot;, err)) } if !exists { // 如果对象已经被删除，就把它从队列中移除 queue.Forget(obj) return } // 在这里添加处理 Pod 的逻辑 pod := podObj.(*v1.Pod) fmt.Printf(\u0026quot;处理 Pod: namespace:%v,podName:%v\\n\u0026quot;, pod.Namespace, pod.Name) // 处理完事件后，把它从队列中移除 queue.Forget(obj) return } // 启动 worker go wait.Until(processNextItem, time.Second, stopCh) // 等待信号 \u0026lt;-signalCh } /* 处理 Pod: namespace:kube-system,podName:kindnet-h25kv 处理 Pod: namespace:kube-system,podName:kube-apiserver-minikube 处理 Pod: namespace:kube-system,podName:metrics-server-c9fb666df-zk4tb 处理 Pod: namespace:kubernetes-dashboard,podName:dashboard-metrics-scraper-b74747df5-4pb7w 处理 Pod: namespace:default,podName:nginx-76d6c9b8c-jqv9h 处理 Pod: namespace:default,podName:nginx-76d6c9b8c-m4g5l 处理 Pod: namespace:kube-system,podName:coredns-7f8cbcb969-48nz6 处理 Pod: namespace:kube-system,podName:kube-proxy-t766g 处理 Pod: namespace:kube-system,podName:kube-scheduler-minikube 处理 Pod: namespace:kube-system,podName:kindnet-44zl6 处理 Pod: namespace:kube-system,podName:kube-controller-manager-minikube 处理 Pod: namespace:kube-system,podName:kube-proxy-gq68w 处理 Pod: namespace:kube-system,podName:kube-proxy-l92vg 处理 Pod: namespace:kube-system,podName:storage-provisioner 处理 Pod: namespace:kubernetes-dashboard,podName:kubernetes-dashboard-57bbdc5f89-466rh 处理 Pod: namespace:default,podName:nginx-76d6c9b8c-kr9d2 处理 Pod: namespace:default,podName:nginx-76d6c9b8c-n8st9 处理 Pod: namespace:kube-system,podName:kindnet-w9f7t 处理 Pod: namespace:default,podName:nginx-76d6c9b8c-8ljkt 处理 Pod: namespace:kube-system,podName:etcd-minikube 处理 Pod: namespace:default,podName:nginx 处理 Pod: namespace:default,podName:ubuntu */ indexer Indexer是client-go中用于本地缓存资源对象的一种方式。它支持多种索引方式，并且可以使用函数func(obj interface{}) ([]string, error)进行索引。在检索时，需要使用相同的indexName参数。借助informer，indexer就可以维护一个特定资源的本地缓存，例如pod、namespace等。这种方法省去了每次get pod都要访问api-server的过程，从而减小了api-server的压力。\n// 如何使用索引器来检索Pod对象 package main import ( \u0026quot;fmt\u0026quot; v1 \u0026quot;k8s.io/api/core/v1\u0026quot; \u0026quot;k8s.io/apimachinery/pkg/api/meta\u0026quot; metav1 \u0026quot;k8s.io/apimachinery/pkg/apis/meta/v1\u0026quot; \u0026quot;k8s.io/client-go/tools/cache\u0026quot; ) const ( NamespaceIndexName = \u0026quot;namespace\u0026quot; // 定义一个索引器名称，用于按照命名空间检索Pod NodeNameIndexName = \u0026quot;nodeName\u0026quot; // 定义一个索引器名称，用于按照节点名称检索Pod ) // NamespaceIndexFunc是一个函数，用于从对象中提取命名空间作为索引键 func NamespaceIndexFunc(obj interface{}) ([]string, error) { m, err := meta.Accessor(obj) // 获取对象的元数据 if err != nil { return []string{\u0026quot;\u0026quot;}, fmt.Errorf(\u0026quot;object has no meta: %v\u0026quot;, err) } return []string{m.GetNamespace()}, nil // 返回对象的命名空间 } // NodeNameIndexFunc是一个函数，用于从Pod对象中提取节点名称作为索引键 func NodeNameIndexFunc(obj interface{}) ([]string, error) { pod, ok := obj.(*v1.Pod) // 判断对象是否是Pod类型 if !ok { return []string{}, nil // 如果不是，返回空切片 } return []string{pod.Spec.NodeName}, nil // 如果是，返回Pod的节点名称 } func main() { index := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{ NamespaceIndexName: NamespaceIndexFunc, NodeNameIndexName: NodeNameIndexFunc, }) // 创建一个新的索引器，指定主键函数和辅助键函数 pod1 := \u0026amp;v1.Pod{ ObjectMeta: metav1.ObjectMeta{ Name: \u0026quot;index-pod-1\u0026quot;, Namespace: \u0026quot;default\u0026quot;, }, Spec: v1.PodSpec{NodeName: \u0026quot;node1\u0026quot;}, } // 创建一个Pod对象，属于default命名空间和node1节点 pod2 := \u0026amp;v1.Pod{ ObjectMeta: metav1.ObjectMeta{ Name: \u0026quot;index-pod-2\u0026quot;, Namespace: \u0026quot;default\u0026quot;, }, Spec: v1.PodSpec{NodeName: \u0026quot;node2\u0026quot;}, } // 创建另一个Pod对象，属于default命名空间和node2节点 pod3 := \u0026amp;v1.Pod{ ObjectMeta: metav1.ObjectMeta{ Name: \u0026quot;index-pod-3\u0026quot;, Namespace: \u0026quot;kube-system\u0026quot;, }, Spec: v1.PodSpec{NodeName: \u0026quot;node2\u0026quot;}, } // 创建第三个Pod对象，属于kube-system命名空间和node2节点 index.Add(pod1) // 将pod1添加到索引器中 index.Add(pod2) // 将pod2添加到索引器中 index.Add(pod3) // 将pod3添加到索引器中 pods, err := index.ByIndex(NamespaceIndexName, \u0026quot;default\u0026quot;) // 按照命名空间为default检索Pod列表 if err != nil { panic(err) } for _, pod := range pods { fmt.Println(pod.(*v1.Pod).Name) } // 遍历并打印检索到的Pod名称 fmt.Println(\u0026quot;*****************\u0026quot;) pods, err = index.ByIndex(NodeNameIndexName, \u0026quot;node2\u0026quot;) // 按照节点名称为node2检索Pod列表 if err != nil { panic(err) } for _, pod := range pods { fmt.Println(pod.(*v1.Pod).Name) } // 遍历并打印 } /* index-pod-2 index-pod-1 ***************** index-pod-2 index-pod-3 */ lister Lister是对Indexer的封装，提供了一种方便的方式来获取已经索引的Kubernetes资源对象列表。\n具体而言，Lister是一个接口，包含了获取所有已索引对象的列表以及根据名称获取单个对象的方法。这些方法可以帮助开发者在应用程序中快速访问已经缓存的资源对象，而无需直接与Indexer交互。\nLister的主要功能包括：\n提供方便的接口：Lister接口的方法定义清晰简洁，使用起来非常方便，可以快速地获取已经索引的资源对象列表。 提高代码可读性：通过使用Lister接口，代码可读性得到提高。开发者可以更加专注于业务逻辑，而无需关注底层的Indexer实现细节。 提高代码复用性：由于Lister接口已经提供了通用的方法，因此可以更容易地在不同的代码模块中重用相同的逻辑，减少代码重复。 总之，Lister作为client-go包中的一个重要组件，可以帮助开发者更加高效地处理Kubernetes资源对象，提高代码的可读性和可重用性。\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;os\u0026quot; \u0026quot;time\u0026quot; \u0026quot;k8s.io/apimachinery/pkg/labels\u0026quot; \u0026quot;k8s.io/client-go/informers\u0026quot; \u0026quot;k8s.io/client-go/kubernetes\u0026quot; \u0026quot;k8s.io/client-go/tools/cache\u0026quot; \u0026quot;k8s.io/client-go/tools/clientcmd\u0026quot; ) func main() { // 获取 kubeconfig 文件路径 kubeconfigPath := os.Getenv(\u0026quot;KUBECONFIG\u0026quot;) if kubeconfigPath == \u0026quot;\u0026quot; { kubeconfigPath = os.Getenv(\u0026quot;HOME\u0026quot;) + \u0026quot;/.kube/config\u0026quot; } // 使用 kubeconfig 文件创建 kubernetes 客户端 config, err := clientcmd.BuildConfigFromFlags(\u0026quot;\u0026quot;, kubeconfigPath) if err != nil { panic(err) } clientset, err := kubernetes.NewForConfig(config) if err != nil { panic(err.Error()) } // 创建Informer factory := informers.NewSharedInformerFactory(clientset, time.Minute) podInformer := factory.Core().V1().Pods() // 创建Lister lister := podInformer.Lister() // 等待Informer同步完成 stopCh := make(chan struct{}) defer close(stopCh) factory.Start(stopCh) cache.WaitForCacheSync(stopCh, podInformer.Informer().HasSynced) // 获取namespace为\u0026quot;default\u0026quot;的Pod对象 podList, err := lister.Pods(\u0026quot;default\u0026quot;).List(labels.Everything()) if err != nil { panic(err.Error()) } // 打印Pod对象 for _, pod := range podList { fmt.Printf(\u0026quot;Pod name: %s, Namespace: %s\\n\u0026quot;, pod.Name, pod.Namespace) } } /* Pod name: nginx-76d6c9b8c-m4g5l, Namespace: default Pod name: nginx, Namespace: default Pod name: ubuntu, Namespace: default Pod name: nginx-76d6c9b8c-kr9d2, Namespace: default Pod name: nginx-76d6c9b8c-n8st9, Namespace: default Pod name: nginx-76d6c9b8c-8ljkt, Namespace: default Pod name: nginx-76d6c9b8c-jqv9h, Namespace: default */ Reference sample-controller/controller-client-go.md at master · kubernetes/sample-controller (github.com) K8s二开之 client-go 初探 - 掘金 (juejin.cn) ","date":"2024-03-08","permalink":"https://daemon365.dev/2024/03/08/kubernetes-client-go%E5%8A%9F%E8%83%BD%E4%BB%8B%E7%BB%8D/","tags":["client-go","kubernetes","golang"],"title":"kubernetes client-go功能介绍"},{"content":"启动流程 containerd 作为一个 api 服务，提供了一系列的接口供外部调用，比如创建容器、删除容器、创建镜像、删除镜像等等。使用 docker 和 ctr 等工具，都是通过调用 containerd 的 api 来实现的。 kubelet 通过 cri 调用 containerd 和这些不一样，后续我会介绍到。\ncontainerd 创建容器流程如下：\n接收到 api 请求，通过调用 containerd-shim-runc-v2 调用 runc 创建容器，主要是做解压文件和准备环境的工作。 接收到 api 请求，创建一个 task，task 是一个容器的抽象，包含了容器的所有信息，比如容器的 id、容器的状态、容器的配置等等。 containerd 启动一个 containerd-shim-runc-v2 进程。 containerd-shim-runc-v2 进程 在启动一个 containerd-shim-runc-v2 进程，然后第一个 containerd-shim-runc-v2 进程退出。 containerd 通过 IPC 通信，让第二个 containerd-shim-runc-v2 启动容器。 containerd-shim-runc-v2 进程通过调用 runc start 启动容器。 runc 会调用 runc init 启动容器的 init 进程。 runc init 进程会调用 unix.Exec 的方式，替换自己的进程，启动容器的第一个进程。这个进程既是容器的启动命令，也是容器的 pid 1 进程。完成之后，runc create 进程退出。 这样 containerd-shim-runc-v2 的父进程就是 init 进程（1），而 init 进程的父进程是 containerd-shim-runc-v2 进程，这样就形成了一个进程树。\n我通过 docker 启动一个容器，示例一下：\n❯ docker run -d --rm -it docker.m.daocloud.io/ubuntu:22.10 sleep 3000 ❯ ps -ef|grep \u0026quot;sleep 3000\u0026quot; root 15042 15021 0 22:02 pts/0 00:00:00 sleep 3000 ❯ ps -ef|grep \u0026quot;15021\u0026quot; root 15021 1 0 22:02 ? 00:00:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 4346ca602cd85d35b0a4a81762be6142bc6a2222f859f4af47563992efc3c59c -address /run/containerd/containerd.sock root 15042 15021 0 22:02 pts/0 00:00:00 sleep 3000 可以看到我们的结论是正确的。\n疑问解答 1.为什么要创建两个 containerd-shim 不嫌麻烦吗？\n因为 第一个 containerd-shim 会在创建完第二个 containerd-shim 后退出，而作为第一个进程子进程的第二个 containerd-shim 会成为孤儿进程，这样就会被 init 进程接管，而和 containerd 本身脱离了关系。\n2.为什么要想法设法把 containerd-shim 挂在 init 进程下面，而不是 containerd？\n为了保证稳定性和独立性。这样做可以确保即使 containerd 崩溃或重启，由 containerd-shim 管理的容器进程仍然可以继续运行，不受影响。此外，这种设计还有助于更好地管理资源和防止资源泄露。\n3.为什么 runc start 进程退出了 runc init 进程（用户进程）没有变成 init 的子进程 而是containerd-shim的子进程？\n因为 containerd-shim 做了 unix 的 PR_SET_CHILD_SUBREAPER 调用, 这个系统调用大概作用为 当这个进程的子子孙孙进程变成孤儿进程的时候，这个进程会接管这个孤儿进程，而不是 init 进程接管。\n架构图 代码分析 containerd api 注册 代码分析 var register = struct { sync.RWMutex r plugin.Registry }{} type Registry []*Registration type Registration struct { // Type of the plugin Type Type // ID of the plugin ID string // Config specific to the plugin Config interface{} // Requires is a list of plugins that the registered plugin requires to be available Requires []Type // InitFn is called when initializing a plugin. The registration and // context are passed in. The init function may modify the registration to // add exports, capabilities and platform support declarations. InitFn func(*InitContext) (interface{}, error) // ConfigMigration allows a plugin to migrate configurations from an older // version to handle plugin renames or moving of features from one plugin // to another in a later version. // The configuration map is keyed off the plugin name and the value // is the configuration for that objects, with the structure defined // for the plugin. No validation is done on the value before performing // the migration. ConfigMigration func(context.Context, int, map[string]interface{}) error } 通过 init 把接口注册进去 比如 task api 注册\n代码位置 ： services/tasks/local.go\nfunc init() { registry.Register(\u0026amp;plugin.Registration{ Type: plugins.ServicePlugin, ID: services.TasksService, Requires: tasksServiceRequires, Config: \u0026amp;Config{}, InitFn: initFunc, }) timeout.Set(stateTimeout, 2*time.Second) } func initFunc(ic *plugin.InitContext) (interface{}, error) { config := ic.Config.(*Config) v2r, err := ic.GetByID(plugins.RuntimePluginV2, \u0026quot;task\u0026quot;) if err != nil { return nil, err } m, err := ic.GetSingle(plugins.MetadataPlugin) if err != nil { return nil, err } ep, err := ic.GetSingle(plugins.EventPlugin) if err != nil { return nil, err } monitor, err := ic.GetSingle(plugins.TaskMonitorPlugin) if err != nil { if !errors.Is(err, plugin.ErrPluginNotFound) { return nil, err } monitor = runtime.NewNoopMonitor() } db := m.(*metadata.DB) l := \u0026amp;local{ containers: metadata.NewContainerStore(db), store: db.ContentStore(), publisher: ep.(events.Publisher), monitor: monitor.(runtime.TaskMonitor), v2Runtime: v2r.(runtime.PlatformRuntime), } v2Tasks, err := l.v2Runtime.Tasks(ic.Context, true) if err != nil { return nil, err } for _, t := range v2Tasks { l.monitor.Monitor(t, nil) } if err := blockio.SetConfig(config.BlockIOConfigFile); err != nil { log.G(ic.Context).WithError(err).Errorf(\u0026quot;blockio initialization failed\u0026quot;) } if err := rdt.SetConfig(config.RdtConfigFile); err != nil { log.G(ic.Context).WithError(err).Errorf(\u0026quot;RDT initialization failed\u0026quot;) } return l, nil } 然后在 containerd 启动的时候 注册api\nloaded := registry.Graph(filter(config.DisabledPlugins)) for _, p := range loaded { result := p.Init(initContext) if err := initialized.Add(result); err != nil { return nil, fmt.Errorf(\u0026quot;could not add plugin result to plugin set: %w\u0026quot;, err) } instance, err := result.Instance() delete(required, id) // check for grpc services that should be registered with the server if src, ok := instance.(grpcService); ok { grpcServices = append(grpcServices, src) } if src, ok := instance.(ttrpcService); ok { ttrpcServices = append(ttrpcServices, src) } if service, ok := instance.(tcpService); ok { tcpServices = append(tcpServices, service) } s.plugins = append(s.plugins, result) } // register services after all plugins have been initialized for _, service := range grpcServices { if err := service.Register(grpcServer); err != nil { return nil, err } } for _, service := range ttrpcServices { if err := service.RegisterTTRPC(ttrpcServer); err != nil { return nil, err } } for _, service := range tcpServices { if err := service.RegisterTCP(tcpServer); err != nil { return nil, err } } create task func (l *local) Create(ctx context.Context, r *api.CreateTaskRequest, _ ...grpc.CallOption) (*api.CreateTaskResponse, error) { rtime := l.v2Runtime _, err = rtime.Get(ctx, r.ContainerID) if err != nil \u0026amp;\u0026amp; !errdefs.IsNotFound(err) { return nil, errdefs.ToGRPC(err) } if err == nil { return nil, errdefs.ToGRPC(fmt.Errorf(\u0026quot;task %s: %w\u0026quot;, r.ContainerID, errdefs.ErrAlreadyExists)) } c, err := rtime.Create(ctx, r.ContainerID, opts) } func (m *TaskManager) Create(ctx context.Context, taskID string, opts runtime.CreateOpts) (runtime.Task, error) { // 启动第一个 containerd-shim-runc-v2 进程 shimTask, err := newShimTask(shim) if err != nil { return nil, err } // 给第二个 containerd-shim-runc-v2 进程传递参数 t, err := shimTask.Create(ctx, opts) return t, nil } cri 代码 cri 和 task 和上述的是一样的， 通过 register 注册 api.\nfunc init() { registry.Register(\u0026amp;plugin.Registration{ Type: plugins.GRPCPlugin, ID: \u0026quot;cri\u0026quot;, Requires: []plugin.Type{ plugins.CRIImagePlugin, plugins.InternalPlugin, plugins.SandboxControllerPlugin, plugins.NRIApiPlugin, plugins.EventPlugin, plugins.ServicePlugin, plugins.LeasePlugin, plugins.SandboxStorePlugin, }, InitFn: initCRIService, }) } startContainer 接口\nfunc (in *instrumentedService) StartContainer(ctx context.Context, r *runtime.StartContainerRequest) (_ *runtime.StartContainerResponse, err error) { if err := in.checkInitialized(); err != nil { return nil, err } log.G(ctx).Infof(\u0026quot;StartContainer for %q\u0026quot;, r.GetContainerId()) defer func() { if err != nil { log.G(ctx).WithError(err).Errorf(\u0026quot;StartContainer for %q failed\u0026quot;, r.GetContainerId()) } else { log.G(ctx).Infof(\u0026quot;StartContainer for %q returns successfully\u0026quot;, r.GetContainerId()) } }() res, err := in.c.StartContainer(ctrdutil.WithNamespace(ctx), r) return res, errdefs.ToGRPC(err) } func (c *criService) StartContainer(ctx context.Context, r *runtime.StartContainerRequest) (retRes *runtime.StartContainerResponse, retErr error) { task, err := container.NewTask(ctx, ioCreation, taskOpts...) } func (c *container) NewTask(ctx context.Context, ioCreate cio.Creator, opts ...NewTaskOpts) (_ Task, err error) { // 通过 unix socket 的方式调用上述的 create task 接口 response, err := c.client.TaskService().Create(ctx, request) } containerd-shim func run(ctx context.Context, manager Manager, config Config) error { // Handle explicit actions switch action { case \u0026quot;delete\u0026quot;: case \u0026quot;start\u0026quot;: // 如果是 start 参数的话启动一个 containerd-shim-runc-v2 进程 opts := StartOpts{ Address: addressFlag, TTRPCAddress: ttrpcAddress, Debug: debugFlag, } params, err := manager.Start(ctx, id, opts) if err != nil { return err } data, err := json.Marshal(\u0026amp;params) if err != nil { return fmt.Errorf(\u0026quot;failed to marshal bootstrap params to json: %w\u0026quot;, err) } if _, err := os.Stdout.Write(data); err != nil { return err } return nil } } // manager.Start 中创建的 command 指定三个参数 Namespace 容器 id 和 containerd socket 文件的地址 func newCommand(ctx context.Context, id, containerdAddress, containerdTTRPCAddress string, debug bool) (*exec.Cmd, error) { ns, err := namespaces.NamespaceRequired(ctx) if err != nil { return nil, err } self, err := os.Executable() if err != nil { return nil, err } cwd, err := os.Getwd() if err != nil { return nil, err } args := []string{ \u0026quot;-namespace\u0026quot;, ns, \u0026quot;-id\u0026quot;, id, \u0026quot;-address\u0026quot;, containerdAddress, } if debug { args = append(args, \u0026quot;-debug\u0026quot;) } cmd := exec.Command(self, args...) cmd.Dir = cwd cmd.Env = append(os.Environ(), \u0026quot;GOMAXPROCS=4\u0026quot;) cmd.SysProcAttr = \u0026amp;syscall.SysProcAttr{ Setpgid: true, } return cmd, nil } 第二个 containerd-shim 也会开启一些api服务 ，比如启动容器\nfunc (s *service) Start(ctx context.Context, r *taskAPI.StartRequest) (*taskAPI.StartResponse, error) { p, err := container.Start(ctx, r) } func (c *Container) Start(ctx context.Context, r *task.StartRequest) (process.Process, error) { p, err := c.Start(r.ExecID) } // command 及调用 runc 启动 runc create 的进程 func (r *Runc) Start(context context.Context, id string) error { return r.runOrError(r.command(context, \u0026quot;start\u0026quot;, id)) } runc func (r *runner) run(config *specs.Process) (int, error) { switch r.action { case CT_ACT_CREATE: err = r.container.Start(process) case CT_ACT_RESTORE: err = r.container.Restore(process, r.criuOpts) case CT_ACT_RUN: err = r.container.Run(process) default: panic(\u0026quot;Unknown action\u0026quot;) } } func (c *Container) Start(process *Process) error { c.m.Lock() defer c.m.Unlock() if c.config.Cgroups.Resources.SkipDevices { return errors.New(\u0026quot;can't start container with SkipDevices set\u0026quot;) } if process.Init { if err := c.createExecFifo(); err != nil { return err } } if err := c.start(process); err != nil { if process.Init { c.deleteExecFifo() } return err } return nil } // 调用 runc init 进程 /proc/self/exe 是自己的二进制文件 func (c *Container) newParentProcess(p *Process) (parentProcess, error) { comm, err := newProcessComm() if err != nil { return nil, err } // Make sure we use a new safe copy of /proc/self/exe or the runc-dmz // binary each time this is called, to make sure that if a container // manages to overwrite the file it cannot affect other containers on the // system. For runc, this code will only ever be called once, but // libcontainer users might call this more than once. p.closeClonedExes() var ( exePath string // only one of dmzExe or safeExe are used at a time dmzExe, safeExe *os.File ) if dmz.IsSelfExeCloned() { // /proc/self/exe is already a cloned binary -- no need to do anything logrus.Debug(\u0026quot;skipping binary cloning -- /proc/self/exe is already cloned!\u0026quot;) exePath = \u0026quot;/proc/self/exe\u0026quot; } cmd := exec.Command(exePath, \u0026quot;init\u0026quot;) cmd.Args[0] = os.Args[0] cmd.Stdin = p.Stdin cmd.Stdout = p.Stdout cmd.Stderr = p.Stderr cmd.Dir = c.config.Rootfs if cmd.SysProcAttr == nil { cmd.SysProcAttr = \u0026amp;unix.SysProcAttr{} } } runc init\nfunc init() { if len(os.Args) \u0026gt; 1 \u0026amp;\u0026amp; os.Args[1] == \u0026quot;init\u0026quot; { // This is the golang entry point for runc init, executed // before main() but after libcontainer/nsenter's nsexec(). libcontainer.Init() } } // libcontainer.Init() 中调用的 func startInitialization() (retErr error) { return containerInit(it, \u0026amp;config, syncPipe, consoleSocket, pidfdSocket, fifofd, logFD, dmzExe, mountFds{sourceFds: mountSrcFds, idmapFds: idmapFds}) } // linuxSetnsInit 是 exec 的时候调用的 在启动的容器执行命令 // initStandard 是启动容器 func containerInit(t initType, config *initConfig, pipe *syncSocket, consoleSocket, pidfdSocket *os.File, fifoFd, logFd int, dmzExe *os.File, mountFds mountFds) error { if err := populateProcessEnvironment(config.Env); err != nil { return err } switch t { case initSetns: // mount and idmap fds must be nil in this case. We don't mount while doing runc exec. if mountFds.sourceFds != nil || mountFds.idmapFds != nil { return errors.New(\u0026quot;mount and idmap fds must be nil; can't mount from exec\u0026quot;) } i := \u0026amp;linuxSetnsInit{ pipe: pipe, consoleSocket: consoleSocket, pidfdSocket: pidfdSocket, config: config, logFd: logFd, dmzExe: dmzExe, } return i.Init() case initStandard: i := \u0026amp;linuxStandardInit{ pipe: pipe, consoleSocket: consoleSocket, pidfdSocket: pidfdSocket, parentPid: unix.Getppid(), config: config, fifoFd: fifoFd, logFd: logFd, dmzExe: dmzExe, mountFds: mountFds, } return i.Init() } return fmt.Errorf(\u0026quot;unknown init type %q\u0026quot;, t) } func (l *linuxStandardInit) Init() error { return system.Exec(name, l.config.Args, os.Environ()) } // 替换进程 func Exec(cmd string, args []string, env []string) error { for { err := unix.Exec(cmd, args, env) if err != unix.EINTR { return \u0026amp;os.PathError{Op: \u0026quot;exec\u0026quot;, Path: cmd, Err: err} } } } ","date":"2023-12-07","permalink":"https://daemon365.dev/2023/12/07/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8Bcontainerd-%E5%92%8C-runc/","tags":["docker","containerd","runc","containerd-shim","kubernetes","源码分析"],"title":"容器启动流程（containerd 和 runc）"},{"content":"CDI 是什么？ Container Device Interface (CDI) 是一个提议的标准，它定义了如何在容器运行时环境中向容器提供设备。这个提议的目的是使得设备供应商能够更容易地将其设备集成到 Kubernetes 集群中，而不必修改 Kubernetes 核心代码。\nCDI 插件通常负责：\n配置设备以供容器使用（例如，分配设备文件或设置必要的环境变量）。 在容器启动时将设备资源注入到容器中。 官网\nhttps://github.com/cncf-tags/container-device-interface 为什么需要CDI？ 如果我们想在容器内使用 nvidia 的 gpu，在没有 CDI 之前，我们需要修改 containerd 的 low-level container runtime(runc) 到 nvidia runtime。这么做的原因就是使用 gpu 不单单要绑定 gpu device 文件到容器内，还需要绑定一些驱动文件和可执行命令（比如 nvidia-smi）等到容器内，还有就执行一些 hooks。 nvidia runtime 的作用就是绑定一些文件和执行一些 hooks 然后调用 runc。\n现在我们可以使用 CDI 做这些事情，除了无需修改 runtime 外，还有抽象和插件化等优点。\n版本及准备工作 kubelet version \u0026gt;= 1.28.0 containerd version \u0026gt;= 1.7.0 而且这在 k8s 1.28 (1.29版本是 beta 了 默认就打开了) 版本中是一个 alpha 版本的功能，所以我们需要在 kubelet 的启动参数中加入开启特性门：--feature-gates=DevicePluginCDIDevices =true。\nsudo vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS --feature-gates=DevicePluginCDIDevices=true containerd 也需要开启 CDI cdi_spec_dirs 为 cdi 配置文件的目录，enable_cdi 为开启 CDI 功能。\nsudo vim /etc/containerd/config.toml cdi_spec_dirs = [\u0026quot;/etc/cdi\u0026quot;, \u0026quot;/var/run/cdi\u0026quot;] enable_cdi = true 重启 containerd 和 kubelet\nsudo systemctl restart kubelet.service containerd.service mock 因为我的集群里没有 gpu , 所以我就随便 mock 几个文件作为 device 了。\nsudo mkdir /dev/mock cd /dev/mock sudo mknod /dev/mock/device_0 c 89 1 sudo mknod /dev/mock/device_1 c 89 1 sudo mknod /dev/mock/device_2 c 89 1 sudo mknod /dev/mock/device_3 c 89 1 sudo mknod /dev/mock/device_4 c 89 1 sudo mknod /dev/mock/device_5 c 89 1 sudo mknod /dev/mock/device_6 c 89 1 sudo mknod /dev/mock/device_7 c 89 1 sudo vim /mock/bin/list_device.sh #!/bin/bash # 定义目录数组 directories=(/dev /dev/mock) # 遍历目录数组 for dir in \u0026quot;${directories[@]}\u0026quot;; do # 检查目录是否存在 if [ -d \u0026quot;$dir\u0026quot; ]; then # 目录存在，打印目录下的所有文件 ls \u0026quot;$dir\u0026quot; fi done sudo chmod a+x /mock/bin/list_device.sh sudo mkdir /mock/so cd /mock/so sudo touch device_0.so device_1.so device_2.so device_3.so device_5.so device_6.so device_7.so device_4.so 开启 kubelet 的 device plugin 下面是简单写的一个 device plugin，及其 dockerfile 还有部署到 k8s 的 yaml 文件。\npackage main import ( \u0026quot;context\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;time\u0026quot; \u0026quot;github.com/kubevirt/device-plugin-manager/pkg/dpm\u0026quot; pluginapi \u0026quot;k8s.io/kubelet/pkg/apis/deviceplugin/v1beta1\u0026quot; ) type PluginLister struct { ResUpdateChan chan dpm.PluginNameList } var ResourceNamespace = \u0026quot;mock.com\u0026quot; var PluginName = \u0026quot;gpu\u0026quot; func (p *PluginLister) GetResourceNamespace() string { return ResourceNamespace } func (p *PluginLister) Discover(pluginListCh chan dpm.PluginNameList) { pluginListCh \u0026lt;- dpm.PluginNameList{PluginName} } func (p *PluginLister) NewPlugin(name string) dpm.PluginInterface { return \u0026amp;Plugin{} } type Plugin struct { } func (p *Plugin) GetDevicePluginOptions(ctx context.Context, e *pluginapi.Empty) (*pluginapi.DevicePluginOptions, error) { options := \u0026amp;pluginapi.DevicePluginOptions{ PreStartRequired: true, } return options, nil } func (p *Plugin) PreStartContainer(ctx context.Context, r *pluginapi.PreStartContainerRequest) (*pluginapi.PreStartContainerResponse, error) { return \u0026amp;pluginapi.PreStartContainerResponse{}, nil } func (p *Plugin) GetPreferredAllocation(ctx context.Context, r *pluginapi.PreferredAllocationRequest) (*pluginapi.PreferredAllocationResponse, error) { return \u0026amp;pluginapi.PreferredAllocationResponse{}, nil } func (p *Plugin) ListAndWatch(e *pluginapi.Empty, r pluginapi.DevicePlugin_ListAndWatchServer) error { devices := []*pluginapi.Device{} for i := 0; i \u0026lt; 8; i++ { devices = append(devices, \u0026amp;pluginapi.Device{ // 和 device 名称保持一致 ID: fmt.Sprintf(\u0026quot;device_%d\u0026quot;, i), Health: pluginapi.Healthy, }) } for { fmt.Println(\u0026quot;register devices\u0026quot;) // 每分钟注册一次 r.Send(\u0026amp;pluginapi.ListAndWatchResponse{ Devices: devices, }) time.Sleep(time.Second * 60) } } func (p *Plugin) Allocate(ctx context.Context, r *pluginapi.AllocateRequest) (*pluginapi.AllocateResponse, error) { // 使用cdi插件 responses := \u0026amp;pluginapi.AllocateResponse{} for _, req := range r.ContainerRequests { cdidevices := []*pluginapi.CDIDevice{} for _, id := range req.DevicesIDs { cdidevices = append(cdidevices, \u0026amp;pluginapi.CDIDevice{ Name: fmt.Sprintf(\u0026quot;%s/%s=%s\u0026quot;, ResourceNamespace, PluginName, id), }) } responses.ContainerResponses = append(responses.ContainerResponses, \u0026amp;pluginapi.ContainerAllocateResponse{ CDIDevices: cdidevices, }) } return responses, nil } func main() { m := dpm.NewManager(\u0026amp;PluginLister{}) m.Run() } FROM golang:1.21.3 as builder COPY . /src WORKDIR /src RUN go env -w GO111MODULE=on \u0026amp;\u0026amp; go env -w GOPROXY=https://goproxy.io,direct RUN go build FROM debian:bookworm-slim RUN sed -i 's/deb.debian.org/mirrors.ustc.edu.cn/g' /etc/apt/sources.list.d/debian.sources RUN apt-get update \u0026amp;\u0026amp; apt-get install -y --no-install-recommends \\ ca-certificates \\ netbase \\ pciutils \\ curl \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/ \\ \u0026amp;\u0026amp; apt-get autoremove -y \u0026amp;\u0026amp; apt-get autoclean -y RUN update-pciids COPY --from=builder /src /app WORKDIR /app apiVersion: v1 kind: Namespace metadata: name: mock-plugin --- apiVersion: apps/v1 kind: DaemonSet metadata: name: mock-plugin-daemonset namespace: mock-plugin spec: selector: matchLabels: name: mock-plugin template: metadata: labels: name: mock-plugin app.kubernetes.io/component: mock-plugin app.kubernetes.io/name: mock-plugin spec: containers: - image: zhaohaiyu/mock:v1 name: mock-plugin command: ['/app/mock'] imagePullPolicy: Always securityContext: privileged: true tty: true volumeMounts: - name: kubelet mountPath: /var/lib/kubelet volumes: - name: kubelet hostPath: path: /var/lib/kubelet 执行完毕使用 kubectl 查看\n❯ kubectl -n mock-plugin get pod NAME READY STATUS RESTARTS AGE mock-plugin-daemonset-8w2r8 1/1 Running 0 3m27s 查看 node 是否已经注册了 device plugin\nkubectl describe node node1 Capacity: cpu: 8 ephemeral-storage: 102626232Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 24570324Ki mock.com/gpu: 8 pods: 110 Allocatable: cpu: 8 ephemeral-storage: 94580335255 hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 24467924Ki mock.com/gpu: 8 pods: 110 可以看到已经注册了 8 个 gpu 设备 名字叫 mock.com/gpu 也就是我们代码中定义的。\nCDI配置文件 CDI Spec: https://github.com/cncf-tags/container-device-interface/blob/main/SPEC.md\n我们也生成了一个 CDI 的配置文件，这个配置文件会被 containerd 读取，然后根据配置文件中的信息去调用 device plugin。\n# vim /etc/cdi/mock.yaml cdiVersion: 0.5.0 kind: mock.com/gpu devices: - name: device_0 containerEdits: deviceNodes: - hostPath: \u0026quot;/dev/mock/device_0\u0026quot; path: \u0026quot;/dev/mock/device_0\u0026quot; type: c permissions: rw mounts: - hostPath: \u0026quot;/mock/so/device_0.so\u0026quot; containerPath: \u0026quot;/mock/so/device_0.so\u0026quot; options: - ro - nosuid - nodev - bind - name: device_1 containerEdits: deviceNodes: - hostPath: \u0026quot;/dev/mock/device_1\u0026quot; path: \u0026quot;/dev/mock/device_1\u0026quot; type: c permissions: rw mounts: - hostPath: \u0026quot;/mock/so/device_1.so\u0026quot; containerPath: \u0026quot;/mock/so/device_1.so\u0026quot; options: - ro - nosuid - nodev - bind - name: device_2 containerEdits: deviceNodes: - hostPath: \u0026quot;/dev/mock/device_2\u0026quot; path: \u0026quot;/dev/mock/device_2\u0026quot; type: c permissions: rw mounts: - hostPath: \u0026quot;/mock/so/device_2.so\u0026quot; containerPath: \u0026quot;/mock/so/device_2.so\u0026quot; options: - ro - nosuid - nodev - bind - name: device_3 containerEdits: deviceNodes: - hostPath: \u0026quot;/dev/mock/device_3\u0026quot; path: \u0026quot;/dev/mock/device_3\u0026quot; type: c permissions: rw mounts: - hostPath: \u0026quot;/mock/so/device_3.so\u0026quot; containerPath: \u0026quot;/mock/so/device_3.so\u0026quot; options: - ro - nosuid - nodev - bind - name: device_4 containerEdits: deviceNodes: - hostPath: \u0026quot;/dev/mock/device_4\u0026quot; path: \u0026quot;/dev/mock/device_4\u0026quot; type: c permissions: rw mounts: - hostPath: \u0026quot;/mock/so/device_4.so\u0026quot; containerPath: \u0026quot;/mock/so/device_4.so\u0026quot; options: - ro - nosuid - nodev - bind - name: device_5 containerEdits: deviceNodes: - hostPath: \u0026quot;/dev/mock/device_5\u0026quot; path: \u0026quot;/dev/mock/device_5\u0026quot; type: c permissions: rw mounts: - hostPath: \u0026quot;/mock/so/device_5.so\u0026quot; containerPath: \u0026quot;/mock/so/device_5.so\u0026quot; options: - ro - nosuid - nodev - bind - name: device_6 containerEdits: deviceNodes: - hostPath: \u0026quot;/dev/mock/device_6\u0026quot; path: \u0026quot;/dev/mock/device_6\u0026quot; type: c permissions: rw mounts: - hostPath: \u0026quot;/mock/so/device_6.so\u0026quot; containerPath: \u0026quot;/mock/so/device_6.so\u0026quot; options: - ro - nosuid - nodev - bind - name: device_7 containerEdits: deviceNodes: - hostPath: \u0026quot;/dev/mock/device_7\u0026quot; path: \u0026quot;/dev/mock/device_7\u0026quot; type: c permissions: rw mounts: - hostPath: \u0026quot;/mock/so/device_7.so\u0026quot; containerPath: \u0026quot;/mock/so/device_7.so\u0026quot; options: - ro - nosuid - nodev - bind containerEdits: mounts: - hostPath: \u0026quot;/mock/bin/list_device.sh\u0026quot; containerPath: \u0026quot;/usr/local/bin/list_device.sh\u0026quot; options: - ro - nosuid - nodev - bind 这里我只是简单示例，还有 hooks 和 env 等用法查看官方文档。\n部署 pod 我部署一个 pod 使用 mock gpu 这个资源 4个。\napiVersion: v1 kind: Pod metadata: name: ubuntu1 spec: containers: - name: ubuntu-container image: ubuntu:latest command: [\u0026quot;sleep\u0026quot;] args: [\u0026quot;3600\u0026quot;] resources: requests: memory: \u0026quot;64Mi\u0026quot; cpu: \u0026quot;250m\u0026quot; mock.com/gpu: \u0026quot;4\u0026quot; limits: memory: \u0026quot;128Mi\u0026quot; cpu: \u0026quot;500m\u0026quot; mock.com/gpu: \u0026quot;4\u0026quot; ubuntu1 1/1 Running 0 49s 现在我们 使用 kubectl exec -it ubuntu1 bash 进入容器看一看。\nls /dev/mock/ device_0 device_1 device_6 device_7 ls /mock/so/ device_0.so device_1.so device_6.so device_7.so list_device.sh so device_0 device_1 device_6 device_7 可以看到我们 cdi 配置文件配置的 device 和 so 文件和还有我们的 list_device.sh 都已经挂载到容器内了。\n我现在再启动一个 pod 使用 mock gpu 这个资源 3 个。\napiVersion: v1 kind: Pod metadata: name: ubuntu2 spec: containers: - name: ubuntu-container image: ubuntu:latest command: [\u0026quot;sleep\u0026quot;] args: [\u0026quot;3600\u0026quot;] resources: requests: memory: \u0026quot;64Mi\u0026quot; cpu: \u0026quot;250m\u0026quot; mock.com/gpu: \u0026quot;3\u0026quot; limits: memory: \u0026quot;128Mi\u0026quot; cpu: \u0026quot;500m\u0026quot; mock.com/gpu: \u0026quot;3\u0026quot; ls /dev/mock/ device_2 device_3 device_5 查看node使用了多少资源\nAllocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 1550m (19%) 1 (12%) memory 668Mi (2%) 596Mi (2%) ephemeral-storage 0 (0%) 0 (0%) hugepages-1Gi 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) mock.com/gpu 7 7 可以看到我们使用了 7 个 mock.com/gpu 资源，还剩下 1 个。\nnvdia gpu 我找了一台带有 nvidia gpu 的机器，然后安装了 nvidia-container-toolkit-base。\n使用 nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml 生产 cdi 配置文件。\n--- cdiVersion: 0.5.0 containerEdits: deviceNodes: - path: /dev/nvidia-modeset - path: /dev/nvidia-uvm - path: /dev/nvidia-uvm-tools - path: /dev/nvidiactl hooks: - args: - nvidia-ctk - hook - create-symlinks - --link - libglxserver_nvidia.so.525.147.05::/usr/lib/x86_64-linux-gnu/nvidia/xorg/libglxserver_nvidia.so hookName: createContainer path: /usr/bin/nvidia-ctk - args: - nvidia-ctk - hook - update-ldcache - --folder - /usr/lib/x86_64-linux-gnu hookName: createContainer path: /usr/bin/nvidia-ctk mounts: - containerPath: /run/nvidia-persistenced/socket hostPath: /run/nvidia-persistenced/socket options: - ro - nosuid - nodev - bind - noexec - containerPath: /usr/bin/nvidia-cuda-mps-control hostPath: /usr/bin/nvidia-cuda-mps-control options: - ro - nosuid - nodev - bind - containerPath: /usr/bin/nvidia-cuda-mps-server hostPath: /usr/bin/nvidia-cuda-mps-server options: - ro - nosuid - nodev - bind - containerPath: /usr/bin/nvidia-debugdump hostPath: /usr/bin/nvidia-debugdump options: - ro - nosuid - nodev - bind - containerPath: /usr/bin/nvidia-persistenced hostPath: /usr/bin/nvidia-persistenced options: - ro - nosuid - nodev - bind - containerPath: /usr/bin/nvidia-smi hostPath: /usr/bin/nvidia-smi options: - ro - nosuid - nodev - bind - containerPath: /usr/lib/x86_64-linux-gnu/libEGL_nvidia.so.525.147.05 hostPath: /usr/lib/x86_64-linux-gnu/libEGL_nvidia.so.525.147.05 options: - ro - nosuid - nodev - bind - containerPath: /usr/lib/x86_64-linux-gnu/libGLESv1_CM_nvidia.so.525.147.05 hostPath: /usr/lib/x86_64-linux-gnu/libGLESv1_CM_nvidia.so.525.147.05 options: - ro - nosuid - nodev - bind - containerPath: /usr/lib/x86_64-linux-gnu/libGLESv2_nvidia.so.525.147.05 hostPath: /usr/lib/x86_64-linux-gnu/libGLESv2_nvidia.so.525.147.05 options: - ro - nosuid - nodev - bind - containerPath: /usr/lib/x86_64-linux-gnu/libGLX_nvidia.so.525.147.05 hostPath: /usr/lib/x86_64-linux-gnu/libGLX_nvidia.so.525.147.05 options: - ro - nosuid - nodev - bind - containerPath: /usr/lib/x86_64-linux-gnu/libcuda.so.525.147.05 hostPath: /usr/lib/x86_64-linux-gnu/libcuda.so.525.147.05 options: - ro - nosuid - nodev - bind - containerPath: /usr/lib/x86_64-linux-gnu/libcudadebugger.so.525.147.05 hostPath: /usr/lib/x86_64-linux-gnu/libcudadebugger.so.525.147.05 options: - ro - nosuid - nodev - bind - containerPath: /usr/lib/x86_64-linux-gnu/libnvcuvid.so.525.147.05 hostPath: /usr/lib/x86_64-linux-gnu/libnvcuvid.so.525.147.05 options: - ro - nosuid - nodev - bind - containerPath: /usr/lib/x86_64-linux-gnu/libnvidia-allocator.so.525.147.05 hostPath: /usr/lib/x86_64-linux-gnu/libnvidia-allocator.so.525.147.05 options: - ro - nosuid - nodev - bind - containerPath: /usr/lib/x86_64-linux-gnu/libnvidia-cfg.so.525.147.05 hostPath: /usr/lib/x86_64-linux-gnu/libnvidia-cfg.so.525.147.05 options: - ro - nosuid - nodev - bind - containerPath: /usr/lib/x86_64-linux-gnu/libnvidia-compiler.so.525.147.05 hostPath: /usr/lib/x86_64-linux-gnu/libnvidia-compiler.so.525.147.05 options: - ro - nosuid - nodev - bind - containerPath: /usr/lib/x86_64-linux-gnu/libnvidia-egl-gbm.so.1.1.0 hostPath: /usr/lib/x86_64-linux-gnu/libnvidia-egl-gbm.so.1.1.0 options: - ro - nosuid - nodev - bind - containerPath: /usr/lib/x86_64-linux-gnu/libnvidia-eglcore.so.525.147.05 hostPath: /usr/lib/x86_64-linux-gnu/libnvidia-eglcore.so.525.147.05 options: - ro - nosuid - nodev - bind - containerPath: /usr/lib/x86_64-linux-gnu/libnvidia-encode.so.525.147.05 hostPath: /usr/lib/x86_64-linux-gnu/libnvidia-encode.so.525.147.05 options: - ro - nosuid - nodev - bind - containerPath: /usr/lib/x86_64-linux-gnu/libnvidia-fbc.so.525.147.05 hostPath: /usr/lib/x86_64-linux-gnu/libnvidia-fbc.so.525.147.05 options: - ro - nosuid - nodev - bind - containerPath: /usr/lib/x86_64-linux-gnu/libnvidia-glcore.so.525.147.05 hostPath: /usr/lib/x86_64-linux-gnu/libnvidia-glcore.so.525.147.05 options: - ro - nosuid - nodev - bind - containerPath: /usr/lib/x86_64-linux-gnu/libnvidia-glsi.so.525.147.05 hostPath: /usr/lib/x86_64-linux-gnu/libnvidia-glsi.so.525.147.05 options: - ro - nosuid - nodev - bind - containerPath: /usr/lib/x86_64-linux-gnu/libnvidia-glvkspirv.so.525.147.05 hostPath: /usr/lib/x86_64-linux-gnu/libnvidia-glvkspirv.so.525.147.05 options: - ro - nosuid - nodev - bind - containerPath: /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.525.147.05 hostPath: /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.525.147.05 options: - ro - nosuid - nodev - bind - containerPath: /usr/lib/x86_64-linux-gnu/libnvidia-ngx.so.525.147.05 hostPath: /usr/lib/x86_64-linux-gnu/libnvidia-ngx.so.525.147.05 options: - ro - nosuid - nodev - bind - containerPath: /usr/lib/x86_64-linux-gnu/libnvidia-nvvm.so.525.147.05 hostPath: /usr/lib/x86_64-linux-gnu/libnvidia-nvvm.so.525.147.05 options: - ro - nosuid - nodev - bind - containerPath: /usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.525.147.05 hostPath: /usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.525.147.05 options: - ro - nosuid - nodev - bind - containerPath: /usr/lib/x86_64-linux-gnu/libnvidia-opticalflow.so.525.147.05 hostPath: /usr/lib/x86_64-linux-gnu/libnvidia-opticalflow.so.525.147.05 options: - ro - nosuid - nodev - bind - containerPath: /usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.525.147.05 hostPath: /usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.525.147.05 options: - ro - nosuid - nodev - bind - containerPath: /usr/lib/x86_64-linux-gnu/libnvidia-rtcore.so.525.147.05 hostPath: /usr/lib/x86_64-linux-gnu/libnvidia-rtcore.so.525.147.05 options: - ro - nosuid - nodev - bind - containerPath: /usr/lib/x86_64-linux-gnu/libnvidia-tls.so.525.147.05 hostPath: /usr/lib/x86_64-linux-gnu/libnvidia-tls.so.525.147.05 options: - ro - nosuid - nodev - bind - containerPath: /usr/lib/x86_64-linux-gnu/libnvoptix.so.525.147.05 hostPath: /usr/lib/x86_64-linux-gnu/libnvoptix.so.525.147.05 options: - ro - nosuid - nodev - bind - containerPath: /lib/firmware/nvidia/525.147.05/gsp_ad10x.bin hostPath: /lib/firmware/nvidia/525.147.05/gsp_ad10x.bin options: - ro - nosuid - nodev - bind - containerPath: /lib/firmware/nvidia/525.147.05/gsp_tu10x.bin hostPath: /lib/firmware/nvidia/525.147.05/gsp_tu10x.bin options: - ro - nosuid - nodev - bind - containerPath: /usr/share/X11/xorg.conf.d/10-nvidia.conf hostPath: /usr/share/X11/xorg.conf.d/10-nvidia.conf options: - ro - nosuid - nodev - bind - containerPath: /usr/share/egl/egl_external_platform.d/15_nvidia_gbm.json hostPath: /usr/share/egl/egl_external_platform.d/15_nvidia_gbm.json options: - ro - nosuid - nodev - bind - containerPath: /usr/share/glvnd/egl_vendor.d/10_nvidia.json hostPath: /usr/share/glvnd/egl_vendor.d/10_nvidia.json options: - ro - nosuid - nodev - bind - containerPath: /usr/share/vulkan/icd.d/nvidia_icd.json hostPath: /usr/share/vulkan/icd.d/nvidia_icd.json options: - ro - nosuid - nodev - bind - containerPath: /usr/lib/x86_64-linux-gnu/nvidia/xorg/libglxserver_nvidia.so.525.147.05 hostPath: /usr/lib/x86_64-linux-gnu/nvidia/xorg/libglxserver_nvidia.so.525.147.05 options: - ro - nosuid - nodev - bind - containerPath: /usr/lib/x86_64-linux-gnu/nvidia/xorg/nvidia_drv.so hostPath: /usr/lib/x86_64-linux-gnu/nvidia/xorg/nvidia_drv.so options: - ro - nosuid - nodev - bind devices: - containerEdits: deviceNodes: - path: /dev/nvidia0 - path: /dev/dri/card0 - path: /dev/dri/renderD128 hooks: - args: - nvidia-ctk - hook - create-symlinks - --link - ../card0::/dev/dri/by-path/pci-0000:01:00.0-card - --link - ../renderD128::/dev/dri/by-path/pci-0000:01:00.0-render hookName: createContainer path: /usr/bin/nvidia-ctk - args: - nvidia-ctk - hook - chmod - --mode - \u0026quot;755\u0026quot; - --path - /dev/dri hookName: createContainer path: /usr/bin/nvidia-ctk name: \u0026quot;0\u0026quot; - containerEdits: deviceNodes: - path: /dev/nvidia0 - path: /dev/dri/card0 - path: /dev/dri/renderD128 hooks: - args: - nvidia-ctk - hook - create-symlinks - --link - ../card0::/dev/dri/by-path/pci-0000:01:00.0-card - --link - ../renderD128::/dev/dri/by-path/pci-0000:01:00.0-render hookName: createContainer path: /usr/bin/nvidia-ctk - args: - nvidia-ctk - hook - chmod - --mode - \u0026quot;755\u0026quot; - --path - /dev/dri hookName: createContainer path: /usr/bin/nvidia-ctk name: all kind: nvidia.com/gpu 可以看到 nvidia 的 cdi 配置文件比 mock 的要复杂很多，因为 nvidia 的 gpu 需要绑定很多文件到容器内，还有 hooks 等。这些工作之前都是在 runtime 中做的，现在都可以通过 cdi 插件来做了。\n","date":"2023-11-19","permalink":"https://daemon365.dev/2023/11/19/kubernetes-container-device-interface-cdi/","tags":["cdi","kubernetes"],"title":"kubernetes container device interface (CDI)"},{"content":"责任链模式 责任链模式是一种行为设计模式， 允许你将请求沿着处理者链进行发送。 收到请求后， 每个处理者均可对请求进行处理， 或将其传递给链上的下个处理者。比如 kratos,gin等开源库的中间件实现。\n代码实现 package main import ( \u0026quot;context\u0026quot; \u0026quot;fmt\u0026quot; ) type Handler func(ctx context.Context, req interface{}) (resp interface{}, err error) type Middleware func(next Handler) Handler func Chain(middlewares ...Middleware) Middleware { return func(next Handler) Handler { for i := len(middlewares) - 1; i \u0026gt;= 0; i-- { next = middlewares[i](next) } return next } } func main() { c := Chain(func(next Handler) Handler { return func(ctx context.Context, req interface{}) (resp interface{}, err error) { fmt.Println(\u0026quot;handler 1 before\u0026quot;) resp, err = next(ctx, req) fmt.Println(\u0026quot;handler 1 after\u0026quot;) return resp, err } }, func(next Handler) Handler { return func(ctx context.Context, req interface{}) (resp interface{}, err error) { fmt.Println(\u0026quot;handler 2 before\u0026quot;) resp, err = next(ctx, req) fmt.Println(\u0026quot;handler 2 after\u0026quot;) return resp, err } }) resp, err := c(func(ctx context.Context, req interface{}) (resp interface{}, err error) { fmt.Println(\u0026quot;handler req:\u0026quot;, req) return req, nil })(context.Background(), \u0026quot;hello\u0026quot;) fmt.Println(resp, err) } /* handler 1 before handler 2 before handler req: hello handler 2 after handler 1 after hello \u0026lt;nil\u0026gt; */ 观察者模式 观察者模式用于触发联动。一个对象的改变会触发其它观察者的相关动作，而此对象无需关心连动对象的具体实现。\n代码实现 package main import \u0026quot;fmt\u0026quot; type subject interface { register(Observer observer) deregister(Observer observer) notifyAll() } type observer interface { update(string) getID() string } type item struct { observerList []observer name string inStock bool } func newItem(name string) *item { return \u0026amp;item{ name: name, } } func (i *item) updateAvailability() { fmt.Printf(\u0026quot;Item %s is now in stock\\n\u0026quot;, i.name) i.inStock = true i.notifyAll() } func (i *item) register(o observer) { i.observerList = append(i.observerList, o) } func (i *item) deregister(o observer) { i.observerList = removeFromslice(i.observerList, o) } func (i *item) notifyAll() { for _, observer := range i.observerList { observer.update(i.name) } } func removeFromslice(observerList []observer, observerToRemove observer) []observer { observerListLength := len(observerList) for i, observer := range observerList { if observerToRemove.getID() == observer.getID() { observerList[observerListLength-1], observerList[i] = observerList[i], observerList[observerListLength-1] return observerList[:observerListLength-1] } } return observerList } type customer struct { id string } func (c *customer) update(itemName string) { fmt.Printf(\u0026quot;Sending email to customer %s for item %s\\n\u0026quot;, c.id, itemName) } func (c *customer) getID() string { return c.id } func main() { shirtItem := newItem(\u0026quot;Nike Shirt\u0026quot;) observerFirst := \u0026amp;customer{id: \u0026quot;abc@gmail.com\u0026quot;} observerSecond := \u0026amp;customer{id: \u0026quot;xyz@gmail.com\u0026quot;} shirtItem.register(observerFirst) shirtItem.register(observerSecond) shirtItem.updateAvailability() } /* Item Nike Shirt is now in stock Sending email to customer abc@gmail.com for item Nike Shirt Sending email to customer xyz@gmail.com for item Nike Shirt */ 模板方法模式 模版方法模式使用继承机制，把通用步骤和通用方法放到父类中，把具体实现延迟到子类中实现。使得实现符合开闭原则。\n如实例代码中通用步骤在父类中实现（准备、下载、保存、收尾）下载和保存的具体实现留到子类中，并且提供 保存方法的默认实现。\n因为Golang不提供继承机制，需要使用匿名组合模拟实现继承。\n此处需要注意：因为父类需要调用子类方法，所以子类需要匿名组合父类的同时，父类需要持有子类的引用。\n代码实现 package main import \u0026quot;fmt\u0026quot; type Downloader interface { Download(uri string) } type template struct { implement uri string } type implement interface { download() save() } func newTemplate(impl implement) *template { return \u0026amp;template{ implement: impl, } } func (t *template) Download(uri string) { t.uri = uri fmt.Print(\u0026quot;prepare downloading\\n\u0026quot;) t.implement.download() t.implement.save() fmt.Print(\u0026quot;finish downloading\\n\u0026quot;) } func (t *template) save() { fmt.Print(\u0026quot;default save\\n\u0026quot;) } type HTTPDownloader struct { *template } func NewHTTPDownloader() Downloader { downloader := \u0026amp;HTTPDownloader{} template := newTemplate(downloader) downloader.template = template return downloader } func (d *HTTPDownloader) download() { fmt.Printf(\u0026quot;download %s via http\\n\u0026quot;, d.uri) } func (*HTTPDownloader) save() { fmt.Printf(\u0026quot;http save\\n\u0026quot;) } type FTPDownloader struct { *template } func NewFTPDownloader() Downloader { downloader := \u0026amp;FTPDownloader{} template := newTemplate(downloader) downloader.template = template return downloader } func (d *FTPDownloader) download() { fmt.Printf(\u0026quot;download %s via ftp\\n\u0026quot;, d.uri) } func main() { downloader := NewHTTPDownloader() downloader.Download(\u0026quot;http://example.com/abc.zip\u0026quot;) downloader = NewFTPDownloader() downloader.Download(\u0026quot;ftp://example.com/abc.zip\u0026quot;) } /* prepare downloading download http://example.com/abc.zip via http http save finish downloading prepare downloading download ftp://example.com/abc.zip via ftp default save finish downloading */ 命令模式 命令模式是一种行为设计模式， 它可将请求转换为一个包含与请求相关的所有信息的独立对象。 该转换让你能根据不同的请求将方法参数化、 延迟请求执行或将其放入队列中， 且能实现可撤销操作。\n命令模式本质是把某个对象的方法调用封装到对象中，方便传递、存储、调用。\n示例中把主板单中的启动(start)方法和重启(reboot)方法封装为命令对象，再传递到主机(box)对象中。于两个按钮进行绑定：\n第一个机箱(box1)设置按钮1(button1) 为开机按钮2(button2)为重启。 第二个机箱(box1)设置按钮2(button2) 为开机按钮1(button1)为重启。 从而得到配置灵活性。\n除了配置灵活外，使用命令模式还可以用作：\n批处理 任务队列 undo, redo 等把具体命令封装到对象中使用的场合\n代码实现 package command import \u0026quot;fmt\u0026quot; type Command interface { Execute() } type StartCommand struct { mb *MotherBoard } func NewStartCommand(mb *MotherBoard) *StartCommand { return \u0026amp;StartCommand{ mb: mb, } } func (c *StartCommand) Execute() { c.mb.Start() } type RebootCommand struct { mb *MotherBoard } func NewRebootCommand(mb *MotherBoard) *RebootCommand { return \u0026amp;RebootCommand{ mb: mb, } } func (c *RebootCommand) Execute() { c.mb.Reboot() } type MotherBoard struct{} func (*MotherBoard) Start() { fmt.Print(\u0026quot;system starting\\n\u0026quot;) } func (*MotherBoard) Reboot() { fmt.Print(\u0026quot;system rebooting\\n\u0026quot;) } type Box struct { button1 Command button2 Command } func NewBox(button1, button2 Command) *Box { return \u0026amp;Box{ button1: button1, button2: button2, } } func (b *Box) PressButton1() { b.button1.Execute() } func (b *Box) PressButton2() { b.button2.Execute() } 策略模式 它能让你定义一系列算法， 并将每种算法分别放入独立的类中， 以使算法的对象能够相互替换。\n代码实现 package main import \u0026quot;fmt\u0026quot; type Payment struct { context *PaymentContext strategy PaymentStrategy } type PaymentContext struct { Name, CardID string Money int } func NewPayment(name, cardid string, money int, strategy PaymentStrategy) *Payment { return \u0026amp;Payment{ context: \u0026amp;PaymentContext{ Name: name, CardID: cardid, Money: money, }, strategy: strategy, } } func (p *Payment) Pay() { p.strategy.Pay(p.context) } type PaymentStrategy interface { Pay(*PaymentContext) } type Cash struct{} func (*Cash) Pay(ctx *PaymentContext) { fmt.Printf(\u0026quot;Pay $%d to %s by cash\\n\u0026quot;, ctx.Money, ctx.Name) } type Bank struct{} func (*Bank) Pay(ctx *PaymentContext) { fmt.Printf(\u0026quot;Pay $%d to %s by bank account %s\\n\u0026quot;, ctx.Money, ctx.Name, ctx.CardID) } func main() { payment := NewPayment(\u0026quot;Ada\u0026quot;, \u0026quot;\u0026quot;, 123, \u0026amp;Cash{}) payment.Pay() payment = NewPayment(\u0026quot;Bob\u0026quot;, \u0026quot;0002\u0026quot;, 888, \u0026amp;Bank{}) payment.Pay() } /* Pay $123 to Ada by cash Pay $888 to Bob by bank account 0002 */ 状态模式 让你能在一个对象的内部状态变化时改变其行为， 使其看上去就像改变了自身所属的类一样。\n代码实现 package main import ( \u0026quot;fmt\u0026quot; ) // Machine 状态机 type Machine struct { state IState } // SetState 更新状态 func (m *Machine) SetState(state IState) { m.state = state } // GetStateName 获取当前状态 func (m *Machine) GetStateName() string { return m.state.GetName() } func (m *Machine) Approval() { m.state.Approval(m) } func (m *Machine) Reject() { m.state.Reject(m) } // IState 状态 type IState interface { // 审批通过 Approval(m *Machine) // 驳回 Reject(m *Machine) // 获取当前状态名称 GetName() string } // leaderApproveState 直属领导审批 type leaderApproveState struct{} // Approval 获取状态名字 func (leaderApproveState) Approval(m *Machine) { fmt.Println(\u0026quot;leader 审批成功\u0026quot;) m.SetState(GetFinanceApproveState()) } // GetName 获取状态名字 func (leaderApproveState) GetName() string { return \u0026quot;LeaderApproveState\u0026quot; } // Reject 获取状态名字 func (leaderApproveState) Reject(m *Machine) {} func GetLeaderApproveState() IState { return \u0026amp;leaderApproveState{} } // financeApproveState 财务审批 type financeApproveState struct{} // Approval 审批通过 func (f financeApproveState) Approval(m *Machine) { fmt.Println(\u0026quot;财务审批成功\u0026quot;) fmt.Println(\u0026quot;出发打款操作\u0026quot;) } // 拒绝 func (f financeApproveState) Reject(m *Machine) { m.SetState(GetLeaderApproveState()) } // GetName 获取名字 func (f financeApproveState) GetName() string { return \u0026quot;FinanceApproveState\u0026quot; } // GetFinanceApproveState GetFinanceApproveState func GetFinanceApproveState() IState { return \u0026amp;financeApproveState{} } func main() { m := \u0026amp;Machine{state: GetLeaderApproveState()} fmt.Println(\u0026quot;LeaderApproveState\u0026quot;, m.GetStateName()) m.Approval() fmt.Println(\u0026quot;FinanceApproveState\u0026quot;, m.GetStateName()) m.Reject() fmt.Println(\u0026quot;LeaderApproveState\u0026quot;, m.GetStateName()) m.Approval() fmt.Println(\u0026quot;FinanceApproveState\u0026quot;, m.GetStateName()) m.Approval() } /* LeaderApproveState LeaderApproveState leader 审批成功 FinanceApproveState FinanceApproveState LeaderApproveState LeaderApproveState leader 审批成功 FinanceApproveState FinanceApproveState 财务审批成功 出发打款操作 */ 迭代器模式 让你能在不暴露集合底层表现形式 （列表、 栈和树等） 的情况下遍历集合中所有的元素。\n代码实现 package main import \u0026quot;fmt\u0026quot; type collection interface { createIterator() iterator } type userCollection struct { users []*user } func (u *userCollection) createIterator() iterator { return \u0026amp;userIterator{ users: u.users, } } type iterator interface { hasNext() bool getNext() *user } type userIterator struct { index int users []*user } func (u *userIterator) hasNext() bool { if u.index \u0026lt; len(u.users) { return true } return false } func (u *userIterator) getNext() *user { if u.hasNext() { user := u.users[u.index] u.index++ return user } return nil } type user struct { name string age int } func main() { user1 := \u0026amp;user{ name: \u0026quot;a\u0026quot;, age: 30, } user2 := \u0026amp;user{ name: \u0026quot;b\u0026quot;, age: 20, } userCollection := \u0026amp;userCollection{ users: []*user{user1, user2}, } iterator := userCollection.createIterator() for iterator.hasNext() { user := iterator.getNext() fmt.Printf(\u0026quot;User is %+v\\n\u0026quot;, user) } } /* User is \u0026amp;{name:a age:30} User is \u0026amp;{name:b age:20} */ 访问者模式 访问者模式可以给一系列对象透明的添加功能，并且把相关代码封装到一个类中。\n对象只要预留访问者接口Accept则后期为对象添加功能的时候就不需要改动对象。\n代码实现 package main import \u0026quot;fmt\u0026quot; type Customer interface { Accept(Visitor) } type Visitor interface { Visit(Customer) } type EnterpriseCustomer struct { name string } type CustomerCol struct { customers []Customer } func (c *CustomerCol) Add(customer Customer) { c.customers = append(c.customers, customer) } func (c *CustomerCol) Accept(visitor Visitor) { for _, customer := range c.customers { customer.Accept(visitor) } } func NewEnterpriseCustomer(name string) *EnterpriseCustomer { return \u0026amp;EnterpriseCustomer{ name: name, } } func (c *EnterpriseCustomer) Accept(visitor Visitor) { visitor.Visit(c) } type IndividualCustomer struct { name string } func NewIndividualCustomer(name string) *IndividualCustomer { return \u0026amp;IndividualCustomer{ name: name, } } func (c *IndividualCustomer) Accept(visitor Visitor) { visitor.Visit(c) } type ServiceRequestVisitor struct{} func (*ServiceRequestVisitor) Visit(customer Customer) { switch c := customer.(type) { case *EnterpriseCustomer: fmt.Printf(\u0026quot;serving enterprise customer %s\\n\u0026quot;, c.name) case *IndividualCustomer: fmt.Printf(\u0026quot;serving individual customer %s\\n\u0026quot;, c.name) } } // only for enterprise type AnalysisVisitor struct{} func (*AnalysisVisitor) Visit(customer Customer) { switch c := customer.(type) { case *EnterpriseCustomer: fmt.Printf(\u0026quot;analysis enterprise customer %s\\n\u0026quot;, c.name) } } func main() { c := \u0026amp;CustomerCol{} c.Add(NewEnterpriseCustomer(\u0026quot;A company\u0026quot;)) c.Add(NewEnterpriseCustomer(\u0026quot;B company\u0026quot;)) c.Add(NewIndividualCustomer(\u0026quot;bob\u0026quot;)) c.Accept(\u0026amp;ServiceRequestVisitor{}) c = \u0026amp;CustomerCol{} c.Add(NewEnterpriseCustomer(\u0026quot;A company\u0026quot;)) c.Add(NewIndividualCustomer(\u0026quot;bob\u0026quot;)) c.Add(NewEnterpriseCustomer(\u0026quot;B company\u0026quot;)) c.Accept(\u0026amp;AnalysisVisitor{}) } /* serving enterprise customer A company serving enterprise customer B company serving individual customer bob analysis enterprise customer A company analysis enterprise customer B company */ 备忘录模式 备忘录模式用于保存程序内部状态到外部，又不希望暴露内部状态的情形。\n程序内部状态使用窄接口传递给外部进行存储，从而不暴露程序实现细节。\n备忘录模式同时可以离线保存内部状态，如保存到数据库，文件等。\n代码实现 package main import \u0026quot;fmt\u0026quot; type Memento interface{} type Game struct { hp, mp int } type gameMemento struct { hp, mp int } func (g *Game) Play(mpDelta, hpDelta int) { g.mp += mpDelta g.hp += hpDelta } func (g *Game) Save() Memento { return \u0026amp;gameMemento{ hp: g.hp, mp: g.mp, } } func (g *Game) Load(m Memento) { gm := m.(*gameMemento) g.mp = gm.mp g.hp = gm.hp } func (g *Game) Status() { fmt.Printf(\u0026quot;Current HP:%d, MP:%d\\n\u0026quot;, g.hp, g.mp) } func main() { game := \u0026amp;Game{ hp: 10, mp: 10, } game.Status() progress := game.Save() game.Play(-2, -3) game.Status() game.Load(progress) game.Status() } /* Current HP:10, MP:10 Current HP:7, MP:8 Current HP:10, MP:10 */ 解释器模式 解释器模式定义一套语言文法，并设计该语言解释器，使用户能使用特定文法控制解释器行为。\n解释器模式的意义在于，它分离多种复杂功能的实现，每个功能只需关注自身的解释。\n对于调用者不用关心内部的解释器的工作，只需要用简单的方式组合命令就可以。\n代码实现 package main import ( \u0026quot;fmt\u0026quot; \u0026quot;strconv\u0026quot; \u0026quot;strings\u0026quot; ) type Node interface { Interpret() int } type ValNode struct { val int } func (n *ValNode) Interpret() int { return n.val } type AddNode struct { left, right Node } func (n *AddNode) Interpret() int { return n.left.Interpret() + n.right.Interpret() } type MinNode struct { left, right Node } func (n *MinNode) Interpret() int { return n.left.Interpret() - n.right.Interpret() } type Parser struct { exp []string index int prev Node } func (p *Parser) Parse(exp string) { p.exp = strings.Split(exp, \u0026quot; \u0026quot;) for { if p.index \u0026gt;= len(p.exp) { return } switch p.exp[p.index] { case \u0026quot;+\u0026quot;: p.prev = p.newAddNode() case \u0026quot;-\u0026quot;: p.prev = p.newMinNode() default: p.prev = p.newValNode() } } } func (p *Parser) newAddNode() Node { p.index++ return \u0026amp;AddNode{ left: p.prev, right: p.newValNode(), } } func (p *Parser) newMinNode() Node { p.index++ return \u0026amp;MinNode{ left: p.prev, right: p.newValNode(), } } func (p *Parser) newValNode() Node { v, _ := strconv.Atoi(p.exp[p.index]) p.index++ return \u0026amp;ValNode{ val: v, } } func (p *Parser) Result() Node { return p.prev } func main() { p := \u0026amp;Parser{} p.Parse(\u0026quot;1 + 2 + 3 - 4 + 5 - 6\u0026quot;) res := p.Result().Interpret() expect := 1 if res != expect { fmt.Println(res,expect) } } 中介模式 中介者模式封装对象之间互交，使依赖变的简单，并且使复杂互交简单化，封装在中介者中。\n例子中的中介者使用单例模式生成中介者。\n中介者的change使用switch判断类型。\n代码实现 package main import ( \u0026quot;fmt\u0026quot; \u0026quot;strings\u0026quot; ) type CDDriver struct { Data string } func (c *CDDriver) ReadData() { c.Data = \u0026quot;music,image\u0026quot; fmt.Printf(\u0026quot;CDDriver: reading data %s\\n\u0026quot;, c.Data) GetMediatorInstance().changed(c) } type CPU struct { Video string Sound string } func (c *CPU) Process(data string) { sp := strings.Split(data, \u0026quot;,\u0026quot;) c.Sound = sp[0] c.Video = sp[1] fmt.Printf(\u0026quot;CPU: split data with Sound %s, Video %s\\n\u0026quot;, c.Sound, c.Video) GetMediatorInstance().changed(c) } type VideoCard struct { Data string } func (v *VideoCard) Display(data string) { v.Data = data fmt.Printf(\u0026quot;VideoCard: display %s\\n\u0026quot;, v.Data) GetMediatorInstance().changed(v) } type SoundCard struct { Data string } func (s *SoundCard) Play(data string) { s.Data = data fmt.Printf(\u0026quot;SoundCard: play %s\\n\u0026quot;, s.Data) GetMediatorInstance().changed(s) } type Mediator struct { CD *CDDriver CPU *CPU Video *VideoCard Sound *SoundCard } var mediator *Mediator func GetMediatorInstance() *Mediator { if mediator == nil { mediator = \u0026amp;Mediator{} } return mediator } func (m *Mediator) changed(i interface{}) { switch inst := i.(type) { case *CDDriver: m.CPU.Process(inst.Data) case *CPU: m.Sound.Play(inst.Sound) m.Video.Display(inst.Video) } } func main() { mediator := GetMediatorInstance() mediator.CD = \u0026amp;CDDriver{} mediator.CPU = \u0026amp;CPU{} mediator.Video = \u0026amp;VideoCard{} mediator.Sound = \u0026amp;SoundCard{} //Tiggle mediator.CD.ReadData() fmt.Printf(\u0026quot;%#v\\n\u0026quot;, mediator) } /* CDDriver: reading data music,image CPU: split data with Sound music, Video image SoundCard: play music VideoCard: display image \u0026amp;main.Mediator{CD:(*main.CDDriver)(0xc000010250), CPU:(*main.CPU)(0xc000060040), Video:(*main.VideoCard)(0xc000010260), Sound:(*main.SoundCard)(0xc000010270)} */ References https://github.com/senghoo/golang-design-pattern\nhttps://refactoringguru.cn/design-patterns\nhttps://lailin.xyz/post/singleton.html\n","date":"2023-09-25","permalink":"https://daemon365.dev/2023/09/25/%E8%A1%8C%E4%B8%BA%E6%A8%A1%E5%BC%8F/","tags":["go","设计模式"],"title":"行为模式"},{"content":"适配器模式 适配器模式用于转换一种接口适配另一种接口。比如，现在有个借口是对json字符串进行分析等，现在有一些yaml文件也要分析，这时候我我们就应该给yaml字符串就个适配器，转换成json字符串，然后就行分析。\n代码实现 package main import ( \u0026quot;fmt\u0026quot; \u0026quot;github.com/ghodss/yaml\u0026quot; ) type Analysis interface { Analyze(string) error } type JsonAnalysis struct{} func (*JsonAnalysis) Analyze(jsonStr string) error { // 函数逻辑 fmt.Println(jsonStr) return nil } type yamlAnalysis struct { ja *JsonAnalysis } func (y *yamlAnalysis) Analyze(yamlStr string) error { bs, err := yaml.YAMLToJSON([]byte(yamlStr)) if err != nil { return err } return y.ja.Analyze(string(bs)) } func main() { ja := \u0026amp;JsonAnalysis{} err := ja.Analyze(\u0026quot;{\\\u0026quot;name\\\u0026quot;:\\\u0026quot;zhy\\\u0026quot;,\\\u0026quot;age\\\u0026quot;:18}\u0026quot;) if err != nil { fmt.Println(err) } ya := \u0026amp;yamlAnalysis{ja: ja} err = ya.Analyze(\u0026quot;name: you\\nage: 88\u0026quot;) if err != nil { fmt.Println(err) } } /* {\u0026quot;name\u0026quot;:\u0026quot;zhy\u0026quot;,\u0026quot;age\u0026quot;:18} {\u0026quot;age\u0026quot;:88,\u0026quot;name\u0026quot;:\u0026quot;you\u0026quot;} */ 桥接模式 桥接模式分离抽象部分和实现部分。使得两部分独立扩展。\n桥接模式类似于策略模式，区别在于策略模式封装一系列算法使得算法可以互相替换。\n策略模式使抽象部分和实现部分分离，可以独立变化。\n比如要发消息，可以发很多种方式，微信、qq、email、短信等等，也可以发很多类型，日常、紧急等的。这时我们可以把发送的的方法做抽象，把类型做实现。\n代码实现 package main import \u0026quot;fmt\u0026quot; type Message interface { SendMessage(s string) error } type MessageMethod interface { Send(string) error } type qq struct{} func (*qq) Send(s string) error { fmt.Println(\u0026quot;send qq:\u0026quot;, s) return nil } type weixin struct{} func (*weixin) Send(s string) error { fmt.Println(\u0026quot;send weixin:\u0026quot;, s) return nil } type email struct{} func (*email) Send(s string) error { fmt.Println(\u0026quot;send email:\u0026quot;, s) return nil } type InfoMessage struct { method MessageMethod } func (i *InfoMessage) SendMessage(s string) error { s = \u0026quot;info message: \u0026quot; + s return i.method.Send(s) } type UrgencyMessage struct { method MessageMethod } func (u *UrgencyMessage) SendMessage(s string) error { s = \u0026quot;urgency message: \u0026quot; + s return u.method.Send(s) } func main() { qq := new(qq) weixin := new(weixin) email := new(email) info := new(InfoMessage) info.method = qq info.SendMessage(\u0026quot;hello\u0026quot;) info.method = weixin info.SendMessage(\u0026quot;hello\u0026quot;) info.method = email info.SendMessage(\u0026quot;hello\u0026quot;) urgency := new(UrgencyMessage) urgency.method = qq urgency.SendMessage(\u0026quot;hello\u0026quot;) urgency.method = weixin urgency.SendMessage(\u0026quot;hello\u0026quot;) urgency.method = email urgency.SendMessage(\u0026quot;hello\u0026quot;) } /* send qq: info message: hello send weixin: info message: hello send email: info message: hello send qq: urgency message: hello send weixin: urgency message: hello send email: urgency message: hello */ 装饰器模式 装饰模式使用对象组合的方式动态改变或增加对象行为。Go语言借助于匿名组合和非入侵式接口可以很方便实现装饰模式。使用匿名组合，在装饰器中不必显式定义转调原对象方法。\n代码实现 package main import \u0026quot;fmt\u0026quot; type Component interface { Calc() int } type ConcreteComponent struct{} func (*ConcreteComponent) Calc() int { return 10 } type MulDecorator struct { Component num int } func WarpMulDecorator(c Component, num int) Component { return \u0026amp;MulDecorator{ Component: c, num: num, } } func (d *MulDecorator) Calc() int { return d.Component.Calc() * d.num } type AddDecorator struct { Component num int } func WarpAddDecorator(c Component, num int) Component { return \u0026amp;AddDecorator{ Component: c, num: num, } } func (d *AddDecorator) Calc() int { return d.Component.Calc() + d.num } func main() { c := \u0026amp;ConcreteComponent{} md := WarpMulDecorator(c, 2) ad := WarpAddDecorator(c, 3) fmt.Println(md.Calc()) fmt.Println(ad.Calc()) } /* 20 13 */ 代理模式 代理模式用于延迟处理操作或者在进行实际操作前后进行其它处理。比如在限流中间件中\n代码实现 package main import ( \u0026quot;errors\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;sync\u0026quot; \u0026quot;time\u0026quot; ) type Serve interface { handle(name string) (string, error) } type server struct { lock sync.RWMutex serve Serve maxAllowedRequest int rateLimiter map[string]int } func (s *server) getService(name string) (string, error) { s.lock.RLock() nowNumber := s.rateLimiter[name] s.lock.RUnlock() if nowNumber \u0026gt;= s.maxAllowedRequest { return \u0026quot;\u0026quot;, errors.New(\u0026quot;rate limit\u0026quot;) } // 更新计数器 s.lock.Lock() s.rateLimiter[name]++ s.lock.Unlock() str, err := s.serve.handle(name) // 执行后减少计数器 s.lock.Lock() s.rateLimiter[name]-- s.lock.Unlock() if err != nil { return \u0026quot;\u0026quot;, err } return str, nil } type hand struct{} func (h *hand) handle(name string) (string, error) { time.Sleep(time.Microsecond * 500) return fmt.Sprintf(\u0026quot;hello %s\u0026quot;, name), nil } func main() { wg := \u0026amp;sync.WaitGroup{} wg.Add(20) s := \u0026amp;server{ serve: \u0026amp;hand{}, maxAllowedRequest: 10, rateLimiter: map[string]int{}, } for i := 0; i \u0026lt; 20; i++ { go func(i int) { defer wg.Done() res, err := s.getService(\u0026quot;world\u0026quot;) if err != nil { fmt.Println(i, \u0026quot;error:\u0026quot;, err) } else { fmt.Println(i, \u0026quot;success:\u0026quot;, res) } }(i) } wg.Wait() } /* 15 error: rate limit 18 error: rate limit 10 error: rate limit 4 error: rate limit 7 error: rate limit 6 error: rate limit 16 error: rate limit 9 error: rate limit 11 error: rate limit 17 error: rate limit 14 success: hello world 19 success: hello world 13 success: hello world 2 success: hello world 12 success: hello world 1 success: hello world 3 success: hello world 8 success: hello world 5 success: hello world 0 success: hello world */ 组合模式 组合模式统一对象和对象集，使得使用相同接口使用对象和对象集。\n组合模式常用于树状结构，用于统一叶子节点和树节点的访问，并且可以用于应用某一操作到所有子节点。\n比如要搜索文件夹下的所有文件名\n代码实现 package main import \u0026quot;fmt\u0026quot; type component interface { search(string) } type file struct { name string } func (f *file) search(keyword string) { fmt.Printf(\u0026quot;Searching for keyword %s in file %s\\n\u0026quot;, keyword, f.name) } func (f *file) getName() string { return f.name } type folder struct { components []component name string } func (f *folder) search(keyword string) { fmt.Printf(\u0026quot;Serching recursively for keyword %s in folder %s\\n\u0026quot;, keyword, f.name) for _, composite := range f.components { composite.search(keyword) } } func (f *folder) add(c component) { f.components = append(f.components, c) } func main() { file1 := \u0026amp;file{name: \u0026quot;File1\u0026quot;} file2 := \u0026amp;file{name: \u0026quot;File2\u0026quot;} file3 := \u0026amp;file{name: \u0026quot;File3\u0026quot;} folder1 := \u0026amp;folder{ name: \u0026quot;Folder1\u0026quot;, } folder1.add(file1) folder2 := \u0026amp;folder{ name: \u0026quot;Folder2\u0026quot;, } folder2.add(file2) folder2.add(file3) folder2.add(folder1) folder2.search(\u0026quot;rose\u0026quot;) } /* Serching recursively for keyword rose in folder Folder2 Searching for keyword rose in file File2 Searching for keyword rose in file File3 Serching recursively for keyword rose in folder Folder1 Searching for keyword rose in file File1 */ 外观模式 API 为facade 模块的外观接口，大部分代码使用此接口简化对facade类的访问。\nfacade模块同时暴露了a和b 两个Module 的NewXXX和interface，其它代码如果需要使用细节功能时可以直接调用。\n代码实现 package main import \u0026quot;fmt\u0026quot; func NewAPI() API { return \u0026amp;apiImpl{ a: NewAModuleAPI(), b: NewBModuleAPI(), } } //API is facade interface of facade package type API interface { Test() string } //facade implement type apiImpl struct { a AModuleAPI b BModuleAPI } func (a *apiImpl) Test() string { aRet := a.a.TestA() bRet := a.b.TestB() return fmt.Sprintf(\u0026quot;%s\\n%s\u0026quot;, aRet, bRet) } //NewAModuleAPI return new AModuleAPI func NewAModuleAPI() AModuleAPI { return \u0026amp;aModuleImpl{} } //AModuleAPI ... type AModuleAPI interface { TestA() string } type aModuleImpl struct{} func (*aModuleImpl) TestA() string { return \u0026quot;A module running\u0026quot; } //NewBModuleAPI return new BModuleAPI func NewBModuleAPI() BModuleAPI { return \u0026amp;bModuleImpl{} } //BModuleAPI ... type BModuleAPI interface { TestB() string } type bModuleImpl struct{} func (*bModuleImpl) TestB() string { return \u0026quot;B module running\u0026quot; } func main() { api := NewAPI() fmt.Println(api.Test()) } /* A module running B module running */ 享元模式 享元模式从对象中剥离出不发生改变且多个实例需要的重复数据，独立出一个享元，使多个对象共享，从而节省内存以及减少对象数量。\n代码实现 package main import \u0026quot;fmt\u0026quot; type ImageFlyweightFactory struct { maps map[string]*ImageFlyweight } var imageFactory *ImageFlyweightFactory func GetImageFlyweightFactory() *ImageFlyweightFactory { if imageFactory == nil { imageFactory = \u0026amp;ImageFlyweightFactory{ maps: make(map[string]*ImageFlyweight), } } return imageFactory } func (f *ImageFlyweightFactory) Get(filename string) *ImageFlyweight { image := f.maps[filename] if image == nil { image = NewImageFlyweight(filename) f.maps[filename] = image } return image } type ImageFlyweight struct { data string } func NewImageFlyweight(filename string) *ImageFlyweight { // Load image file data := fmt.Sprintf(\u0026quot;image data %s\u0026quot;, filename) return \u0026amp;ImageFlyweight{ data: data, } } func (i *ImageFlyweight) Data() string { return i.data } type ImageViewer struct { *ImageFlyweight } func NewImageViewer(filename string) *ImageViewer { image := GetImageFlyweightFactory().Get(filename) return \u0026amp;ImageViewer{ ImageFlyweight: image, } } func (i *ImageViewer) Display() { fmt.Printf(\u0026quot;Display: %s\\n\u0026quot;, i.Data()) } References https://github.com/senghoo/golang-design-pattern\nhttps://refactoringguru.cn/design-patterns\nhttps://lailin.xyz/post/singleton.html\n","date":"2023-09-24","permalink":"https://daemon365.dev/2023/09/24/%E7%BB%93%E6%9E%84%E5%9E%8B%E6%A8%A1%E5%BC%8F/","tags":["go","设计模式"],"title":"结构型模式"},{"content":"单例模式 为什么要用单例模式 保证一个对象只有一个实例 ，减少内存开销。比如一些可以复用一个连接的网络，比如http2 client等，而且可以减少网络开销。\n为什么不用个全局变量控制 因为任何代码都有可能覆盖掉那些变量的内容， 从而引发程序崩溃。\n代码实现 package main import ( \u0026quot;fmt\u0026quot; \u0026quot;sync\u0026quot; ) type Single struct { } var single *Single var once = \u0026amp;sync.Once{} func NewSingle() *Single { once.Do(func() { single = \u0026amp;Single{ // 初始化 } }) return single } func main() { for i := 0; i \u0026lt; 1000; i++ { s := NewSingle() fmt.Printf(\u0026quot;create %d,address %p\\n\u0026quot;, i, s) } } /* 结果： create 0,address 0x1164fe0 create 1,address 0x1164fe0 create 2,address 0x1164fe0 create 3,address 0x1164fe0 create 4,address 0x1164fe0 create 5,address 0x1164fe0 create 6,address 0x1164fe0 create 7,address 0x1164fe0 create 8,address 0x1164fe0 ... */ 工厂模式 我们在创建对象时不会对客户端暴露创建逻辑，并且是通过使用一个共同的接口来指向新创建的对象。比如电脑支持intel cpu，现在要支持amd cpu，我们就可以让所有cpu实现接口。\n简单工厂模式 实现简单，不适合复杂场景\npackage main import ( \u0026quot;errors\u0026quot; \u0026quot;fmt\u0026quot; ) // 工厂接口 type CpuFactory interface { Run() string } type IntelCpu struct{} func (*IntelCpu) Run() string { return \u0026quot;intel cpu is running\u0026quot; } type AmdCpu struct{} func (*AmdCpu) Run() string { return \u0026quot;amd cpu is running\u0026quot; } func NewCpu(name string) (CpuFactory, error) { switch name { case \u0026quot;intel\u0026quot;: return \u0026amp;IntelCpu{}, nil case \u0026quot;amd\u0026quot;: return \u0026amp;AmdCpu{}, nil default: return nil, errors.New(\u0026quot;no such cpu\u0026quot;) } } func main() { c1, err := NewCpu(\u0026quot;intel\u0026quot;) if err != nil { panic(err) } fmt.Println(c1.Run()) c2, err := NewCpu(\u0026quot;amd\u0026quot;) if err != nil { panic(err) } fmt.Println(c2.Run()) _, err = NewCpu(\u0026quot;other\u0026quot;) fmt.Println(err) } /* 结果： intel cpu is running amd cpu is running no such cpu */ 工厂方法模式 大部分时候，我们创建对象要创建很多逻辑，比如初始化变量，从远端请求config等等。这是我们需要每次struct提供个创建方法。\npackage main import ( \u0026quot;errors\u0026quot; \u0026quot;fmt\u0026quot; ) // 工厂接口 type CpuFactory interface { Run() string } type IntelCpu struct{} func (*IntelCpu) Run() string { return \u0026quot;intel cpu is running\u0026quot; } func NewIntelCpu() *IntelCpu { // 做创建逻辑 return \u0026amp;IntelCpu{} } type AmdCpu struct{} func (*AmdCpu) Run() string { return \u0026quot;amd cpu is running\u0026quot; } func NewAmdCpu() *AmdCpu { // 做创建逻辑 return \u0026amp;AmdCpu{} } func NewCpu(name string) (CpuFactory, error) { switch name { case \u0026quot;intel\u0026quot;: return NewIntelCpu(), nil case \u0026quot;amd\u0026quot;: return NewAmdCpu(), nil default: return nil, errors.New(\u0026quot;no such cpu\u0026quot;) } } func main() { c1, err := NewCpu(\u0026quot;intel\u0026quot;) if err != nil { panic(err) } fmt.Println(c1.Run()) c2, err := NewCpu(\u0026quot;amd\u0026quot;) if err != nil { panic(err) } fmt.Println(c2.Run()) _, err = NewCpu(\u0026quot;other\u0026quot;) fmt.Println(err) } /* 结果： intel cpu is running amd cpu is running no such cpu */ 抽象工厂模式 抽象工厂模式则是针对的多个产品等级结构， 我们可以将一种产品等级想象为一个产品族，所谓的产品族，是指位于不同产品等级结构中功能相关联的产品组成的家族。用于复杂场景，比如amd和intel都生成cpu和gpu\npackage main import \u0026quot;fmt\u0026quot; // 抽象工厂接口 type ElementAbstractFactory interface { CreateCpu() Cpu // cpu CreateGpu() Gpu // Gpu } func GetElementAbstractFactory(brand string) (ElementAbstractFactory, error) { if brand == \u0026quot;intel\u0026quot; { return \u0026amp;intel{}, nil } if brand == \u0026quot;amd\u0026quot; { return \u0026amp;amd{}, nil } return nil, fmt.Errorf(\u0026quot;Wrong brand type passed\u0026quot;) } // cpu 具体工厂 type Cpu interface { Run() string } type Gpu interface { Graphics() string } type intel struct{} func (*intel) CreateCpu() Cpu { return \u0026amp;intelCpu{} } func (*intel) CreateGpu() Gpu { return \u0026amp;intelGpu{} } type intelCpu struct{} func (*intelCpu) Run() string { return \u0026quot;intel cpu is running\u0026quot; } type intelGpu struct{} func (*intelGpu) Graphics() string { return \u0026quot;intel gpu is working on graphics\u0026quot; } type amd struct{} func (*amd) CreateCpu() Cpu { return \u0026amp;amdCpu{} } func (*amd) CreateGpu() Gpu { return \u0026amp;amdGpu{} } type amdCpu struct{} func (*amdCpu) Run() string { return \u0026quot;amd cpu is running\u0026quot; } type amdGpu struct{} func (*amdGpu) Graphics() string { return \u0026quot;amd gpu is working on graphics\u0026quot; } func main() { e, _ := GetElementAbstractFactory(\u0026quot;intel\u0026quot;) cpu := e.CreateCpu() gpu := e.CreateGpu() fmt.Println(cpu.Run()) fmt.Println(gpu.Graphics()) e2, _ := GetElementAbstractFactory(\u0026quot;amd\u0026quot;) cpu2 := e2.CreateCpu() gpu2 := e2.CreateGpu() fmt.Println(cpu2.Run()) fmt.Println(gpu2.Graphics()) } /* intel cpu is running intel gpu is working on graphics amd cpu is running amd gpu is working on graphics */ 生成器模式 在对其进行构造时需要对诸多成员变量和嵌套对象进行繁复的初始化工作。 这些初始化代码通常深藏于一个包含众多参数且让人基本看不懂的构造函数中； 甚至还有更糟糕的情况， 那就是这些代码散落在客户端代码的多个位置。比如在go中，就可以利用指针传递完成初始化。\npackage main import \u0026quot;fmt\u0026quot; // Computer 是生成器接口 type Computer interface { Cpu() Gpu() } type Director struct { builder Computer } // NewDirector ... func NewDirector(builder Computer) *Director { return \u0026amp;Director{ builder: builder, } } // Construct Product func (d *Director) Construct() { d.builder.Cpu() d.builder.Gpu() } type Computer1 struct { cpu string gpu string } func (c *Computer1) Cpu() { c.cpu = \u0026quot;intel\u0026quot; } func (c *Computer1) Gpu() { c.gpu = \u0026quot;nvida\u0026quot; } type Computer2 struct { cpu string gpu string } func (c *Computer2) Cpu() { c.cpu = \u0026quot;amd\u0026quot; } func (c *Computer2) Gpu() { c.gpu = \u0026quot;amd\u0026quot; } func main() { c1 := Computer1{} d := NewDirector(\u0026amp;c1) fmt.Printf(\u0026quot;%+v\\n\u0026quot;, c1) d.Construct() fmt.Printf(\u0026quot;%+v\\n\u0026quot;, c1) c2 := Computer2{} d2 := NewDirector(\u0026amp;c2) fmt.Printf(\u0026quot;%+v\\n\u0026quot;, c2) d2.Construct() fmt.Printf(\u0026quot;%+v\\n\u0026quot;, c2) } /* {cpu: gpu:} {cpu:intel gpu:nvida} {cpu: gpu:} {cpu:amd gpu:amd} */ 原型模式 原型模式使对象能复制自身，并且暴露到接口中，使客户端面向接口编程时，不知道接口实际对象的情况下生成新的对象。\n原型模式配合原型管理器使用，使得客户端在不知道具体类的情况下，通过接口管理器得到新的实例，并且包含部分预设定配置。\npackage main //Cloneable 是原型对象需要实现的接口 type Cloneable interface { Clone() Cloneable } type PrototypeManager struct { prototypes map[string]Cloneable } func NewPrototypeManager() *PrototypeManager { return \u0026amp;PrototypeManager{ prototypes: make(map[string]Cloneable), } } func (p *PrototypeManager) Get(name string) Cloneable { return p.prototypes[name].Clone() } func (p *PrototypeManager) Set(name string, prototype Cloneable) { p.prototypes[name] = prototype } References https://github.com/senghoo/golang-design-pattern\nhttps://refactoringguru.cn/design-patterns\nhttps://lailin.xyz/post/singleton.html\n","date":"2023-09-23","permalink":"https://daemon365.dev/2023/09/23/%E5%88%9B%E5%BB%BA%E8%80%85%E6%A8%A1%E5%BC%8F/","tags":["go","设计模式"],"title":"创建者模式"},{"content":"redis主从同步 原理：\n从服务器向主服务器发送 SYNC 命令。\n接到 SYNC 命令的主服务器会调用BGSAVE 命令，创建一个 RDB 文件，并使用缓冲区记录接下来执行的所有写命令。\n当主服务器执行完 BGSAVE 命令时，它会向从服务器发送 RDB 文件，而从服务器则会接收并载入这个文件。\n主服务器将缓冲区储存的所有写命令发送给从服务器执行。\n\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- 1、在开启主从复制的时候，使用的是RDB方式的，同步主从数据的 2、同步开始之后，通过主库命令传播的方式，主动的复制方式实现 3、2.8以后实现PSYNC的机制，实现断线重连\n环境准备 6380.conf\n1、环境： 准备两个或两个以上redis实例 mkdir /data/638{0..2} #创建6380 6381 6382文件夹 配置文件示例： vim /data/6380/redis.conf port 6380 daemonize yes pidfile /data/6380/redis.pid loglevel notice logfile \u0026quot;/data/6380/redis.log\u0026quot; dbfilename dump.rdb dir /data/6380 protected-mode no 6381.conf\nvim /data/6381/redis.conf port 6381 daemonize yes pidfile /data/6381/redis.pid loglevel notice logfile \u0026quot;/data/6381/redis.log\u0026quot; dbfilename dump.rdb dir /data/6381 protected-mode no 6382.conf\nport 6382 daemonize yes pidfile /data/6382/redis.pid loglevel notice logfile \u0026quot;/data/6382/redis.log\u0026quot; dbfilename dump.rdb dir /data/6382 protected-mode no 启动三个redis实例\nredis-server /data/6380/redis.conf redis-server /data/6381/redis.conf redis-server /data/6382/redis.conf 主从规划\n主节点：6380 从节点：6381、6382 配置主从同步 6381/6382命令行\nredis-cli -p 6381 SLAVEOF 127.0.0.1 6380 #指明主的地址\nredis-cli -p 6382 SLAVEOF 127.0.0.1 6380 #指明主的地址\n检查主从状态\n从库：\n127.0.0.1:6382\u0026gt; info replication 127.0.0.1:6381\u0026gt; info replication 主库：\n127.0.0.1:6380\u0026gt; info replication 测试写入数据，主库写入数据，检查从库数据 主 127.0.0.1:6380\u0026gt; set name chaoge 从 127.0.0.1:6381\u0026gt;get name 手动进行主从复制故障切换 #关闭主库6380redis-cli -p 6380 shutdown 检查从库主从信息，此时master_link_status:down\nredis-cli -p 6381 info replication redis-cli -p 6382 info replication 既然主库挂了，我想要在6381 6382之间选一个新的主库\n1.关闭6381的从库身份\nredis-cli -p 6381 info replication slaveof no one 2.将6382设为6381的从库\n6382连接到6381： [root@db03 ~]## redis-cli -p 6382 127.0.0.1:6382\u0026gt; SLAVEOF no one 127.0.0.1:6382\u0026gt; SLAVEOF 127.0.0.1 6381 3.检查6382，6381的主从信息\n","date":"2023-08-20","permalink":"https://daemon365.dev/2023/08/20/redis%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5/","tags":["redis"],"title":"redis主从同步"},{"content":"概念 kratos 为了使http协议的逻辑代码和grpc的逻辑代码使用同一份，选择了基于protobuf的IDL文件使用proto插件生成辅助代码的方式。\nprotoc http插件的地址为：https://github.com/go-kratos/kratos/tree/main/cmd/protoc-gen-go-http\n示例 syntax = \u0026quot;proto3\u0026quot;; package helloworld; option go_package = \u0026quot;test/helloworld;helloworld\u0026quot;; option java_multiple_files = true; option java_package = \u0026quot;helloworld\u0026quot;; import \u0026quot;google/api/annotations.proto\u0026quot;; service Greeter { rpc SayHello (HelloRequest) returns (HelloReply) { option (google.api.http) = { post: \u0026quot;/helloworld\u0026quot;, // 声明路由 body: \u0026quot;*\u0026quot; }; } } message HelloRequest { string name = 1; } message HelloReply { string msg = 1; } 使用kratos proto client xxx 生成的代码为：\n// Code generated by protoc-gen-go-http. DO NOT EDIT. // versions: // - protoc-gen-go-http v2.4.0 // - protoc v3.19.4 // source: helloworld/helloworld.proto package helloworld import ( context \u0026quot;context\u0026quot; http \u0026quot;github.com/go-kratos/kratos/v2/transport/http\u0026quot; binding \u0026quot;github.com/go-kratos/kratos/v2/transport/http/binding\u0026quot; ) // This is a compile-time assertion to ensure that this generated file // is compatible with the kratos package it is being compiled against. var _ = new(context.Context) var _ = binding.EncodeURL const _ = http.SupportPackageIsVersion1 const OperationGreeterSayHello = \u0026quot;/helloworld.Greeter/SayHello\u0026quot; type GreeterHTTPServer interface { SayHello(context.Context, *HelloRequest) (*HelloReply, error) } func RegisterGreeterHTTPServer(s *http.Server, srv GreeterHTTPServer) { r := s.Route(\u0026quot;/\u0026quot;) r.POST(\u0026quot;/helloworld\u0026quot;, _Greeter_SayHello0_HTTP_Handler(srv)) } func _Greeter_SayHello0_HTTP_Handler(srv GreeterHTTPServer) func(ctx http.Context) error { return func(ctx http.Context) error { var in HelloRequest if err := ctx.Bind(\u0026amp;in); err != nil { return err } http.SetOperation(ctx, OperationGreeterSayHello) h := ctx.Middleware(func(ctx context.Context, req interface{}) (interface{}, error) { return srv.SayHello(ctx, req.(*HelloRequest)) }) out, err := h(ctx, \u0026amp;in) if err != nil { return err } reply := out.(*HelloReply) return ctx.Result(200, reply) } } type GreeterHTTPClient interface { SayHello(ctx context.Context, req *HelloRequest, opts ...http.CallOption) (rsp *HelloReply, err error) } type GreeterHTTPClientImpl struct { cc *http.Client } func NewGreeterHTTPClient(client *http.Client) GreeterHTTPClient { return \u0026amp;GreeterHTTPClientImpl{client} } func (c *GreeterHTTPClientImpl) SayHello(ctx context.Context, in *HelloRequest, opts ...http.CallOption) (*HelloReply, error) { var out HelloReply pattern := \u0026quot;/helloworld\u0026quot; path := binding.EncodeURL(pattern, in, false) opts = append(opts, http.Operation(OperationGreeterSayHello)) opts = append(opts, http.PathTemplate(pattern)) err := c.cc.Invoke(ctx, \u0026quot;POST\u0026quot;, path, in, \u0026amp;out, opts...) if err != nil { return nil, err } return \u0026amp;out, err } 开启一个grpc及http服务:\npackage main import ( \u0026quot;context\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;log\u0026quot; \u0026quot;test/helloworld\u0026quot; \u0026quot;github.com/go-kratos/kratos/v2\u0026quot; \u0026quot;github.com/go-kratos/kratos/v2/middleware/recovery\u0026quot; \u0026quot;github.com/go-kratos/kratos/v2/transport/grpc\u0026quot; \u0026quot;github.com/go-kratos/kratos/v2/transport/http\u0026quot; ) type server struct { helloworld.UnimplementedGreeterServer } func (s *server) SayHello(ctx context.Context, in *helloworld.HelloRequest) (*helloworld.HelloReply, error) { return \u0026amp;helloworld.HelloReply{Msg: fmt.Sprintf(\u0026quot;Hello %+v\u0026quot;, in.Name)}, nil } func main() { s := \u0026amp;server{} httpSrv := http.NewServer( http.Address(\u0026quot;:8000\u0026quot;), http.Middleware( recovery.Recovery(), ), ) grpcSrv := grpc.NewServer( grpc.Address(\u0026quot;:9000\u0026quot;), grpc.Middleware( recovery.Recovery(), ), ) helloworld.RegisterGreeterServer(grpcSrv, s) helloworld.RegisterGreeterHTTPServer(httpSrv, s) app := kratos.New( kratos.Name(\u0026quot;test\u0026quot;), kratos.Server( httpSrv, grpcSrv, ), ) if err := app.Run(); err != nil { log.Fatal(err) } } http client：\npackage main import ( \u0026quot;context\u0026quot; \u0026quot;log\u0026quot; \u0026quot;test/helloworld\u0026quot; \u0026quot;github.com/go-kratos/kratos/v2/middleware/recovery\u0026quot; transhttp \u0026quot;github.com/go-kratos/kratos/v2/transport/http\u0026quot; ) func main() { callHTTP() } func callHTTP() { conn, err := transhttp.NewClient( context.Background(), transhttp.WithMiddleware( recovery.Recovery(), ), transhttp.WithEndpoint(\u0026quot;127.0.0.1:8000\u0026quot;), ) if err != nil { panic(err) } defer conn.Close() client := helloworld.NewGreeterHTTPClient(conn) reply, err := client.SayHello(context.Background(), \u0026amp;helloworld.HelloRequest{Name: \u0026quot;kratos\u0026quot;}) if err != nil { log.Fatal(err) } log.Printf(\u0026quot;[http] SayHello %s\\n\u0026quot;, reply.Msg) } http server端实现原理 核心流程为下图 ：\n首先新建一个struct 并实现 http_pb.go种 GreeterHTTPServer interface 的方法，GreeterHTTPServer的命名方式为protobuf文件中的 service+HTTPServer，interface的方法为protobuf中使用google.api.http生命http路由所有的method。\n然后使用RegisterGreeterHTTPServer方法把服务注册进去。大体的流程如下：\nconst OperationGreeterSayHello = \u0026quot;/helloworld.Greeter/SayHello\u0026quot; func RegisterGreeterHTTPServer(s *http.Server, srv GreeterHTTPServer) { r := s.Route(\u0026quot;/\u0026quot;) r.POST(\u0026quot;/helloworld\u0026quot;, _Greeter_SayHello0_HTTP_Handler(srv)) // 注册路由 } func _Greeter_SayHello0_HTTP_Handler(srv GreeterHTTPServer) func(ctx http.Context) error { return func(ctx http.Context) error { var in HelloRequest // protobuf 中声明的request if err := ctx.Bind(\u0026amp;in); err != nil { // 把http的参数绑定到 in return err } http.SetOperation(ctx, OperationGreeterSayHello) // 设置Operation 和grpc一值，用于middleware select 等 h := ctx.Middleware(func(ctx context.Context, req interface{}) (interface{}, error) { return srv.SayHello(ctx, req.(*HelloRequest)) // 这个方法也就是上文提到的GreeterHTTPServer接口的方法，也就是我们自己实现的struct server里的SayHello方法 }) // 使用责任链模式middleware 这里没有任何中间件 out, err := h(ctx, \u0026amp;in) // 执行 if err != nil { return err } reply := out.(*HelloReply) return ctx.Result(200, reply) } } 什么事责任链模式？\nhttps://haiyux.cc/post/designmode/behavioral/#责任链模式\n上段代码中的POST方法为：\n代码在https://github.com/go-kratos/kratos/blob/main/transport/http/router.go#L76\nfunc (r *Router) POST(path string, h HandlerFunc, m ...FilterFunc) { r.Handle(http.MethodPost, path, h, m...) // MethodPost = POST net/http下的常量 } // h 为上段xxx_http_pb.go代码中_Greeter_SayHello0_HTTP_Handler的返回值 func (r *Router) Handle(method, relativePath string, h HandlerFunc, filters ...FilterFunc) { next := http.Handler(http.HandlerFunc(func(res http.ResponseWriter, req *http.Request) { ctx := r.pool.Get().(Context) ctx.Reset(res, req) // 把 net/http的http.ResponseWriter 和*http.Request 设置ctx中 if err := h(ctx); err != nil { // 执行h r.srv.ene(res, req, err) // 如果出错了 执行 ene(EncodeErrorFunc) } ctx.Reset(nil, nil) r.pool.Put(ctx) })) next = FilterChain(filters...)(next) next = FilterChain(r.filters...)(next) // 添加filter 责任链模式 r.srv.router.Handle(path.Join(r.prefix, relativePath), next).Methods(method) // router 为 mux的router 把方法注册到路由中 } 当我们访问 path.Join(r.prefix, relativePath)也就是/helloworld 时，会执行上段代码中的next方法，next是一个责任链。\n核心为会执行_Greeter_SayHello0_HTTP_Handler方法，\n如果没发生错误，执行ctx.Result(200, reply)\ntype wrapper struct { router *Router req *http.Request res http.ResponseWriter w responseWriter } func (c *wrapper) Result(code int, v interface{}) error { c.w.WriteHeader(code) return c.router.srv.enc(\u0026amp;c.w, c.req, v) } enc也就是EncodeResponseFunc, 为kratos预留的返回值函数\ntype EncodeResponseFunc func(http.ResponseWriter, *http.Request, interface{}) error kratos提供了默认的EncodeResponseFunc\nfunc DefaultResponseEncoder(w http.ResponseWriter, r *http.Request, v interface{}) error { if v == nil { return nil } if rd, ok := v.(Redirector); ok { // 检查有无Redirect方法，如果实现了interface 为跳转路由 也就是http的301 302等 url, code := rd.Redirect() http.Redirect(w, r, url, code) // 跳转 return nil } codec, _ := CodecForRequest(r, \u0026quot;Accept\u0026quot;) // 查看需要返回的参数类型 比如json data, err := codec.Marshal(v) // 把数据Marshal成[]byte if err != nil { return err } w.Header().Set(\u0026quot;Content-Type\u0026quot;, httputil.ContentType(codec.Name())) // 设置header _, err = w.Write(data) // 写数据 if err != nil { return err } return nil } 如果没发生错误，执行ene,也就是EncodeErrorFunc, 为kratos预留的错误返回值删除\ntype EncodeErrorFunc func(http.ResponseWriter, *http.Request, error) kratos提供了默认的EncodeErrorFunc\nfunc DefaultErrorEncoder(w http.ResponseWriter, r *http.Request, err error) { se := errors.FromError(err) // 把error变成自定义的实现error的结构体 codec, _ := CodecForRequest(r, \u0026quot;Accept\u0026quot;) // 查看需要返回的参数类型 比如json body, err := codec.Marshal(se) if err != nil { w.WriteHeader(http.StatusInternalServerError) return } w.Header().Set(\u0026quot;Content-Type\u0026quot;, httputil.ContentType(codec.Name())) w.WriteHeader(int(se.Code)) // 写入 error中的code _, _ = w.Write(body) // 返回错误信息 } http client端实现原理 在上传的代码中http client的部分为\ntype GreeterHTTPClient interface { SayHello(ctx context.Context, req *HelloRequest, opts ...http.CallOption) (rsp *HelloReply, err error) } type GreeterHTTPClientImpl struct { // 实现 GreeterHTTPClient 接口 cc *http.Client } func NewGreeterHTTPClient(client *http.Client) GreeterHTTPClient { return \u0026amp;GreeterHTTPClientImpl{client} } func (c *GreeterHTTPClientImpl) SayHello(ctx context.Context, in *HelloRequest, opts ...http.CallOption) (*HelloReply, error) { var out HelloReply // 返回值 pattern := \u0026quot;/helloworld\u0026quot; path := binding.EncodeURL(pattern, in, false) // 整理path 传入in 是由于可能有path参数或者query opts = append(opts, http.Operation(OperationGreeterSayHello)) opts = append(opts, http.PathTemplate(pattern)) err := c.cc.Invoke(ctx, \u0026quot;POST\u0026quot;, path, in, \u0026amp;out, opts...) // 访问接口 if err != nil { return nil, err } return \u0026amp;out, err } 上段代码中的Invoke方法为：\n代码在https://github.com/go-kratos/kratos/blob/main/transport/http/client.go#L192\nfunc (client *Client) Invoke(ctx context.Context, method, path string, args interface{}, reply interface{}, opts ...CallOption) error { var ( contentType string body io.Reader ) c := defaultCallInfo(path) for _, o := range opts { if err := o.before(\u0026amp;c); err != nil { return err } } if args != nil { data, err := client.opts.encoder(ctx, c.contentType, args) if err != nil { return err } contentType = c.contentType body = bytes.NewReader(data) } url := fmt.Sprintf(\u0026quot;%s://%s%s\u0026quot;, client.target.Scheme, client.target.Authority, path) req, err := http.NewRequest(method, url, body) if err != nil { return err } if contentType != \u0026quot;\u0026quot; { req.Header.Set(\u0026quot;Content-Type\u0026quot;, c.contentType) } if client.opts.userAgent != \u0026quot;\u0026quot; { req.Header.Set(\u0026quot;User-Agent\u0026quot;, client.opts.userAgent) } ctx = transport.NewClientContext(ctx, \u0026amp;Transport{ endpoint: client.opts.endpoint, reqHeader: headerCarrier(req.Header), operation: c.operation, request: req, pathTemplate: c.pathTemplate, }) return client.invoke(ctx, req, args, reply, c, opts...) } func (client *Client) invoke(ctx context.Context, req *http.Request, args interface{}, reply interface{}, c callInfo, opts ...CallOption) error { h := func(ctx context.Context, in interface{}) (interface{}, error) { res, err := client.do(req.WithContext(ctx)) if res != nil { cs := csAttempt{res: res} for _, o := range opts { o.after(\u0026amp;c, \u0026amp;cs) } } if err != nil { return nil, err } defer res.Body.Close() if err := client.opts.decoder(ctx, res, reply); err != nil { return nil, err } return reply, nil } var p selector.Peer ctx = selector.NewPeerContext(ctx, \u0026amp;p) if len(client.opts.middleware) \u0026gt; 0 { h = middleware.Chain(client.opts.middleware...)(h) } _, err := h(ctx, args) return err } ","date":"2023-06-29","permalink":"https://daemon365.dev/2023/06/29/kratos-http%E5%8E%9F%E7%90%86/","tags":["go","kratos","源码分析"],"title":"kratos http原理"},{"content":"Namespace 什么是 Namespace ？ 这里的 \u0026ldquo;namespace\u0026rdquo; 指的是 Linux namespace 技术，它是 Linux 内核实现的一种隔离方案。简而言之，Linux 操作系统能够为不同的进程分配不同的 namespace，每个 namespace 都具有独立的资源分配，从而实现了进程间的隔离。如果你的 Linux 安装了 GCC，可以通过运行 man namespaces 命令来查看相关文档，或者你也可以访问在线手册获取更多信息。\n介绍 下图为各种 namespace 的参数，支持的起始内核版本，以及隔离内容。\nNamespace 系统调用参数 内核版本 隔离内容 UTS (Unix Time-sharing System) CLONE_NEWUTS Linux 2.4.19 主机名与域名 IPC (Inter-Process Communication) CLONE_NEWIPC Linux 2.6.19 信号量、消息队列和共享内存 PID (Process ID) CLONE_NEWPID Linux 2.6.19 进程编号 Network CLONE_NEWNET Linux 2.6.24 网络设备、网络栈、端口等等 Mount CLONE_NEWNS Linux 2.6.29 挂载点（文件系统） User CLONE_NEWUSER Linux 3.8 用户和用户组 PID Namespace： 不同用户的进程通过 PID Namespace 进行隔离，并且不同的 Namespace 中可以有相同的进程 ID。在 Docker 中，所有的 LXC（Linux 容器）进程的父进程是 Docker 进程，每个 LXC 进程具有不同的 Namespace。由于支持嵌套 Namespace，因此可以方便地实现 Docker 中的 Docker（Docker in Docker）。 Net Namespace： 有了 PID Namespace，每个 Namespace 中的进程能够相互隔离，但是网络端口仍然共享主机的端口。通过 Net Namespace 实现网络隔离，每个 Net Namespace 具有独立的网络设备、IP 地址、IP 路由表和 /proc/net 目录。这样，每个容器的网络就能够得到隔离。Docker 默认使用 veth（虚拟以太网）方式将容器中的虚拟网卡与主机上的 Docker 桥接器（docker0）连接起来。 IPC Namespace： 容器中的进程仍然使用常见的 Linux 进程间通信（IPC）方法，包括信号量、消息队列和共享内存。然而，与虚拟机不同的是，容器中的进程实际上是在具有相同 PID Namespace 的主机进程之间进行通信，因此在申请 IPC 资源时需要加入 Namespace 信息，每个 IPC 资源都有一个唯一的 32 位 ID。 MNT Namespace： 类似于 chroot，将进程限制在特定的目录下执行。MNT Namespace 允许不同 Namespace 的进程看到不同的文件结构，从而隔离了每个 Namespace 中进程所看到的文件目录。与 chroot 不同的是，每个 Namespace 中的容器在 /proc/mounts 中的信息仅包含所在 Namespace 的挂载点。 UTS Namespace： UTS（\u0026ldquo;UNIX Time-sharing System\u0026rdquo;）Namespace 允许每个容器拥有独立的主机名和域名，使其在网络上可以被视为一个独立的节点，而不仅仅是主机上的一个进程。 User Namespace： 每个容器可以具有不同的用户和组 ID，这意味着容器内部的程序可以使用容器内部的用户执行，而不是主机上的用户。 涉及到三个系统调用（system call）的 API：\nclone()：用于创建新进程。与 fork() 创建新进程不同的是，clone() 创建进程时可以传递 CLONE_NEW* 类型的命名空间隔离参数，以控制子进程共享的内容。要了解更多信息，请查阅clone 手册。 setns()：用于将某个进程与指定的命名空间分离。通过 setns()，进程可以脱离一个特定的命名空间，使其不再与该命名空间中的其他进程共享资源。 unshare()：用于将某个进程加入到指定的命名空间中。通过 unshare()，进程可以加入到一个特定的命名空间，与该命名空间中的其他进程共享资源。 namespace 的操作 查看当前系统的 namespace lsns –t \u0026lt;type\u0026gt; 查看某进程的 namespace ls -la /proc/\u0026lt;pid\u0026gt;/ns/ 进入某 namespace 运行命令 nsenter -t \u0026lt;pid\u0026gt; -n ip addr Test:\n# Linux命令行中，可以使用`unshare`命令结合`clone()`创建一个新的进程，并在其中使用命名空间隔离参数。 # 创建一个新的进程，并在其中使用命名空间隔离参数 unshare --pid --net -- sleep 600 ps -ef|grep sleep root 37915 34572 0 08:53 pts/1 00:00:00 sudo unshare --pid --net -- sleep 600 root 37916 37915 0 08:53 pts/3 00:00:00 sudo unshare --pid --net -- sleep 600 root 37917 37916 0 08:53 pts/3 00:00:00 sleep 600 zhy 37919 37896 0 08:53 pts/2 00:00:00 grep --color=auto sleep sudo lsns -t net [sudo] password for zhy: NS TYPE NPROCS PID USER NETNSID NSFS COMMAND 4026531840 net 277 1 root unassigned /sbin/init 4026532656 net 1 37347 root 0 /run/docker/netns/c986b82be683 bash 4026532718 net 1 37917 root unassigned sleep 600 sudo nsenter -t 37917 -n ip a 1: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 docker 启动一个 ubuntu docker run --rm -it docker.m.daocloud.io/ubuntu:22.10 bash 用另一个窗口 找到这个进程 ps -ef|grep ubuntu # zhy 37247 34017 0 08:20 pts/0 00:00:00 docker run --rm -it docker.m.daocloud.io/ubuntu:22.10 bash 查看这个进程的 namespace ls -la /proc/37247/ns/ total 0 dr-x--x--x 2 zhy zhy 0 May 27 08:24 . dr-xr-xr-x 9 zhy zhy 0 May 27 08:23 .. lrwxrwxrwx 1 zhy zhy 0 May 27 08:24 cgroup -\u0026gt; 'cgroup:[4026531835]' lrwxrwxrwx 1 zhy zhy 0 May 27 08:24 ipc -\u0026gt; 'ipc:[4026531839]' lrwxrwxrwx 1 zhy zhy 0 May 27 08:24 mnt -\u0026gt; 'mnt:[4026531841]' lrwxrwxrwx 1 zhy zhy 0 May 27 08:24 net -\u0026gt; 'net:[4026531840]' lrwxrwxrwx 1 zhy zhy 0 May 27 08:24 pid -\u0026gt; 'pid:[4026531836]' lrwxrwxrwx 1 zhy zhy 0 May 27 08:24 pid_for_children -\u0026gt; 'pid:[4026531836]' lrwxrwxrwx 1 zhy zhy 0 May 27 08:24 time -\u0026gt; 'time:[4026531834]' lrwxrwxrwx 1 zhy zhy 0 May 27 08:24 time_for_children -\u0026gt; 'time:[4026531834]' lrwxrwxrwx 1 zhy zhy 0 May 27 08:24 user -\u0026gt; 'user:[4026531837]' lrwxrwxrwx 1 zhy zhy 0 May 27 08:24 uts -\u0026gt; 'uts:[4026531838]' 查看namespace sudo lsns -t pid NS TYPE NPROCS PID USER COMMAND 4026531836 pid 275 1 root /sbin/init 4026532654 pid 1 37347 root bash sudo lsns -t net NS TYPE NPROCS PID USER NETNSID NSFS COMMAND 4026531840 net 275 1 root unassigned /sbin/init 4026532656 net 1 37347 root 0 /run/docker/netns/c986b82be683 bash 为什么查出来执行 **bash** 的 pid 和 **ps -ef** 的不一样？\n一个是docker run的进程 PID\n一个是 容器内部 \u0026lsquo;bash\u0026rsquo; 进程的 PID 这个进程是由docker run的进程通过进程复制（process cloning）创建的子进程。\n在 ubuntu 中执行 ip addr 在主机执行 nsenter -t \u0026lt;pid\u0026gt; -n ip addr # 容器内 ip addr 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 11: eth0@if12: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 08:23 pts/0 00:00:00 bash root 360 1 0 09:12 pts/0 00:00:00 ps -ef # 主机 sudo nsenter -t 37347 -n -- ip addr # -n 进入网络namespace执行 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 11: eth0@if12: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever sudo nsenter -t 37347 -a -- ps -ef # -a 进入所有namespace执行 UID PID PPID C STIME TTY TIME CMD root 1 0 0 08:23 pts/0 00:00:00 bash root 359 0 0 09:12 ? 00:00:00 ps -ef Cgroup 什么是 Cgroup Linux cgroups 的全称是 Linux Control Groups，它是 Linux 内核的特性，主要作用是限制、记录和隔离进程组（process groups）使用的物理资源（cpu、memory、IO 等）。\n为什么要使用Cgroup? 可以做到对 cpu，内存等资源实现精细化的控制，容器技术就使用了 cgroups 提供的资源限制能力来完成cpu，内存等部分的资源控制。\n核心概念 task：任务，对应于系统中运行的一个实体，一般是指进程 subsystem：子系统，具体的资源控制器（resource class 或者 resource controller），控制某个特定的资源使用。比如 CPU 子系统可以控制 CPU 时间，memory 子系统可以控制内存使用量 cgroup：控制组，一组任务和子系统的关联关系，表示对这些任务进行怎样的资源管理策略 hierarchy：层级有一系列 cgroup 以一个树状结构排列而成，每个层级通过绑定对应的子系统进行资源控制。层级中的 cgroup 节点可以包含零个或多个子节点，子节点继承父节点挂载的子系统。一个操作系统中可以有多个层级。 subsystem subsystem 是一组资源控制的模块，一般包含有：\nblkio 设置对块设备 (比如硬盘) 的输入输出的访问控制 (block/io) cpu 设置 cgroup 中的进程的 CPU 被调度的策略 cpuacct 可以统计 cgroup 中的进程的 CPU 占用 (cpu account) cpuset 在多核机器上设置 cgroup 中的进程可以使用的 CPU 和内存 (此处内存仅使用于 NUMA 架构) devices 控制 cgroup 中进程对设备的访问 freezer 用于挂起 (suspends) 和恢复 (resumes) cgroup 中的进程 memory 用于控制 cgroup 中进程的内存占用 net_cls 用于将 cgroup 中进程产生的网络包分类 (classify)，以便 Linux 的 tc (traffic controller) (net_classify) 可以根据分类 (classid) 区分出来自某个 cgroup 的包并做限流或监控。 net_prio 设置 cgroup 中进程产生的网络流量的优先级 ns 这个 subsystem 比较特殊，它的作用是 cgroup 中进程在新的 namespace fork 新进程 (NEWNS) 时，创建出一个新的 cgroup，这个 cgroup 包含新的 namespace 中进程。 v2 Cgroup v2手册\n是否加载了Cgroup v2内核模块\ncat /sys/fs/cgroup/cgroup.controllers cpuset cpu io memory hugetlb pids rdma misc test Cpu 执行一段go代码\npackage main func main() { go func() { for{} }() for {} } /* 执行 go run test.go top PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 39268 zhy 20 0 709572 868 584 R 200.0 0.0 2:12.27 test 可以看到使用了2个cpu 因为开个两个goroutine for阻塞 */ 限制cpu\nsudo mkdir /sys/fs/cgroup/test sudo echo \u0026quot;100000 100000\u0026quot; | sudo tee /sys/fs/cgroup/test/cpu.max \u0026gt;/dev/null sudo echo \u0026quot;39268\u0026quot; | sudo tee /sys/fs/cgroup/test/cgroup.procs \u0026gt;/dev/null # top # PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND # 39268 zhy 20 0 709572 868 584 R 100.3 0.0 7:45.04 test # 马上就只占用一个cpu了 Memory #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #define BLOCK_SIZE (100 * 1024 * 1024) #define NUM_ALLOCATIONS 10 #define SLEEP_SECONDS 30 char* allocMemory(int size) { char* out = (char*)malloc(size); memset(out, 'A', size); return out; } int main() { int i; for (i = 1; i \u0026lt;= NUM_ALLOCATIONS; i++) { char* block = allocMemory(i * BLOCK_SIZE); printf(\u0026quot;Allocated memory block of size %dMB at address: %p\\n\u0026quot;, i * 100, block); sleep(SLEEP_SECONDS); } return 0; } /* ps -p 3243 -o rss=,unit=M,cmd= M 308512 session-4.scope ./test2 */ 限制内存\nsudo echo \u0026quot;300000000\u0026quot; |sudo tee /sys/fs/cgroup/test/memory.max \u0026gt;/dev/null sudo echo \u0026quot;64417\u0026quot; | sudo tee /sys/fs/cgroup/test/cgroup.procs \u0026gt;/dev/null #cat memory.current #299839488 UnionFS 联合文件系统（UnionFS）是一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下 (unite several directories into a single virtual filesystem)。\n联合文件系统是 Docker 镜像的基础。镜像可以通过分层来进行继承，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像。\n另外，不同 Docker 容器就可以共享一些基础的文件系统层，同时再加上自己独有的改动层，大大提高了存储的效率。\n最新版 Docker 使用的是 overlay2。\noverlay2 现在主流基本都是 overlayFS\nOverlayFS 属于文件级的存储驱动，包含了最初的 Overlay 和更新更稳定的 overlay2。\nOverlay 只有两层：upper 层和 lower 层，Lower 层代表镜像层，upper 层代表容器可写层。\ntest mkdir test \u0026amp;\u0026amp; cd test mkdir upper lower merged work echo \u0026quot;file1 from lower\u0026quot; \u0026gt; lower/file1.txt echo \u0026quot;file2 from lowerr\u0026quot; \u0026gt; lower/file2.txt echo \u0026quot;file3 from lower\u0026quot; \u0026gt; lower/file3.txt echo \u0026quot;file2 from upper\u0026quot; \u0026gt; upper/file2.txt echo \u0026quot;file4 from upper\u0026quot; \u0026gt; upper/file4.txt current_dir=$(pwd) sudo mount -t overlay -o lowerdir=\u0026quot;$current_dir/lower\u0026quot;,upperdir=\u0026quot;$current_dir/upper\u0026quot;,workdir=\u0026quot;$current_dir/work\u0026quot; overlay \u0026quot;$current_dir/merged\u0026quot; cat merged/file1.txt file1 from lower cat merged/file2.txt file2 from upper cat merged/file3.txt file3 from lower cat merged/file4.txt file4 from upper docker image 每一条指令是一层, 下层可以共用\nDocker 的文件系统 典型的Linux文件系统组成如下：\nBootfs（引导文件系统） Bootloader（引导加载程序）：负责加载内核。 Kernel（内核）：一旦内核加载到内存中，就会卸载bootfs。 Rootfs（根文件系统） /dev、/proc、/bin、/etc等标准目录和文件。 对于不同的Linux发行版，bootfs基本上是一致的，但rootfs会有所差异。 Docker 启动 Linux\n在启动后，首先将 rootfs 设置为 readonly, 进行一系列检查，然后将其切换为 “readwrite” 供用户使用。 Docker 启动\n初始化时也是将 rootfs 以 readonly 方式加载并检查，然而接下来利用 union mount 的方式将一个 readwrite 文件系统挂载在 readonly 的 rootfs 之上； 并且允许再次将下层的 FS（file system） 设定为 readonly 并且向上叠加。 这样一组 readonly 和一个 writeable 的结构构成一个 container 的运行时态，每一个 FS 被称作一个 FS 层。 写操作 由于镜像具有共享特性，所以对容器可写层的操作需要依赖存储驱动提供的写时复制和用时分配机制，以此来 支持对容器可写层的修改，进而提高对存储和内存资源的利用率。\n写时复制 即 Copy-on-Write。 一个镜像可以被多个容器使用，但是不需要在内存和磁盘上做多个拷贝。 在需要对镜像提供的文件进行修改时，该文件会从镜像的文件系统被复制到容器的可写层的文件系统 进行修改，而镜像里面的文件不会改变。 不同容器对文件的修改都相互独立、互不影响。 用时分配 按需分配空间，而非提前分配，即当一个文件被创建出来后，才会分配空间。 ","date":"2023-06-24","permalink":"https://daemon365.dev/2023/06/24/%E5%AE%B9%E5%99%A8%E5%9F%BA%E7%A1%80--namespacecgroup-%E5%92%8C-unionfs/","tags":["docker","containerd","kubernetes","namespace","Cgroup","UnionFS"],"title":"容器基础-- namespace,Cgroup 和 UnionFS"},{"content":"消息队列 本篇文章主要介绍了 RabbitMQ 这种消息队列，从消息队列的概念、应用场景、安装方式到它的核心概念、五种工作模式。在安装的时候推荐使用 Docker 方式进行安装。重点需要理解的就是消息队列的应用场景、核心概念和 RabbitMQ 的五种工作模式，其中用的比较多的就是发布订阅模式、主题模式。\n队列 (Queue) 是一种常见的数据结构，其最大的特性就是先进先出(Firist In First Out)，作为最基础的数据结构，队列应用很广泛，比如我们熟知的 Redis 基础数据类型 List，其底层数据结构就是队列。\n消息队列 (Messaeg Queue) 是一种使用队列 (Queue) 作为底层存储数据结构，可用于解决不同进程与应用之间通讯的分布式消息容器，也称为消息中间件。\n目前使用得比较多的消息队列有 ActiveMQ，RabbitMQ，Kafka，RocketMQ 等。本文主要讲述的是 RabbitMQ，RabbitMQ 是用 Erlang 语言开发的一个实现了 AMQP 协议的消息队列服务器，相比其他同类型的消息队列，最大的特点在保证可观的单机吞吐量的同时，延时方面非常出色。\nRabbitMQ 支持多种客户端，比如：GO、Python、Ruby、.NET、Java、JMS、C、PHP、ActionScript、XMPP、STOMP 等。\nAMQP，即 Advanced Message Queuing Protocol，高级消息队列协议，是应用层协议的一个开放标准，为面向消息的中间件设计，它是一种应用程序之间的通信方法，消息队列在分布式系统开发中应用非常广泛。这里是 AMQP 官网 amqp.org\n消息队列使用广泛，其应用场景有很多，下面我们列举比较常见的四个场景：\n1、消息通讯 消息队列最主要功能收发消息，其内部有高效的通讯机制，因此非常适合用于消息通讯。\n我们可以基于消息队列开发点对点聊天系统，也可以开发广播系统，用于将消息广播给大量接收者。\n2、异步处理 一般我们写的程序都是顺序执行 (同步执行)，比如一个用户注册函数，其执行顺序如下：\n1、写入用户注册数据。 2、发送注册邮件。 3、发送注册成功的短信通知。 4、更新统计数据。 按照上面的执行顺序，要全部执行完毕，才能返回成功，但其实在第 1 步执行成功后，其他的步骤完全可以异步执行，我们可以将后面的逻辑发给消息队列，再由其他程序异步执行。使用消息队列进行异步处理，可以更快地返回结果，加快服务器的响应速度，提升了服务器的性能。\n3、服务解耦 在我们的系统中，应用与应用之间的通讯是很常见的，一般我们应用之间直接调用，比如说应用 A 调用应用 B 的接口，这时候应用之间的关系是强耦合的。\n如果应用 B 处于不可用的状态，那么应用 A 也会受影响。\n在应用 A 与应用 B 之间引入消息队列进行服务解耦，如果应用 B 挂掉，也不会影响应用 A 的使用。\n4、流量削峰 对于高并发的系统来说，在访问高峰时，突发的流量就像洪水般向应用系统涌过来，尤其是一些高并发写操作，随时会导致数据库服务器瘫痪，无法继续提供服务。\n而引入消息队列则可以减少突发流量对应用系统的冲击。消息队列就像水库一样，拦蓄上游的洪水，削减进入下游河道的洪峰流量，从而达到减免洪水灾害的目的。而在旱季水流量小的时候又可以把水放出来灌溉庄稼。\n这方面最常见的例子就是秒杀系统，一般秒杀活动瞬间流量很高，如果流量全部涌向秒杀系统，会压垮秒杀系统，通过引入消息队列，可以有效缓冲突发流量，达到削峰填谷的作用。\n安装 Docker 安装方式如下：\ndocker run -d --name rabbitmq -p 5672:5672 -p 15672:15672 rabbitmq:3-management RabbitMQ 有属于自己的一套核心概念，对这些概念的理解很重要，只有理解了这些核心概念，才有可能建立对 RabbitMQ 的全面理解：\nRabbitMQ核心概念 Broker Broker 概念比较简单，我们可以把 Broker 理解为一个 RabitMQ Server。\nProducer 与 Consumer 生产者与消费者相对于 RabbitMQ 服务器来说，都是 RabbitMQ 服务器的客户端。\n生产者 (Producer)：连到 RabbitMQ 服务器，将消息发送到 RabbitMQ 服务器的队列，是消息的发送方。 消费者 (Consumer)：连接到 RabbitMQ 则是为了消费队列中的消息，是消息的接收方。 生产者与消费者一般由我们的应用程序充当。\nConnection Connection 是 RabbitMQ 内部对象之一，用于管理每个到 RabbitMQ 的 TCP 网络连接。\nChannel Channel 是我们与 RabbitMQ 打交道的最重要的一个接口，我们大部分的业务操作是在 Channel 这个接口中完成的，包括定义 Queue、定义 Exchange、绑定 Queue 与 Exchange、发布消息等。\nExchnage 消息交换机，作用是接收来自生产者的消息，并根据路由键转发消息到所绑定的队列。\n生产者发送上的消息，就是先通过 Exchnage 按照绑定 (binding) 规则转发到队列的。\n交换机类型 (Exchange Type) 有四种：fanout、direct、topic，headers，其中 headers 并不常用。\nfanout：这种类型不处理路由键 (RoutingKey)，很像子网广播，每台子网内的主机都获得了一份复制的消息，发布 / 订阅模式就是指使用 fanout 交换机类型，fanout 类型交换机转发消息是最快的。 direct：模式处理路由键，需要路由键完全匹配的队列才能收到消息，路由模式使用的是 direct 类型的交换机。 topic：将路由键和某模式进行匹配。主题模式使用的是 topic 类型的交换机。 路由模式，发布订阅模式，主题模式，这些工作模式我们下面会讲。\nQueue Queue 即队列，RabbitMQ 内部用于存储消息的对象，是真正用存储消息的结构，在生产端，生产者的消息最终发送到指定队列，而消费者也是通过订阅某个队列，达到获取消息的目的。\nBinding Binding 是一种操作，其作用是建立消息从 Exchange 转发到 Queue 的规则，在进行 Exchange 与 Queue 的绑定时，需要指定一个 BindingKey，Binding 操作一般用于 RabbitMQ 的路由工作模式和主题工作模式。\nBindingKey 的概念，下面在讲 RabbitMQ 的工作模式会详细讲解。\nVirtual Host Virutal host 也叫虚拟主机，一个 VirtualHost 下面有一组不同 Exchnage 与 Queue，不同的 Virtual host 的 Exchnage 与 Queue 之间互相不影响。应用隔离与权限划分，Virtual host 是 RabbitMQ 中最小颗粒的权限单位划分。\n如果要类比的话，我们可以把 Virtual host 比作 MySQL 中的数据库，通常我们在使用 MySQL 时，会为不同的项目指定不同的数据库，同样的，在使用 RabbitMQ 时，我们可以为不同的应用程序指定不同的 Virtual host。\n请参考：www.rabbitmq.com/getstarted.…\n简单 (simple) 模式 simple 模式，是 RabbitMQ 几种模式中最简单的一种模式，其结构如下图所示：\n从上面的示意图，我们可以看出simple模式有以下几个特征：\n只有一个生产者、一个消费者和一个队列。 生产者和消费者在发送和接收消息时，只需要指定队列名，而不需要指定发送到哪个 Exchange，RabbitMQ 服务器会自动使用 Virtual host 的默认的 Exchange，默认 Exchange 的 type 为 direct。 package main // 引入amqo包 import ( \u0026quot;fmt\u0026quot; \u0026quot;github.com/streadway/amqp\u0026quot; \u0026quot;log\u0026quot; ) const ( LOGIN string = \u0026quot;zhaohaiyu\u0026quot; PASSWORD string = \u0026quot;zhy.1996\u0026quot; HOST string = \u0026quot;127.0.0.1\u0026quot; PORT string = \u0026quot;5672\u0026quot; VIRTUALHOST string = \u0026quot;/\u0026quot; ) func Pushlish() { // 创建链接 url := fmt.Sprintf(\u0026quot;amqp://%s:%s@%s:%s%s\u0026quot;, LOGIN, PASSWORD, HOST, PORT, VIRTUALHOST) conn, err := amqp.Dial(url) failOnError(err, \u0026quot;Failed to connect to RabbitMQ\u0026quot;) defer conn.Close() // 打开一个通道 ch, err := conn.Channel() failOnError(err, \u0026quot;Failed to open a channel\u0026quot;) defer ch.Close() // 生成一个交换机（交换机不存在的情况下） err = ch.ExchangeDeclare(\u0026quot;test\u0026quot;,\u0026quot;direct\u0026quot;, true,false,false, false, nil) failOnError(err, \u0026quot;Failed to declare an exchange\u0026quot;) // 生成一个队列队列（队列不存在的情况下） _, err = ch.QueueDeclare(\u0026quot;test1\u0026quot;, true, false, false, false, nil) failOnError(err, \u0026quot;Failed to declare an queue\u0026quot;) //列队与交换机绑定 err = ch.QueueBind(\u0026quot;test1\u0026quot;, \u0026quot;zhaohaiyu\u0026quot;, \u0026quot;test\u0026quot;, false, nil) failOnError(err, \u0026quot;Bind queue to exchange failure\u0026quot;) //指定交换机发布消息 err = ch.Publish(\u0026quot;test\u0026quot;, \u0026quot;zhaohaiyu\u0026quot;, false, false, amqp.Publishing{ ContentType: \u0026quot;text/plain\u0026quot;, Body: []byte(\u0026quot;生产者测试1\u0026quot;), }) failOnError(err, \u0026quot;Message publish failure\u0026quot;) } func Get() { // 创建链接 url := fmt.Sprintf(\u0026quot;amqp://%s:%s@%s:%s%s\u0026quot;, LOGIN, PASSWORD, HOST, PORT, VIRTUALHOST) conn, err := amqp.Dial(url) failOnError(err, \u0026quot;Failed to connect to RabbitMQ\u0026quot;) defer conn.Close() // 打开一个通道 ch, err := conn.Channel() failOnError(err, \u0026quot;Failed to open a channel\u0026quot;) defer ch.Close() // 指定队列获取消息 msg, ok, err := ch.Get(\u0026quot;test1\u0026quot;, true) failOnError(err, \u0026quot;Message empty\u0026quot;) fmt.Println(string(msg.Body), ok) } func failOnError(err error, msg string) { if err != nil { log.Fatalf(\u0026quot;%s: %s\u0026quot;, msg, err) } } func main() { Pushlish() Get() } /* 结果：产者测试1 true */ 工作 (work) 模式 在 simple 模式下只有一个生产者和消费者，当生产者生产消息的速度大于消费者的消费速度时，我们可以添加一个或多个消费者来加快消费速度，这种在 simple 模式下增加消费者的模式，称为 work 模式，如下图所示：\nwork 模式有以下两个特征：\n可以有多个消费者，但一条消息只能被一个消费者获取。 发送到队列中的消息，由服务器平均分配给不同消费者进行消费 package main // 引入amqo包 import ( \u0026quot;fmt\u0026quot; \u0026quot;github.com/streadway/amqp\u0026quot; \u0026quot;log\u0026quot; \u0026quot;time\u0026quot; ) const ( LOGIN string = \u0026quot;zhaohaiyu\u0026quot; PASSWORD string = \u0026quot;zhy.1996\u0026quot; HOST string = \u0026quot;127.0.0.1\u0026quot; PORT string = \u0026quot;5672\u0026quot; VIRTUALHOST string = \u0026quot;/\u0026quot; ) func Pushlish() { // 创建链接 url := fmt.Sprintf(\u0026quot;amqp://%s:%s@%s:%s%s\u0026quot;, LOGIN, PASSWORD, HOST, PORT, VIRTUALHOST) conn, err := amqp.Dial(url) failOnError(err, \u0026quot;Failed to connect to RabbitMQ\u0026quot;) defer conn.Close() // 打开一个通道 ch, err := conn.Channel() failOnError(err, \u0026quot;Failed to open a channel\u0026quot;) defer ch.Close() // 生成一个交换机（交换机不存在的情况下） err = ch.ExchangeDeclare(\u0026quot;test\u0026quot;,\u0026quot;direct\u0026quot;, true,false,false, false, nil) failOnError(err, \u0026quot;Failed to declare an exchange\u0026quot;) // 生成一个队列队列（队列不存在的情况下） _, err = ch.QueueDeclare(\u0026quot;test1\u0026quot;, true, false, false, false, nil) failOnError(err, \u0026quot;Failed to declare an queue\u0026quot;) //列队与交换机绑定 err = ch.QueueBind(\u0026quot;test1\u0026quot;, \u0026quot;zhaohaiyu\u0026quot;, \u0026quot;test\u0026quot;, false, nil) failOnError(err, \u0026quot;Bind queue to exchange failure\u0026quot;) //指定交换机发布消息 for { time.Sleep(time.Second) err = ch.Publish(\u0026quot;test\u0026quot;, \u0026quot;zhaohaiyu\u0026quot;, false, false, amqp.Publishing{ ContentType: \u0026quot;text/plain\u0026quot;, Body: []byte(\u0026quot;生产者测试1\u0026quot;), }) failOnError(err, \u0026quot;Message publish failure\u0026quot;) } } func Get1() { // 创建链接 url := fmt.Sprintf(\u0026quot;amqp://%s:%s@%s:%s%s\u0026quot;, LOGIN, PASSWORD, HOST, PORT, VIRTUALHOST) conn, err := amqp.Dial(url) failOnError(err, \u0026quot;Failed to connect to RabbitMQ\u0026quot;) defer conn.Close() // 打开一个通道 ch, err := conn.Channel() failOnError(err, \u0026quot;Failed to open a channel\u0026quot;) defer ch.Close() // 指定队列获取消息 for { time.Sleep(time.Second) msg, ok, err := ch.Get(\u0026quot;test1\u0026quot;, true) failOnError(err, \u0026quot;Message empty\u0026quot;) fmt.Println(\u0026quot;Get1\u0026quot;, string(msg.Body), ok) } } func Get2() { // 创建链接 url := fmt.Sprintf(\u0026quot;amqp://%s:%s@%s:%s%s\u0026quot;, LOGIN, PASSWORD, HOST, PORT, VIRTUALHOST) conn, err := amqp.Dial(url) failOnError(err, \u0026quot;Failed to connect to RabbitMQ\u0026quot;) defer conn.Close() // 打开一个通道 ch, err := conn.Channel() failOnError(err, \u0026quot;Failed to open a channel\u0026quot;) defer ch.Close() // 指定队列获取消息 for { time.Sleep(time.Second) msg, ok, err := ch.Get(\u0026quot;test1\u0026quot;, true) failOnError(err, \u0026quot;Message empty\u0026quot;) fmt.Println(\u0026quot;Get2\u0026quot;, string(msg.Body), ok) } } func failOnError(err error, msg string) { if err != nil { log.Fatalf(\u0026quot;%s: %s\u0026quot;, msg, err) } } func main() { go Pushlish() go Get1() go Get2() time.Sleep(time.Second * 20) } /* 结果：Get1 false Get2 生产者测试1 true Get2 false Get1 生产者测试1 true Get1 false Get2 生产者测试1 true Get2 false Get1 生产者测试1 true Get1 生产者测试1 true Get2 false Get2 false Get1 生产者测试1 true Get2 false Get1 生产者测试1 true Get2 false Get1 生产者测试1 true Get1 生产者测试1 true Get2 false Get2 false Get1 生产者测试1 true Get1 生产者测试1 true Get2 false Get1 生产者测试1 true Get2 false Get2 false Get1 生产者测试1 true Get2 false Get1 生产者测试1 true Get1 生产者测试1 true Get2 false Get1 生产者测试1 true Get2 false Get2 生产者测试1 true Get1 false Get2 false Get1 生产者测试1 true Get1 生产者测试1 true Get2 false */ 发布 / 订阅 (pub/sub) 模式 work 模式可以将消息转到多个消费者，但每条消息只能由一个消费者获取，如果我们想一条消息可以同时给多个消费者消费呢？\n这时候就需要发布 / 订阅模式，其示意图如下所示：\n从上面的示意图我们可以看出来，在发布 / 订阅模式下，需要指定发送到哪个 Exchange 中，上面图中的 X 表示 Exchange。\n发布 / 订阅模式中，Echange 的 type 为 fanout。 生产者发送消息时，不需要指定具体的队列名，Exchange 会将收到的消息转发到所绑定的队列。 消息被 Exchange 转到多个队列，一条消息可以被多个消费者获取。 在上图中，oneQueue 中的消息要么被 CustomerA 获取，要么被 CustomerB 获取。也就是同一条消息，要么是 CustomerA + CustomerC 消费、要么是 CustomerB + CustomerC 消费。\n生产者：\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;github.com/streadway/amqp\u0026quot; \u0026quot;log\u0026quot; \u0026quot;os\u0026quot; \u0026quot;strings\u0026quot; ) const ( LOGIN string = \u0026quot;zhaohaiyu\u0026quot; PASSWORD string = \u0026quot;zhy.1996\u0026quot; HOST string = \u0026quot;127.0.0.1\u0026quot; PORT string = \u0026quot;5672\u0026quot; VIRTUALHOST string = \u0026quot;/\u0026quot; ) //url := fmt.Sprintf() func failOnError(err error, msg string) { if err != nil { log.Fatalf(\u0026quot;%s: %s\u0026quot;, msg, err) } } func main() { conn, err := amqp.Dial(fmt.Sprintf(\u0026quot;amqp://%s:%s@%s:%s%s\u0026quot;, LOGIN, PASSWORD, HOST, PORT, VIRTUALHOST)) failOnError(err, \u0026quot;Failed to connect to RabbitMQ\u0026quot;) defer conn.Close() ch, err := conn.Channel() failOnError(err, \u0026quot;Failed to open a channel\u0026quot;) defer ch.Close() err = ch.ExchangeDeclare( \u0026quot;logs\u0026quot;, // name \u0026quot;fanout\u0026quot;, // type true, // durable false, // auto-deleted false, // internal false, // no-wait nil, // arguments ) failOnError(err, \u0026quot;Failed to declare an exchange\u0026quot;) body := bodyFrom(os.Args) err = ch.Publish( \u0026quot;logs\u0026quot;, // exchange \u0026quot;\u0026quot;, // routing key false, // mandatory false, // immediate amqp.Publishing{ ContentType: \u0026quot;text/plain\u0026quot;, Body: []byte(body), }) failOnError(err, \u0026quot;Failed to publish a message\u0026quot;) log.Printf(\u0026quot; [x] Sent %s\u0026quot;, body) } func bodyFrom(args []string) string { var s string if (len(args) \u0026lt; 2) || os.Args[1] == \u0026quot;\u0026quot; { s = \u0026quot;hello\u0026quot; } else { s = strings.Join(args[1:], \u0026quot; \u0026quot;) } return s } 不同的 Exchange 之间互不影响，相同 Exchange，相同队列的情况下，消息均等消费：\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;github.com/streadway/amqp\u0026quot; \u0026quot;log\u0026quot; ) const ( LOGIN string = \u0026quot;zhaohaiyu\u0026quot; PASSWORD string = \u0026quot;zhy.1996\u0026quot; HOST string = \u0026quot;127.0.0.1\u0026quot; PORT string = \u0026quot;5672\u0026quot; VIRTUALHOST string = \u0026quot;/\u0026quot; ) func failOnError(err error, msg string) { if err != nil { log.Fatalf(\u0026quot;%s: %s\u0026quot;, msg, err) } } func main() { conn, err := amqp.Dial(fmt.Sprintf(\u0026quot;amqp://%s:%s@%s:%s%s\u0026quot;, LOGIN, PASSWORD, HOST, PORT, VIRTUALHOST)) failOnError(err, \u0026quot;Failed to connect to RabbitMQ\u0026quot;) defer conn.Close() ch, err := conn.Channel() failOnError(err, \u0026quot;Failed to open a channel\u0026quot;) defer ch.Close() err = ch.ExchangeDeclare( \u0026quot;logs\u0026quot;, // name \u0026quot;fanout\u0026quot;, // type true, // durable false, // auto-deleted false, // internal false, // no-wait nil, // arguments ) failOnError(err, \u0026quot;Failed to declare an exchange\u0026quot;) q, err := ch.QueueDeclare( \u0026quot;\u0026quot;, // name false, // durable false, // delete when unused true, // exclusive false, // no-wait nil, // arguments ) failOnError(err, \u0026quot;Failed to declare a queue\u0026quot;) err = ch.QueueBind( q.Name, // queue name \u0026quot;\u0026quot;, // routing key \u0026quot;logs\u0026quot;, // exchange false, nil, ) failOnError(err, \u0026quot;Failed to bind a queue\u0026quot;) msgs, err := ch.Consume( q.Name, // queue \u0026quot;\u0026quot;, // consumer true, // auto-ack false, // exclusive false, // no-local false, // no-wait nil, // args ) failOnError(err, \u0026quot;Failed to register a consumer\u0026quot;) forever := make(chan bool) go func() { for d := range msgs { log.Printf(\u0026quot; [x] %s\u0026quot;, d.Body) } }() log.Printf(\u0026quot; [*] Waiting for logs. To exit press CTRL+C\u0026quot;) \u0026lt;-forever } 相同 Exchange，不同队列的情况下，一条消息可以被多个消费者获取。\n路由 (routing) 模式 前面几种模式，消息的目标队列无法由生产者指定，而在路由模式下，消息的目标队列，可以由生产者指定，其示意图如下所示：\n路由模式下Exchange的 type 为direct。 消息的目标队列可以由生产者按照routingKey规则指定。 消费者通过BindingKey绑定自己所关心的队列。 一条消息队可以被多个消息者获取。 只有RoutingKey与BidingKey相匹配的队列才会收到消息。 RoutingKey用于生产者指定Exchange最终将消息路由到哪个队列，BindingKey用于消费者绑定到某个队列。\n生产者：\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;log\u0026quot; \u0026quot;os\u0026quot; \u0026quot;strings\u0026quot; \u0026quot;github.com/streadway/amqp\u0026quot; ) const ( LOGIN string = \u0026quot;zhaohaiyu\u0026quot; PASSWORD string = \u0026quot;zhy.1996\u0026quot; HOST string = \u0026quot;127.0.0.1\u0026quot; PORT string = \u0026quot;5672\u0026quot; VIRTUALHOST string = \u0026quot;/\u0026quot; ) func failOnError(err error, msg string) { if err != nil { log.Fatalf(\u0026quot;%s: %s\u0026quot;, msg, err) } } func main() { conn, err := amqp.Dial(fmt.Sprintf(\u0026quot;amqp://%s:%s@%s:%s%s\u0026quot;, LOGIN, PASSWORD, HOST, PORT, VIRTUALHOST)) failOnError(err, \u0026quot;Failed to connect to RabbitMQ\u0026quot;) defer conn.Close() ch, err := conn.Channel() failOnError(err, \u0026quot;Failed to open a channel\u0026quot;) defer ch.Close() err = ch.ExchangeDeclare( \u0026quot;logs_direct\u0026quot;, // name \u0026quot;direct\u0026quot;, // type true, // durable false, // auto-deleted false, // internal false, // no-wait nil, // arguments ) failOnError(err, \u0026quot;Failed to declare an exchange\u0026quot;) body := bodyFrom(os.Args) err = ch.Publish( \u0026quot;logs_direct\u0026quot;, // exchange severityFrom(os.Args), // routing key false, // mandatory false, // immediate amqp.Publishing{ ContentType: \u0026quot;text/plain\u0026quot;, Body: []byte(body), }) failOnError(err, \u0026quot;Failed to publish a message\u0026quot;) log.Printf(\u0026quot; [x] Sent %s\u0026quot;, body) } func bodyFrom(args []string) string { var s string if (len(args) \u0026lt; 3) || os.Args[2] == \u0026quot;\u0026quot; { s = \u0026quot;hello\u0026quot; } else { s = strings.Join(args[2:], \u0026quot; \u0026quot;) } return s } func severityFrom(args []string) string { var s string if (len(args) \u0026lt; 2) || os.Args[1] == \u0026quot;\u0026quot; { s = \u0026quot;info\u0026quot; } else { s = os.Args[1] } return s } 消费者：\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;log\u0026quot; \u0026quot;os\u0026quot; \u0026quot;github.com/streadway/amqp\u0026quot; ) const ( LOGIN string = \u0026quot;zhaohaiyu\u0026quot; PASSWORD string = \u0026quot;zhy.1996\u0026quot; HOST string = \u0026quot;127.0.0.1\u0026quot; PORT string = \u0026quot;5672\u0026quot; VIRTUALHOST string = \u0026quot;/\u0026quot; ) func failOnError(err error, msg string) { if err != nil { log.Fatalf(\u0026quot;%s: %s\u0026quot;, msg, err) } } func main() { conn, err := amqp.Dial(fmt.Sprintf(\u0026quot;amqp://%s:%s@%s:%s%s\u0026quot;, LOGIN, PASSWORD, HOST, PORT, VIRTUALHOST)) failOnError(err, \u0026quot;Failed to connect to RabbitMQ\u0026quot;) defer conn.Close() ch, err := conn.Channel() failOnError(err, \u0026quot;Failed to open a channel\u0026quot;) defer ch.Close() err = ch.ExchangeDeclare( \u0026quot;logs_direct\u0026quot;, // name \u0026quot;direct\u0026quot;, // type true, // durable false, // auto-deleted false, // internal false, // no-wait nil, // arguments ) failOnError(err, \u0026quot;Failed to declare an exchange\u0026quot;) q, err := ch.QueueDeclare( \u0026quot;\u0026quot;, // name false, // durable false, // delete when unused true, // exclusive false, // no-wait nil, // arguments ) failOnError(err, \u0026quot;Failed to declare a queue\u0026quot;) if len(os.Args) \u0026lt; 2 { log.Printf(\u0026quot;Usage: %s [info] [warning] [error]\u0026quot;, os.Args[0]) os.Exit(0) } for _, s := range os.Args[1:] { log.Printf(\u0026quot;Binding queue %s to exchange %s with routing key %s\u0026quot;, q.Name, \u0026quot;logs_direct\u0026quot;, s) err = ch.QueueBind( q.Name, // queue name s, // routing key \u0026quot;logs_direct\u0026quot;, // exchange false, nil) failOnError(err, \u0026quot;Failed to bind a queue\u0026quot;) } msgs, err := ch.Consume( q.Name, // queue \u0026quot;\u0026quot;, // consumer true, // auto ack false, // exclusive false, // no local false, // no wait nil, // args ) failOnError(err, \u0026quot;Failed to register a consumer\u0026quot;) forever := make(chan bool) go func() { for d := range msgs { log.Printf(\u0026quot; [x] %s\u0026quot;, d.Body) } }() log.Printf(\u0026quot; [*] Waiting for logs. To exit press CTRL+C\u0026quot;) \u0026lt;-forever } 主题 (Topic) 模式 主题模式是在路由模式的基础上，将路由键和某模式进行匹配。其中#表示匹配多个词，*表示匹配一个词，消费者可以通过某种模式的 BindKey 来达到订阅某个主题消息的目的，如示意图如下所示：\n主题模式 Exchange 的 type 取值为 topic。 一条消息可以被多个消费者获取。 package main import ( \u0026quot;fmt\u0026quot; \u0026quot;github.com/streadway/amqp\u0026quot; \u0026quot;log\u0026quot; \u0026quot;time\u0026quot; ) const ( LOGIN string = \u0026quot;zhaohaiyu\u0026quot; PASSWORD string = \u0026quot;zhy.1996\u0026quot; HOST string = \u0026quot;127.0.0.1\u0026quot; PORT string = \u0026quot;5672\u0026quot; VIRTUALHOST string = \u0026quot;/\u0026quot; ) func publish() { url := fmt.Sprintf(\u0026quot;amqp://%s:%s@%s:%s%s\u0026quot;, LOGIN, PASSWORD, HOST, PORT, VIRTUALHOST) conn, err := amqp.Dial(url) failOnError(err, \u0026quot;Failed to connect to RabbitMQ\u0026quot;) defer conn.Close() ch, err := conn.Channel() failOnError(err, \u0026quot;Failed to open a channel\u0026quot;) defer ch.Close() err = ch.ExchangeDeclare(\u0026quot;testTopic\u0026quot;, amqp.ExchangeTopic, true, false, false, false, nil) failOnError(err, \u0026quot;Failed to declare an exchange\u0026quot;) for i := 0; i \u0026lt; 10; i++ { err = ch.Publish(\u0026quot;testTopic\u0026quot;, \u0026quot;yuemoxi\u0026quot;, false, false, amqp.Publishing{ ContentType: \u0026quot;text/plain\u0026quot;, Body: []byte(fmt.Sprintf(\u0026quot;time:%v\u0026quot;, time.Now())), }) if err == nil { fmt.Println(\u0026quot;发布成功!\u0026quot;) } time.Sleep(time.Second) } } func get1() { url := fmt.Sprintf(\u0026quot;amqp://%s:%s@%s:%s%s\u0026quot;, LOGIN, PASSWORD, HOST, PORT, VIRTUALHOST) conn, err := amqp.Dial(url) failOnError(err, \u0026quot;Failed to connect to RabbitMQ\u0026quot;) defer conn.Close() ch, err := conn.Channel() failOnError(err, \u0026quot;Failed to open a channel\u0026quot;) defer ch.Close() err = ch.ExchangeDeclare(\u0026quot;testTopic\u0026quot;, amqp.ExchangeTopic, true, false, false, false, nil) failOnError(err, \u0026quot;Failed to declare an exchange\u0026quot;) q, err := ch.QueueDeclare(\u0026quot;testTopic_get1\u0026quot;, false, false, false, false, nil) failOnError(err, \u0026quot;Failed to declare a queue\u0026quot;) err = ch.QueueBind(q.Name, \u0026quot;yuemox*\u0026quot;, \u0026quot;testTopic\u0026quot;, false, nil) failOnError(err, \u0026quot;Failed to bind a queue\u0026quot;) msgs, err := ch.Consume(q.Name, \u0026quot;get1\u0026quot;, true, false, false, false, nil) failOnError(err, \u0026quot;Failed to register a consumer\u0026quot;) for msg := range msgs { log.Printf(\u0026quot;get1: %s\u0026quot;, msg.Body) time.Sleep(time.Millisecond * 500) } } func get2() { url := fmt.Sprintf(\u0026quot;amqp://%s:%s@%s:%s%s\u0026quot;, LOGIN, PASSWORD, HOST, PORT, VIRTUALHOST) conn, err := amqp.Dial(url) failOnError(err, \u0026quot;Failed to connect to RabbitMQ\u0026quot;) defer conn.Close() ch, err := conn.Channel() failOnError(err, \u0026quot;Failed to open a channel\u0026quot;) defer ch.Close() err = ch.ExchangeDeclare(\u0026quot;testTopic\u0026quot;, amqp.ExchangeTopic, true, false, false, false, nil) failOnError(err, \u0026quot;Failed to declare an exchange\u0026quot;) q, err := ch.QueueDeclare(\u0026quot;testTopic_get2\u0026quot;, false, false, false, false, nil) failOnError(err, \u0026quot;Failed to declare a queue\u0026quot;) err = ch.QueueBind(q.Name, \u0026quot;#\u0026quot;, \u0026quot;testTopic\u0026quot;, false, nil) failOnError(err, \u0026quot;Failed to bind a queue\u0026quot;) msgs, err := ch.Consume(q.Name, \u0026quot;get2\u0026quot;, true, false, false, false, nil) failOnError(err, \u0026quot;Failed to register a consumer\u0026quot;) for msg := range msgs { log.Printf(\u0026quot;get2: %s\u0026quot;, msg.Body) time.Sleep(time.Millisecond * 500) } } func failOnError(err error, msg string) { if err != nil { log.Fatalf(\u0026quot;%s: %s\u0026quot;, msg, err) } } func main() { go publish() go get1() go get2() time.Sleep(time.Second * 20) } /* 结果： 发布成功! 2021/08/31 22:45:09 get1: time:2021-08-31 22:45:09.017664 +0800 CST m=+0.012626409 发布成功! 2021/08/31 22:45:10 get2: time:2021-08-31 22:45:10.022005 +0800 CST m=+1.016996649 2021/08/31 22:45:10 get1: time:2021-08-31 22:45:10.022005 +0800 CST m=+1.016996649 发布成功! 2021/08/31 22:45:11 get2: time:2021-08-31 22:45:11.023542 +0800 CST m=+2.018558384 2021/08/31 22:45:11 get1: time:2021-08-31 22:45:11.023542 +0800 CST m=+2.018558384 发布成功! 2021/08/31 22:45:12 get1: time:2021-08-31 22:45:12.028649 +0800 CST m=+3.023687209 2021/08/31 22:45:12 get2: time:2021-08-31 22:45:12.028649 +0800 CST m=+3.023687209 发布成功! 2021/08/31 22:45:13 get2: time:2021-08-31 22:45:13.033497 +0800 CST m=+4.028553608 2021/08/31 22:45:13 get1: time:2021-08-31 22:45:13.033497 +0800 CST m=+4.028553608 发布成功! 2021/08/31 22:45:14 get1: time:2021-08-31 22:45:14.03647 +0800 CST m=+5.031571881 2021/08/31 22:45:14 get2: time:2021-08-31 22:45:14.03647 +0800 CST m=+5.031571881 发布成功! 2021/08/31 22:45:15 get2: time:2021-08-31 22:45:15.036783 +0800 CST m=+6.031867631 2021/08/31 22:45:15 get1: time:2021-08-31 22:45:15.036783 +0800 CST m=+6.031867631 发布成功! 2021/08/31 22:45:16 get2: time:2021-08-31 22:45:16.041189 +0800 CST m=+7.036283180 2021/08/31 22:45:16 get1: time:2021-08-31 22:45:16.041189 +0800 CST m=+7.036283180 发布成功! 2021/08/31 22:45:17 get1: time:2021-08-31 22:45:17.043382 +0800 CST m=+8.038484117 2021/08/31 22:45:17 get2: time:2021-08-31 22:45:17.043382 +0800 CST m=+8.038484117 发布成功! 2021/08/31 22:45:18 get1: time:2021-08-31 22:45:18.04643 +0800 CST m=+9.041536815 2021/08/31 22:45:18 get2: time:2021-08-31 22:45:18.04643 +0800 CST m=+9.041536815 */ 延迟队列 什么是延迟队列 延时队列，首先，它是一种队列，队列意味着内部的元素是有序的，元素出队和入队是有方向性的，元素从一端进入，从另一端取出。\n其次，延时队列，最重要的特性就体现在它的延时属性上，跟普通的队列不一样的是，普通队列中的元素总是等着希望被早点取出处理，而延时队列中的元素则是希望被在指定时间得到取出和处理，所以延时队列中的元素是都是带时间属性的，通常来说是需要被处理的消息或者任务。\n简单来说，延时队列就是用来存放需要在指定时间被处理的元素的队列。\n使用场景 订单在十分钟之内未支付则自动取消。 新创建的店铺，如果在十天内都没有上传过商品，则自动发送消息提醒。 账单在一周内未支付，则自动结算。 用户注册成功后，如果三天内没有登陆则进行短信提醒。 用户发起退款，如果三天内没有得到处理则通知相关运营人员。 预定会议后，需要在预定的时间点前十分钟通知各个与会人员参加会议。 rabbitMQ中的TTL 在介绍延时队列之前，还需要先介绍一下RabbitMQ中的一个高级特性——TTL（Time To Live）。\nTTL是什么呢？TTL是RabbitMQ中一个消息或者队列的属性，表明一条消息或者该队列中的所有消息的最大存活时间，单位是毫秒。换句话说，如果一条消息设置了TTL属性或者进入了设置TTL属性的队列，那么这条消息如果在TTL设置的时间内没有被消费，则会成为“死信”。如果同时配置了队列的TTL和消息的TTL，那么较小的那个值将会被使用。\n如何利用rabbitMQ实现延迟队列 想想看，延时队列，不就是想要消息延迟多久被处理吗，TTL则刚好能让消息在延迟多久之后成为死信，另一方面，成为死信的消息都会被投递到死信队列里，这样只需要消费者一直消费死信队列里的消息就万事大吉了，因为里面的消息都是希望被立即处理的消息。\n从下图可以大致看出消息的流向：\n生产者生产一条延时消息，根据需要延时时间的不同，利用不同的routingkey将消息路由到不同的延时队列，每个队列都设置了不同的TTL属性，并绑定在同一个死信交换机中，消息过期后，根据routingkey的不同，又会被路由到不同的死信队列中，消费者只需要监听对应的死信队列进行处理即可。\ngo实现\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;github.com/streadway/amqp\u0026quot; \u0026quot;log\u0026quot; \u0026quot;time\u0026quot; ) const ( LOGIN string = \u0026quot;zhaohaiyu\u0026quot; PASSWORD string = \u0026quot;zhy.1996\u0026quot; HOST string = \u0026quot;127.0.0.1\u0026quot; PORT string = \u0026quot;5672\u0026quot; VIRTUALHOST string = \u0026quot;/\u0026quot; ) func publish() { url := fmt.Sprintf(\u0026quot;amqp://%s:%s@%s:%s%s\u0026quot;, LOGIN, PASSWORD, HOST, PORT, VIRTUALHOST) conn, err := amqp.Dial(url) failOnError(err, \u0026quot;Failed to connect to RabbitMQ\u0026quot;) defer conn.Close() ch, err := conn.Channel() failOnError(err, \u0026quot;Failed to open a channel\u0026quot;) defer ch.Close() err = ch.ExchangeDeclare(\u0026quot;testTopic\u0026quot;, amqp.ExchangeTopic, true, false, false, false, nil) failOnError(err, \u0026quot;Failed to declare an exchange\u0026quot;) for i := 0; i \u0026lt; 10; i++ { err = ch.Publish(\u0026quot;testTopic\u0026quot;, \u0026quot;yuemoxi\u0026quot;, false, false, amqp.Publishing{ ContentType: \u0026quot;text/plain\u0026quot;, Body: []byte(fmt.Sprintf(\u0026quot;time:%v\u0026quot;, time.Now())), Expiration: \u0026quot;600000\u0026quot;, }) if err == nil { fmt.Println(\u0026quot;发布成功!\u0026quot;) } time.Sleep(time.Second) } } func get2() { url := fmt.Sprintf(\u0026quot;amqp://%s:%s@%s:%s%s\u0026quot;, LOGIN, PASSWORD, HOST, PORT, VIRTUALHOST) conn, err := amqp.Dial(url) failOnError(err, \u0026quot;Failed to connect to RabbitMQ\u0026quot;) defer conn.Close() ch, err := conn.Channel() failOnError(err, \u0026quot;Failed to open a channel\u0026quot;) defer ch.Close() err = ch.ExchangeDeclare(\u0026quot;testTopic\u0026quot;, amqp.ExchangeTopic, true, false, false, false, nil) failOnError(err, \u0026quot;Failed to declare an exchange\u0026quot;) q, err := ch.QueueDeclare(\u0026quot;testTopic_get2\u0026quot;, false, false, false, false, nil) failOnError(err, \u0026quot;Failed to declare a queue\u0026quot;) //声明延时队列队列，该队列中消息如果过期，就将消息发送到交换器上，交换器就分发消息到普通队列 q1, err := ch.QueueDeclare( \u0026quot;test_delay\u0026quot;, //队列名 true, //持久化 false, //不用时是否自动删除 true, false, amqp.Table{ //当消息过期时把消息发送到logs这个交换器 \u0026quot;x-dead-letter-exchange\u0026quot;: \u0026quot;ttlTopic\u0026quot;, \u0026quot;x-dead-letter-routing-key\u0026quot;: \u0026quot;ttlKey\u0026quot;, }, ) failOnError(err, \u0026quot;Failed to bind a queue\u0026quot;) err = ch.QueueBind(q.Name, \u0026quot;#\u0026quot;, \u0026quot;testTopic\u0026quot;, false, nil) failOnError(err, \u0026quot;Failed to bind a queue\u0026quot;) err = ch.QueueBind( q1.Name, \u0026quot;ttlKey\u0026quot;, \u0026quot;ttlTopic\u0026quot;, false, nil, ) msgs, err := ch.Consume(q.Name, \u0026quot;get2\u0026quot;, true, false, false, false, nil) failOnError(err, \u0026quot;Failed to register a consumer\u0026quot;) for msg := range msgs { log.Printf(\u0026quot;get2: %s\u0026quot;, msg.Body) time.Sleep(time.Millisecond * 500) } } func failOnError(err error, msg string) { if err != nil { log.Fatalf(\u0026quot;%s: %s\u0026quot;, msg, err) } } func main() { go publish() go get2() time.Sleep(time.Second * 20) } 参考文章 https://juejin.cn/post/6916148736414466061 rabbitmq官网 https://www.cnblogs.com/mfrank/p/11260355.html ","date":"2023-05-20","permalink":"https://daemon365.dev/2023/05/20/rabbitmq%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/","tags":["RabbitMQ"],"title":"RabbitMQ消息队列"},{"content":"HTTP协议是什么？ HTTP协议是超文本传输协议的缩写，英文是Hyper Text Transfer Protocol。它是从WEB服务器传输超文本标记语言(HTML)到本地浏览器的传送协议。 设计HTTP最初的目的是为了提供一种发布和接收HTML页面的方法。 HTPP有多个版本，目前广泛使用的是HTTP/1.1版本。 HTTP原理 HTTP是一个基于TCP/IP通信协议来传递数据的协议，传输的数据类型为HTML 文件,、图片文件, 查询结果等。\nHTTP协议一般用于B/S架构（）。浏览器作为HTTP客户端通过URL向HTTP服务端即WEB服务器发送所有请求。\n我们以访问百度为例：\n访问百度流程\nhttp1.*和http2 这个Akamai公司建立的一个官方的演示，使用HTTP/1.1和HTTP/2同时请求379张图片，观察请求的时间，明显看出HTTP/2性能占优势。\n多路复用：通过单一的HTTP/2连接请求发起多重的请求-响应消息，多个请求stream共享一个TCP连接，实现多流并行而不是依赖建立多个TCP连接。\nHTTP报文格式\nHTTP特点 http协议支持客户端/服务端模式，也是一种请求/响应模式的协议。 简单快速：客户向服务器请求服务时，只需传送请求方法和路径。请求方法常用的有GET、HEAD、POST。 灵活：HTTP允许传输任意类型的数据对象。传输的类型由Content-Type加以标记。 无连接：限制每次连接只处理一个请求。服务器处理完请求，并收到客户的应答后，即断开连接，但是却不利于客户端与服务器保持会话连接，为了弥补这种不足，产生了两项记录http状态的技术，一个叫做Cookie,一个叫做Session。 无状态：无状态是指协议对于事务处理没有记忆，后续处理需要前面的信息，则必须重传。 为什么要用https？ 实际使用中，绝大说的网站现在都采用的是https协议，这也是未来互联网发展的趋势。下面是通过wireshark抓取的一个博客网站的登录请求过程。\n博客登录抓包\n可以看到访问的账号密码都是明文传输， 这样客户端发出的请求很容易被不法分子截取利用，因此，HTTP协议不适合传输一些敏感信息，比如：各种账号、密码等信息，使用http协议传输隐私信息非常不安全。\n一般http中存在如下问题：\n请求信息明文传输，容易被窃听截取。 数据的完整性未校验，容易被篡改 没有验证对方身份，存在冒充危险 什么是HTTPS? 为了解决上述HTTP存在的问题，就用到了HTTPS。\nHTTPS 协议（HyperText Transfer Protocol over Secure Socket Layer）：一般理解为HTTP+SSL/TLS，通过 SSL证书来验证服务器的身份，并为浏览器和服务器之间的通信进行加密。\n那么SSL又是什么？\nSSL（Secure Socket Layer，安全套接字层）：1994年为 Netscape 所研发，SSL 协议位于 TCP/IP 协议与各种应用层协议之间，为数据通讯提供安全支持。\nTLS（Transport Layer Security，传输层安全）：其前身是 SSL，它最初的几个版本（SSL 1.0、SSL 2.0、SSL 3.0）由网景公司开发，1999年从 3.1 开始被 IETF 标准化并改名，发展至今已经有 TLS 1.0、TLS 1.1、TLS 1.2 三个版本。SSL3.0和TLS1.0由于存在安全漏洞，已经很少被使用到。TLS 1.3 改动会比较大，目前还在草案阶段，目前使用最广泛的是TLS 1.1、TLS 1.2。\nSSL发展史（互联网加密通信）\n1994年NetSpace公司设计SSL协议（Secure Sockets Layout）1.0版本，但未发布。 1995年NetSpace发布SSL/2.0版本，很快发现有严重漏洞 1996年发布SSL/3.0版本，得到大规模应用 1999年，发布了SSL升级版TLS/1.0版本，目前应用最广泛的版本 2006年和2008年，发布了TLS/1.1版本和TLS/1.2版本 浏览器在使用HTTPS传输数据的流程是什么？ HTTPS数据传输流程\n首先客户端通过URL访问服务器建立SSL连接。 服务端收到客户端请求后，会将网站支持的证书信息（证书中包含公钥）传送一份给客户端。 客户端的服务器开始协商SSL连接的安全等级，也就是信息加密的等级。 客户端的浏览器根据双方同意的安全等级，建立会话密钥，然后利用网站的公钥将会话密钥加密，并传送给网站。 服务器利用自己的私钥解密出会话密钥。 服务器利用会话密钥加密与客户端之间的通信。 HTTPS的缺点 HTTPS协议多次握手，导致页面的加载时间延长近50%； HTTPS连接缓存不如HTTP高效，会增加数据开销和功耗； 申请SSL证书需要钱，功能越强大的证书费用越高。 SSL涉及到的安全算***消耗 CPU 资源，对服务器资源消耗较大。 总结HTTPS和HTTP的区别 HTTPS是HTTP协议的安全版本，HTTP协议的数据传输是明文的，是不安全的，HTTPS使用了SSL/TLS协议进行了加密处理。 http和https使用连接方式不同，默认端口也不一样，http是80，https是443。 参考文章:\nhttps://zhuanlan.zhihu.com/p/72616216 https://baijiahao.baidu.com/s?id=1673721127616042356 ","date":"2023-01-25","permalink":"https://daemon365.dev/2023/01/25/http%E5%92%8Chttps/","tags":["network"],"title":"http和https"},{"content":"etcd是近几年比较火热的一个开源的、分布式的键值对数据存储系统，提供共享配置、服务的注册和发现，本文主要介绍etcd的安装和使用。\netcd介绍 etcd是使用Go语言开发的一个开源的、高可用的分布式key-value存储系统，可以用于配置共享和服务的注册和发现。\n类似项目有zookeeper和consul。\netcd具有以下特点：\n完全复制：集群中的每个节点都可以使用完整的存档 高可用性：Etcd可用于避免硬件的单点故障或网络问题 一致性：每次读取都会返回跨多主机的最新写入 简单：包括一个定义良好、面向用户的API（gRPC） 安全：实现了带有可选的客户端证书身份验证的自动化TLS 快速：每秒10000次写入的基准速度 可靠：使用Raft算法实现了强一致、高可用的服务存储目录 etcd应用场景 服务发现 服务发现要解决的也是分布式系统中最常见的问题之一，即在同一个分布式集群中的进程或服务，要如何才能找到对方并建立连接。本质上来说，服务发现就是想要了解集群中是否有进程在监听 udp 或 tcp 端口，并且通过名字就可以查找和连接。\n配置中心 将一些配置信息放到 etcd 上进行集中管理。\n这类场景的使用方式通常是这样：应用在启动的时候主动从 etcd 获取一次配置信息，同时，在 etcd 节点上注册一个 Watcher 并等待，以后每次配置有更新的时候，etcd 都会实时通知订阅者，以此达到获取最新配置信息的目的。\n分布式锁 因为 etcd 使用 Raft 算法保持了数据的强一致性，某次操作存储到集群中的值必然是全局一致的，所以很容易实现分布式锁。锁服务有两种使用方式，一是保持独占，二是控制时序。\n保持独占即所有获取锁的用户最终只有一个可以得到。etcd 为此提供了一套实现分布式锁原子操作 CAS（CompareAndSwap）的 API。通过设置prevExist值，可以保证在多个节点同时去创建某个目录时，只有一个成功。而创建成功的用户就可以认为是获得了锁。 控制时序，即所有想要获得锁的用户都会被安排执行，但是获得锁的顺序也是全局唯一的，同时决定了执行顺序。etcd 为此也提供了一套 API（自动创建有序键），对一个目录建值时指定为POST动作，这样 etcd 会自动在目录下生成一个当前最大的值为键，存储这个新的值（客户端编号）。同时还可以使用 API 按顺序列出所有当前目录下的键值。此时这些键的值就是客户端的时序，而这些键中存储的值可以是代表客户端的编号。 为什么用 etcd 而不用ZooKeeper？ etcd 实现的这些功能，ZooKeeper都能实现。那么为什么要用 etcd 而非直接使用ZooKeeper呢？\n为什么不选择ZooKeeper？ 部署维护复杂，其使用的Paxos强一致性算法复杂难懂。官方只提供了Java和C两种语言的接口。 使用Java编写引入大量的依赖。运维人员维护起来比较麻烦。 最近几年发展缓慢，不如etcd和consul等后起之秀。 为什么选择etcd？ 简单。使用 Go 语言编写部署简单；支持HTTP/JSON API,使用简单；使用 Raft 算法保证强一致性让用户易于理解。 etcd 默认数据一更新就进行持久化。 etcd 支持 SSL 客户端安全认证。 最后，etcd 作为一个年轻的项目，正在高速迭代和开发中，这既是一个优点，也是一个缺点。优点是它的未来具有无限的可能性，缺点是无法得到大项目长时间使用的检验。然而，目前CoreOS、Kubernetes和CloudFoundry等知名项目均在生产环境中使用了etcd，所以总的来说，etcd值得你去尝试。\netcd集群 etcd 作为一个高可用键值存储系统，天生就是为集群化而设计的。由于 Raft 算法在做决策时需要多数节点的投票，所以 etcd 一般部署集群推荐奇数个节点，推荐的数量为 3、5 或者 7 个节点构成一个集群。\n搭建一个3节点集群示例： 在每个etcd节点指定集群成员，为了区分不同的集群最好同时配置一个独一无二的token。\n下面是提前定义好的集群信息，其中n1、n2和n3表示3个不同的etcd节点。\nTOKEN=token-01 CLUSTER_STATE=new CLUSTER=n1=http://10.240.0.17:2380,n2=http://10.240.0.18:2380,n3=http://10.240.0.19:2380 在n1这台机器上执行以下命令来启动etcd：\netcd --data-dir=data.etcd --name n1 \\ --initial-advertise-peer-urls http://10.240.0.17:2380 --listen-peer-urls http://10.240.0.17:2380 \\ --advertise-client-urls http://10.240.0.17:2379 --listen-client-urls http://10.240.0.17:2379 \\ --initial-cluster ${CLUSTER} \\ --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN} 在n2这台机器上执行以下命令启动etcd：\netcd --data-dir=data.etcd --name n2 \\ --initial-advertise-peer-urls http://10.240.0.18:2380 --listen-peer-urls http://10.240.0.18:2380 \\ --advertise-client-urls http://10.240.0.18:2379 --listen-client-urls http://10.240.0.18:2379 \\ --initial-cluster ${CLUSTER} \\ --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN} 在n3这台机器上执行以下命令启动etcd：\netcd --data-dir=data.etcd --name n3 \\ --initial-advertise-peer-urls http://10.240.0.19:2380 --listen-peer-urls http://10.240.0.19:2380 \\ --advertise-client-urls http://10.240.0.19:2379 --listen-client-urls http://10.240.0.19:2379 \\ --initial-cluster ${CLUSTER} \\ --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN} etcd 官网提供了一个可以公网访问的 etcd 存储地址。你可以通过如下命令得到 etcd 服务的目录，并把它作为-discovery参数使用。\ncurl https://discovery.etcd.io/new?size=3 https://discovery.etcd.io/a81b5818e67a6ea83e9d4daea5ecbc92 # grab this token TOKEN=token-01 CLUSTER_STATE=new DISCOVERY=https://discovery.etcd.io/a81b5818e67a6ea83e9d4daea5ecbc92 etcd --data-dir=data.etcd --name n1 \\ --initial-advertise-peer-urls http://10.240.0.17:2380 --listen-peer-urls http://10.240.0.17:2380 \\ --advertise-client-urls http://10.240.0.17:2379 --listen-client-urls http://10.240.0.17:2379 \\ --discovery ${DISCOVERY} \\ --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN} etcd --data-dir=data.etcd --name n2 \\ --initial-advertise-peer-urls http://10.240.0.18:2380 --listen-peer-urls http://10.240.0.18:2380 \\ --advertise-client-urls http://10.240.0.18:2379 --listen-client-urls http://10.240.0.18:2379 \\ --discovery ${DISCOVERY} \\ --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN} etcd --data-dir=data.etcd --name n3 \\ --initial-advertise-peer-urls http://10.240.0.19:2380 --listen-peer-urls http://10.240.0.19:2380 \\ --advertise-client-urls http://10.240.0.19:2379 --listen-client-urls http:/10.240.0.19:2379 \\ --discovery ${DISCOVERY} \\ --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN} 到此etcd集群就搭建起来了，可以使用etcdctl来连接etcd。\nexport ETCDCTL_API=3 HOST_1=10.240.0.17 HOST_2=10.240.0.18 HOST_3=10.240.0.19 ENDPOINTS=$HOST_1:2379,$HOST_2:2379,$HOST_3:2379 etcdctl --endpoints=$ENDPOINTS member list Go语言操作etcd 这里使用官方的etcd/clientv3包来连接etcd并进行相关操作。\n安装 go get go.etcd.io/etcd/clientv3 put和get操作 put命令用来设置键值对数据，get命令用来根据key获取值。\npackage main import ( \u0026quot;context\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;time\u0026quot; \u0026quot;go.etcd.io/etcd/clientv3\u0026quot; ) // etcd client put/get demo // use etcd/clientv3 func main() { cli, err := clientv3.New(clientv3.Config{ Endpoints: []string{\u0026quot;127.0.0.1:2379\u0026quot;}, DialTimeout: 5 * time.Second, }) if err != nil { // handle error! fmt.Printf(\u0026quot;connect to etcd failed, err:%v\\n\u0026quot;, err) return } fmt.Println(\u0026quot;connect to etcd success\u0026quot;) defer cli.Close() // put ctx, cancel := context.WithTimeout(context.Background(), time.Second) _, err = cli.Put(ctx, \u0026quot;q1mi\u0026quot;, \u0026quot;dsb\u0026quot;) cancel() if err != nil { fmt.Printf(\u0026quot;put to etcd failed, err:%v\\n\u0026quot;, err) return } // get ctx, cancel = context.WithTimeout(context.Background(), time.Second) resp, err := cli.Get(ctx, \u0026quot;q1mi\u0026quot;) cancel() if err != nil { fmt.Printf(\u0026quot;get from etcd failed, err:%v\\n\u0026quot;, err) return } for _, ev := range resp.Kvs { fmt.Printf(\u0026quot;%s:%s\\n\u0026quot;, ev.Key, ev.Value) } } watch操作 watch用来获取未来更改的通知。\npackage main import ( \u0026quot;context\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;time\u0026quot; \u0026quot;go.etcd.io/etcd/clientv3\u0026quot; ) // watch demo func main() { cli, err := clientv3.New(clientv3.Config{ Endpoints: []string{\u0026quot;127.0.0.1:2379\u0026quot;}, DialTimeout: 5 * time.Second, }) if err != nil { fmt.Printf(\u0026quot;connect to etcd failed, err:%v\\n\u0026quot;, err) return } fmt.Println(\u0026quot;connect to etcd success\u0026quot;) defer cli.Close() // watch key:q1mi change rch := cli.Watch(context.Background(), \u0026quot;q1mi\u0026quot;) // \u0026lt;-chan WatchResponse for wresp := range rch { for _, ev := range wresp.Events { fmt.Printf(\u0026quot;Type: %s Key:%s Value:%s\\n\u0026quot;, ev.Type, ev.Kv.Key, ev.Kv.Value) } } } 将上面的代码保存编译执行，此时程序就会等待etcd中q1mi这个key的变化。\n例如：我们打开终端执行以下命令修改、删除、设置q1mi这个key。\netcd\u0026gt; etcdctl.exe --endpoints=http://127.0.0.1:2379 put q1mi \u0026quot;dsb2\u0026quot; OK etcd\u0026gt; etcdctl.exe --endpoints=http://127.0.0.1:2379 del q1mi 1 etcd\u0026gt; etcdctl.exe --endpoints=http://127.0.0.1:2379 put q1mi \u0026quot;dsb3\u0026quot; OK 上面的程序都能收到如下通知。\nwatch\u0026gt;watch.exe connect to etcd success Type: PUT Key:q1mi Value:dsb2 Type: DELETE Key:q1mi Value: Type: PUT Key:q1mi Value:dsb3 lease租约 package main import ( \u0026quot;fmt\u0026quot; \u0026quot;time\u0026quot; ) // etcd lease import ( \u0026quot;context\u0026quot; \u0026quot;log\u0026quot; \u0026quot;go.etcd.io/etcd/clientv3\u0026quot; ) func main() { cli, err := clientv3.New(clientv3.Config{ Endpoints: []string{\u0026quot;127.0.0.1:2379\u0026quot;}, DialTimeout: time.Second * 5, }) if err != nil { log.Fatal(err) } fmt.Println(\u0026quot;connect to etcd success.\u0026quot;) defer cli.Close() // 创建一个5秒的租约 resp, err := cli.Grant(context.TODO(), 5) if err != nil { log.Fatal(err) } // 5秒钟之后, /nazha/ 这个key就会被移除 _, err = cli.Put(context.TODO(), \u0026quot;/nazha/\u0026quot;, \u0026quot;dsb\u0026quot;, clientv3.WithLease(resp.ID)) if err != nil { log.Fatal(err) } } keepAlive package main import ( \u0026quot;context\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;log\u0026quot; \u0026quot;time\u0026quot; \u0026quot;go.etcd.io/etcd/clientv3\u0026quot; ) // etcd keepAlive func main() { cli, err := clientv3.New(clientv3.Config{ Endpoints: []string{\u0026quot;127.0.0.1:2379\u0026quot;}, DialTimeout: time.Second * 5, }) if err != nil { log.Fatal(err) } fmt.Println(\u0026quot;connect to etcd success.\u0026quot;) defer cli.Close() resp, err := cli.Grant(context.TODO(), 5) if err != nil { log.Fatal(err) } _, err = cli.Put(context.TODO(), \u0026quot;/nazha/\u0026quot;, \u0026quot;dsb\u0026quot;, clientv3.WithLease(resp.ID)) if err != nil { log.Fatal(err) } // the key 'foo' will be kept forever ch, kaerr := cli.KeepAlive(context.TODO(), resp.ID) if kaerr != nil { log.Fatal(kaerr) } for { ka := \u0026lt;-ch fmt.Println(\u0026quot;ttl:\u0026quot;, ka.TTL) } } 基于etcd实现分布式锁 go.etcd.io/etcd/clientv3/concurrency在etcd之上实现并发操作，如分布式锁、屏障和选举。\n导入该包：\nimport \u0026quot;go.etcd.io/etcd/clientv3/concurrency\u0026quot; 基于etcd实现的分布式锁示例：\ncli, err := clientv3.New(clientv3.Config{Endpoints: endpoints}) if err != nil { log.Fatal(err) } defer cli.Close() // 创建两个单独的会话用来演示锁竞争 s1, err := concurrency.NewSession(cli) if err != nil { log.Fatal(err) } defer s1.Close() m1 := concurrency.NewMutex(s1, \u0026quot;/my-lock/\u0026quot;) s2, err := concurrency.NewSession(cli) if err != nil { log.Fatal(err) } defer s2.Close() m2 := concurrency.NewMutex(s2, \u0026quot;/my-lock/\u0026quot;) // 会话s1获取锁 if err := m1.Lock(context.TODO()); err != nil { log.Fatal(err) } fmt.Println(\u0026quot;acquired lock for s1\u0026quot;) m2Locked := make(chan struct{}) go func() { defer close(m2Locked) // 等待直到会话s1释放了/my-lock/的锁 if err := m2.Lock(context.TODO()); err != nil { log.Fatal(err) } }() if err := m1.Unlock(context.TODO()); err != nil { log.Fatal(err) } fmt.Println(\u0026quot;released lock for s1\u0026quot;) \u0026lt;-m2Locked fmt.Println(\u0026quot;acquired lock for s2\u0026quot;) 输出：\nacquired lock for s1 released lock for s1 acquired lock for s2 查看文档了解更多\n文章转自 https://www.liwenzhou.com/posts/Go/go_etcd/ ","date":"2023-01-08","permalink":"https://daemon365.dev/2023/01/08/golang%E6%93%8D%E4%BD%9Cetcd/","tags":["boltdb","etcd","golang","数据库","kubernetes"],"title":"golang操作etcd"},{"content":"什么是隔离？ 隔离，本质上是对系统或资源进行分割，从而实现当系统发生故障时能限定传播范围和影响范围，即发生故障后只有出问题的服务不可用，保证其他服务仍然可用。\n服务隔离 动静隔离 例如 CDN\n小到 CPU 的 cacheline false sharing、数据库 mysql 表设计中避免 bufferpool 频繁过期，隔离动静表，大到架构设计中的图片、静态资源等缓存加速。本质上都体现的一样的思路，即加速/缓存访问变换频次小的。比如 CDN 场景中，将静态资源和动态 API 分离，也是体现了隔离的思路:\n降低应用服务器负载，静态文件访问负载全部通过CDN。 对象存储存储费用最低。 海量存储空间，无需考虑存储架构升级。 静态CDN带宽加速，延迟低。 archive: 稿件表，存储稿件的名称、作者、分类、tag、状态等信息，表示稿件的基本信息。 在一个投稿流程中，一旦稿件创建改动的频率比较低。 archive_stat: 稿件统计表，表示稿件的播放、点赞、收藏、投币数量，比较高频的更新。 随着稿件获取流量，稿件被用户所消费，各类计数信息更新比较频繁。 MySQL BufferPool 是用于缓存 DataPage 的，DataPage 可以理解为缓存了表的行，那么如果频繁更新 DataPage 不断会置换，会导致命中率下降的问题，所以我们在表设计中，仍然可以沿用类似的思路，其主表基本更新，在上游 Cache 未命中，透穿到 MySQL，仍然有 BufferPool 的缓存。\n读写隔离 例如主从，除此之外还有常见的 CQRS 模式，分库分表等\n常见的隔离技术，当用于读取操作的服务器出现故障时，写服务器照常可以运作，反之也一样。\n轻重隔离 核心隔离：例如上面讲到将核心业务独立部署，非核心业务共享资源 热点隔离：例如上面讲到的 remote cache 到 local cache 用户隔离：不同的用户可能有不同的级别，例如上面讲到的外部用户和管理员 物理隔离 线程 常见的例子就是线程池，这个在 Golang 中一般不用过多考虑，runtime 已经帮我们管理好了\n主要通过线程池进行隔离，也是实现服务隔离的基础。（可将图中隔离媒介换成线程池即可）\n把业务进行分类并交给不同的线程池进行处理，当某个线程池处理一种业务请求发生问题时，不会讲故障扩散和影响到其他线程池，保证服务可用。\n假设系统存在商品服务、用户服务和订单服务3个微服务，通过设置运行时环境得到3个服务一共使用200个线程，客户端调用这3个微服务共享线程池时可能会引发服务雪崩，将线程分别隔离后则不会触发整体雪崩。\n进程 我们现在一般使用容器化服务，跑在 k8s 上这就是一种进程级别的隔离\n将系统拆分为多个子系统来实现物理隔离，各个子系统运行在独立的容器和JVM中，通过进程隔离使得一个子系统出现问题不会影响其他子系统。\n机房 我们目前在 K8s 的基础上做一些开发，常见的一种做法就是将我们的服务的不同副本尽量的分配在不同的可用区，实际上就是云厂商的不同机房，避免机房停电或者着火之类的影响\n如果有条件，对于大型高可用系统，会进行多机房部署，每个机房的服务都有自己的服务分组，本机房的服务应该只调用同机房服务。\n当一个机房出现故障，将请求快速切换到其他机房确保服务继续可用。\n集群 非常重要的服务我们可以部署多套，在物理上进行隔离，常见的有异地部署，也可能就部署在同一个区域\n将某些服务单独部署成集群，或对于某些服务可以进行分组集群管理，某一个集群出现问题之后就不会影响到其他集群，从而实现隔离。\n参考文章 https://blog.csdn.net/xiaofeng10330111/article/details/86772740 https://lailin.xyz/post/go-training-week6-usability-1-bulkhe.html ","date":"2022-12-25","permalink":"https://daemon365.dev/2022/12/25/%E9%9A%94%E7%A6%BB/","tags":["go","grpc"],"title":"隔离"},{"content":"令牌桶算法 是一个存放固定容量令牌的桶，按照固定速率往桶里添加令牌。令牌桶算法的描述如下：\n假设限制2r/s，则按照500毫秒的固定速率往桶中添加令牌。 桶中最多存放 b 个令牌，当桶满时，新添加的令牌被丢弃或拒绝。 当一个 n 个字节大小的数据包到达，将从桶中删除n 个令牌，接着数据包被发送到网络上。 如果桶中的令牌不足 n 个，则不会删除令牌，且该数据包将被限流（要么丢弃，要么缓冲区等待）。 令牌桶速率限制算法: golang.org/x/time/rate\n漏桶算法 作为计量工具(The Leaky Bucket Algorithm as a Meter)时，可以用于流量整形(Traffic Shaping)和流量控制(TrafficPolicing)，漏桶算法的描述如下：\n一个固定容量的漏桶，按照常量固定速率流出水滴。 如果桶是空的，则不需流出水滴。 可以以任意速率流入水滴到漏桶。 如果流入水滴超出了桶的容量，则流入的水滴溢出了（被丢弃），而漏桶容量是不变的。 漏桶率限制算法: go.uber.org/ratelimit\n过载保护 令牌桶与漏桶的缺点 漏斗桶/令牌桶确实能够保护系统不被拖垮, 但不管漏斗桶还是令牌桶, 其防护思路都是设定一个指标, 当超过该指标后就阻止或减少流量的继续进入，当系统负载降低到某一水平后则恢复流量的进入。但其通常都是被动的，其实际效果取决于限流阈值设置是否合理，但往往设置合理不是一件容易的事情。\n集群增加机器或者减少机器限流阈值是否要重新设置? 设置限流阈值的依据是什么? 人力运维成本是否过高? 当调用方反馈429时, 这个时候重新设置限流, 其实流量高峰已经过了重新评估限流是否有意义? 这些其实都是采用漏斗桶/令牌桶的缺点, 总体来说就是太被动, 不能快速适应流量变化。 因此我们需要一种自适应的限流算法，即: 过载保护，根据系统当前的负载自动丢弃流量。\n过载保护方法 计算系统临近过载时的峰值吞吐作为限流的阈值来进行流量控制，达到系统保护。\n服务器临近过载时，主动抛弃一定量的负载，目标是自保。 在系统稳定的前提下，保持系统的吞吐量。 利特尔法则 计算吞吐量：利特尔法则 L = λ * W\n利特尔法则由麻省理工大学斯隆商学院（MIT Sloan School of Management）的教授 John Little﹐于 1961 年所提出与证明。它是一个有关提前期与在制品关系的简单数学公式，这一法则为精益生产的改善方向指明了道路。 —- MBA 智库百科 (mbalib.com)\n如上图所示，如果我们开一个小店，平均每分钟进店 2 个客人(λ)，每位客人从等待到完成交易需要 4 分钟(W)，那我们店里能承载的客人数量就是 2 * 4 = 8 个人\n同理，我们可以将 λ 当做 QPS， W 呢是每个请求需要花费的时间，那我们的系统的吞吐就是 L = λ * W ，所以我们可以使用利特尔法则来计算系统的吞吐量。\n什么时候系统的吞吐量就是最大的吞吐量？ 首先我们可以通过统计过去一段时间的数据，获取到平均每秒的请求量，也就是 QPS，以及请求的耗时时间，为了避免出现前面 900ms 一个请求都没有最后 100ms 请求特别多的情况，我们可以使用滑动窗口算法来进行统计。\n最容易想到的就是我们从系统启动开始，就把这些值给保存下来，然后计算一个吞吐的最大值，用这个来表示我们的最大吞吐量就可以了。但是这样存在一个问题是，我们很多系统其实都不是独占一台机器的，一个物理机上面往往有很多服务，并且一般还存在一些超卖，所以可能第一个小时最大处理能力是 100，但是这台节点上其他服务实例同时都在抢占资源的时候，这个处理能力最多就只能到 80 了\n所以我们需要一个数据来做启发阈值，只要这个指标达到了阈值那我们就进入流控当中。常见的选择一般是 CPU、Memory、System Load，这里我们以 CPU 为例\n只要我们的 CPU 负载超过 80% 的时候，获取过去 5s 的最大吞吐数据，然后再统计当前系统中的请求数量，只要当前系统中的请求数大于最大吞吐那么我们就丢弃这个请求。\n如何计算接近峰值时的系统吞吐？\nCPU: 使用一个独立的线程采样，每隔 250ms 触发一次。在计算均值时，使用了简单滑动平均去除峰值的影响。 Inflight: 当前服务中正在进行的请求的数量。 Pass\u0026amp;RT: 最近5s，pass 为每100ms采样窗口内成功请求的数量，rt 为单个采样窗口中平均响应时间。 我们使用 CPU 的滑动均值(CPU \u0026gt; 800)作为启发阈值，一旦触发进入到过载保护阶段，算法为：(pass* rt) \u0026lt; inflight 限流效果生效后，CPU 会在临界值(800)附近抖动，如果不使用冷却时间，那么一个短时间的 CPU 下降就可能导致大量请求被放行，严重时会打满 CPU。 在冷却时间后，重新判断阈值(CPU \u0026gt; 800 )，是否持续进入过载保护。 什么是限流 限流是指在一段时间内，定义某个客户或应用可以接收或处理多少个请求的技术。例如，通过限流，你可以过滤掉产生流量峰值的客户和微服务，或者可以确保你的应用程序在自动扩展(Auto Scaling)失效前都不会出现过载的情况。\n令牌桶、漏桶 针对单个节点，无法分布式限流。 QPS 限流 不同的请求可能需要数量迥异的资源来处理。 某种静态 QPS 限流不是特别准。 给每个用户设置限制 全局过载发生时候，针对某些“异常”进行控制。 一定程度的“超卖”配额。 按照优先级丢弃。 拒绝请求也需要成本。 分布式限流 分布式限流，是为了控制某个应用全局的流量，而非真对单个节点纬度。\n单个大流量的接口，使用 redis 容易产生热点。 pre-request 模式对性能有一定影响，高频的网络往返。 思考：\n从获取单个 quota 升级成批量 quota。quota: 表示速率，获取后使用令牌桶算法来限制。 每次心跳后，异步批量获取 quota，可以大大减少请求 redis 的频次，获取完以后本地消费，基于令牌桶拦截。 每次申请的配额需要手动设定静态值略欠灵活，比如每次要20，还是50。 如何基于单个节点按需申请，并且避免出现不公平的现象？ 初次使用默认值，一旦有过去历史窗口的数据，可以基于历史窗口数据进行 quota 请求。 思考：\n我们经常面临给一组用户划分稀有资源的问题，他们都享有等价的权利来获取资源，但是其中一些用户实际上只需要比其他用户少的资源。 那么我们如何来分配资源呢？一种在实际中广泛使用的分享技术称作“最大最小公平分享”(Max-Min Fairness)。 直观上，公平分享分配给每个用户想要的可以满足的最小需求，然后将没有使用的资源均匀的分配给需要‘大资源’的用户。 最大最小公平分配算法的形式化定义如下：\n资源按照需求递增的顺序进行分配。 不存在用户得到的资源超过自己的需求。 未得到满足的用户等价的分享资源。 限流的重要性 每个接口配置阈值，运营工作繁重，最简单的我们配置服务级别 quota，更细粒度的，我们可以根据不同重要性设定 quota，我们引入了重要性(criticality):\n最重要 CRITICAL_PLUS，为最终的要求预留的类型，拒绝这些请求会造成非常严重的用户可见的问题。 重要 CRITICAL，生产任务发出的默认请求类型。拒绝这些请求也会造成用户可见的问题。但是可能没那么严重。 可丢弃的 SHEDDABLE_PLUS 这些流量可以容忍某种程度的不可用性。这是批量任务发出的请求的默认值。这些请求通常可以过几分钟、几小时后重试。 可丢弃的 SHEDDABLE 这些流量可能会经常遇到部分不可用情况，偶尔会完全不可用。 gRPC 系统之间，需要自动传递重要性信息。如果后端接受到请求 A，在处理过程中发出了请求 B 和 C 给其他后端，请求 B 和 C 会使用与 A 相同的重要性属性。\n全局配额不足时，优先拒绝低优先级的。 全局配额，可以按照重要性分别设置。 过载保护时，低优先级的请求先被拒绝。 熔断 断路器(Circuit Breakers): 为了限制操作的持续时间，我们可以使用超时，超时可以防止挂起操作并保证系统可以响应。因为我们处于高度动态的环境中，几乎不可能确定在每种情况下都能正常工作的准确的时间限制。断路器以现实世界的电子元件命名，因为它们的行为是都是相同的。断路器在分布式系统中非常有用，因为重复的故障可能会导致雪球效应，并使整个系统崩溃。\n服务依赖的资源出现大量错误。 某个用户超过资源配额时，后端任务会快速拒绝请求，返回“配额不足”的错误，但是拒绝回复仍然会消耗一定资源。有可能后端忙着不停发送拒绝请求，导致过载。 如上图所示，熔断器存在三个状态:\n关闭(closed): 关闭状态下没有触发断路保护，所有的请求都正常通行 打开(open): 当错误阈值触发之后，就进入开启状态，这个时候所有的流量都会被节流，不运行通行 半打开(half-open): 处于打开状态一段时间之后，会尝试尝试放行一个流量来探测当前 server 端是否可以接收新流量，如果这个没有问题就会进入关闭状态，如果有问题又会回到打开状态 Google SRE 过载保护算法 max(0, (requests - K*accepts) / (requests + 1)) 算法如上所示，这个公式计算的是请求被丢弃的概率[3]\nrequests: 一段时间的请求数量 accepts: 成功的请求数量 K: 倍率，K 越小表示越激进，越小表示越容易被丢弃请求 这个算法的好处是不会直接一刀切的丢弃所有请求，而是计算出一个概率来进行判断，当成功的请求数量越少，K越小的时候 requests−K∗accepts 的值就越大，计算出的概率也就越大，表示这个请求被丢弃的概率越大\nGutter 基于熔断的 gutter kafka ，用于接管自动修复系统运行过程中的负载，这样只需要付出10%的资源就能解决部分系统可用性问题。 我们经常使用 failover 的思路，但是完整的 failover 需要翻倍的机器资源，平常不接受流量时，资源浪费。高负载情况下接管流量又不一定完整能接住。所以这里核心利用熔断的思路，是把抛弃的流量转移到 gutter 集群，如果 gutter 也接受不住的流量，重新回抛到主集群，最大力度来接受。\n客户端流控 positive feedback: 用户总是积极重试，访问一个不可达的服务。\n客户端需要限制请求频次，retry backoff 做一定的请求退让。 可以通过接口级别的error_details，挂载到每个 API 返回的响应里。 参考文章 https://blog.csdn.net/m__l__/article/details/109175787 https://lailin.xyz/post/go-training-week6-4-auto-limiter.html https://lailin.xyz/post/go-training-week6-6-breaker.html ","date":"2022-12-24","permalink":"https://daemon365.dev/2022/12/24/%E9%99%90%E6%B5%81/","tags":["grpc","go"],"title":"限流"},{"content":"什么是超时控制？ 超时控制，使我们的服务之间调用可以快速抛错。比如API接口设置1s超时API调用A服务用了500ms，服务A调用和服务B用了600ms，n那么现在已经超时，还要调用服务C等等，再返回超时错误吗？这回事使服务C后面的链路做了无用功，浪费服务器资源。\nGRPC的截止时间 截止时间以请求开始的绝对时间来表示（即使 API 将它们表示为持续时间偏移），并且应 用于多个服务调用。发起请求的应用程序设置截止时间，整个请求链需要在截止时间之前 进行响应。 gRPC API 支持为 RPC 使用截止时间，出于多种原因，在 gRPC 应用程序中使 用截止时间始终是一种最佳实践。由于 gRPC 通信是在网络上发生的，因此在 RPC 和响应 之间会有延迟。另外，在一些特定的场景中， gRPC 服务本身可能要花费更多的时间来响 应，这取决于服务的业务逻辑。如果客户端应用程序在开发时没有指定截止时间，那么它 们会无限期地等待自己所发起的 RPC 请求的响应，而资源都会被正在处理的请求所占用。 这会让服务和客户端都面临资源耗尽的风险，增加服务的延迟，甚至可能导致整个 gRPC 服务崩溃。\n客户端应用程序的截止时间设置为 50 毫秒（截止时间 = 当前时间 + 偏移量）。客户端和 ProductMgt 服务之间的网络延迟为 0 毫秒， ProductMgt 服务的处理延迟为 20 毫秒。 商 品管理服务（ ProductMgt 服务）必须将截止时间的偏移量设置为 30 毫秒。 因为库存服 务（ Inventory 服务）需要 30 毫秒来响应， 所以截止时间的事件会在两个客户端上发生 （ ProductMgt 调用 Inventory 服务和客户端应用程序）。\nProductMgt 服务的业务逻辑将延迟时间增加了 20 毫秒。 随后， ProductMgt 服务的调用逻 辑触发了超出截止时间的场景，并且传播回客户端应用程序。因此，在使用截止时间时， 要明确它们适用于所有服务场景。\nconn, err := grpc.Dial(address, grpc.WithInsecure()) if err != nil { log.Fatalf(\u0026quot;did not connect: %v\u0026quot;, err) } defer conn.Close() client := pb.NewOrderManagementClient(conn) clientDeadline := time.Now().Add( time.Duration(2 * time.Second)) ctx, cancel := context.WithDeadline(context.Background(), clientDeadline) defer cancel() // 调用方法传入ctx ","date":"2022-12-23","permalink":"https://daemon365.dev/2022/12/23/%E8%B6%85%E6%97%B6%E6%8E%A7%E5%88%B6/","tags":["grpc","go"],"title":"超时控制"},{"content":"关于 Cobra 是 Go 的 CLI 框架。它包含一个用于创建功能强大的现代 CLI 应用程序的库，以及一个用于快速生成基于 Cobra 的应用程序和命令文件的工具。\nCobra 由 Go 项目成员和 hugo 作者 spf13 创建，已经被许多流行的 Go 项目采用，比如 kubernetes、docker等\n特性 简单的基于子命令的 CLIs：app server、app fetch 等； 完全兼容 POSIX（可移植操作系统接口） 的标志（包括短版和长版） 嵌套子命令 全局、局部和级联的标志 使用 cobra init appname 和 cobra add cmdname 轻松生成应用程序和命令 智能提示（app srver \u0026hellip;did you mean app server） 自动生成命令和标志的帮助 自动识别 -h、--help 等帮助标识 自动为你的应用程序生成的 bash 自动完成 自动为你的应用程序生成 man 手册 命令别名，以便你可以更改内容而不会破坏它们 定义自己的帮助，用法等的灵活性。 可选与 viper 紧密集成，可用于 12factor 应用程序 概念 Cobra 构建在命令（commands）、参数（arguments）和 标志（flags）上。\nCommands 代表动作，Args 是事物，Flags 是这些动作的修饰符。\n最好的应用程序在使用时会像句子一样读起来。用户将知道如何使用该应用程序，因为他们将自然地了解如何使用它。\n遵循的模式是 APPNAME VERB NOUN --ADJECTIVE。 或 APPNAME COMMAND ARG --FLAG\n一些真实的例子可以更好地说明这一点。\n在以下示例中，server 是命令，port 是标志：\nhugo server --port=1313 在此命令中，我们告诉 Git 克隆 url 的内容：\ngit clone URL --bare 命令（Command） 命令是应用程序的核心。应用程序提供的每一个交互都包含在 Command 中。一个命令可以有子命令和可选的运行一个动作。\n在上面的示例中，server 是命令。\nCobra.Command API)\n标志（Flags） 一个标志是一种修饰命令行为的方式。Cobra 支持完全符合 [https://zh.wikipedia.org/wiki/可移植操作系统接口) 包。\nCobra 命令可以定义一直保留到子命令的标志和仅可用于该命令的标志。\n在上面的例子中，port 是标志。\n标志的功能是 pflag 库提供的，该库是一个标准库的 fork，在维护相同接口的基础上兼容了 POSIX（可移植操作系统接口）。\n简单使用 // 目录结构 ├── add │ └── add.go ├── go.mod ├── go.sum └── main.go // main.go package main import ( \u0026quot;log\u0026quot; \u0026quot;test/add\u0026quot; \u0026quot;github.com/spf13/cobra\u0026quot; ) var rootCmd = \u0026amp;cobra.Command{ Use: \u0026quot;test\u0026quot;, Short: \u0026quot;测试\u0026quot;, Long: `我要写博客做个测试呢,这是个常提示`, Version: \u0026quot;v1.1\u0026quot;, } func init() { rootCmd.AddCommand(add.CmdAdd) } func main() { if err := rootCmd.Execute(); err != nil { log.Fatal(err) } } // add/add.go package add import ( \u0026quot;fmt\u0026quot; \u0026quot;github.com/spf13/cobra\u0026quot; ) var CmdAdd = \u0026amp;cobra.Command{ Use: \u0026quot;add\u0026quot;, Short: \u0026quot;新键\u0026quot;, Long: \u0026quot;新建个文件\u0026quot;, RunE: RunE, } var path string func init() { CmdAdd.Flags().StringVarP(\u0026amp;path, \u0026quot;path\u0026quot;, \u0026quot;p\u0026quot;, path, \u0026quot;file path\u0026quot;) } func RunE(cmd *cobra.Command, args []string) error { fmt.Println(\u0026quot;假装创建个文件 path=\u0026quot;, path) return nil } 执行结果\n# go run main.go --help 我要写博客做个测试呢,这是个常提示 Usage: test [command] Available Commands: add 新键 completion Generate the autocompletion script for the specified shell help Help about any command Flags: -h, --help help for test -v, --version version for test Use \u0026quot;test [command] --help\u0026quot; for more information about a command. # go run main.go add --help 新建个文件 Usage: test add [flags] Flags: -h, --help help for add -p, --path string file path # go run main.go add --path=/user/pass 假装创建个文件 path= /user/pass # go run main.go --version test version v1.1 Command参数 例子中的cobra.Command是个结构体，有很多字段，都是做什么用的呢？\n这些参数是Go语言中cobra库中的Command结构体的字段，用于定义命令行工具的行为和选项。它们的作用如下：\nUse: 命令名称。 Aliases: 命令的别名。 SuggestFor: 命令建议使用的单词列表。 Short: 命令简短描述。 GroupID: 命令所属的命令组。 Long: 命令详细描述。 Example: 命令的使用示例。 ValidArgs: 命令接受的参数列表。 ValidArgsFunction: 命令用于提供动态参数补全的函数。 Args: 命令的位置参数列表。 ArgAliases: 位置参数的别名。 BashCompletionFunction: 生成Bash补全的函数。 Deprecated: 命令是否已经过时的标志。 Annotations: 命令的附加注释信息。 Version: 命令版本号。 PersistentPreRun: 每次执行该命令之前都会执行的函数。 PersistentPreRunE: 每次执行该命令之前都会执行的返回错误的函数。 PreRun: 每次执行该命令之前都会执行的函数。 PreRunE: 每次执行该命令之前都会执行的返回错误的函数。 Run: 执行命令的函数。 RunE: 执行命令的返回错误的函数。 PostRun: 每次执行该命令之后都会执行的函数。 PostRunE: 每次执行该命令之后都会执行的返回错误的函数。 PersistentPostRun: 每次执行该命令之后都会执行的函数。 PersistentPostRunE: 每次执行该命令之后都会执行的返回错误的函数。 FParseErrWhitelist : 忽略特定的解析错误 CompletionOptions :控制 shell 自动完成的选项 TraverseChildren: 解析父命令的标志后再执行子命令 Hidden : 隐藏命令，不在可用命令列表中显示 SilenceErrors : 静默下游错误 SilenceUsage : 静默错误时不显示用法 DisableFlagParsing : 禁用标志解析 DisableAutoGenTag : 禁用自动生成的标记 DisableFlagsInUseLine : 在打印帮助或生成文档时禁用“[flags]”在用法行中的添加 DisableSuggestions : 禁用基于Levenshtein距离的建议 SuggestionsMinimumDistance : 显示建议的最小Levenshtein距离 Reference https://juejin.cn/post/6924541628031959047 Cobra. Dev ","date":"2022-12-21","permalink":"https://daemon365.dev/2022/12/21/go%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7cobra/","tags":["go"],"title":"Go命令行工具cobra"},{"content":"什么是lua Lua是一个小巧的脚本语言。它是巴西里约热内卢天主教大学（Pontifical Catholic University of Rio de Janeiro）里的一个由Roberto Ierusalimschy、Waldemar Celes 和 Luiz Henrique de Figueiredo三人所组成的研究小组于1993年开发的。 其设计目的是为了通过灵活嵌入应用程序中从而为应用程序提供灵活的扩展和定制功能。Lua由标准C编写而成，几乎在所有操作系统和平台上都可以编译，运行。Lua并没有提供强大的库，这是由它的定位决定的。所以Lua不适合作为开发独立应用程序的语言。Lua 有一个同时进行的JIT项目，提供在特定平台上的即时编译功能。\n安装 mac：\nbrew install lua 检验：\nlua -v Lua 5.4.2 Copyright (C) 1994-2020 Lua.org, PUC-Rio 注释 单行注释 两个减号是单行注释:\n-- 多行注释 --[[ 多行注释 多行注释 --]] 标示符 Lua 标示符用于定义一个变量，函数获取其他用户定义的项。标示符以一个字母 A 到 Z 或 a 到 z 或下划线 _ 开头后加上 0 个或多个字母，下划线，数字（0 到 9）。\n最好不要使用下划线加大写字母的标示符，因为Lua的保留字也是这样的。\nLua 不允许使用特殊字符如 @, $, 和 % 来定义标示符。 Lua 是一个区分大小写的编程语言。\n关键词 以下列出了 Lua 的保留关键词。保留关键字不能作为常量或变量或其他用户自定义标示符：\nand break do else elseif end false for function if in local nil not or repeat return then true until while goto 一般约定，以下划线开头连接一串大写字母的名字（比如 _VERSION）被保留用于 Lua 内部全局变量。\n全局变量 在默认情况下，变量总是认为是全局的。\n全局变量不需要声明，给一个变量赋值后即创建了这个全局变量，访问一个没有初始化的全局变量也不会出错，只不过得到的结果是：nil。\n\\\u0026gt; print(b) nil \\\u0026gt; b=10 \\\u0026gt; print(b) 10 \\\u0026gt; 如果你想删除一个全局变量，只需要将变量赋值为nil。\nb = nil print(b) --\u0026gt; nil 这样变量b就好像从没被使用过一样。换句话说, 当且仅当一个变量不等于nil时，这个变量即存在。\nLua 数据类型 Lua 是动态类型语言，变量不要类型定义,只需要为变量赋值。 值可以存储在变量中，作为参数传递或结果返回。\nLua 中有 8 个基本类型分别为：nil、boolean、number、string、userdata、function、thread 和 table。\n数据类型 描述 nil 这个最简单，只有值nil属于该类，表示一个无效值（在条件表达式中相当于false）。 boolean 包含两个值：false和true。 number 表示双精度类型的实浮点数 string 字符串由一对双引号或单引号来表示 function 由 C 或 Lua 编写的函数 userdata 表示任意存储在变量中的C数据结构 thread 表示执行的独立线路，用于执行协同程序 table Lua 中的表（table）其实是一个\u0026quot;关联数组\u0026quot;（associative arrays），数组的索引可以是数字、字符串或表类型。在 Lua 里，table 的创建是通过\u0026quot;构造表达式\u0026quot;来完成，最简单构造表达式是{}，用来创建一个空表。 table Lua 中的表（table）其实是一个\u0026quot;关联数组\u0026quot;（associative arrays），数组的索引可以是数字或者是字符串。\na = {\u0026quot;zhaohaiyu\u0026quot;, \u0026quot;baidu\u0026quot;, \u0026quot;taobao\u0026quot;, \u0026quot;qq\u0026quot;} for key, val in pairs(a) do print(\u0026quot;Key\u0026quot;, key,\u0026quot;value\u0026quot;,val) end lua 1.lua Key\t1\tvalue\tzhaohaiyu Key\t2\tvalue\tbaidu Key\t3\tvalue\ttaobao Key\t4\tvalue\tqq a = {} a[\u0026quot;zhaohaiyu\u0026quot;] = \u0026quot;https://www.zhaohaiyu.com/\u0026quot; a[\u0026quot;百度\u0026quot;] = \u0026quot;https://www.baidu.com/\u0026quot; a[\u0026quot;淘宝\u0026quot;] = 11 a[\u0026quot;淘宝\u0026quot;] = a[\u0026quot;淘宝\u0026quot;] + 12 for k, v in pairs(a) do print(k .. \u0026quot; : \u0026quot; .. v) end lua 1.lua zhaohaiyu : https://www.zhaohaiyu.com/ 淘宝 : 23 百度 : https://www.baidu.com/ function（函数） 在 Lua 中，函数是被看作是\u0026quot;第一类值（First-Class Value）\u0026quot;，函数可以存在变量里:\n-- function_test.lua 脚本文件 function factorial1(n) if n == 0 then return 1 else return n * factorial1(n - 1) end end print(factorial1(5)) factorial2 = factorial1 print(factorial2(5)) 脚本执行结果为：\n$ lua function_test.lua 120 120 thread（线程） 在 Lua 里，最主要的线程是协同程序（coroutine）。它跟线程（thread）差不多，拥有自己独立的栈、局部变量和指令指针，可以跟其他协同程序共享全局变量和其他大部分东西。\n线程跟协程的区别：线程可以同时多个运行，而协程任意时刻只能运行一个，并且处于运行状态的协程只有被挂起（suspend）时才会暂停。\nuserdata（自定义类型） userdata 是一种用户自定义数据，用于表示一种由应用程序或 C/C++ 语言库所创建的类型，可以将任意 C/C++ 的任意数据类型的数据（通常是 struct 和 指针）存储到 Lua 变量中调用。\nLua 变量 变量在使用前，需要在代码中进行声明，即创建该变量。\n编译程序执行代码之前编译器需要知道如何给语句变量开辟存储区，用于存储变量的值。\nLua 变量有三种类型：全局变量、局部变量、表中的域。\nLua 中的变量全是全局变量，那怕是语句块或是函数里，除非用 local 显式声明为局部变量。\n局部变量的作用域为从声明位置开始到所在语句块结束。\n变量的默认值均为 nil。\na = 5 -- 全局变量 local b = 5 -- 局部变量 for while if a = {1,2,3,4,5,6,7,8,9,10} for i, v in ipairs(a) do print(i,v) end 数值for循环 Lua 编程语言中数值 for 循环语法格式:\nfor var=exp1,exp2,exp3 do \u0026lt;执行体\u0026gt; end var 从 exp1 变化到 exp2，每次变化以 exp3 为步长递增 var，并执行一次 \u0026ldquo;执行体\u0026rdquo;。exp3 是可选的，如果不指定，默认为1。\nfor i=10,1,-1 do print(i) end --while a=10 while( a \u0026lt; 20 ) do print(\u0026quot;a 的值为:\u0026quot;, a) a = a+1 end -- repeat a = 10 repeat print(\u0026quot;a的值为:\u0026quot;, a) a = a + 1 until( a \u0026gt; 15 ) break Lua 编程语言 break 语句插入在循环体中，用于退出当前循环或语句，并开始脚本执行紧接着的语句。\n如果你使用循环嵌套，break语句将停止最内层循环的执行，并开始执行的外层的循环语句。\ngoto Lua 语言中的 goto 语句允许将控制流程无条件地转到被标记的语句处。\nlocal a = 1 ::label:: print(\u0026quot;--- goto label ---\u0026quot;) a = a+1 if a \u0026lt; 3 then goto label -- a 小于 3 的时候跳转到标签 label end --if if(0) then print(\u0026quot;0 为 true\u0026quot;) end Lua 函数 在Lua中，函数是对语句和表达式进行抽象的主要方法。既可以用来处理一些特殊的工作，也可以用来计算一些值。\nLua 提供了许多的内建函数，你可以很方便的在程序中调用它们，如print()函数可以将传入的参数打印在控制台上。\nLua 函数主要有两种用途：\n1.完成指定的任务，这种情况下函数作为调用语句使用； 2.计算并返回值，这种情况下函数作为赋值语句的表达式使用。 --[[ 函数返回两个值的最大值 --]] function max(num1, num2) if (num1 \u0026gt; num2) then result = num1; else result = num2; end return result; end -- 调用函数 print(\u0026quot;两值比较最大值为 \u0026quot;,max(10,4)) print(\u0026quot;两值比较最大值为 \u0026quot;,max(5,6)) ","date":"2022-10-25","permalink":"https://daemon365.dev/2022/10/25/lua%E5%9F%BA%E7%A1%80/","tags":["lua"],"title":"lua基础"},{"content":"原文地址：https://haiyux.cc/2022/09/21/k8s-install/ 虚拟机准备 我这里准备了三台虚拟机，分别部署一个master和两个node，操作系统位ubuntu 20.04。以下为特殊说明为三台机器都要做此操作\n安装容器runtime 之前，我们用的容器runtime基本都是docker，但是docker并没有实现k8s的CRI，是在kubelet的有一个组件叫docker-shim做转化，在kubernetes v1.24版本以上这个组件已经废弃，这里选择containerd做容器runtime。当然，containerd是可以使用docker的镜像的。如果非要使用docker的话，被kubernetes废弃的docker-shim被docker自己维护起来了，可以试试看。但是不建议纯纯的浪费资源。\n安装 apt install -y containerd 生成默认配置\nmkdir /etc/containerd containerd config default \u0026gt; /etc/containerd/config.toml 配置systemd cgroup驱动程序\nsed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml 设置代理和修改pause镜像\n重所周知的原因\n镜像加速 我这里用的网易docker源 你也可以用别的 阿里源等\n限免的的 https://xxxxx.mirror.aliyuncs.com 是阿里云加速，xxxx是我屏蔽字段\nhttps://cr.console.aliyun.com/cn-hangzhou/instances/mirrors 可以自啊这个地址申请自己的\nsed -i 's|config_path = \u0026quot;\u0026quot;|config_path = \u0026quot;/etc/containerd/certs.d/\u0026quot;|g' /etc/containerd/config.toml mkdir -p /etc/containerd/certs.d/docker.io mkdir -p /etc/containerd/certs.d/docker.io cat \u0026gt;/etc/containerd/certs.d/docker.io/hosts.toml \u0026lt;\u0026lt;EOF server = \u0026quot;https://docker.io\u0026quot; [host.\u0026quot;https://xxxxx.mirror.aliyuncs.com\u0026quot;] capabilities = [\u0026quot;pull\u0026quot;,\u0026quot;resolve\u0026quot;] [host.\u0026quot;https://docker.mirrors.ustc.edu.cn\u0026quot;] capabilities = [\u0026quot;pull\u0026quot;,\u0026quot;resolve\u0026quot;] [host.\u0026quot;https://registry-1.docker.io\u0026quot;] capabilities = [\u0026quot;pull\u0026quot;,\u0026quot;resolve\u0026quot;,\u0026quot;push\u0026quot;] EOF 把sandbox_image 修改成阿里云镜像版本自己看着办 不然kube-apiserver可能起不来 vim /etc/containerd/config.toml sandbox_image = \u0026quot;registry.aliyuncs.com/google_containers/pause:3.8\u0026quot; 启动\nsystemctl daemon-reload systemctl enable containerd systemctl start containerd 测试 这里使用 nerdctl工具测试\nnerdctl 是 containerd 房官方提供的加强版命令行工具 https://github.com/containerd/nerdctl\n下载方式\nwget https://ghproxy.com/https://github.com/containerd/nerdctl/releases/download/v0.23.0/nerdctl-0.23.0-linux-amd64.tar.gz tar xzvf nerdctl-0.23.0-linux-amd64.tar.gz -C /usr/local/bin nerdctl --debug pull busybox DEBU[0000] verification process skipped DEBU[0000] Found hosts dir \u0026quot;/etc/containerd/certs.d\u0026quot; DEBU[0000] Ignoring hosts dir \u0026quot;/etc/docker/certs.d\u0026quot; error=\u0026quot;stat /etc/docker/certs.d: no such file or directory\u0026quot; DEBU[0000] The image will be unpacked for platform {\u0026quot;amd64\u0026quot; \u0026quot;linux\u0026quot; \u0026quot;\u0026quot; [] \u0026quot;\u0026quot;}, snapshotter \u0026quot;overlayfs\u0026quot;. DEBU[0000] fetching image=\u0026quot;docker.io/library/busybox:latest\u0026quot; DEBU[0000] loading host directory dir=/etc/containerd/certs.d/docker.io DEBU[0000] resolving host=hub-mirror.c.163.com DEBU[0000] do request host=hub-mirror.c.163.com request.header.accept=\u0026quot;application/vnd.docker.distribution.manifest.v2+json, application/vnd.docker.distribution.manifest.list.v2+json, application/vnd.oci.image.manifest.v1+json, application/vnd.oci.image.index.v1+json, */*\u0026quot; request.header.user-agent=containerd/1.6.0+unknown request.method=HEAD url=\u0026quot;http://hub-mirror.c.163.com/v2/library/busybox/manifests/latest?ns=docker.io\u0026quot; 看到 host=hub-mirror.c.163.com 代表配置成功\n其他准备工作 防火墙 # 查看状态 ufw status # 如果打开着呢 请关闭 ufw disable 时间同步 apt install -y ntpdate ntpdate time.windows.com 关闭swap分区 # 永久生效 需要重启 sed -ri 's/.*swap.*/#\u0026amp;/' /etc/fstab # 临时关闭，重启后无效 swapoff -a 将桥接的IPv4流量传递到iptables的链 在每个节点上将桥接的IPv4流量传递到iptables的链 cat \u0026gt; /etc/sysctl.d/k8s.conf \u0026lt;\u0026lt; EOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 vm.swappiness = 0 EOF # 加载br_netfilter模块 modprobe br_netfilter # 查看是否加载 lsmod | grep br_netfilter # 生效 sysctl --system echo 1 \u0026gt; /proc/sys/net/bridge/bridge-nf-call-iptables echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward echo 1 \u0026gt; /proc/sys/net/bridge/bridge-nf-call-iptables 开启ipvs 在kubernetes中service有两种代理模型，一种是基于iptables，另一种是基于ipvs的。ipvs的性能要高于iptables的，但是如果要使用它，需要手动载入ipvs模块。\napt install -y ipset ipvsadm mkdir -p /etc/sysconfig/modules cat \u0026gt; /etc/sysconfig/modules/ipvs.modules \u0026lt;\u0026lt;EOF #!/bin/bash modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack EOF 授权、运行、检查是否加载\nchmod 755 /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; bash /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4 检查是否加载\nlsmod | grep -e ipvs -e nf_conntrack sysctl --system 设置主机名 设置主机名\nhostnamectl set-hostname \u0026lt;hostname\u0026gt; 三台机器分别为\n# 192.168.56.100 hostnamectl set-hostname k8s-master # 192.168.56.101 hostnamectl set-hostname k8s-node1 # 192.168.56.102 hostnamectl set-hostname k8s-node2 安装kubeadm、kubelet和kubectl 安装https工具\napt install -y apt-transport-https ca-certificates curl 下载阿里云cloud公钥\n为什么下载阿里云的，不去下载 kubernetes 官方的 你懂得\nsudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg 添加 Kubernetes apt 仓库\necho \u0026quot;deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main\u0026quot; | sudo tee /etc/apt/sources.list.d/kubernetes.list 更新 apt 包索引，安装 kubelet、kubeadm 和 kubectl，并锁定其版本：\napt update apt install -y kubelet kubeadm kubectl apt-mark hold kubelet kubeadm kubectl 查看k8s所需镜像 kubeadm config images list egistry.k8s.io/kube-apiserver:v1.25.2 registry.k8s.io/kube-controller-manager:v1.25.2 registry.k8s.io/kube-scheduler:v1.25.2 registry.k8s.io/kube-proxy:v1.25.2 registry.k8s.io/pause:3.8 registry.k8s.io/etcd:3.5.4-0 registry.k8s.io/coredns/coredns:v1.9.3 初始化（只有master执行） 如果带上debug日志可以在后面加 \u0026ndash;v=9\nkubeadm init \\ --apiserver-advertise-address=192.168.56.100 \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version v1.25.2 \\ --service-cidr=10.96.0.0/12 \\ --pod-network-cidr=10.244.0.0/16 出现这个代表 init 成功\nYour Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \u0026quot;kubectl apply -f [podnetwork].yaml\u0026quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.56.100:6443 --token qsmewy.fd3hlnkr6b3tb570 \\ --discovery-token-ca-cert-hash sha256:08afdf5077a0ee0f72553640e09356f19846d030552c35357d05032f95a14b89 根据提示执行\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 根据提示在两台node上执行命令 加入集群（这个写你自己master弹出来的命令）\nkubeadm join 192.168.56.100:6443 --token qsmewy.fd3hlnkr6b3tb570 \\ --discovery-token-ca-cert-hash sha256:08afdf5077a0ee0f72553640e09356f19846d030552c35357d05032f95a14b89 出现这个代表节点加入集群成功\nThis node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster. 部署CNI网络插件 kubernetes支持多种网络插件，比如flannel、calico、canal等，任选一种即可，本次选择flannel kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 这个是网络地址，可能是失败这里提供一个yaml下载，然后 apply，kube-flannel.yml\n测试 kubectl get node NAME STATUS ROLES AGE VERSION k8s-master Ready control-plane 31m v1.25.2 k8s-node1 Ready \u0026lt;none\u0026gt; 31m v1.25.2 k8s-node2 Ready \u0026lt;none\u0026gt; 30m v1.25.2 kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE coredns-c676cc86f-chtqm 1/1 Running 0 31m coredns-c676cc86f-ph8wl 1/1 Running 0 31m etcd-k8s-master 1/1 Running 1 32m kube-apiserver-k8s-master 1/1 Running 1 32m kube-controller-manager-k8s-master 1/1 Running 1 32m kube-proxy-949st 1/1 Running 0 31m kube-proxy-9zjnb 1/1 Running 0 31m kube-proxy-g98kp 1/1 Running 0 31m kube-scheduler-k8s-master 1/1 Running 1 32m kubectl get pod -n kube-flannel NAME READY STATUS RESTARTS AGE kube-flannel-ds-jk8fp 1/1 Running 0 2m2s kube-flannel-ds-pmmcs 1/1 Running 0 2m2s kube-flannel-ds-r5j7s 1/1 Running 0 2m2s 创建一个 nginx pod\nkubectl run nginx --image=nginx:1.17.1 kubectl get pod -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx 1/1 Running 0 27s 10.244.1.2 k8s-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 创建一个 service\n# vim nginx-svc.yaml apiVersion: v1 kind: Service metadata: name: nginx spec: type: ClusterIP ports: - port: 8080 targetPort: 80 protocol: TCP name: http selector: run: nginx kubectl apply -f nginx-svc.yaml kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 43m nginx ClusterIP 10.110.94.194 \u0026lt;none\u0026gt; 8080/TCP 92s 之后加入node master执行\nkubeadm token create --ttl 0 --print-join-command 执行打印出来的命令\n","date":"2022-09-22","permalink":"https://daemon365.dev/2022/09/22/kubernetes%E9%9B%86%E7%BE%A4%E6%9C%80%E6%96%B0%E7%89%88%E5%AE%89%E8%A3%85/","tags":["kubernetes"],"title":"kubernetes集群最新版安装"},{"content":"什么是发布和订阅 Redis 发布订阅 (pub/sub) 是一种消息通信模式：发送者 (pub) 发送消息，订阅者 (sub) 接收消息。\nRedis 客户端可以订阅任意数量的频道。\n发布和订阅 1、客户端可以订阅频道如下图\n2、当给这个频道发布消息后，消息就会发送给订阅的客户端\n发布订阅命令行实现 1、 打开一个客户端订阅channel1\nSUBSCRIBE channel1\n2、打开另一个客户端，给channel1发布消息hello\npublish channel1 hello\n返回的1是订阅者数量\n3、打开第一个客户端可以看到发送的消息\n注：发布的消息没有持久化，如果在订阅的客户端收不到hello，只能收到订阅后发布的消息\n","date":"2022-08-20","permalink":"https://daemon365.dev/2022/08/20/redis%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85/","tags":["redis"],"title":"redis发布订阅"},{"content":"macOS 全局 Command + Shift + P / F1 显示命令面板 Command + P 快速打开 Command + Shift + N 打开新窗口 Command + W 关闭窗口 基本 Command + X 剪切（未选中文本的情况下，剪切光标所在行） Command + C 复制（未选中文本的情况下，复制光标所在行） ``Option + Up` 向上移动行 Option + Down 向下移动行 Option + Shift + Up 向上复制行 Option + Shift + Down 向下复制行 Command + Shift + K 删除行 Command + Enter 下一行插入 Command + Shift + Enter 上一行插入 Command + Shift + \\ 跳转到匹配的括号 Command + [ 减少缩进 Command + ] 增加缩进 Home 跳转至行首 End 跳转到行尾 Command + Up 跳转至文件开头 Command + Down 跳转至文件结尾 Ctrl + PgUp 按行向上滚动 Ctrl + PgDown 按行向下滚动 Command + PgDown 按屏向下滚动 Command + PgUp 按屏向上滚动 Command + Shift + [ 折叠代码块 Command + Shift + ] 展开代码块 Command + K Command + [ 折叠全部子代码块 Command + K Command + ] 展开全部子代码块 Command + K Command + 0 折叠全部代码块 Command + K Command + J 展开全部代码块 Command + K Command + C 添加行注释 Command + K Command + U 移除行注释 Command + / 添加、移除行注释 Option + Shift + A 添加、移除块注释 Option + Z 自动换行、取消自动换行 多光标与选择 Option + 点击 插入多个光标 Command + Option + Up 向上插入光标 Command + Option + Down 向下插入光标 Command + U 撤销上一个光标操作 Option + Shift + I 在所选行的行尾插入光标 Command + I 选中当前行 Command + Shift + L 选中所有与当前选中内容相同部分 Command + F2 选中所有与当前选中单词相同的单词 Command + Ctrl + Shift + Left 折叠选中 Command + Ctrl + Shift + Right 展开选中 Alt + Shift + 拖动鼠标 选中代码块 Command + Shift + Option + Up 列选择 向上 Command + Shift + Option + Down 列选择 向下 Command + Shift + Option + Left 列选择 向左 Command + Shift + Option + Right 列选择 向右 Command + Shift + Option + PgUp 列选择 向上翻页 Command + Shift + Option + PgDown 列选择 向下翻页 查找替换 Command + F 查找 Command + Option + F 替换 Command + G 查找下一个 Command + Shift + G 查找上一个 Option + Enter 选中所有匹配项 Command + D 向下选中相同内容 Command + K Command + D 移除前一个向下选中相同内容 进阶 Ctrl + Space 打开建议 Command + Shift + Space 参数提示 Tab Emmet插件缩写补全 Option + Shift + F 格式化 Command + K Command + F 格式化选中内容 F12 跳转到声明位置 Option + F12 查看具体声明内容 Command + K F12 分屏查看具体声明内容 Command + . 快速修复 Shift + F12 显示引用 F2 重命名符号 Command + Shift + . 替换为上一个值 Command + Shift + , 替换为下一个值 Command + K Command + X 删除行尾多余空格 Command + K M 更改文件语言 导航 Command + T 显示所有符号 Ctrl + G 跳转至某行 Command + P 跳转到某个文件 Command + Shift + O 跳转到某个符号 Command + Shift + M 打开问题面板 F8下一个错误或警告位置 Shift + F8 上一个错误或警告位置 Ctrl + Shift + Tab 编辑器历史记录 Ctrl + -后退 Ctrl + Shift + - 前进 Ctrl + Shift + M Tab 切换焦点 编辑器管理 Command + W 关闭编辑器 Command + K F 关闭文件夹 Command + \\ 编辑器分屏 Command + 1 切换到第一分组 Command + 2 切换到第二分组 Command + 3 切换到第三分组 Command + K Command + Left 切换到上一分组 Command + K Command + Right 切换到下一分组 Command + K Command + Shift + Left 左移编辑器 Command + K Command + Shift + Right 右移编辑器 Command + K Left 激活左侧编辑组 Command + K Right 激活右侧编辑组 文件管理 Command + N 新建文件 Command + O 打开文件 Command + S 保存文件 Command + Shift + S 另存为 Command + Option + S 全部保存 Command + W 关闭 Command + K Command + W 全部关闭 Command + Shift + T 重新打开被关闭的编辑器 Command + K Enter 保持打开 Ctrl + Tab 打开下一个 Ctrl + Shift + Tab 打开上一个 Command + K P 复制当前文件路径 Command + K R 在资源管理器中查看当前文件 Command + K O 新窗口打开当前文件 显示 Command + Ctrl + F 全屏、退出全屏 Command + Option + 1 切换编辑器分屏方式（横、竖） Command + + 放大 Command + - 缩小 Command + B 显示、隐藏侧边栏 Command + Shift + E 显示资源管理器 或 切换焦点 Command + Shift + F 显示搜索框 Ctrl + Shift + G 显示Git面板 Command + Shift + D 显示调试面板 Command + Shift + X 显示插件面板 Command + Shift + H 全局搜索替换 Command + Shift + J 显示、隐藏高级搜索 Command + Shift + C 打开新终端 Command + Shift + U 显示输出面板 Command + Shift + V Markdown预览窗口 Command + K V 分屏显示 Markdown预览窗口 调试 F9 设置 或 取消断点 F5 开始 或 继续 F11 进入 Shift + F11 跳出 F10 跳过 Command + K Command + I 显示悬停信息 集成终端 Ctrl +显示终端 Ctrl + Shift +新建终端 Command + Up 向上滚动 Command + Down 向下滚动 PgUp 向上翻页 PgDown 向下翻页 Command + Home 滚动到顶部 Command + End 滚动到底部 windows \u0026amp; linux 注释:\n单行注释:[ctrl+k,ctrl+c] 或 ctrl+/\n取消单行注释:[ctrl+k,ctrl+u] (按下ctrl不放，再按k + u)\n多行注释:[alt+shift+A]\n多行注释:/**\n移动行:alt+up/down\n显示/隐藏左侧目录栏 ctrl + b\n复制当前行:shift + alt +up/down\n删除当前行:shift + ctrl + k\n控制台终端显示与隐藏:ctrl + ~\n查找文件/安装vs code 插件地址:ctrl + p\n代码格式化:shift + alt +f\n新建一个窗口: ctrl + shift + n\n行增加缩进: ctrl + [\n行减少缩进: ctrl + ]\n裁剪尾随空格(去掉一行的末尾那些没用的空格) : ctrl + shift + x\n字体放大/缩小: ctrl + ( + 或 - )\n拆分编辑器 :ctrl + 1/2/3\n切换窗口: ctrl + shift + left/right\n关闭编辑器窗口: ctrl + w\n关闭所有窗口 : ctrl + k + w\n切换全屏 :F11\n自动换行: alt + z\n显示git: ctrl + shift + g\n全局查找文件:ctrl + p\n显示相关插件的命令(如:git log):ctrl + shift + p\n选中文字:shift + left / right / up / down\n折叠代码: ctrl + k + 0-9 (0是完全折叠)\n展开代码: ctrl + k + j (完全展开代码)\n删除行 : ctrl + shift + k\n快速切换主题:ctrl + k / ctrl + t\n快速回到顶部 : ctrl + home\n快速回到底部 : ctrl + end\n格式化选定代码 :ctrl + k / ctrl +f\n选中代码 : shift + 鼠标左键\n多行同时添加内容（光标）:ctrl + alt + up/down\n全局替换:ctrl + shift + h\n当前文件替换:ctrl + h\n打开最近打开的文件:ctrl + r\n打开新的命令窗:ctrl + shift + c\nvscode常用插件 Bracket Pair Colorize 2 彩虹括号 Material Icon Theme 文件样式 open in browser 游览器打开html Remote Development 远程开发 protobuf proto buffer协议文件编写 Gitlens git加强版 go go语言 Python Python语言 Vetur vue框架 todo-tree todo 原文地址 https://zhaohaiyu.com/post/editor/vscode/ ","date":"2022-03-25","permalink":"https://daemon365.dev/2022/03/25/vscode%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E5%8F%8A%E6%8F%92%E4%BB%B6/","tags":["vscode"],"title":"vscode常用快捷键及插件"},{"content":"make make是一个构建自动化工具，会在当前目录下寻找Makefile或makefile文件。如果存在相应的文件，它就会依据其中定义好的规则完成构建任务。\nmakefile 什么是makefile？或许很多Winodws的程序员都不知道这个东西，因为那些Windows的IDE都为你做了这个工作，但我觉得要作一个好的和professional的程序员，makefile还是要懂。这就好像现在有这么多的HTML的编辑器，但如果你想成为一个专业人士，你还是要了解HTML的标识的含义。特别在Unix下的软件编译，你就不能不自己写makefile了，会不会写makefile，从一个侧面说明了一个人是否具备完成大型工程的能力。因为，makefile关系到了整个工程的编译规则。一个工程中的源文件不计数，其按类型、功能、模块分别放在若干个目录中，makefile定义了一系列的规则来指定，哪些文件需要先编译，哪些文件需要后编译，哪些文件需要重新编译，甚至于进行更复杂的功能操作，因为makefile就像一个Shell脚本一样，其中也可以执行操作系统的命令。makefile带来的好处就是——“自动化编译”，一旦写好，只需要一个make命令，整个工程完全自动编译，极大的提高了软件开发的效率。make是一个命令工具，是一个解释makefile中指令的命令工具，一般来说，大多数的IDE都有这个命令，比如：Delphi的make，Visual C++的nmake，Linux下GNU的make。可见，makefile都成为了一种在工程方面的编译方法。\n规则概述 Makefile由多条规则组成，每条规则主要由两个部分组成，分别是依赖的关系和执行的命令。\n其结构如下所示：\n[target] ... : [prerequisites] ... [command] ... ... 其中：\ntargets：规则的目标 prerequisites：可选的要生成 targets 需要的文件或者是目标。 command：make 需要执行的命令（任意的 shell 命令）。可以有多条命令，每一条命令占一行。 举个例子：\nbuild: CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o xx 示例 .PHONY: all build run gotool clean help BINARY=\u0026quot;coursemanager\u0026quot; all: build build: CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o ${BINARY} run: @go run ./ gotool: go fmt ./ go vet ./ clean: @if [ -f ${BINARY} ] ; then rm ${BINARY} ; fi help: @echo \u0026quot;make - 格式化 Go 代码, 并编译生成二进制文件\u0026quot; @echo \u0026quot;make build - 编译 Go 代码, 生成二进制文件\u0026quot; @echo \u0026quot;make run - 直接运行 Go 代码\u0026quot; @echo \u0026quot;make clean - 移除二进制文件和 vim swap files\u0026quot; @echo \u0026quot;make gotool - 运行 Go 工具 'fmt' and 'vet'\u0026quot; 参考文章:\nhttps://www.liwenzhou.com/posts/Go/makefile/ https://blog.csdn.net/weixin_38391755/article/details/80380786 ","date":"2021-12-15","permalink":"https://daemon365.dev/2021/12/15/makefile/","tags":["makefile"],"title":"makefile"},{"content":"我们在微服务框架kratos v2的默认项目模板中kratos-layout使用了google/wire进行依赖注入，也建议开发者在维护项目时使用该工具。\nwire 乍看起来比较违反直觉，导致很多同学不理解为什么要用或不清楚如何用（也包括曾经的我），本文来帮助大家理解 wire 的使用。\nWhat wire是由 google 开源的一个供 Go 语言使用的依赖注入代码生成工具。它能够根据你的代码，生成相应的依赖注入 go 代码。\n而与其它依靠反射实现的依赖注入工具不同的是，wire 能在编译期（准确地说是代码生成时）如果依赖注入有问题，在代码生成时即可报出来，不会拖到运行时才报，更便于 debug。\nWhy 理解依赖注入 什么是依赖注入？为什么要依赖注入？ 依赖注入就是 Java 遗毒（不是）\n依赖注入 (Dependency Injection，缩写为 DI)，可以理解为一种代码的构造模式（就是写法），按照这样的方式来写，能够让你的代码更加容易维护。\n对于很多软件设计模式和架构的理念，我们都无法理解他们要绕好大一圈做复杂的体操、用奇怪的方式进行实现的意义。他们通常都只是丢出来一段样例，说这样写就很好很优雅，由于省略掉了这种模式是如何发展出来的推导过程，我们只看到了结果，导致理解起来很困难。那么接下来我们来尝试推导还原一下整个过程，看看代码是如何和为什么演进到依赖注入模式的，以便能够更好理解使用依赖注入的意义。\n依赖是什么？ 这里的依赖是个名词，不是指软件包的依赖（比如那坨塞在 node_modules 里面的东西），而是指软件中某一个模块（对象/实例）所依赖的其它外部模块（对象/实例）。\n注入到哪里？ 被依赖的模块，在创建模块时，被注入到（即当作参数传入）模块的里面。\n不 DI 是啥样？DI 了又样子？ 下面用 go 伪代码来做例子，领会精神即可。\n假设个场景，你在打工搞一个 web 应用，它有一个简单接口。最开始的项目代码可能长这个样子：\n# 下面为伪代码，忽略了很多与主题无关的细节 type App struct { } # 假设这个方法将会匹配并处理 GET /biu/\u0026lt;id\u0026gt; 这样的请求 func (a *App) GetData(id string) string { # todo: write your data query return \u0026quot;some data\u0026quot; } func NewApp() *App { return \u0026amp;App{} } app := App() app.Run() 你要做的是接一个 mysql，从里面把数据按照 id 查出来，返回。 要连 mysql 的话，假设我们已经有了个NewMySQLClient的方法返回 client 给你，初始化时传个地址进去就能拿到数据库连接，并假设它有个Exec的方法给你执行参数。\n不用 DI，通过全局变量传递依赖实例 一种写法是，在外面全局初始化好 client，然后 App 直接拿来调用。\nvar mysqlUrl = \u0026quot;mysql://blabla\u0026quot; var db = NewMySQLClient(mysqlUrl) type App struct { } func (a *App) GetData(id string) string { data := db.Exec(\u0026quot;select data from biu where id = ? limit 1\u0026quot;, id) return data } func NewApp() *App { return \u0026amp;App{} } func main() { app := App() app.Run() } 这就是没用依赖注入，app 依赖了全局变量 db，这是比较糟糕的一种做法。db 这个对象游离在全局作用域，暴露给包下的其他模块，比较危险。（设想如果这个包里其他代码在运行时悄悄把你的这个 db 变量替换掉会发生啥）\n不用 DI，在 App 的初始化方法里创建依赖实例 另一种方式是这样的：\ntype App struct { db *MySQLClient } func (a *App) GetData(id string) string { data := a.db.Exec(\u0026quot;select data from biu where id = ? limit 1\u0026quot;, id) return data } func NewApp() *App { return \u0026amp;App{db: NewMySQLClient(mysqlUrl)} } func main() { app := NewApp(\u0026quot;mysql://blabla\u0026quot;) app.Run() } 这种方法稍微好一些，db 被塞到 app 里面了，不会有 app 之外的无关代码碰它，比较安全，但这依然不是依赖注入，而是在内部创建了依赖，接下来你会看到它带来的问题。\n老板：我们的数据要换个地方存 （需要变更实现） 你的老板不知道从哪听说——Redis 贼特么快，要不我们的数据改从 Redis 里读吧。这个时候你的内心有点崩溃，但毕竟要恰饭的，就硬着头皮改上面的代码。\ntype App struct { ds *RedisClient } func (a *App) GetData(id string) string { data := a.ds.Do(\u0026quot;GET\u0026quot;, \u0026quot;biu_\u0026quot;+id) return data } func NewApp() *App { return \u0026amp;App{ds: NewRedisClient(redisAddr)} } func main() { app := NewApp(\u0026quot;redis://ooo\u0026quot;) app.Run() } 上面基本进行了 3 处修改：\nApp 初始化方法里改成了初始化 RedisClient get_data 里取数据时改用 run 方法，并且查询语句也换了 App 实例化时传入的参数改成了 redis 地址 老板：要不，我们再换个地方存？/我们要加测试，需要 Mock 老板的思路总是很广的，又过了两天他又想换成 Postgres 存了；或者让你们给 App 写点测试代码，只测接口里面的逻辑，通常我们不太愿意在旁边再起一个数据库，那么就需要 mock 掉数据源这块东西，让它直接返回数据给请求的 handler 用，来进行针对性的测试。\n这种情况怎么办？再改里面的代码？这不科学。\n面向接口编程 一个很重要的思路就是要面向接口(interface)编程，而不是面向具体实现编程。\n什么叫面向具体实现编程呢？比如上述的例子里改动的部分：调 mysqlclient 的 exec_sql 执行一条 sql，被改成了：调 redisclient 的 do 执行一句 get 指令。由于每种 client 的接口设计不同，每换一个实现，就得改一遍。\n而面向接口编程的思路，则完全不同。我们不要听老板想用啥就马上写代码。首先就得预料到，这个数据源的实现很有可能被更换，因此在一开始就应该做好准备（设计）。\n设计接口 Python 里面有个概念叫鸭子类型(duck-typing)，就是如果你叫起来像鸭子，走路像鸭子，游泳像鸭子，那么你就是一只鸭子。这里的叫、走路、游泳就是我们约定的鸭子接口，而你如果完整实现了这些接口，我们可以像对待一个鸭子一样对待你。\n在我们上面的例子中，不论是 Mysql 实现还是 Redis 实现，他们都有个共同的功能：用一个 id，查一个数据出来，那么这就是共同的接口。\n我们可以约定一个叫 DataSource 的接口，它必须有一个方法叫 GetById，功能是要接收一个 id，返回一个字符串\ntype DataSource interface { GetById(id string) string } 然后我们就可以把各个数据源分别进行封装，按照这个 interface 定义实现接口，这样我们的 App 里处理请求的部分就可以稳定地调用 GetById 这个方法，而底层数据实现只要实现了 DataSource 这个 interface 就能花式替换，不用改 App 内部的代码了。\n// 封装个redis type redis struct { r *RedisClient } func NewRedis(addr string) *redis { return \u0026amp;redis{r: NewRedisClient(addr)} } func (r *redis) GetById(id string) string { return r.r.Do(\u0026quot;GET\u0026quot;, \u0026quot;biu_\u0026quot;+id) } // 再封装个mysql type mysql struct { m *MySQLClient } func NewMySQL(addr string) *redis { return \u0026amp;mysql{m: NewMySQLClient(addr)} } func (m *mysql) GetById(id string) string { return r.m.Exec(\u0026quot;select data from biu where id = ? limit 1\u0026quot;, id) } type App struct { ds DataSource } func NewApp(addr string) *App { //需要用Mysql的时候 return \u0026amp;App{ds: NewMySQLClient(addr)} //需要用Redis的时候 return \u0026amp;App{ds: NewRedisClient(addr)} } 由于两种数据源都实现了 DataSource 接口，因此可以直接创建一个塞到 App 里面了，想用哪个用哪个，看着还不错？\n等一等，好像少了些什么 addr 作为参数，是不是有点简单？通常初始化一个数据库连接，可能有一堆参数，配在一个 yaml 文件里，需要解析到一个 struct 里面，然后再传给对应的 New 方法。\n配置文件可能是这样的：\nredis: addr: 127.0.0.1:6379 read_timeout: 0.2s write_timeout: 0.2s 解析结构体是这样的：\ntype RedisConfig struct { Network string `json:\u0026quot;network,omitempty\u0026quot;` Addr string `json:\u0026quot;addr,omitempty\u0026quot;` ReadTimeout *duration.Duration `json:\u0026quot;read_timeout,omitempty\u0026quot;` WriteTimeout *duration.Duration `json:\u0026quot;write_timeout,omitempty\u0026quot;` } 结果你的NewApp方法可能就变成了这个德性：\nfunc NewApp() *App { var conf *RedisConfig yamlFile, err := ioutil.ReadFile(\u0026quot;redis_conf.yaml\u0026quot;) if err != nil { panic(err) } err = yaml.Unmarshal(yamlFile, \u0026amp;conf) if err != nil { panic(err) } return \u0026amp;App{ds: NewRedisClient(conf)} } NewApp 说，停停，你们年轻人不讲武德，我的责任就是创建一个 App 实例，我只需要一个 DataSource 注册进去，至于这个 DataSource 是怎么来的我不想管，这么一坨处理 conf 的代码凭什么要放在我这里，我也不想关心你这配置文件是通过网络请求拿来的还是从本地磁盘读的，我只想把 App 组装好扔出去直接下班。\n依赖注入终于可以登场了 还记得前面是怎么说依赖注入的吗？被依赖的模块，在创建模块时，被注入到（即当作参数传入）初始化函数里面。通过这种模式，正好可以让 NewApp 早点下班。我们在外面初始化好 NewRedis 或者 NewMysql，得到的 DataSource 直接扔给 NewApp。\n也就是这样\nfunc NewApp(ds DataSource) *App { return \u0026amp;App{ds: ds} } 那坨读配置文件初始化 redis 的代码扔到初始化 DataSource 的方法里去\nfunc NewRedis() DataSource { var conf *RedisConfig yamlFile, err := ioutil.ReadFile(\u0026quot;redis_conf.yaml\u0026quot;) if err != nil { panic(err) } err = yaml.Unmarshal(yamlFile, \u0026amp;conf) if err != nil { panic(err) } return \u0026amp;redis{r: NewRedisClient(conf)} } 更进一步，NewRedis 这个方法甚至也不需要关心文件是怎么读的，它的责任只是通过 conf 初始化一个 DataSource 出来，因此你可以继续把读 config 的代码往外抽，把 NewRedis 做成接收一个 conf，输出一个 DataSource\nfunc GetRedisConf() *RedisConfig func NewRedis(conf *RedisConfig) DataSource 因为之前整个组装过程是散放在 main 函数下面的，我们把它抽出来搞成一个独立的 initApp 方法。最后你的 App 初始化逻辑就变成了这样\nfunc initApp() *App { c := GetRedisConf() r := NewRedis(c) app := NewApp(r) return app } func main() { app := initApp() app.Run() } 然后你可以通过实现 DataSource 的接口，更换前面的读取配置文件的方法，和更换创建 DataSource 的方法，来任意修改你的底层实现（读配置文件的实现，和用哪种 DataSource 来查数据），而不用每次都改一大堆代码。这使得你的代码层次划分得更加清楚，更容易维护了。\n这就是依赖注入。\n手工依赖注入的问题 上文这一坨代码，把各个实例初始化好，再按照各个初始化方法的需求塞进去，最终构造出 app 的这坨代码，就是注入依赖的过程。\nc := GetRedisConf() r := NewRedis(c) app := NewApp(r) 目前只有一个 DataSource，这样手写注入过程还可以，一旦你要维护的东西多了，比如你的 NewApp 是这样的NewApp(r *Redis, es *ES, us *UserSerivce, db *MySQL) *App然后其中 UserService 是这样的UserService(pg *Postgres, mm *Memcached)，这样形成了多层次的一堆依赖需要注入，徒手去写非常麻烦。\n而这部分，就是 wire 这样的依赖注入工具能够起作用的地方了——他的功能只是通过生成代码帮你注入依赖，而实际的依赖实例需要你自己创建（初始化）。\nHow wire 的主要问题是，看文档学不会。反正我最初看完文档之后是一头雾水——这是啥，这要干啥？但通过我们刚才的推导过程，应该大概理解了为什么要用依赖注入，以及 wire 在这其中起到什么作用——通过生成代码帮你注入依赖，而实际的依赖实例需要你自己创建（初始化）。\n接下来就比较清楚了。\n首先要实现一个wire.go的文件，里面定义好 Injector。\n// +build wireinject func initApp() (*App) { panic(wire.Build(GetRedisConf, NewRedis, SomeProviderSet, NewApp)) } 然后分别实现好 Provider。\n执行wire命令后 他会扫描整个项目，并帮你生成一个wire_gen.go文件，如果你有什么没有实现好，它会报错出来。\n你学会了吗？\n重新理解 等一等，先别放弃治疗，让我们用神奇的中文编程来解释一下要怎么做。\n谁参与编译？ 上面那个initApp方法，官方文档叫它 Injector，由于文件里首行// +build wireinject这句注释，这个 wire.go 文件只会由 wire 读取，在 go 编译器在编译代码时不会去管它，实际会读的是生成的 wire_gen.go 文件。\n而 Provider 就是你代码的一部分，肯定会参与到编译过程。\nInjector 是什么鬼东西？ Injector 就是你最终想要的结果——最终的 App 对象的初始化函数，也就是前面那个例子里的initApp方法。\n把它理解为你去吃金拱门，进门看到点餐机，噼里啪啦点了一堆，最后打出一张单子。\n// +build wireinject func 来一袋垃圾食品() 一袋垃圾食品 { panic(wire.Build(来一份巨无霸套餐, 来一份双层鳕鱼堡套餐, 来一盒麦乐鸡, 垃圾食品打包)) } 这就是你点的单子，它不参与编译，实际参与编译的代码是由 wire 帮你生成的。\nProvider 是什么鬼东西？ Provider 就是创建各个依赖的方法，比如前面例子里的 NewRedis 和 NewApp 等。\n你可以理解为，这些是金拱门的服务员和后厨要干的事情： 金拱门后厨需要提供这些食品的制作服务——实现这些实例初始化方法。\nfunc 来一盒麦乐鸡() 一盒麦乐鸡 {} func 垃圾食品打包(一份巨无霸套餐, 一份双层鳕鱼堡套餐, 一盒麦乐鸡) 一袋垃圾食品 {} wire 里面还有个 ProviderSet 的概念，就是把一组 Provider 打包，因为通常你点单的时候很懒，不想这样点你的巨无霸套餐：我要一杯可乐，一包薯条，一个巨无霸汉堡；你想直接戳一下就好了，来一份巨无霸套餐。这个套餐就是 ProviderSet，一组约定好的配方，不然你的点单列表（injector 里的 Build）就会变得超级长，这样你很麻烦，服务员看着也很累。\n用其中一个套餐举例\n// 先定义套餐内容 var 巨无霸套餐 = wire.NewSet(来一杯可乐，来一包薯条，来一个巨无霸汉堡) // 然后实现各个食品的做法 func 来一杯可乐() 一杯可乐 {} func 来一包薯条() 一包薯条 {} func 来一个巨无霸汉堡() 一个巨无霸汉堡 {} wire 工具做了啥？ 重要的事情说三遍，通过生成代码帮你注入依赖。\n在金拱门的例子里就是，wire 就是个服务员，它按照你的订单，去叫做相应的同事把各个食物/套餐做好，然后最终按需求打包给你。这个中间协调构建的过程，就是注入依赖。\n这样的好处就是， 对于金拱门，假设他们突然换可乐供应商了，直接把来一杯可乐替换掉就行，返回一种新的可乐，而对于顾客不需要有啥改动。 对于顾客来说，点单内容可以变换，比如我今天不想要麦乐鸡了，或者想加点别的，只要改动我的点单(只要金拱门能做得出来)，然后通过 wire 重新去生成即可，不需要关注这个服务员是如何去做这个订单的。\n现在你应该大概理解 wire 的用处和好处了。\n总结 让我们从金拱门回来，重新总结一下用 wire 做依赖注入的过程。\n1. 定义 Injector 创建wire.go文件，定义下你最终想用的实例初始化函数例如initApp（即 Injector），定好它返回的东西*App，在方法里用panic(wire.Build(NewRedis, SomeProviderSet, NewApp))罗列出它依赖哪些实例的初始化方法（即 Provider）/或者哪些组初始化方法（ProviderSet）\n2. 定义 ProviderSet（如果有的话） ProviderSet 就是一组初始化函数，是为了少写一些代码，能够更清晰的组织各个模块的依赖才出现的。也可以不用，但 Injector 里面的东西就需要写一堆。 像这样 var SomeProviderSet = wire.NewSet(NewES,NewDB)定义 ProviderSet 里面包含哪些 Provider\n3. 实现各个 Provider Provider 就是初始化方法，你需要自己实现，比如 NewApp，NewRedis，NewMySQL，GetConfig 等，注意他们们各自的输入输出\n4. 生成代码 执行 wire 命令生成代码，工具会扫描你的代码，依照你的 Injector 定义来组织各个 Provider 的执行顺序，并自动按照 Provider 们的类型需求来按照顺序执行和安排参数传递，如果有哪些 Provider 的要求没有满足，会在终端报出来，持续修复执行 wire，直到成功生成wire_gen.go文件。接下来就可以正常使用initApp来写你后续的代码了。\n如果需要替换实现，对 Injector 进行相应的修改，实现必须的 Provider，重新生成即可。\n它生成的代码其实就是类似我们之前需要手写的这个\nfunc initApp() *App { // injector c := GetRedisConf() // provider r := NewRedis(c) // provider app := NewApp(r) // provider return app } 由于我们的例子比较简单，通过 wire 生成体现不出优势，但如果我们的软件复杂，有很多层级的依赖，使用 wire 自动生成注入逻辑，无疑更加方便和准确。\n5. 高级用法 wire 还有更多功能，比如 cleanup, bind 等等，请参考官方文档来使用。\n最后，其实多折腾几次，就会使用了，希望本文能对您起到一定程度上的帮助。\n文章转自 https://go-kratos.dev/blog/go-project-wire ","date":"2021-09-30","permalink":"https://daemon365.dev/2021/09/30/go%E5%B7%A5%E7%A8%8B%E5%8C%96-%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5/","tags":["go","kratos","wire"],"title":"Go工程化 - 依赖注入"},{"content":"使用 下载 go install github.com/go-kratos/kratos/cmd/kratos/v2@latest 查看是否安装成功\nkratos -v kratos version v2.1.3 升级 kratos upgrade 查看帮助 kratos --help Kratos: An elegant toolkit for Go microservices. Usage: kratos [command] Available Commands: changelog Get a kratos change log completion generate the autocompletion script for the specified shell help Help about any command new Create a service template proto Generate the proto files run Run project upgrade Upgrade the kratos tools Flags: -h, --help help for kratos -v, --version version for kratos Use \u0026quot;kratos [command] --help\u0026quot; for more information about a command. new命令 kratos new 命令为创建一个kratos项目\n参数：\n-r repo地址 默认为https://github.com/go-kratos/kratos-layout\n-b git版本 默认为main分支\n-t 超时时间 默认为60s\n也可添加环境变量KRATOS_LAYOUT_REPO 知道远程repo\n创建一个项目\nkratos new helloworld 因为默认远程仓库地址是 github上的，在国内很容易创建失败，所以要需要设置终端或者git代理（什么是终端代理和git代理可以百度或者google一下）。\n当然你也可以使用-r 知道国内仓库 我们提供一个国内镜像https://gitee.com/go-kratos/kratos-layout。\n如果嫌弃每次都要-r指定麻烦，也可以把KRATOS_LAYOUT_REPO=https://gitee.com/go-kratos/kratos-layout 加入到path中。\nkratos new helloworld -r https://gitee.com/go-kratos/kratos-layout proto命令 proto命令下有 add client 和 server子命令\nadd kratos proto add 为创建一个proto模板\nkratos proto add api/helloworld/v2/hello.proto 在目录api/helloworld/v2 下可以看到生成的文件\nsyntax = \u0026quot;proto3\u0026quot;; package api.helloworld.v2; option go_package = \u0026quot;helloworld/api/helloworld/v2;v2\u0026quot;; option java_multiple_files = true; option java_package = \u0026quot;api.helloworld.v2\u0026quot;; service Hello { rpc CreateHello (CreateHelloRequest) returns (CreateHelloReply); rpc UpdateHello (UpdateHelloRequest) returns (UpdateHelloReply); rpc DeleteHello (DeleteHelloRequest) returns (DeleteHelloReply); rpc GetHello (GetHelloRequest) returns (GetHelloReply); rpc ListHello (ListHelloRequest) returns (ListHelloReply); } message CreateHelloRequest {} message CreateHelloReply {} message UpdateHelloRequest {} message UpdateHelloReply {} message DeleteHelloRequest {} message DeleteHelloReply {} message GetHelloRequest {} message GetHelloReply {} message ListHelloRequest {} message ListHelloReply {} client kratos proto client 为生成 Proto 代码\n使用这个命令需要下载 protobuf 工具 protoc，可以在官网下载对应版本 Protobuf release版本\nkratos proto client api/helloworld/v2/ 这条命令就可以编译api/helloworld/v2/下的所有.proto文件\n如果我们需要 import 其他proto文件 可以在命令后面加上protoc的参数\n比如\nkratos proto client api/helloworld/v2/ --proto_path=api/helloworld/v2 默认也会把 ./third_party 下import 进来 需要第三方的proto文件 可以放在这里\nserver kratos proto server为指定proto文件生成简单的service代码\n参数：\n-t 生成代码的位置 默认是internal/service 比如\nkratos proto server api/helloworld/v2/hello.proto -t=internal/service/hello 生成的代码\npackage service import ( \u0026quot;context\u0026quot; pb \u0026quot;helloworld/api/helloworld/v2\u0026quot; ) type HelloService struct { pb.UnimplementedHelloServer } func NewHelloService() *HelloService { return \u0026amp;HelloService{} } func (s *HelloService) ListHello(ctx context.Context, req *pb.ListHelloRequest) (*pb.ListHelloReply, error) { return \u0026amp;pb.ListHelloReply{}, nil } run命令 启动服务\nkratos run 原文地址 kratos v2版本命令行工具使用 - haiyux\u0026rsquo;s blog ","date":"2021-09-12","permalink":"https://daemon365.dev/2021/09/12/kratos-v2%E7%89%88%E6%9C%AC%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/","tags":["go","kratos"],"title":"kratos v2版本命令行工具使用"},{"content":"为什么要用熔断 前面我们讲过限流保证服务的可用性，不被突如其来的流量打爆。但是两种情况是限流解决不了的。\n如果我们服务只能处理1000QPS，但是有10wQPS打过来，服务还是会炸。因为拒绝请求也需要成本。 服务但是io型的，会把mysql，redis，mq等中间件打挂。 所以，我们遵循一个思路，可不可以client端在失败的多的时候就不调用了，直接返回错误呢？\n什么是熔断 熔断器是为了当依赖的服务已经出现故障时，主动阻止对依赖服务的请求。保证自身服务的正常运行不受依赖服务影响，防止雪崩效应。\n源码分析 源码地址 https://github.com/go-kratos/aegis/tree/main/circuitbreaker CircuitBreaker 接口 type CircuitBreaker interface { Allow() error MarkSuccess() MarkFailed() } Allow() 判断熔断器是否允许通过 MarkSuccess() 熔断器成功的回调 MarkFailed() 熔断器失败的回调 Group 结构体 type Group struct { mutex sync.Mutex val atomic.Value New func() CircuitBreaker } mutex 互斥锁，使val这个map不产生数据竞争 val map，存储name -\u0026gt; CircuitBreaker New 生成一个CircuitBreaker Get方法 // Get . func (g *Group) Get(name string) CircuitBreaker { m, ok := g.val.Load().(map[string]CircuitBreaker) if ok { breaker, ok := m[name] if ok { return breaker // 很具name从val拿出 breaker 如果存在返回 } } // slowpath for group don`t have specified name breaker. g.mutex.Lock() nm := make(map[string]CircuitBreaker, len(m)+1) for k, v := range m { nm[k] = v } breaker := g.New() nm[name] = breaker // 如果不存在 生成一个 并放入map 并返回 g.val.Store(nm) g.mutex.Unlock() return breaker } Breaker 结构体 // Breaker is a sre CircuitBreaker pattern. type Breaker struct { stat window.RollingCounter r *rand.Rand // rand.New(...) returns a non thread safe object randLock sync.Mutex // Reducing the k will make adaptive throttling behave more aggressively, // Increasing the k will make adaptive throttling behave less aggressively. k float64 request int64 state int32 } stat 滑动窗口，记录成功失败 r 随机数 randLock 读写锁 k 成功系数 total(总数) = success * k request 请求数 当总数 \u0026lt; request时，不判断是否熔断 state 熔断器状态 打开或者关闭 Allow()方法 // Allow request if error returns nil. func (b *Breaker) Allow() error { success, total := b.summary() // 从活动窗口获取成功数和总数 k := b.k * float64(success) // 根据k成功系数 获取 // check overflow requests = K * success if total \u0026lt; b.request || float64(total) \u0026lt; k { // 如果总数\u0026lt;request 或者 总数 \u0026lt; k if atomic.LoadInt32(\u0026amp;b.state) == StateOpen { atomic.CompareAndSwapInt32(\u0026amp;b.state, StateOpen, StateClosed) // 如果state是打开 关闭 } return nil } if atomic.LoadInt32(\u0026amp;b.state) == StateClosed { atomic.CompareAndSwapInt32(\u0026amp;b.state, StateClosed, StateOpen) // 如果state是关闭 打开 } dr := math.Max(0, (float64(total)-k)/float64(total+1)) // 获取系数，当k越大 dr越小 drop := b.trueOnProba(dr) // trueOnProba 获取水机数 // 返回是否\u0026lt;dr if drop { // 如果是 拒绝请求 return circuitbreaker.ErrNotAllowed } return nil } func (b *Breaker) trueOnProba(proba float64) (truth bool) { b.randLock.Lock() truth = b.r.Float64() \u0026lt; proba b.randLock.Unlock() return } 使用trueOnProba的原因是，当熔断器关闭时，随机让一部分请求通过，当success越大，请求的通过的数量就越多。用这些数据成功与否，放入窗口统计，当成功数达到要求时，就可以关闭熔断器了。\nMarkSuccess()以及MarkFailed()方法 // MarkSuccess mark requeest is success. func (b *Breaker) MarkSuccess() { b.stat.Add(1) // 成功数+1 } // MarkFailed mark request is failed. func (b *Breaker) MarkFailed() { // NOTE: when client reject requets locally, continue add counter let the // drop ratio higher. b.stat.Add(0) // 失败数+1 } 流程图 原文地址 https://zhaohaiyu.com/posts/microservice/breaker/ ","date":"2021-09-04","permalink":"https://daemon365.dev/2021/09/04/%E4%BB%8Ekratos%E5%88%86%E6%9E%90breaker%E7%86%94%E6%96%AD%E5%99%A8%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0/","tags":["go","kratos","breaker","源码分析"],"title":"从kratos分析breaker熔断器源码实现"},{"content":"什么是自适应限流 自适应限流从整体维度对应用入口流量进行控制，结合应用的 Load、CPU 使用率、总体平均 RT、入口 QPS 和并发线程数等几个维度的监控指标，通过自适应的流控策略，让系统的入口流量和系统的负载达到一个平衡，让系统尽可能跑在最大吞吐量的同时保证系统整体的稳定性。\n核心目标：\n自动嗅探负载和 qps，减少人工配置 削顶，保证超载时系统不被拖垮，并能以高水位 qps 继续运行 限流规则 计算吞吐量：利特尔法则 L = λ * W\n如上图所示，如果我们开一个小店，平均每分钟进店 2 个客人(λ)，每位客人从等待到完成交易需要 4 分钟(W)，那我们店里能承载的客人数量就是 2 * 4 = 8 个人\n同理，我们可以将 λ 当做 QPS， W 呢是每个请求需要花费的时间，那我们的系统的吞吐就是 L = λ * W ，所以我们可以使用利特尔法则来计算系统的吞吐量。\n指标介绍 指标名称 指标含义 cpu 最近 1s 的 CPU 使用率均值，使用滑动平均计算，采样周期是 250ms inflight 当前处理中正在处理的请求数量 pass 请求处理成功的量 rt 请求成功的响应耗时 滑动窗口 在自适应限流保护中，采集到的指标的时效性非常强，系统只需要采集最近一小段时间内的 qps、rt 即可，对于较老的数据，会自动丢弃。为了实现这个效果，kratos 使用了滑动窗口来保存采样数据。\n如上图，展示了一个具有两个桶（bucket）的滑动窗口（rolling window）。整个滑动窗口用来保存最近 1s 的采样数据，每个小的桶用来保存 500ms 的采样数据。 当时间流动之后，过期的桶会自动被新桶的数据覆盖掉，在图中，在 1000-1500ms 时，bucket 1 的数据因为过期而被丢弃，之后 bucket 3 的数据填到了窗口的头部。\n限流公式 判断是否丢弃当前请求的算法如下：\ncpu \u0026gt; 800 AND (Now - PrevDrop) \u0026lt; 1s AND (MaxPass * MinRt * windows / 1000) \u0026lt; InFlight MaxPass 表示最近 5s 内，单个采样窗口中最大的请求数。 MinRt 表示最近 5s 内，单个采样窗口中最小的响应时间。 windows 表示一秒内采样窗口的数量，默认配置中是 5s 50 个采样，那么 windows 的值为 10。\n源码分析 代码地址： https://github.com/go-kratos/aegis/tree/main/ratelimit/bbr BBR struct type BBR struct { cpu cpuGetter passStat window.RollingCounter rtStat window.RollingCounter inFlight int64 bucketPerSecond int64 bucketSize time.Duration // prevDropTime defines previous start drop since initTime prevDropTime atomic.Value maxPASSCache atomic.Value minRtCache atomic.Value opts *options } cpu cpu的指标函数，CPU的使用率， 这里为了减小误差，把数字扩大化，乘以1000，比赛使用率60%，也就是0.6 cpu的值就为600 passStat 请求数的采样数据，使用滑动窗口进行统计 rtStat 响应时间的采样数据，同样使用滑动窗口进行统计 inFlight 当前系统中的请求数，数据得来方法是：中间件原理在处理前+1，处理handle之后不管成功失败都减去1 bucketPerSecond 一个 bucket 的时间 bucketSize 桶的数量 prevDropTime 上次触发限流时间 maxPASSCache 单个采样窗口中最大的请求数的缓存数据 minRtCache 单个采样窗口中最小的响应时间的缓存数据 Allow接口 // Allow checks all inbound traffic. // Once overload is detected, it raises limit.ErrLimitExceed error. func (l *BBR) Allow(ctx context.Context) (func(), error) { if l.shouldDrop() { // shouldDrop 判断是否需要限流，如果true表示拒绝 之后重点讲 return nil, ErrLimitExceed } atomic.AddInt64(\u0026amp;l.inFlight, 1) // 之前说的，正在处理数+1 stime := time.Since(initTime) // 现在时间减去程序初始化时间 表示程序开始执行时刻 return func() { // allow返回函数 在中间件（拦截器）中handle执行完成后调用 rt := int64((time.Since(initTime) - stime) / time.Millisecond) // 执行完handle的时间减去stime 表示 程序执行的总时间 单位ms l.rtStat.Add(rt) // 把处理时间放进采样数据window atomic.AddInt64(\u0026amp;l.inFlight, -1) // 正在处理数-1 便是处理完成 l.passStat.Add(1) // 成功了，把通过数的采样数据window加1 }, nil } shouldDrop方法 func (l *BBR) shouldDrop() bool { curTime := time.Since(initTime) if l.cpu() \u0026lt; l.opts.CPUThreshold { // current cpu payload below the threshold prevDropTime, _ := l.prevDropTime.Load().(time.Duration) if prevDropTime == 0 { // haven't start drop, // accept current request return false } if curTime-prevDropTime \u0026lt;= time.Second { // just start drop one second ago, // check current inflight count inFlight := atomic.LoadInt64(\u0026amp;l.inFlight) return inFlight \u0026gt; 1 \u0026amp;\u0026amp; inFlight \u0026gt; l.maxInFlight() } l.prevDropTime.Store(time.Duration(0)) return false } // current cpu payload exceeds the threshold inFlight := atomic.LoadInt64(\u0026amp;l.inFlight) drop := inFlight \u0026gt; 1 \u0026amp;\u0026amp; inFlight \u0026gt; l.maxInFlight() if drop { prevDrop, _ := l.prevDropTime.Load().(time.Duration) if prevDrop != 0 { // already started drop, return directly return drop } // store start drop time l.prevDropTime.Store(curTime) } return drop } maxInFlight()方法代表过去的负载\nint64(math.Floor(float64(l.maxPASS()*l.minRT()*l.bucketPerSecond)/1000.0) + 0.5) 参考算法：https://github.com/alibaba/Sentinel/wiki/%E7%B3%BB%E7%BB%9F%E8%87%AA%E9%80%82%E5%BA%94%E9%99%90%E6%B5%81\nmaxPass * bucketPerSecond / 1000 为每毫秒处理的请求数 l.minRT() 为 单个采样窗口中最小的响应时间 T ≈ QPS * Avg(RT) + 0.5为向上取整 流程图 压测报告 场景1，请求以每秒增加1个的速度不停上升，压测效果如下：\n左测是没有限流的压测效果，右侧是带限流的压测效果。 可以看到，没有限流的场景里，系统在 700qps 时开始抖动，在 1k qps 时被拖垮，几乎没有新的请求能被放行，然而在使用限流之后，系统请求能够稳定在 600 qps 左右，rt 没有暴增，服务也没有被打垮，可见，限流有效的保护了服务。\n原文地址： https://zhaohaiyu.com/posts/microservice/overload/ 参考文章：\nhttps://v1.go-kratos.dev/#/ratelimit https://github.com/alibaba/Sentinel/wiki/%E7%B3%BB%E7%BB%9F%E8%87%AA%E9%80%82%E5%BA%94%E9%99%90%E6%B5%81 ","date":"2021-09-04","permalink":"https://daemon365.dev/2021/09/04/%E4%BB%8Ekratos%E5%88%86%E6%9E%90bbr%E9%99%90%E6%B5%81%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0/","tags":["go","kratos","BBR","源码分析"],"title":"从kratos分析BBR限流源码实现"},{"content":"您好，地球人，欢迎来到Kratos漫游指南。\n对于刚开始研究Kratos框架的开发者来说，目前的文档有些零散，这与我们的模块化设计有一些关系，不过Don\u0026rsquo;t panic，从这篇文章开始，我将试图打破这一现状，漫游指南系列将循序渐进地介绍Kratos框架，理顺框架的使用思路，使您更快上手Kratos。\n同时，这个系列也会逐步整合进官方文档中，同时重新组织整个文档的结构和内容，敬请期待。\n本篇是该系列的第一篇，主要介绍Kratos的整体概况。\n设计哲学 Kratos是一个Go语言实现的微服务框架，说得更准确一点，它更类似于一个使用Go构建微服务的工具箱，开发者可以按照自己的习惯选用或定制其中的的组件，来打造自己的微服务。也正是由于这样的原因，Kratos并不绑定于特定的基础设施，不限定于某种注册中心，或数据库ORM等，所以您可以十分轻松地将任意库集成进项目里，与Kratos共同运作。\n围绕这样的核心设计理念，我们设计了如下的项目生态：\nkratos Kratos框架核心，主要包含了基础的CLI工具，内置的HTTP/gRPC接口生成和服务生命周期管理，提供链路追踪、配置文件、日志、服务发现、监控等组件能力和相关接口定义。 contrib 基于上述核心定义的基础接口，对配置文件、日志、服务发现、监控等服务进行具体实现所形成的一系列插件，可以直接使用它们，也可以参考它们的代码，做您需要的服务的适配，从而集成进kratos项目中来。 aegis 我们将服务可用性相关的算法：如限流、熔断等算法放在了这个独立的项目里，几乎没有外部依赖，它更不依赖Kratos，您可以在直接在任意项目中使用。您也可以轻松将它集成到Kratos中使用，提高服务的可用性。 layout 我们设计的一个默认的项目模板，它包含一个参考了DDD和简洁架构设计的项目结构、Makefile脚本和Dockerfile文件。但这个项目模板不是必需的，您可以任意修改它，或使用自己设计的项目结构，Kratos依然可以正常工作。框架本身不对项目结构做任何假设和限制，您可以按照自己的想法来使用，具有很强的可定制性。 gateway 这个是我们刚刚起步，用Go开发的API Gateway，后续您可以使用它来作为您Kratos微服务的网关，用于微服务API的治理，项目正在施工中，欢迎关注。 仓库、文档和社区 GitHub仓库：https://github.com/go-kratos 文档：https://go-kratos.dev/ 微信群：go-kratos 官方微信群 Discord：go-kratos\n为什么v2完全重新设计 以前关注过kratos项目的可能知道，Kratos的v1版本已经开源了很久，也是个较为完善的框架。那么为什么不直接基于v1继续迭代，而是要推倒重来，推出完全重新设计的v2呢？\n经验源自踩坑。\n在业务不断迭代、项目不断膨胀的情况下，我们发现，过去的框架和项目结构设计，导致代码变更成本逐渐升高，而没有进行合理的抽象，导致\b\b更难进行模块的测试，也更难对第三方基础库进行适配和迁移，这在一定程度上拉低了生产力。\n因此，我们参考了大量的DDD和Clean Architecture等业界先进设计理念，重新设计了微服务的项目结构，并且这个结构随着我们的后续研究，会进一步进行迭代，让它成为微服务项目结构的最佳实践。\n没错，新版本的是从kratos-layout开始的。也许刚接触这个项目结构时会觉得不适应，但随着项目迭代，代码复杂度的提高，这个定义良好的结构，将使项目保持优秀的代码可读性、可测试性，以及令人满意的开发效率和可维护性。\n更重要的一点是，这一次我们想面向社区来设计和开发这个框架。让更多的开发者能够使用我们的框架来提高生产力，同时参与到我们的项目中来。\n所以我们把整个框架设计成为一个插座，我们希望整个框架轻量，插件化，可定制。对于几乎每一个微服务相关的功能模块，我们都设计了标准化接口，对于第三方库设计为插件，这样就能迅速把任意基础设施集成到使用Kratos的项目里，因此，无论您的公司使用何种基础设施，有何种规范，您都可以轻松将Kratos定制成与您的开发、生产环境相匹配的样子。\n不破不立，v2是一次从内到外的彻底革新，我们无法在旧版本上修修补补，而是选择重新设计和开发新版本。而目前v2版本也已经在很多生产环境使用，我们也将持续迭代和完善这个框架，同时也更欢迎各位开发者参与进来，一起让它变得更好。\n数据库/缓存/消息队列/\u0026hellip; 正如前文提到的，Kratos框架不限制您使用任何第三方库来进行项目开发，因此您可以根据喜好来选择库进行集成。我们也会逐步针对更多被广泛使用的第三方库开发插件。\n这里给出一些被广泛使用的库供参考：\n数据库：\ndatabase/sql 官方库 gorm ent 缓存：\ngo-redis redigo gomemcache 消息队列：\nsarama kafka客户端 kafka-go 其它更多的优秀go库，可以在awesome-go这个仓库中找找。\nCLI工具 kratos命令目前主要用于从模板创建项目，维护依赖包版本等。具体请参考文档\nProtobuf定义API Kratos使用Protobuf进行API定义。Protobuf是由Google开发的一种语言中立的数据序列化协议。它有结构定义清晰、可扩展性好、体积小、性能优秀等特点，在众多公司和项目被广泛使用。\n在使用Kratos的项目中，您将使用如下的IDL进行您的接口定义，并且通过protoc工具生成相应的.pb.go文件，其中包含根据定义生成的的服务端和客户端代码。随后您就可以在自己的项目内部注册服务端代码使用，或引用客户端代码进行远程调用。\nKratos默认仅生成gRPC接口的代码，如果需要生成HTTP代码，请在proto文件中使用option (google.api.http)来添加HTTP部分的定义后再进行生成。默认情况下，HTTP接口将使用JSON作为序列化格式，如果想使用其它序列化格式（form，XML等），请参考文档序列化进行相应的配置即可。\nsyntax = \u0026quot;proto3\u0026quot;; package helloworld.v1; import \u0026quot;google/api/annotations.proto\u0026quot;; option go_package = \u0026quot;github.com/go-kratos/kratos-layout/api/helloworld/v1;v1\u0026quot;; // The greeting service definition. service Greeter { // Sends a greeting rpc SayHello (HelloRequest) returns (HelloReply) { option (google.api.http) = { get: \u0026quot;/helloworld/{name}\u0026quot; }; } } // The request message containing the user's name. message HelloRequest { string name = 1; } // The response message containing the greetings message HelloReply { string message = 1; } 需要注意，虽然Protobuf定义的API的可靠性更强，但字段结构灵活性相对JSON要弱一些，因此如果您有诸如文件上传接口，或者某些无法对应到proto的\bJSON结构需要使用，我门还提供了“逃生门”，在我们的Protobuf体系之外定义这些接口，实现为普通的http.Handler并且挂载到路由上，或者用struct来定义您的字段。可以参考我们的upload例子进行实现。\n元信息传递 服务之间的API调用，如果有某些元信息需要传递过去，而不是写在payload消息中，可以使用Metadata包进行字段设置和提取，具体细节参考元信息传递文档\n错误处理 Kratos的errors模块提供了error的封装。框架也预定义了一系列标准错误供使用。\n错误处理这一块的设计也经过了很久的讨论才定下来，主要设计理念如下：\ncode 语义近似HTTP的Status Code（例如客户端传参数错误用400）同时也作为大类错误，在HTTP接口中的HTTP Code会使用它，好处是网关层可以根据这个code触发相应策略（重试、限流、熔断等）。 reason 业务的具体错误码，为可读的字符串，能够表明，在同一个服务中应该唯一。 message 用户可读的信息，可以在客户端（App、浏览器等）进行相应的展示给用户看。 metadata 为一些附加信息，可以作为补充信息使用。 在API返回的错误信息中，以HTTP接口为例，消息结构大概是长这个样子的：\n{ // 错误码，跟 http-status 一致，并且在 grpc 中可以转换成 grpc-status \u0026quot;code\u0026quot;: 500, // 错误原因，定义为业务判定错误码 \u0026quot;reason\u0026quot;: \u0026quot;USER_NOT_FOUND\u0026quot;, // 错误信息，为用户可读的信息，可作为用户提示内容 \u0026quot;message\u0026quot;: \u0026quot;invalid argument error\u0026quot;, // 错误元信息，为错误添加附加可扩展信息 \u0026quot;metadata\u0026quot;: {\u0026quot;some-key\u0026quot;: \u0026quot;some-value\u0026quot;} } 在Kratos中您可以使用proto文件定义您的业务错误，并通过工具生成对应的处理逻辑和方法。（如使用layout中提供的make errors指令。）\n错误定义：\nsyntax = \u0026quot;proto3\u0026quot;; package api.blog.v1; import \u0026quot;errors/errors.proto\u0026quot;; option go_package = \u0026quot;github.com/go-kratos/kratos/examples/blog/api/v1;v1\u0026quot;; enum ErrorReason { // 设置缺省错误码 option (errors.default_code) = 500; // 为某个枚举单独设置错误码 USER_NOT_FOUND = 0 [(errors.code) = 404]; CONTENT_MISSING = 1 [(errors.code) = 400];; } 错误创建：\n// 通过 errors.New() 响应错误 errors.New(500, \u0026quot;USER_NAME_EMPTY\u0026quot;, \u0026quot;user name is empty\u0026quot;) // 通过 proto 生成的代码响应错误，并且包名应替换为自己生成代码后的 package name api.ErrorUserNotFound(\u0026quot;user %s not found\u0026quot;, \u0026quot;kratos\u0026quot;) // 传递metadata err := errors.New(500, \u0026quot;USER_NAME_EMPTY\u0026quot;, \u0026quot;user name is empty\u0026quot;) err = err.WithMetadata(map[string]string{ \u0026quot;foo\u0026quot;: \u0026quot;bar\u0026quot;, }) 错误断言：\nerr := wrong() // 通过 errors.Is() 断言 if errors.Is(err,errors.BadRequest(\u0026quot;USER_NAME_EMPTY\u0026quot;,\u0026quot;\u0026quot;)) { // do something } // 通过判断 *Error.Reason 和 *Error.Code e := errors.FromError(err) if e.Reason == \u0026quot;USER_NAME_EMPTY\u0026quot; \u0026amp;\u0026amp; e.Code == 500 { // do something } // 通过 proto 生成的代码断言错误，并且包名应替换为自己生成代码后的 package name if api.IsUserNotFound(err) { // do something }) 配置文件 Kratos提供了统一的接口，支持配置文件的加载和变更订阅。\n通过实现Source 和 Watcher即可实现任意配置源（本地或远程）的配置文件加载和变更订阅。\n已经实现了下列插件：\nfile 本地文件加载，Kratos内置 apollo etcd kubernetes nacos 服务注册\u0026amp;服务发现 Kratos定义了统一的注册接口，通过实现Registrar和Discovery，您可以很轻松地将Kratos接入到您的注册中心中。\n您也可以直接使用我们已经实现好的插件：\nconsul discovery etcd kubernetes nacos zookeeper 日志 Kratos的日志模块由两部分组成：\nLogger：底层日志接口，用于快速适配各种日志库到框架中来，仅提供一个最简单的Log方法。 Helper：高级日志接口，提供了一系列带有日志等级和格式化方法的帮助函数，通常业务逻辑中建议使用这个，能够简化日志代码。 我们已经实现好的插件用于适配目前一些日志库，您也可以参考它们的代码来实现自己需要的日志库的适配：\nstd 标准输出，Kratos内置 fluent zap 监控 监控告警方面，您可以通过实现metrics相关接口将服务的统计数据上报给监控平台。\n也可以直接使用我们已经实现好的插件：\ndatadog prometheus 链路追踪 Kratos使用OpenTelemetry作为分布式链路追踪所使用的标准，您可以通过对client和server配置tracing来将服务接入到链路追踪平台（如jaeger等），从而对服务的接口调用关系，耗时，错误等进行追踪。\n负载均衡 Kratos内置了若干种负载均衡算法，如Weighted round robin（默认）、P2C，Random等，您可以通过在client初始化时配置来使用他们。\n限流熔断 Kratos提供了限流ratelimit和熔断circuitbreaker中间件，用于微服务出现异常故障时自动对流量进行限制，提升服务的健壮性，避免雪崩。这两个中间件使用的算法，也可以在我们的可用性算法仓库aegis中找到，独立于Kratos直接使用。\n中间件 您可以通过Kratos的middleware机制，统一微服务接口的某些共同逻辑。上面提到的功能插件，您可以通过实现Middleware编写Kratos能够使用的中间件。\n同时在仓库的middleware目录下，我们也提供了一系列中间件供您使用。\n插件 除了上述提到的插件外，我们还提供了一些其它插件，完整的插件列表请参考文档社区插件\n示例代码 如果您看过文档后，对某些功能的使用仍有疑惑，或者是希望寻找一些用Kratos写项目的灵感，在仓库的examples目录下我们提供了很多代码供参考。\n您也可以通过文档中的示例代码清单页面来查阅有哪些示例。\n小结 使用Kratos在一般开发过程中用到大部分常用功能点，在本文已经做了简单介绍，相信您对本框架的情况已经有了大致的了解。限于篇幅原因，无法在一篇文章中涵盖到Kratos的全部细节，比如layout这个相对复杂的项目结构具体是怎么用的，我将在后续的文章中，继续对Kratos的框架设计思想和使用方法做更加详细的介绍，\u001b并且会结合具体的项目实例，介绍使用Kratos开发的完整流程。\n文章转自 https://farer.org/2021/10/20/the-hitchhikers-guide-to-kratos-1-overview/ ","date":"2021-09-02","permalink":"https://daemon365.dev/2021/09/02/kratos%E6%BC%AB%E6%B8%B8%E6%8C%87%E5%8D%97-1-%E6%A6%82%E8%A7%88/","tags":["go","kratos"],"title":"Kratos漫游指南 1 - 概览"},{"content":"链路追踪的前世今生 分布式跟踪（也称为分布式请求跟踪）是一种用于分析和监控应用程序的方法，尤其是使用微服务架构构建的应用程序。分布式跟踪有助于精确定位故障发生的位置以及导致性能差的原因。\n起源 链路追踪(Distributed Tracing)　一词最早出现于谷歌发布的论文 《Dapper, a Large-Scale Distributed Systems Tracing Infrastructure》 中,这篇论文对于实现链路追踪,对于后来出现的 Jaeger、Zipkin 等开源分布式追踪项目设计理念仍有很深的影响。\n微服务架构是一个分布式的架构,会有很多个不同的服务。不同的服务之前相互调用,如果出现了错误由于一个请求经过了 N 个服务。随着业务的增加越来越多的服务之间的调用，如果没有一个工具去记录调用链，解决问题的时候就会像下面图片里小猫咪玩的毛线球一样，毫无头绪，无从下手\n所以需要有一个工具能够清楚的了解一个请求经过了哪些服务,顺序是如何,从而能够轻易的定位问题。\n百家争艳 从谷歌发布 Dapper 后，分布式链路追踪工具越来越多，以下简单列举了一些常用的链路追踪系统\nSkywalking 阿里 鹰眼 大众点评 CAT Twitter Zipkin Naver pinpoint Uber Jaeger 争锋相对？ 随着链路追踪工具越来越多，开源领域主要分为两派，一派是以 CNCF技术委员 会为主的 OpenTracing 的规范，例如 jaeger zipkin 都是遵循了OpenTracing 的规范。而另一派则是谷歌作为发起者的 OpenCensus，而且谷歌本身还是最早提出链路追踪概念的公司，后期连微软也加入了 OpenCensus\nOpenTelemetry 诞生 OpenTelemetric 是一组 API、SDK、模组和集成，专为创建和管理‎‎遥测数据‎‎（如追踪、指标和日志）而设\n微软加入 OpenCensus 后，直接打破了之前平衡的局面，间接的导致了 OpenTelemetry 的诞生 谷歌和微软下定决心结束江湖之乱，首要的问题是如何整合两个两个社区已有的项目，OpenTelemetry 主要的理念就是，兼容 OpenCensus 和 OpenTracing ，可以让使用者无需改动或者很小的改动就可以接入 OpenTelemetry\nKratos 的链路追踪实践 Kratos 一套轻量级 Go 微服务框架，包含大量微服务相关框架及工具。\ntracing 中间件 kratos 框架提供的自带中间件中有一个名为 tracing 中间件，它基于 Opentelemetry 实现了kratos 框架的链路追踪功能，中间件的代码可以从 middleware/tracing 中看到。\n实现原理 kratos 的链路追踪中间件由三个文件组成 carrie.go,tracer.go,tracing.go。client和 server 的实现原理基本相同，本文以 server 实现进行原理解析。\n首先当请求进入时，tracing 中间件会被调用,首先调用了 tracer.go 中的 NewTracer 方法 // Server returns a new server middleware for OpenTelemetry. func Server(opts ...Option) middleware.Middleware { // 调用 tracer.go 中的 NewTracer 传入了一个 SpanKindServer 和配置项 tracer := NewTracer(trace.SpanKindServer, opts...) // ... 省略代码 } tracer.go 中的 NewTracer 方法被调用后会返回一个 Tracer,实现如下 func NewTracer(kind trace.SpanKind, opts ...Option) *Tracer { options := options{} for _, o := range opts { o(\u0026amp;options) } // 判断是否存在 otel 追踪提供者配置，如果存在则设置 if options.TracerProvider != nil { otel.SetTracerProvider(options.TracerProvider) } /* 判断是否存在 Propagators 设置，如果存在设置则覆盖，不存在则设置一个默认的TextMapPropagator 注意如果没有设置默认的TextMapPropagator,链路信息则无法正确的传递 */ if options.Propagators != nil { otel.SetTextMapPropagator(options.Propagators) } else {\totel.SetTextMapPropagator(propagation.NewCompositeTextMapPropagator(propagation.Baggage{}, propagation.TraceContext{})) } var name string // 判断当前中间件的类型，是 server 还是 client if kind == trace.SpanKindServer { name = \u0026quot;server\u0026quot; } else if kind == trace.SpanKindClient { name = \u0026quot;client\u0026quot; } else { panic(fmt.Sprintf(\u0026quot;unsupported span kind: %v\u0026quot;, kind)) } // 调用 otel包的 Tracer 方法 传入 name 用来创建一个 tracer 实例 tracer := otel.Tracer(name) return \u0026amp;Tracer{tracer: tracer, kind: kind} } 判断当前请求类型，处理需要采集的数据，并调用 tracer.go 中的 Start 方法 var ( component string operation string carrier propagation.TextMapCarrier ) // 判断请求类型 if info, ok := http.FromServerContext(ctx); ok { // HTTP component = \u0026quot;HTTP\u0026quot; // 取出请求的地址 operation = info.Request.RequestURI // 调用 otel/propagation包中的 HeaderCarrier，会处理 http.Header 以用来满足TextMapCarrier interface // TextMapCarrier 是一个文本映射载体，用于承载信息 carrier = propagation.HeaderCarrier(info.Request.Header) // otel.GetTextMapPropagator().Extract() 方法用于将文本映射载体，读取到上下文中 ctx = otel.GetTextMapPropagator().Extract(ctx, propagation.HeaderCarrier(info.Request.Header)) } else if info, ok := grpc.FromServerContext(ctx); ok { // Grpc component = \u0026quot;gRPC\u0026quot; operation = info.FullMethod // // 调用 grpc/metadata包中metadata.FromIncomingContext(ctx)传入 ctx，转换 grpc 的元数据 if md, ok := metadata.FromIncomingContext(ctx); ok { // 调用carrier.go 中的 MetadataCarrier 将 MD 转换 成文本映射载体 carrier = MetadataCarrier(md) } } // 调用 tracer.Start 方法 ctx, span := tracer.Start(ctx, component, operation, carrier) // ... 省略代码 } 调用 tracing.go 中的 Start 方法 func (t *Tracer) Start(ctx context.Context, component string, operation string, carrier propagation.TextMapCarrier) (context.Context, trace.Span) { // 判断当前中间件如果是 server则将 carrier 注入到上下文中 if t.kind == trace.SpanKindServer { ctx = otel.GetTextMapPropagator().Extract(ctx, carrier) } // 调用otel/tracer 包中的 start 方法，用来创建一个 span ctx, span := t.tracer.Start(ctx, // tracing.go 中声明的请求路由作为 spanName operation, // 设置 span 的属性，设置了一个 component，component的值为请求类型 trace.WithAttributes(attribute.String(\u0026quot;component\u0026quot;, component)), // 设置 span种类 trace.WithSpanKind(t.kind), ) // 判断如果当前中间件是 client 则将 carrier 注入到请求里面 if t.kind == trace.SpanKindClient { otel.GetTextMapPropagator().Inject(ctx, carrier) } return ctx, span } defer 声明了一个闭包方法 // 这个地方要注意，需要使用闭包，因为 defer 的参数是实时计算的如果异常发生，err 会一直为 nil // https://github.com/go-kratos/kratos/issues/927 defer func() { tracer.End(ctx, span, err) }() 中间件继续执行 // tracing.go 69行 reply, err = handler(ctx, req) 中间件调用结束 defer 中的闭包被调用后执行了 tracer.go 中的 End 方法 func (t *Tracer) End(ctx context.Context, span trace.Span, err error) { // 判断是否有异常发生，如果有则设置一些异常信息 if err != nil { // 记录异常 span.RecordError(err) // 设置span 属性 span.SetAttributes( // 设置事件为异常 attribute.String(\u0026quot;event\u0026quot;, \u0026quot;error\u0026quot;), // 设置 message 为 err.Error(). attribute.String(\u0026quot;message\u0026quot;, err.Error()), ) //设置了 span 的状态 span.SetStatus(codes.Error, err.Error()) } else { // 如果没有发生异常，span 状态则为 ok span.SetStatus(codes.Ok, \u0026quot;OK\u0026quot;) } // 中止 span span.End() } 如何使用 tracing 中间件的使用示例可以从 kratos/examples/traces ,该示例简单的实现了跨服务间的链路追踪,以下代码片段包含部分示例代码。\n// https://github.com/go-kratos/kratos/blob/7f835db398c9d0332e69b81bad4c652b4b45ae2e/examples/traces/app/message/main.go#L38 // 首先调用otel 库方法，得到一个 TracerProvider func tracerProvider(url string) (*tracesdk.TracerProvider, error) { // examples/traces 中使用的是 jaeger，其他方式可以查看 opentelemetry 官方示例 exp, err := jaeger.NewRawExporter(jaeger.WithCollectorEndpoint(jaeger.WithEndpoint(url))) if err != nil { return nil, err } tp := tracesdk.NewTracerProvider( tracesdk.WithSampler(tracesdk.AlwaysSample()), // 设置 Batcher，注册jaeger导出程序 tracesdk.WithBatcher(exp), // 记录一些默认信息 tracesdk.WithResource(resource.NewWithAttributes( semconv.ServiceNameKey.String(pb.User_ServiceDesc.ServiceName), attribute.String(\u0026quot;environment\u0026quot;, \u0026quot;development\u0026quot;), attribute.Int64(\u0026quot;ID\u0026quot;, 1), )), ) return tp, nil } 在 grpc/server 中使用 // https://github.com/go-kratos/kratos/blob/main/examples/traces/app/message/main.go grpcSrv := grpc.NewServer( grpc.Address(\u0026quot;:9000\u0026quot;), grpc.Middleware( // Configuring tracing Middleware tracing.Server( tracing.WithTracerProvider(tp), ), ), ) 在 grpc/client 中使用 // https://github.com/go-kratos/kratos/blob/149fc0195eb62ee1fbc2728adb92e1bcd1a12c4e/examples/traces/app/user/main.go#L63 conn, err := grpc.DialInsecure(ctx, grpc.WithEndpoint(\u0026quot;127.0.0.1:9000\u0026quot;), grpc.WithMiddleware( tracing.Client( tracing.WithTracerProvider(s.tracer), tracing.WithPropagators( propagation.NewCompositeTextMapPropagator(propagation.Baggage{}, propagation.TraceContext{}), ), ) ), grpc.WithTimeout(2*time.Second), ) 在 http/server 中使用 // https://github.com/go-kratos/kratos/blob/main/examples/traces/app/user/main.go httpSrv := http.NewServer(http.Address(\u0026quot;:8000\u0026quot;)) httpSrv.HandlePrefix(\u0026quot;/\u0026quot;, pb.NewUserHandler(s, http.Middleware( // Configuring tracing middleware tracing.Server( tracing.WithTracerProvider(tp), tracing.WithPropagators( propagation.NewCompositeTextMapPropagator(propagation.Baggage{}, propagation.TraceContext{}), ), ), ), ) 在 http/client 中使用 http.NewClient(ctx, http.WithMiddleware( tracing.Client( tracing.WithTracerProvider(s.tracer), ), )) 如何实现一个其他场景的 tracing 我们可以借鉴 kratos 的 tracing 中间件的代码来实现例如数据库的 tracing，如下面的代码片段，作者借鉴了tracing 中间件，实现了 qmgo 库操作 MongoDB 数据库的 tracing。\nfunc mongoTracer(ctx context.Context,tp trace.TracerProvider, command interface{}) { var ( commandName string failure string nanos int64 reply bson.Raw queryId int64 eventName string ) otel.SetTracerProvider(tp) reply = bson.Raw{} switch value := command.(type) { case *event.CommandStartedEvent: commandName = value.CommandName reply = value.Command queryId = value.RequestID eventName = \u0026quot;CommandStartedEvent\u0026quot; case *event.CommandSucceededEvent: commandName = value.CommandName nanos = value.DurationNanos queryId = value.RequestID eventName = \u0026quot;CommandSucceededEvent\u0026quot; case *event.CommandFailedEvent: commandName = value.CommandName failure = value.Failure nanos = value.DurationNanos queryId = value.RequestID eventName = \u0026quot;CommandFailedEvent\u0026quot; } duration, _ := time.ParseDuration(strconv.FormatInt(nanos, 10) + \u0026quot;ns\u0026quot;) tracer := otel.Tracer(\u0026quot;mongodb\u0026quot;) kind := trace.SpanKindServer ctx, span := tracer.Start(ctx, commandName, trace.WithAttributes( attribute.String(\u0026quot;event\u0026quot;, eventName), attribute.String(\u0026quot;command\u0026quot;, commandName), attribute.String(\u0026quot;query\u0026quot;, reply.String()), attribute.Int64(\u0026quot;queryId\u0026quot;, queryId), attribute.String(\u0026quot;ms\u0026quot;, duration.String()), ), trace.WithSpanKind(kind), ) if failure != \u0026quot;\u0026quot; { span.RecordError(errors.New(failure)) } span.End() } 文章转自 https://go-kratos.dev/blog/go-kratos-opentelemetry-practice ","date":"2021-08-23","permalink":"https://daemon365.dev/2021/08/23/%E5%9F%BA%E4%BA%8E-opentelemetry-%E7%9A%84%E9%93%BE%E8%B7%AF%E8%BF%BD%E8%B8%AA/","tags":["go","kratos","opentelemetry"],"title":"基于 OpenTelemetry 的链路追踪"},{"content":"创建项目 首先需要安装好对应的依赖环境，以及工具：\ngo 下载 protoc go install google.golang.org/protobuf/cmd/protoc-gen-go@latest protoc-gen-go go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest # 创建项目模板 kratos new helloworld cd helloworld # 拉取项目依赖 go mod download # 生成proto模板 kratos proto add api/helloworld/helloworld.proto # 生成proto源码 kratos proto client api/helloworld/helloworld.proto # 生成server模板 kratos proto server api/helloworld/helloworld.proto -t internal/service 执行命令后,会在当前目录下生成一个 service 工程,工程骨架如下,具体的工程骨架说明可以访问 layout\n运行项目 # 生成所有proto源码、wire等等 go generate ./... # 编译成可执行文件 go build -o ./bin/ ./... # 运行项目 ./bin/helloworld -conf ./configs 看到如下输出则证明项目启动正常\nlevel=INFO module=app service_id=7114ad8a-b3bf-11eb-a1b9-f0189850d2cb service_name= version= level=INFO module=transport/grpc msg=[gRPC] server listening on: [::]:9000 level=INFO module=transport/http msg=[HTTP] server listening on: [::]:8000 测试接口\ncurl 'http://127.0.0.1:8000/helloworld/krtaos' 输出： { \u0026quot;message\u0026quot;: \u0026quot;Hello kratos\u0026quot; } 应用是如何跑起来的? 通过上面的图例👆,我们可以直观观察到应用的调用链,简化来说如下图流程所示👇\n1. 注入依赖并调用 newApp() 方法 // helloword/cmd/main.go func main() { flag.Parse() logger := log.NewStdLogger(os.Stdout) // 调用 go-kratos/kratos/v2/config,创建 config 实例,并指定了来源和配置解析方法 c := config.New( config.WithSource( file.NewSource(flagconf), ), config.WithDecoder(func(kv *config.KeyValue, v map[string]interface{}) error { return yaml.Unmarshal(kv.Value, v) }), ) if err := c.Load(); err != nil { panic(err) } // 将配置扫描到,通过 proto 声明的 conf struct 上 var bc conf.Bootstrap if err := c.Scan(\u0026amp;bc); err != nil { panic(err) } // 通过 wire 将依赖注入,并调用 newApp 方法 app, cleanup, err := initApp(bc.Server, bc.Data, logger) if err != nil { panic(err) } // 省略代码... } 2. 创建 kratos 实例 项目 main.go 的 newApp() 方法中,调用了 go-kratos/kratos/v2/app.go 中的 kratos.New() 方法\n// helloword/cmd/main.go func newApp(logger log.Logger, hs *http.Server, gs *grpc.Server) *kratos.App { return kratos.New( // 配置应用 kratos.Name(Name), kratos.Version(Version), kratos.Metadata(map[string]string{}), kratos.Logger(logger), // kratos.Server() 传入的 http/grpc 服务会通过 buildInstance() 转换成registry.ServiceInstance struct* kratos.Server( hs, gs, ), ) } 该方法会返回一个 App struct,包含 Run() 和 Stop() 方法\n// go-kratos/kratos/v2/app.go type App struct { opts options //配置 ctx context.Context // 上下文 cancel func() // context 的取消方法 instance *registry.ServiceInstance //通过 kratos.Server()声明的实例,并通过 buildInstance() 转换后的 *registry.ServiceInstance struct log *log.Helper // 日志 } // Run executes all OnStart hooks registered with the application's Lifecycle. func (a *App) Run() error { // 省略代码... } // Stop gracefully stops the application. func (a *App) Stop() error { // 省略代码... } 3. 调用 Run() 方法# 项目在 main 方法中调用了 kratos.App struct 的 Run() 方法.\n// helloword/cmd/main.go // 省略代码... // 启动 Kratos if err := app.Run(); err != nil { panic(err) } Run() 方法的实现细节\n// go-kratos/kratos/v2/app.go func (a *App) Run() error { a.log.Infow( \u0026quot;service_id\u0026quot;, a.opts.id, \u0026quot;service_name\u0026quot;, a.opts.name, \u0026quot;version\u0026quot;, a.opts.version, ) g, ctx := errgroup.WithContext(a.ctx) // 遍历通过 kratos.Server() 声明的服务实例 for _, srv := range a.opts.servers { srv := srv // 执行两个goroutine, 用于处理服务启动和退出 g.Go(func() error { \u0026lt;-ctx.Done() // 阻塞,等待调用 cancel 方法 return srv.Stop() // 协程退出后,调用实例的停止方法 }) g.Go(func() error { return srv.Start() // 调用实例的运行方法 }) } // 判断是否调用 kratos.Registrar() 配置了注册发现中心 if a.opts.registrar != nil { // 将实例注册到注册中心 if err := a.opts.registrar.Register(a.opts.ctx, a.instance); err != nil return err } } // 监听进程退出信号 c := make(chan os.Signal, 1) signal.Notify(c, a.opts.sigs...) // 处理进程退出和 context 退出 g.Go(func() error { for { select { case \u0026lt;-ctx.Done(): return ctx.Err() case \u0026lt;-c: // 调用 kratos.App 的停止方法 a.Stop() } } }) if err := g.Wait(); err != nil \u0026amp;\u0026amp; !errors.Is(err, context.Canceled) { return err } return nil } 4. 应用退出 Kratos 实例在启动时,监听了系统的进程退出信号,当收到退出信号时,kratos 会调用 App struct 的 Stop() 方法\n// go-kratos/kratos/v2/app.go func (a *App) Stop() error { // 判断是否有注册中心配置 if a.opts.registrar != nil { // 在注册中心中将实例注销 if err := a.opts.registrar.Deregister(a.opts.ctx, a.instance); err != nil { return err } } // 控制 goroutine 的退出,当调用 a.cancel()时,Run()方法中 监听的 \u0026lt;-ctx.Done() 收到消息后,没有阻塞后,方法会调用 server 的 Stop()方法,停止服务 if a.cancel != nil { a.cancel() } return nil } 文章转自： https://go-kratos.dev/blog/go-layout-operation-process/ ","date":"2021-08-20","permalink":"https://daemon365.dev/2021/08/20/%E9%80%9A%E8%BF%87-layout-%E6%8E%A2%E7%B4%A2-kratos-%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86/","tags":["go","kratos"],"title":"通过 layout 探索 kratos 运行原理"},{"content":"Prometheus简介 什么是 Prometheus Prometheus 是在 Soundcloud 以开源软件的形式进行研发的系统监控和告警工具包，自此以后，许多公司和组织都采用了 Prometheus 作为监控告警工具。Prometheus 的开发者和用户社区非常活跃，它现在是一个独立的开源项目，可以独立于任何公司进行维护。Prometheus 于 2016 年 5 月加入 CNCF 基金会，成为继 Kubernetes 之后的第二个 CNCF 托管项目。\nPrometheus 的优势 Prometheus 的主要优势有：\n由指标名称和和键/值对标签标识的时间序列数据组成的多维数据模型。 强大的查询语言 PromQL。 不依赖分布式存储；单个服务节点具有自治能力。 时间序列数据是服务端通过 HTTP 协议主动拉取获得的。 也可以通过中间网关来推送时间序列数据。 可以通过静态配置文件或服务发现来获取监控目标。 支持多种类型的图表和仪表盘。 Prometheus 的组件 Prometheus 生态系统由多个组件组成，其中有许多组件是可选的：\nPrometheus Server 作为服务端，用来存储时间序列数据。 客户端库用来检测应用程序代码。 用于支持临时任务的推送网关。 Exporter 用来监控 HAProxy，StatsD，Graphite 等特殊的监控目标，并向 Prometheus 提供标准格式的监控样本数据。 alartmanager 用来处理告警。 其他各种周边工具。 其中大多数组件都是用 Go 编写的，因此很容易构建和部署为静态二进制文件。\nPrometheus 的架构 Prometheus 的整体架构以及生态系统组件如下图所示：\nPrometheus Server 直接从监控目标中或者间接通过推送网关来拉取监控指标，它在本地存储所有抓取到的样本数据，并对此数据执行一系列规则，以汇总和记录现有数据的新时间序列或生成告警。可以通过 Grafana 或者其他工具来实现监控数据的可视化。\n数据类型 Prometheus 所有采集的监控数据均以指标（metric）的形式保存在内置的时间序列数据库当中（TSDB）：属于同一指标名称，同一标签集合的、有时间戳标记的数据流。除了存储的时间序列，Prometheus 还可以根据查询请求产生临时的、衍生的时间序列作为返回结果。\n指标名称和标签 每一条时间序列由指标名称（Metrics Name）以及一组标签（键值对）唯一标识。其中指标的名称（metric name）可以反映被监控样本的含义（例如，http_requests_total— 表示当前系统接收到的 HTTP 请求总量），指标名称只能由 ASCII 字符、数字、下划线以及冒号组成，同时必须匹配正则表达式[a-zA-Z_:][a-zA-Z0-9_:]*。\n样本 在时间序列中的每一个点称为一个样本（sample），样本由以下三部分组成：\n指标（metric）：指标名称和描述当前样本特征的 labelsets； 时间戳（timestamp）：一个精确到毫秒的时间戳； 样本值（value）： 一个 folat64 的浮点型数据表示当前样本的值。 表示方式 通过如下表达方式表示指定指标名称和指定标签集合的时间序列：\n{=, ...} 例如，指标名称为api_http_requests_total，标签为method=\u0026ldquo;POST\u0026quot;和handler=\u0026quot;/messages\u0026quot;的时间序列可以表示为：\napi_http_requests_total{method=\u0026quot;POST\u0026quot;, handler=\u0026quot;/messages\u0026quot;} 这与 OpenTSDB 中使用的标记法相同。\n指标类型 Counter（计数器） Counter 类型代表一种样本数据单调递增的指标，即只增不减，除非监控系统发生了重置。例如，你可以使用 counter 类型的指标来表示服务的请求数、已完成的任务数、错误发生的次数等。counter 主要有两个方法：\n//将counter值加1. Inc() // 将指定值加到counter值上，如果指定值\u0026lt;0 会panic. Add(float64) Counter 类型数据可以让用户方便的了解事件产生的速率的变化，在 PromQL 内置的相关操作函数可以提供相应的分析，比如以 HTTP 应用请求量来进行说明：\n//通过rate()函数获取HTTP请求量的增长率 rate(http_requests_total[5m]) //查询当前系统中，访问量前10的HTTP地址 topk(10, http_requests_total) 不要将 counter 类型应用于样本数据非单调递增的指标，例如：当前运行的进程数量（应该用 Guage 类型）。\n不同语言关于 Counter 的客户端库使用文档：\nGo Java Python Ruby Guage（仪表盘） Guage 类型代表一种样本数据可以任意变化的指标，即可增可减。guage 通常用于像温度或者内存使用率这种指标数据，也可以表示能随时增加或减少的“总数”，例如：当前并发请求的数量。\n对于 Gauge 类型的监控指标，通过 PromQL 内置函数 delta() 可以获取样本在一段时间内的变化情况，例如，计算 CPU 温度在两小时内的差异：\ndalta(cpu_temp_celsius{host=\u0026quot;zeus\u0026quot;}[2h]) 你还可以通过PromQL 内置函数 predict_linear() 基于简单线性回归的方式，对样本数据的变化趋势做出预测。例如，基于 2 小时的样本数据，来预测主机可用磁盘空间在 4 个小时之后的剩余情况：\npredict_linear(node_filesystem_free{job=\u0026quot;node\u0026quot;}[2h], 4 * 3600) \u0026lt; 0 不同语言关于 Guage 的客户端库使用文档：\nGo Java Python Ruby Histogram（直方图） 在大多数情况下人们都倾向于使用某些量化指标的平均值，例如 CPU 的平均使用率、页面的平均响应时间。这种方式的问题很明显，以系统 API 调用的平均响应时间为例：如果大多数 API 请求都维持在 100ms 的响应时间范围内，而个别请求的响应时间需要 5s，那么就会导致某些 WEB 页面的响应时间落到中位数的情况，而这种现象被称为长尾问题。\n为了区分是平均的慢还是长尾的慢，最简单的方式就是按照请求延迟的范围进行分组。例如，统计延迟在 010ms 之间的请求数有多少而 1020ms 之间的请求数又有多少。通过这种方式可以快速分析系统慢的原因。Histogram 和 Summary 都是为了能够解决这样问题的存在，通过 Histogram 和 Summary 类型的监控指标，我们可以快速了解监控样本的分布情况。\nHistogram 在一段时间范围内对数据进行采样（通常是请求持续时间或响应大小等），并将其计入可配置的存储桶（bucket）中，后续可通过指定区间筛选样本，也可以统计样本总数，最后一般将数据展示为直方图。\nHistogram 类型的样本会提供三种指标（假设指标名称为）：\n样本的值分布在 bucket 中的数量，命名为_bucket{le=\u0026rdquo;\u0026quot;}。解释的更通俗易懂一点，这个值表示指标值小于等于上边界的所有样本数量。\n// 在总共2次请求当中。http 请求响应时间 \u0026lt;=0.005 秒 的请求次数为0 io_namespace_http_requests_latency_seconds_histogram_bucket{path=\u0026quot;/\u0026quot;,method=\u0026quot;GET\u0026quot;,code=\u0026quot;200\u0026quot;,le=\u0026quot;0.005\u0026quot;,} 0.0 // 在总共2次请求当中。http 请求响应时间 \u0026lt;=0.01 秒 的请求次数为0 io_namespace_http_requests_latency_seconds_histogram_bucket{path=\u0026quot;/\u0026quot;,method=\u0026quot;GET\u0026quot;,code=\u0026quot;200\u0026quot;,le=\u0026quot;0.01\u0026quot;,} 0.0 // 在总共2次请求当中。http 请求响应时间 \u0026lt;=0.025 秒 的请求次数为0 io_namespace_http_requests_latency_seconds_histogram_bucket{path=\u0026quot;/\u0026quot;,method=\u0026quot;GET\u0026quot;,code=\u0026quot;200\u0026quot;,le=\u0026quot;0.025\u0026quot;,} 0.0 io_namespace_http_requests_latency_seconds_histogram_bucket{path=\u0026quot;/\u0026quot;,method=\u0026quot;GET\u0026quot;,code=\u0026quot;200\u0026quot;,le=\u0026quot;0.05\u0026quot;,} 0.0 io_namespace_http_requests_latency_seconds_histogram_bucket{path=\u0026quot;/\u0026quot;,method=\u0026quot;GET\u0026quot;,code=\u0026quot;200\u0026quot;,le=\u0026quot;0.075\u0026quot;,} 0.0 io_namespace_http_requests_latency_seconds_histogram_bucket{path=\u0026quot;/\u0026quot;,method=\u0026quot;GET\u0026quot;,code=\u0026quot;200\u0026quot;,le=\u0026quot;0.1\u0026quot;,} 0.0 io_namespace_http_requests_latency_seconds_histogram_bucket{path=\u0026quot;/\u0026quot;,method=\u0026quot;GET\u0026quot;,code=\u0026quot;200\u0026quot;,le=\u0026quot;0.25\u0026quot;,} 0.0 io_namespace_http_requests_latency_seconds_histogram_bucket{path=\u0026quot;/\u0026quot;,method=\u0026quot;GET\u0026quot;,code=\u0026quot;200\u0026quot;,le=\u0026quot;0.5\u0026quot;,} 0.0 io_namespace_http_requests_latency_seconds_histogram_bucket{path=\u0026quot;/\u0026quot;,method=\u0026quot;GET\u0026quot;,code=\u0026quot;200\u0026quot;,le=\u0026quot;0.75\u0026quot;,} 0.0 io_namespace_http_requests_latency_seconds_histogram_bucket{path=\u0026quot;/\u0026quot;,method=\u0026quot;GET\u0026quot;,code=\u0026quot;200\u0026quot;,le=\u0026quot;1.0\u0026quot;,} 0.0 io_namespace_http_requests_latency_seconds_histogram_bucket{path=\u0026quot;/\u0026quot;,method=\u0026quot;GET\u0026quot;,code=\u0026quot;200\u0026quot;,le=\u0026quot;2.5\u0026quot;,} 0.0 io_namespace_http_requests_latency_seconds_histogram_bucket{path=\u0026quot;/\u0026quot;,method=\u0026quot;GET\u0026quot;,code=\u0026quot;200\u0026quot;,le=\u0026quot;5.0\u0026quot;,} 0.0 io_namespace_http_requests_latency_seconds_histogram_bucket{path=\u0026quot;/\u0026quot;,method=\u0026quot;GET\u0026quot;,code=\u0026quot;200\u0026quot;,le=\u0026quot;7.5\u0026quot;,} 2.0 // 在总共2次请求当中。http 请求响应时间 \u0026lt;=10 秒 的请求次数为 2 io_namespace_http_requests_latency_seconds_histogram_bucket{path=\u0026quot;/\u0026quot;,method=\u0026quot;GET\u0026quot;,code=\u0026quot;200\u0026quot;,le=\u0026quot;10.0\u0026quot;,} 2.0 io_namespace_http_requests_latency_seconds_histogram_bucket{path=\u0026quot;/\u0026quot;,method=\u0026quot;GET\u0026quot;,code=\u0026quot;200\u0026quot;,le=\u0026quot;+Inf\u0026quot;,} 2.0 所有样本值的大小总和，命名为_sum。\n// 实际含义： 发生的2次 http 请求总的响应时间为 13.107670803000001 秒 io_namespace_http_requests_latency_seconds_histogram_sum{path=\u0026quot;/\u0026quot;,method=\u0026quot;GET\u0026quot;,code=\u0026quot;200\u0026quot;,} 13.107670803000001 样本总数，命名为_count。值和_bucket{le=\u0026quot;+Inf\u0026quot;}相同。\n// 实际含义： 当前一共发生了 2 次 http 请求 io_namespace_http_requests_latency_seconds_histogram_count{path=\u0026quot;/\u0026quot;,method=\u0026quot;GET\u0026quot;,code=\u0026quot;200\u0026quot;,} 2.0 注意\nbucket 可以理解为是对数据指标值域的一个划分，划分的依据应该基于数据值的分布。注意后面的采样点是包含前面的采样点的，假设xxx_bucket{\u0026hellip;,le=\u0026ldquo;0.01\u0026rdquo;}的值为 10，而xxx_bucket{\u0026hellip;,le=\u0026ldquo;0.05\u0026rdquo;}的值为 30，那么意味着这 30 个采样点中，有 10 个是小于 10 ms 的，其余 20 个采样点的响应时间是介于 10 ms 和 50 ms 之间的。\n可以通过 histogram_quantile() 函数来计算 Histogram 类型样本的分位数。分位数可能不太好理解，你可以理解为分割数据的点。我举个例子，假设样本的 9 分位数（quantile=0.9）的值为 x，即表示小于 x 的采样值的数量占总体采样值的 90%。Histogram 还可以用来计算应用性能指标值（Apdex score）。\n不同语言关于 Histogram 的客户端库使用文档：\nGo Java Python Ruby Summary（摘要） 与 Histogram 类型类似，用于表示一段时间内的数据采样结果（通常是请求持续时间或响应大小等），但它直接存储了分位数（通过客户端计算，然后展示出来），而不是通过区间来计算。\nSummary 类型的样本也会提供三种指标（假设指标名称为 ）：\n样本值的分位数分布情况，命名为{quantile=\u0026quot;\u0026quot;}。\n// 含义：这 12 次 http 请求中有 50% 的请求响应时间是 3.052404983s io_namespace_http_requests_latency_seconds_summary{path=\u0026quot;/\u0026quot;,method=\u0026quot;GET\u0026quot;,code=\u0026quot;200\u0026quot;,quantile=\u0026quot;0.5\u0026quot;,} 3.052404983 // 含义：这 12 次 http 请求中有 90% 的请求响应时间是 8.003261666s io_namespace_http_requests_latency_seconds_summary{path=\u0026quot;/\u0026quot;,method=\u0026quot;GET\u0026quot;,code=\u0026quot;200\u0026quot;,quantile=\u0026quot;0.9\u0026quot;,} 8.003261666 所有样本值的大小总和，命名为_sum。\n// 含义：这12次 http 请求的总响应时间为 51.029495508s io_namespace_http_requests_latency_seconds_summary_sum{path=\u0026quot;/\u0026quot;,method=\u0026quot;GET\u0026quot;,code=\u0026quot;200\u0026quot;,} 51.029495508 样本总数，命名为_count。\n// 含义：当前一共发生了 12 次 http 请求 io_namespace_http_requests_latency_seconds_summary_count{path=\u0026quot;/\u0026quot;,method=\u0026quot;GET\u0026quot;,code=\u0026quot;200\u0026quot;,} 12.0 现在可以总结一下 Histogram 与 Summary 的异同：\n它们都包含了_sum和_count指标 Histogram 需要通过_bucket来计算分位数，而 Summary 则直接存储了分位数的值。 关于 Summary 与 Histogram 的详细用法，请参考 histograms and summaries。\n不同语言关于 Summary 的客户端库使用文档：\nGo Java Python Ruby 下载 下载地址: https://prometheus.io/download 下载对应版本安装 golang使用prometheus package main import ( \u0026quot;log\u0026quot; \u0026quot;math/rand\u0026quot; \u0026quot;net/http\u0026quot; \u0026quot;time\u0026quot; \u0026quot;github.com/prometheus/client_golang/prometheus\u0026quot; \u0026quot;github.com/prometheus/client_golang/prometheus/promhttp\u0026quot; ) var ( cpuTemp = prometheus.NewGauge(prometheus.GaugeOpts{ Name: \u0026quot;cpu_temperature_celsius\u0026quot;, Help: \u0026quot;Current temperature of the CPU.\u0026quot;, }) hdFailures = prometheus.NewCounterVec( prometheus.CounterOpts{ Name: \u0026quot;hd_errors_total\u0026quot;, Help: \u0026quot;Number of hard-disk errors.\u0026quot;, }, []string{\u0026quot;device\u0026quot;, \u0026quot;service\u0026quot;}, ) ) func init() { // Metrics have to be registered to be exposed: prometheus.MustRegister(cpuTemp) prometheus.MustRegister(hdFailures) } func main() { go func() { for { val := rand.Float64() * 100 cpuTemp.Set(val) hdFailures.With(prometheus.Labels{ \u0026quot;device\u0026quot;: \u0026quot;/dev/sda\u0026quot;, \u0026quot;service\u0026quot;: \u0026quot;hello.world\u0026quot;, }).Inc() time.Sleep(time.Second) } }() // The Handler function provides a default handler to expose metrics // via an HTTP server. \u0026quot;/metrics\u0026quot; is the usual endpoint for that. http.Handle(\u0026quot;/metrics\u0026quot;, promhttp.Handler()) log.Fatal(http.ListenAndServe(\u0026quot;:8080\u0026quot;, nil)) } 执行代码 并访问\n","date":"2021-08-20","permalink":"https://daemon365.dev/2021/08/20/prometheus/","tags":["prometheus","kubernetes"],"title":"Prometheus"},{"content":"什么是日志 所谓日志（Log）是指系统所指定对象的某些操作和其操作结果按时间有序的集合。log文件就是日志文件，log文件记录了系统和系统的用户之间交互的信息，是自动捕获人与系统终端之间交互的类型、内容或时间的数据收集方法。\n日志是用来记录，用户操作，系统状态，错误信息等等内容的文件，是一个软件系统的重要组成部分。一个良好的日志规范，对于系统运行状态的分析，以及线上问题的解决具有重大的意义。\n日志规范 在开发软件打印日志时，需要注意一些问题，举例可能不全，可以自行百度相关文章或查看文章底部文献：\n重要功能日志尽可能的完善。 不要随意打印无用的日志，过多无用的日志会增加分析日志的难度。 日志要区分等级 如 debug，warn，info，error 等。 捕获到未处理错误时最好打印错误堆栈信息 Go 语言常用的日志库 Go 语言标准库中就为我们提供了一个日志库 log，除了这个以外还有很多日志库，如 logrus，glog，logx，Uber 的 zap 等等，例如 zap 就有很多的优点：\n高性能 配置项丰富 多种日志级别 支持Hook 丰富的工具包 提供了sugar log 多种日志打印格式 \u0026hellip; 简单使用 package main import ( \u0026quot;errors\u0026quot; \u0026quot;go.uber.org/zap\u0026quot; ) var logger *zap.Logger func init() { logger, _ = zap.NewProduction() } func main() { logger.Error( \u0026quot;My name is baobao\u0026quot;, zap.String(\u0026quot;from\u0026quot;, \u0026quot;Hulun Buir\u0026quot;), zap.Error(errors.New(\u0026quot;no good\u0026quot;))) logger.Info(\u0026quot;Worked in the Ministry of national development of China!\u0026quot;, zap.String(\u0026quot;key\u0026quot;, \u0026quot;eat🍚\u0026quot;), zap.String(\u0026quot;key\u0026quot;, \u0026quot;sleep😴\u0026quot;)) defer logger.Sync() } Kratos 日志库原理解析 在私下与 Tony老师 沟通时关于日志库的实现理念时，Tony老师 说：由于目前日志库非常多并且好用，在 Kratos 的日志中，主要考虑以下几个问题：\n统一日志接口设计 组织结构化日志 并且需要有友好的日志级别使用 支持多输出源对接需求，如log-agent 或者 3rd 日志库 kratos 的日志库，不强制具体实现方式，只提供适配器，用户可以自行实现日志功能，只需要实现kratos/log 的 Logger interface 即可接入自己喜欢的日志系统。\nkratos 的日志库，在设计阶段，参考了很多优秀的开源项目和大厂的日志系统实现，经历了多次改动后才呈现给大家。\nlog库的组成 kratos 的 log 库主要由以下几个文件组成\nlevel.go 定义日志级别 log.go 日志核心 helper.go log的helper value.go 实现动态值 源码分析 kratos 的 log 库中, 核心部分就是 log.go 代码非常简洁，符合 kratos 的设计理念。 log.go 中声明了 Logger interface，用户只需要实现接口，即可引入自己的日志实现，主要代码如下：\nlog.go package log import ( \u0026quot;context\u0026quot; \u0026quot;log\u0026quot; ) var ( // DefaultLogger is default logger. DefaultLogger Logger = NewStdLogger(log.Writer()) ) // Logger 接口, 后面实现自定义日志库的时候，就是要实现这个接口。 type Logger interface { Log(level Level, keyvals ...interface{}) error } type logger struct { logs []Logger // logger 数组 prefix []interface{} // 一些默认打印的值,例如通过 With 绑定的 Valuer hasValuer bool // 是否包含 Valuer ctx context.Context // 上下文 } func (c *logger) Log(level Level, keyvals ...interface{}) error { kvs := make([]interface{}, 0, len(c.prefix)+len(keyvals)) kvs = append(kvs, c.prefix...) // 判断是否存在 valuer if c.hasValuer { // 绑定 valuer bindValues(c.ctx, kvs) } kvs = append(kvs, keyvals...) // 遍历 logs，调用所有的 logger 进行日志打印。 for _, l := range c.logs { if err := l.Log(level, kvs...); err != nil { return err } } return nil } // With with logger fields. func With(l Logger, kv ...interface{}) Logger { // 判断是否能 把传入的 logger 断言成 *logger if c, ok := l.(*logger); ok { // 预分配内存,make了一个空间长度为 c.prefix + keyvals长度的 interface数组 kvs := make([]interface{}, 0, len(c.prefix)+len(kv)) // 处理打印的内容 kvs = append(kvs, kv...) kvs = append(kvs, c.prefix...) // containsValuer()用来判断 kvs 里面是否存在 valuer return \u0026amp;logger{ logs: c.logs, prefix: kvs, hasValuer: containsValuer(kvs), ctx: c.ctx, } } return \u0026amp;logger{logs: []Logger{l}, prefix: kv, hasValuer: containsValuer(kv)} } // WithContext 绑定 ctx,注意 ctx 必须非空 func WithContext(ctx context.Context, l Logger) Logger { if c, ok := l.(*logger); ok { return \u0026amp;logger{ logs: c.logs, prefix: c.prefix, hasValuer: c.hasValuer, ctx: ctx, } } return \u0026amp;logger{logs: []Logger{l}, ctx: ctx} } // MultiLogger 包装多个logger，简单说就是同时使用多个logger打印 func MultiLogger(logs ...Logger) Logger { return \u0026amp;logger{logs: logs} } value.go // 返回 valuer 函数. func Value(ctx context.Context, v interface{}) interface{} { if v, ok := v.(Valuer); ok { return v(ctx) } return v } // ...省略一些内置的 valuer 实现 // 绑定 valuer func bindValues(ctx context.Context, keyvals []interface{}) { for i := 1; i \u0026lt; len(keyvals); i += 2 { if v, ok := keyvals[i].(Valuer); ok { keyvals[i] = v(ctx) } } } // 是否包含 valuer func containsValuer(keyvals []interface{}) bool { for i := 1; i \u0026lt; len(keyvals); i += 2 { if _, ok := keyvals[i].(Valuer); ok { return true } } return false } helper.go package log import ( \u0026quot;context\u0026quot; \u0026quot;fmt\u0026quot; ) // Helper is a logger helper. type Helper struct { logger Logger } // 创建一个 logger helper 实例 func NewHelper(logger Logger) *Helper { return \u0026amp;Helper{ logger: logger, } } // 通过 WithContext() 返回包含 ctx 的一个日志的帮助类，包含一些定义好的按级别打印日志的方法 func (h *Helper) WithContext(ctx context.Context) *Helper { return \u0026amp;Helper{ logger: WithContext(ctx, h.logger), } } func (h *Helper) Log(level Level, keyvals ...interface{}) { h.logger.Log(level, keyvals...) } func (h *Helper) Debug(a ...interface{}) { h.logger.Log(LevelDebug, \u0026quot;msg\u0026quot;, fmt.Sprint(a...)) } func (h *Helper) Debugf(format string, a ...interface{}) { h.logger.Log(LevelDebug, \u0026quot;msg\u0026quot;, fmt.Sprintf(format, a...)) } // ...省略一些重复的方法 通过单元测试了解调用逻辑 func TestInfo(t *testing.T) { logger := DefaultLogger logger = With(logger, \u0026quot;ts\u0026quot;, DefaultTimestamp, \u0026quot;caller\u0026quot;, DefaultCaller) logger.Log(LevelInfo, \u0026quot;key1\u0026quot;, \u0026quot;value1\u0026quot;) } 单测中首先声明了一个 logger ，用的默认的 DefaultLogger 调用 log.go 中的 With() 函数， 传入了 logger ,和两个动态值， DefaultTimestamp 和 DefaultCaller。 With方法被调用，判断是否能将参数 l 类型转换成 *logger 如果可以转换，将传入的KV，赋值给 logger.prefix 上，然后调用 value.go 中的 containsValuer() 判断传入的KV中是否存在 Valuer类型的值，将结果赋值给 context.hasValuer，最后返回 Logger 对象 否则则直接返回一个 \u0026amp;logger{logs: []Logger{l}, prefix: kv, hasValuer: containsValuer(kv)} 然后打印日志时，logger struct 的 Log 方法被调用 Log() 方法首先预分配了 keyvals 的空间，然后判断 hasValuer，如果为 true，则调用 valuer.go 中的 bindValuer() 并传入了 ctx 然后获取 valuer 的值if v, ok := v.(Valuer); ok { return v() } 8.最后遍历 logger.logs 打印日志\n使用方法 使用 Logger 打印日志 logger := log.DefaultLogger logger.Log(LevelInfo, \u0026quot;key1\u0026quot;, \u0026quot;value1\u0026quot;) 使用 Helper 打印日志 log := log.NewHelper(DefaultLogger) log.Debug(\u0026quot;test debug\u0026quot;) log.Info(\u0026quot;test info\u0026quot;) log.Warn(\u0026quot;test warn\u0026quot;) log.Error(\u0026quot;test error\u0026quot;) 使用 valuer logger := DefaultLogger logger = With(logger, \u0026quot;ts\u0026quot;, DefaultTimestamp, \u0026quot;caller\u0026quot;, DefaultCaller) logger.Log(LevelInfo, \u0026quot;msg\u0026quot;, \u0026quot;helloworld\u0026quot;) 同时打印多个 logger out := log.NewStdLogger(os.Stdout) err := log.NewStdLogger(os.Stderr) l := log.With(MultiLogger(out, err)) l.Log(LevelInfo, \u0026quot;msg\u0026quot;, \u0026quot;test\u0026quot;) 使用 context logger := log.With(NewStdLogger(os.Stdout), \u0026quot;trace\u0026quot;, Trace(), ) log := log.NewHelper(logger) ctx := context.WithValue(context.Background(), \u0026quot;trace_id\u0026quot;, \u0026quot;2233\u0026quot;) log.WithContext(ctx).Info(\u0026quot;got trace!\u0026quot;) 使用 filter 过滤日志 如果需要过滤日志中某些不应该被打印明文的字段如 password 等，可以通过 log.NewFilter() 来实现过滤功能。\n通过 level 过滤日志 l := log.NewHelper(log.NewFilter(log.DefaultLogger, log.FilterLevel(log.LevelWarn))) l.Log(LevelDebug, \u0026quot;msg1\u0026quot;, \u0026quot;te1st debug\u0026quot;) l.Debug(\u0026quot;test debug\u0026quot;) l.Debugf(\u0026quot;test %s\u0026quot;, \u0026quot;debug\u0026quot;) l.Debugw(\u0026quot;log\u0026quot;, \u0026quot;test debug\u0026quot;) l.Warn(\u0026quot;warn log\u0026quot;) 通过 key 过滤日志 l := log.NewHelper(log.NewFilter(log.DefaultLogger, log.FilterKey(\u0026quot;password\u0026quot;))) l.Debugw(\u0026quot;password\u0026quot;, \u0026quot;123456\u0026quot;) 通过 value 过滤日志 l := log.NewHelper(log.NewFilter(log.DefaultLogger, log.FilterValue(\u0026quot;kratos\u0026quot;))) l.Debugw(\u0026quot;name\u0026quot;, \u0026quot;kratos\u0026quot;) 通过 hook func 过滤日志 l := log.NewHelper(log.NewFilter(log.DefaultLogger, log.FilterFunc(testFilterFunc))) l.Debug(\u0026quot;debug level\u0026quot;) l.Infow(\u0026quot;password\u0026quot;, \u0026quot;123456\u0026quot;) func testFilterFunc(level Level, keyvals ...interface{}) bool { if level == LevelWarn { return true } for i := 0; i \u0026lt; len(keyvals); i++ { if keyvals[i] == \u0026quot;password\u0026quot; { keyvals[i+1] = \u0026quot;***\u0026quot; } } return false } 用 Zap 实现 kratos 的日志接口 实现的代码十分简单，仅有不到100 行代码，仅供大家参考。\n实现 // kratos/examples/log/zap.go package logger import ( \u0026quot;fmt\u0026quot; \u0026quot;os\u0026quot; \u0026quot;github.com/go-kratos/kratos/v2/log\u0026quot; \u0026quot;go.uber.org/zap\u0026quot; \u0026quot;go.uber.org/zap/zapcore\u0026quot; \u0026quot;gopkg.in/natefinch/lumberjack.v2\u0026quot; ) var _ log.Logger = (*ZapLogger)(nil) // Zap 结构体 type ZapLogger struct { log *zap.Logger Sync func() error } // 创建一个 ZapLogger 实例 func NewZapLogger(encoder zapcore.EncoderConfig, level zap.AtomicLevel, opts ...zap.Option) *ZapLogger { writeSyncer := getLogWriter() // 设置 zapcore core := zapcore.NewCore( zapcore.NewConsoleEncoder(encoder), zapcore.NewMultiWriteSyncer( zapcore.AddSync(os.Stdout), ), level) // new 一个 *zap.Logger zapLogger := zap.New(core, opts...) return \u0026amp;ZapLogger{log: zapLogger, Sync: zapLogger.Sync} } // Log 方法实现了 kratos/log/log.go 中的 Logger interface func (l *ZapLogger) Log(level log.Level, keyvals ...interface{}) error { if len(keyvals) == 0 || len(keyvals)%2 != 0{ l.log.Warn(fmt.Sprint(\u0026quot;Keyvalues must appear in pairs: \u0026quot;, keyvals)) return nil } // 按照 KV 传入的时候,使用的 zap.Field var data []zap.Field for i := 0; i \u0026lt; len(keyvals); i += 2 { data = append(data, zap.Any(fmt.Sprint(keyvals[i]), fmt.Sprint(keyvals[i+1]))) } switch level { case log.LevelDebug: l.log.Debug(\u0026quot;\u0026quot;, data...) case log.LevelInfo: l.log.Info(\u0026quot;\u0026quot;, data...) case log.LevelWarn: l.log.Warn(\u0026quot;\u0026quot;, data...) case log.LevelError: l.log.Error(\u0026quot;\u0026quot;, data...) } return nil } // 日志自动切割，采用 lumberjack 实现的 func getLogWriter() zapcore.WriteSyncer { lumberJackLogger := \u0026amp;lumberjack.Logger{ Filename: \u0026quot;./test.log\u0026quot;, MaxSize: 10, MaxBackups: 5, MaxAge: 30, Compress: false, } return zapcore.AddSync(lumberJackLogger) } 使用方法 // kratos/examples/log/zap_test.go package logger import ( \u0026quot;testing\u0026quot; \u0026quot;github.com/go-kratos/kratos/v2/log\u0026quot; \u0026quot;go.uber.org/zap\u0026quot; \u0026quot;go.uber.org/zap/zapcore\u0026quot; ) func TestZapLogger(t *testing.T) { encoder := zapcore.EncoderConfig{ TimeKey: \u0026quot;t\u0026quot;, LevelKey: \u0026quot;level\u0026quot;, NameKey: \u0026quot;logger\u0026quot;, CallerKey: \u0026quot;caller\u0026quot;, MessageKey: \u0026quot;msg\u0026quot;, StacktraceKey: \u0026quot;stack\u0026quot;, EncodeTime: zapcore.ISO8601TimeEncoder, LineEnding: zapcore.DefaultLineEnding, EncodeLevel: zapcore.LowercaseLevelEncoder, EncodeDuration: zapcore.SecondsDurationEncoder, EncodeCaller: zapcore.FullCallerEncoder, } logger := NewZapLogger( encoder, zap.NewAtomicLevelAt(zapcore.DebugLevel), zap.AddStacktrace( zap.NewAtomicLevelAt(zapcore.ErrorLevel)), zap.AddCallerSkip(2), zap.Development(), ) zlog := log.NewHelper(logger) zlog.Infow(\u0026quot;name\u0026quot;,\u0026quot;go 语言进阶\u0026quot;) defer logger.Sync() } 参考文献 关于 log 库的讨论 issue Uber 的日志库 Zap uber/zap 日志割接库 lumberjack 基于 zap 的日志demo log example ","date":"2021-08-19","permalink":"https://daemon365.dev/2021/08/19/kratos-%E6%97%A5%E5%BF%97%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8%E5%A7%BF%E5%8A%BF/","tags":["go","kratos"],"title":"kratos 日志库的使用姿势"},{"content":"Linux基础系统优化 Linux的网络功能相当强悍，一时之间我们无法了解所有的网络命令，在配置服务器基础环境时，先了解下网络参数设定命令。\nifconfig　查询、设置网卡和ip等参数 ifup,ifdown 脚本命令，更简单的方式启动关闭网络 ip　符合指令，直接修改上述功能 在我们刚装好linux的时候，需要用xshell进行远程连接，那就得获取ip地址，有时候网卡默认是没启动的，Linux也就拿不到ip地址，因此我们得手动启动网卡\n编辑网卡配置文件vim /etc/sysconfig/network-scripts/ifcfg-eth0 修改配置参数ONBOOT=yes 网卡配置文件详解 网络配置文件：/etc/sysconfig/network 网络接口配置文件：etc/sysconfig/network-scripts/ifcfg-INTERFACE_NAME DEVICE=: 关联的设备名称，要与文件名的后半部“INTERFACE_NAME”保持一致; BOOTPROTO={static|none|dhcp|bootp}:引导协议；要使用静态地址，使用static或none；dhcp表示使用DHCP服务器获取地址； IPADDR=IP地址 NETMASK=：子网掩码 GATEWAY=：设定默认网关； ONBOOT=：开机时是否自动激活此网络接口； HWADDR=： 硬件地址，要与硬件中的地址保持一致；可省； USERCTL={yes|no}: 是否允许普通用户控制此接口； PEERDNS={yes|no}: 是否在BOOTPROTO为dhcp时接受由DHCP服务器指定的DNS地址； ifup,ifdown命令 启动/关闭一块网卡 ifup eth0 ifdown eth0 ifconfig命令 ifconfig 查看网卡的ip地址 接输入ifconfig会列出已经启动的网卡，也可以输入ifconfig eth0单独显示eth0的信息 选项解释是： th0 网卡的代号 o 回环地址loopback net IPv4的Ip地址 etmask 子网掩码 roadcast 广播地址 X/TX 流量发/收情况 tx是发送（transport），rx是接收(receive) ackets 数据包数 rrors 数据包错误数 ropped 数据包有问题被丢弃的数量 ollisions 数据包碰撞情况，数值太多代表网络状况差 ifup,ifdown命令 ifup和ifdown是直接连接到/etc/sysconfig/network-scripts目录下搜索对应的网卡文件，例如ifcfg-eth0然后加以设置 ip命令 ip是一个命令，不是TCP/IP那个ip，这个ip命令是结合了ifconfig和route两个命令的功能。 ip addr show #查看ip信息 了解了如何查看网卡信息，接下来查看系统信息。\n你的系统是什么版本？\n查看系统版本信息cat /etc/redhat-release 查看内核版本号uname -r 查看系统多少位uname -m 查看内核所有信息uname -a 用户管理与文件权限篇 创建普通用户 添加用户useradd zhaohaiyu 设置密码passwd zhaohaiyuroot用户可以修改其他所有人的密码，且不需要验证 切换用户 su - username\n先看下当前用户（我是谁）whoami\n退出用户登录logoutctrl + d\n一般情况下，在生产环境避免直接用root用户，除非有特殊系统维护需求，使用完立刻退回普通用户\n非交互式设置密码(echo \u0026ldquo;redhat\u0026rdquo;|passwd \u0026ndash;stdin oldboy \u0026amp;\u0026amp; history -c)\nTip: 1.超级用户root切换普通用户无需密码,例如“群主”想踢谁就踢谁 2.普通用户切换root，需要输入密码 3.普通用户权限较小，只能基本查看信息 4.$符号是普通用户命令提示符，#是超级管理员的提示符 root是当前用户，oldboyedu是主机名，~代表当前路径，也是家目录 groupadd命令\ngroup命令用于创建用户组，为了更加高效的指派系统中各个用户的权限，groupadd it_dep\nuserdel删除用户 -f 强制删除用户 -r 同事删除用户以及家目录 userdel -r zhaohaiyu 文件与目录权限 Linux权限的目的是（保护账户的资料）\nLinux权限主要依据三种身份来决定：\nuser/owner 文件使用者,文件属于哪个用户 group 属组,文件属于哪个组 others 既不是user，也不再group，就是other，其他人 什么是权限 在Linux中，每个文件都有所属的所有者，和所有组，并且规定了文件的所有者，所有组以及其他人对文件的，可读，可写，可执行等权限。对于目录的权限来说，可读是读取目录文件列表，可写是表示在目录内新增，修改，删除文件。可执行表示可以进入目录\nLinux权限的观察 使用一条命令查看权限ls -l /var/log/mysqld.log\n权限，第一个字母为文件类型，后续9个字母，每3个一组，是三种身份的权限 文件链接数 文件拥有者-属主 文件拥有组-属组 文件大小 最后一次被修改的时间日期 文件名 先来分析一下文件的类型\n- 一般文件 d 文件夹 l 软连接（快捷方式） b 块设备，存储媒体文件为主 c 代表键盘,鼠标等设备 文件权限 r read可读，可以用cat等命令查看 w write写入，可以编辑或者删除这个文件 x executable 可以执行 目录权限 r 可以对此目录执行ls列出所有文件 w 可以在这个目录创建文件 x 可以cd进入这个目录，或者查看详细信息 权限与数字转化\n软连接 软连接也叫做符号链接，类似于windows的快捷方式。\n常用于安装软件的快捷方式配置，如mysql，nginx等ln -s 目标文件 软连接名\n存在文件/tmp/test.txt-rw-r\u0026ndash;r\u0026ndash; 1 root root 10 10月 15 21:23 test.txt 在/home目录中建立软连接，指向/tmp/test.txt文件=ln -s /tmp/test.txt my_test 查看软连接信息 lrwxrwxrwx 1 root root 13 10月 15 21:35 my_test -\u0026gt; /tmp/test.txt4.通过软连接查看文件cat my_testmy_test只是/tmp/test.txt的一个别名，因此删除my_test不会影响/tmp/test.txt，但是删除了本尊，快捷方式就无意义不存在了 命令 tar解压命令 linux的文件打包工具最出名的是tar。\ntar 命令：用来压缩和解压文件。tar本身不具有压缩功能。他是调用压缩功能实现的\n语法\ntar(选项)(参数) -A或--catenate：新增文件到以存在的备份文件； -B：设置区块大小； -c或--create：建立新的备份文件； -C ：这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。 -d：记录文件的差别； -x或--extract或--get：从备份文件中还原文件； -t或--list：列出备份文件的内容； -z或--gzip或--ungzip：通过gzip指令处理备份文件； -Z或--compress或--uncompress：通过compress指令处理备份文件； -f或--file=：指定备份文件； -v或--verbose：显示指令执行过程； -r：添加文件到已经压缩的文件； -u：添加改变了和现有的文件到已经存在的压缩文件； -j：支持bzip2解压文件； -v：显示操作过程； -l：文件系统边界设置； -k：保留原有文件不覆盖； -m：保留文件不被覆盖； -w：确认压缩文件的正确性； -p或--same-permissions：用原来的文件权限还原文件； -P或--absolute-names：文件名使用绝对名称，不移除文件名称前的“/”号； -N 或 --newer=：只将较指定日期更新的文件保存到备份文件里； --exclude=：排除符合范本样式的文件。 gzip命令 gzip用来压缩文件，是个使用广泛的压缩程序，被压缩的以\u0026quot;.gz\u0026quot;扩展名 gzip可以压缩较大的文件，以60%~70%压缩率来节省磁盘空间 语法\n-d或--decompress或----uncompress：解开压缩文件； -f或——force：强行压缩文件。 -h或——help：在线帮助； -l或——list：列出压缩文件的相关信息； -L或——license：显示版本与版权信息； -r或——recursive：递归处理，将指定目录下的所有文件及子目录一并处理； -v或——verbose：显示指令执行过程； netstat命令 netstat命令用来打印Linux中网络系统的状态信息，可让你得知整个Linux系统的网络情况。\n语法【选项】\nnetstat [选项] -t或--tcp：显示TCP传输协议的连线状况； -u或--udp：显示UDP传输协议的连线状况； -n或--numeric：直接使用ip地址，而不通过域名服务器； -l或--listening：显示监控中的服务器的Socket； -p或--programs：显示正在使用Socket的程序识别码和程序名称；-a或--all：显示所有连线中的Socket； ps命令 ps 命令用于查看系统中的进程状态，格式为“ps [参数]”。\nps　命令常用参数 -a 显示所有进程 -u 用户以及其他详细信息 -x 显示没有控制终端的进程 Kill命令 kill命令用来删除执行中的程序或工作。kill可将指定的信息送至程序。\n选项\n-a：当处理当前进程时，不限制命令名和进程号的对应关系； -l ：若不加选项，则-l参数会列出全部的信息名称； -p：指定kill 命令只打印相关进程的进程号，而不发送任何信号； -s ：指定要送出的信息； -u：指定用户。 只有第9种信号(SIGKILL)才可以无条件终止进程，其他信号进程都有权利忽略，下面是常用的信号：\nHUP 1 终端断线 INT 2 中断（同 Ctrl + C） QUIT 3 退出（同 Ctrl + \\） TERM 15 终止 KILL 9 强制终止 CONT 18 继续（与STOP相反， fg/bg命令） STOP 19 暂停（同 Ctrl + Z） killall命令 通常来讲，复杂软件的服务程序会有多个进程协同为用户提供服务，如果逐个去结束这 些进程会比较麻烦，此时可以使用 killall 命令来批量结束某个服务程序带有的全部进程。 例如nginx启动后有2个进程 killall nginx\nSELinux功能 SELinux(Security-Enhanced Linux) 是美国国家安全局（NSA）对于强制访问控制的实现，这个功能管理员又爱又恨，大多数生产环境也是关闭的做法，安全手段使用其他方法。\n大多数ssh连接不上虚拟机，都是因为防火墙和selinux阻挡了\n永久关闭方式：\n1.修改配置文件，永久生效关闭selinux cp /etc/selinux/config /etc/selinux/config.bak #修改前备份 2.修改方式可以vim编辑,找到 # This file controls the state of SELinux on the system. # SELINUX= can take one of these three values: # enforcing - SELinux security policy is enforced. # permissive - SELinux prints warnings instead of enforcing. # disabled - No SELinux policy is loaded. SELINUX=disabled 3.用sed替换 sed -i 's/SELINUX=enforcing/SELINUX=disabled/' /etc/selinux/config 4.检查状态 grep \u0026quot;SELINUX=disabled\u0026quot; /etc/selinux/config #出现结果即表示修改成功 临时关闭selinux(命令行修改，重启失效)：\ngetenforce #获取selinux状态 #修改selinux状态 setenforce usage: setenforce [ Enforcing | Permissive | 1 | 0 ] 数字0 表示permissive，给出警告，不会阻止，等同disabled 数字1表示enforcing，表示开启 Tip:\n修改selinux配置后，想要生效还得重启系统，技巧就是（修改配置文件+命令行修改，达到立即生效） 生产环境的服务器是禁止随意重启的！！！！ iptables防火墙 在学习阶段，关闭防火墙可以更方便的学习，在企业环境中，一般只有配置外网ip的linux服务器才会开启防火墙，但是对于高并发流量的业务服务器仍然是不能开启的，会有很大性能损失，因此需要更nb的硬件防火墙。\n关闭防火墙具体操作如下：\nsystemctl status firewalld查看防火墙状态 systemctl stop firewalld关闭防火墙 systemctl disable firewalld关闭防火墙开机启动systemctl is-enabled firewalld.service#检查防火墙是否启动 df命令 df命令用于显示磁盘分区上的可使用的磁盘空间。默认显示单位为KB。可以利用该命令来获取硬盘被占用了多少空间，目前还剩下多少空间等信息。\n语法\ndf(选项)(参数) -h或--human-readable：以可读性较高的方式来显示信息； -k或--kilobytes：指定区块大小为1024字节； -T或--print-type：显示文件系统的类型； --help：显示帮助； --version：显示版本信息。 tree命令 tree命令以树状图列出目录的内容。 -a：显示所有文件和目录； -A：使用ASNI绘图字符显示树状图而非以ASCII字符组合； -C：在文件和目录清单加上色彩，便于区分各种类型； -d：先是目录名称而非内容； -D：列出文件或目录的更改时间； -f：在每个文件或目录之前，显示完整的相对路径名称； -F：在执行文件，目录，Socket，符号连接，管道名称名称，各自加上\u0026quot;*\u0026quot;，\u0026quot;/\u0026quot;，\u0026quot;@\u0026quot;，\u0026quot;|\u0026ldquo;号； -g：列出文件或目录的所属群组名称，没有对应的名称时，则显示群组识别码； -i：不以阶梯状列出文件和目录名称； -l： 不显示符号范本样式的文件或目录名称； -l：如遇到性质为符号连接的目录，直接列出该连接所指向的原始目录； -n：不在文件和目录清单加上色彩； -N：直接列出文件和目录名称，包括控制字符； -p：列出权限标示； -P： 只显示符合范本样式的文件和目录名称； -q：用“？”号取代控制字符，列出文件和目录名称； -s：列出文件和目录大小； -t：用文件和目录的更改时间排序； -u：列出文件或目录的拥有者名称，没有对应的名称时，则显示用户识别码； -x：将范围局限在现行的文件系统中，若指定目录下的某些子目录，其存放于另一个文件系统上，则将该目录予以排除在寻找范围外。 设置主机名 hostnamectl set-hostname zhaohaiyu\n查看:hostname\nDNS DNS（Domain Name System，域名系统），万维网上作为域名和IP地址相互映射的一个分布式数据库，能够使用户更方便的访问互联网，而不用去记住能够被机器直接读取的IP数串。 通过域名，最终得到该域名对应的IP地址的过程叫做域名解析（或主机名解析）。 查看Linux的dns，唯一配置文件 配置文件 cat /etc/resolv.conf#dns服务器地址 nameserver 119.29.29.29 nameserver 223.5.5.5 本地强制dns解析文件/etc/hosts 指定本地解析： /etc/hosts 主机IP 主机名 主机别名 127.0.0.1 www.zhaohaiyu.com yum命令 yum命令是在Fedora和RedHat以及SUSE中基于rpm的软件包管理器，它可以使系统管理人员交互和自动化地更细与管理RPM软件包，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软体包，无须繁琐地一次次下载、安装。\n尽管 RPM 能够帮助用户查询软件相关的依赖关系，但问题还是要运维人员自己来解决， 而有些大型软件可能与数十个程序都有依赖关系，在这种情况下安装软件会是非常痛苦的。 Yum 软件仓库便是为了进一步降低软件安装难度和复杂度而设计的技术。Yum 软件仓库可以 根据用户的要求分析出所需软件包及其相关的依赖关系，然后自动从服务器下载软件包并安 装到系统。\nYum 软件仓库中的 RPM 软件包可以是由红帽官方发布的，也可以是第三方发布的，当 然也可以是自己编写的。\n选项\n-h：显示帮助信息； -y：对所有的提问都回答“yes”； -c：指定配置文件； -q：安静模式； -v：详细模式； -d：设置调试等级（0-10）； -e：设置错误等级（0-10）； -R：设置yum处理一个命令的最大等待时间； -C：完全从缓存中运行，而不去下载或者更新任何头文件。 实例\nyum install pip\nyum源配置 yum源的目录/etc/yum.repos.d/\n配置阿里云yum源\n1.备份yum源 mkdir repo_bak mv *.repo repo_bak/ 2.下载阿里云repo文件wget http://mirrors.aliyun.com/repo/Centos-7.repo 3.清空yum缓存并且生成新的yum缓存yum clean allyum makecache4.安装软件扩展源yum install -y epel-release yum repolist all 列出所有仓库 yum list all 列出仓库所有软件包 yum info 软件包名 查看软件包信息 yum install 软件包名 安装软件包 yum reinstall 软件包名 重新安装软件包 yum update 软件包名 升级软件包 yum remove 软件包名 移除软件包 yum clean all 清楚所有仓库缓存 yum check-update 检查可以更新的软件包 yum grouplist 查看系统中已安装的软件包 yum groupinstall 软件包组 安装软件包组 Linux下安装程序的方法 rpm -ivh 包名.rpm　需要手动解决依赖关系 yum install 包名 yum自动处理依赖关系 编译安装（源码安装） 安装Lrzsz yum install lrzsz\n","date":"2021-08-19","permalink":"https://daemon365.dev/2021/08/19/linux%E5%9F%BA%E7%A1%80%E7%B3%BB%E7%BB%9F%E4%BC%98%E5%8C%96/","tags":["linux"],"title":"Linux基础系统优化"},{"content":"电脑：辅助人脑的工具 现在的人们几乎无时无刻都会碰电脑！不管是桌上型电脑(桌机)、笔记型电脑(笔电)、平板电脑、智慧型手机等等，这些东西都算是电脑。虽然接触的这么多，但是，你了解电脑里面的元件有什么吗？以桌机来说，电脑的机壳里面含有什么元件？不同的电脑可以应用在哪些工作？你生活周遭有哪些电器用品内部是含有电脑相关元件的？底下我们就来谈一谈这些东西呢！\n所谓的电脑就是一种计算机，而计算机其实是：『接受使用者输入指令与资料，经由中央处理器的数学与逻辑单元运算处理后，以产生或储存成有用的资讯』。因此，只要有输入设备(不管是键盘还是触控式萤幕)及输出设备(例如电脑萤幕或直接由印表机列印出来)，让你可以输入资料使该机器产生资讯的，那就是一部计算机了。\n好了，根据这个定义你知道哪些东西是计算机了吗？其实包括一般商店用的简易型加减乘除计算机、打电话用的手机、开车用的卫星定位系统(GPS)、提款用的提款机(ATM)、你上课会使用的桌上型个人电脑、外出可能会带的笔记型电脑(包括notebook与netbook)，还有近几年(2015前后)非常热门的平板电脑与智慧型手机，甚至是未来可能会大流行的单版电脑(Xapple pi, banana pi, Raspberry pi, )与智慧型手表，甚至于更多的智慧型穿戴式电脑等等，这些都是计算机喔！\n电脑硬件的组成 关于电脑的硬件组成部分，其实你可以观察你的桌上型电脑来分析一下，依外观来说这家伙主要可分为三部分，分别是：\n输入单元：包括键盘、滑鼠、读卡机、扫描器、手写板、触控萤幕等等一堆； 主机部分：这个就是系统单元，被主机机壳保护住了，里面含有一堆板子、CPU 与主记忆体等； 输出单元：例如萤幕、印表机等等 我们主要透过输入设备如滑鼠与键盘来将一些资料输入到主机里面，然后再由主机的功能处理成为图表或文章等资讯后， 将结果传输到输出设备，如萤幕或印表机上面。那主机里面含有什么元件呢？如果你曾经拆开过电脑主机机壳(包括拆开你的智慧型手机也一样喔！)， 会发现其实主机里面最重要的就是一片主机板，上面安插了中央处理器(CPU) 以及主记忆体、硬碟(或记忆卡) 还有一些介面卡装置而已。当然大部分智慧型手机是将这些元件直接焊接在主机板上面而不是插卡啦！\n整部主机的重点在于中央处理器(Central Processing Unit, CPU)，CPU为一个具有特定功能的晶片，里头含有微指令集，如果你想要让主机进行什么特异的功能，就得要参考这颗CPU是否有相关内建的微指令集才可以。由于CPU的工作主要在于管理与运算，因此在CPU内又可分为两个主要的单元，分别是： 算数逻辑单元与控制单元。其中算数逻辑单元主要负责程式运算与逻辑判断，控制单元则主要在协调各周边元件与各单元间的工作。\n如果不是很了解电脑的运作流程的话，超哥拿个简单的想法来思考好了～假设电脑是一个人体，那么每个元件对应到那个地方呢？可以这样思考：\nCPU=脑袋瓜子：每个人会作的事情都不一样(微指令集的差异)，但主要都是透过脑袋瓜子来进行判断与控制身体各部分的活动；\n主记忆体=脑袋中放置正在被思考的资料的区块：在实际活动过程中，我们的脑袋瓜子需要有外界刺激的资料(例如光线、环境、语言等)来分析，那这些互动资料暂时存放的地方就是主记忆体，主要是用来提供给脑袋瓜子判断用的资讯。\n硬碟=脑袋中放置回忆的记忆区块：跟刚刚的主记忆体不同，主记忆体是提供脑袋目前要思考与处理的资讯，但是有些生活琐事或其他没有要立刻处理的事情，就当成回忆先放置到脑袋的记忆深处吧！那就是硬碟！主要目的是将重要的资料记录起来，以便未来将这些重要的经验再次的使用；\n主机板=神经系统：好像人类的神经一样，将所有重要的元件连接起来，包括手脚的活动都是脑袋瓜子发布命令后，透过神经(主机板)传导给手脚来进行活动啊！\n各项周边设备=人体与外界沟通的手、脚、皮肤、眼睛等：就好像手脚一般，是人体与外界互动的重要关键！\n显示卡=脑袋中的影像：将来自眼睛的刺激转成影像后在脑袋中呈现，所以显示卡所产生的资料来源也是CPU控制的。\n电源供应器(Power)=心脏：所有的元件要能运作得要有足够的电力供给才行！这电力供给就好像心脏一样，如果心脏不够力， 那么全身也就无法动弹的！心脏不稳定呢？那你的身体当然可能断断续续的～不稳定！\n电源 既然是人体的心脏，保障电源供应，就需要质量好的电源，生产环境中单个核心服务器最好是双电源AB线路。\n一个接220V电路，一个可能接蓄电池UPS(不间断电源)\nCPU 常见品牌：Intel、AMD，想当于人体的大脑\n内存 是CPU和磁盘之间的缓冲设备，也叫临时存储器（存放数据），断电时数据丢失\n一般程序运行时会被调度到内存中执行，服务器关闭或程序关闭后，数据从内存中释放掉。\n电脑用途的分类 知道了电脑的基本组成与周边装置，也知道其实电脑的CPU种类非常的多，再来我们想要了解的是，电脑如何分类？电脑的分类非常多种，如果以电脑的复杂度与运算能力进行分类的话，主要可以分为这几类：\n超级电脑(Supercomputer) 超级电脑是运作速度最快的电脑，但是他的维护、操作费用也最高！主要是用于需要有高速计算的计画中。例如：国防军事、气象预测、太空科技，用在模拟的领域较多。详情也可以参考：国家高速网路与计算中心http://www.nchc.org.tw的介绍！至于全世界最快速的前500大超级电脑，则请参考：http://www.top500.org。\n大型电脑(Mainframe Computer) 大型电脑通常也具有数个高速的CPU，功能上虽不及超级电脑，但也可用来处理大量资料与复杂的运算。例如大型企业的主机、全国性的证券交易所等每天需要处理数百万笔资料的企业机构，或者是大型企业的资料库伺服器等等。\n迷你电脑(Minicomputer) 迷你电脑仍保有大型电脑同时支援多使用者的特性，但是主机可以放在一般作业场所，不必像前两个大型电脑需要特殊的空调场所。通常用来作为科学研究、工程分析与工厂的流程管理等。\n工作站(Workstation) 工作站的价格又比迷你电脑便宜许多，是针对特殊用途而设计的电脑。在个人电脑的效能还没有提升到目前的状况之前，工作站电脑的性能/价格比是所有电脑当中较佳的，因此在学术研究与工程分析方面相当常见。\n微电脑(Microcomputer) 个人电脑就属于这部份的电脑分类，也是我们本章主要探讨的目标！体积最小，价格最低，但功能还是五脏俱全的！大致又可分为桌上型、笔记型等等。\n若光以效能来说，目前的个人电脑效能已经够快了，甚至已经比工作站等级以上的电脑运算速度还要快！但是工作站电脑强调的是稳定不当机，并且运算过程要完全正确，因此工作站以上等级的电脑在设计时的考量与个人电脑并不相同啦！这也是为啥工作站等级以上的电脑售价较贵的原因。\n互联网常见服务器介绍 DELL（大多数公司在用） HP IBM（百度，银行，政府）（贵） 浪潮 联想 服务器： 服务器指的是网络中能对其他机器提供某些服务的计算机系统，相对普通PC，服务器指的是高性能计算机，稳定性、安全性要求更高 服务器的高性能体现在高速的运转能力，长时间的可靠运行，强大的数据吞吐能力 服务器分类 包括大型机、小型机和UNIX服务器，价格昂贵，体系封闭，但是稳定性极强，性能强、主要用在金融、电信等大型企业核心系统中。\n也就是通常所说的PC服务器，价格便宜、兼容性好，稳定性差，不够安全，常用在中小型企业。\n机房托管服务器是按照服务器的厚度来收费的，每一个机柜规格有限，标准机柜可以放16台1u服务器，如果你的服务器太厚太大，机柜能放的服务器就少一些 1U 单位是==unit==厚度是4.45cm\n总结 计算机必须有的组成部分（cpu、硬盘、内存、电源） 服务器 \u0026gt; 个人台式机　-稳定性更高 ","date":"2021-08-15","permalink":"https://daemon365.dev/2021/08/15/linux%E6%A0%B8%E5%BF%83%E7%9F%A5%E8%AF%86/","tags":["linux"],"title":"Linux核心知识"},{"content":"常用指令 ls\t显示文件或目录\n-l 列出文件详细信息l(list) -a 列出当前目录下所有文件及目录，包括隐藏的a(all) mkdir 创建目录\n-p 创建目录，若无父目录，则创建p(parent) cd 切换目录\ntouch 创建空文件\necho 创建带有内容的文件。\ncat 查看文件内容\ncp 拷贝\nmv 移动或重命名\nrm 删除文件\n-r 递归删除，可删除子目录及文件 -f 强制删除 find 在文件系统中搜索某文件\nwc 统计文本中行数、字数、字符数\ngrep 在文本文件中查找某个字符串\nrmdir 删除空目录\ntree 树形结构显示目录，需要安装tree包\npwd 显示当前目录\nln 创建链接文件\nmore、less 分页显示文本文件内容\nhead、tail 显示文件头、尾内容\nctrl+alt+F1 命令行全屏模式\n系统管理命令 stat 显示指定文件的详细信息，比ls更详细 who 显示在线登陆用户 whoami 显示当前操作用户 hostname 显示主机名 uname 显示系统信息 top 动态显示当前耗费资源最多进程信息 ps 显示瞬间进程状态 ps -aux du 查看目录大小 du -h /home带有单位显示目录信息 df 查看磁盘大小 df -h 带有单位显示磁盘信息 ifconfig 查看网络情况 ping 测试网络连通 netstat 显示网络状态信息 man 命令不会用了，找男人 如：man ls clear 清屏 alias 对命令重命名 如：alias showmeit=\u0026quot;ps -aux\u0026quot; ，另外解除使用unaliax showmeit kill 杀死进程，可以先用ps 或 top命令查看进程的id，然后再用kill命令杀死进程。 打包压缩相关命令 gzip 例如：zip -r mysql.zip mysql 该句命令的含义是：将mysql文件夹压缩成mysql.zip zip -r abcdef.zip abc def.txt 这句命令的意思是将文件夹abc和文件def.txt压缩成一个压缩包abcdef.zip bzip2 与zip命令相反，这是解压命令，用起来很简单。 如：unzip mysql.zip 在当前目录下直接解压mysql.zip。 tar -c 归档文件 -x 压缩文件 -z gzip压缩文件 -j bzip2压缩文件 -v 显示压缩或解压缩过程 v(view) -f 使用档名 例： tar -cvf /home/abc.tar /home/abc 只打包，不压缩 tar -zcvf /home/abc.tar.gz /home/abc 打包，并用gzip压缩 tar -jcvf /home/abc.tar.bz2 /home/abc 打包，并用bzip2压缩 当然，如果想解压缩，就直接替换上面的命令 tar -cvf / tar -zcvf / tar -jcvf 中的“c” 换成“x” 就可以了。 关机/重启机器 shutdown\n-r 关机重启 -h 关机不重启 now 立刻关机 halt 关机\nreboot 重启\nLinux管道 将一个命令的标准输出作为另一个命令的标准输入。也就是把几个命令组合起来使用，后一个命令除以前一个命令的结果。\n例：grep -r \u0026quot;close\u0026quot; /home/* | more 在home目录下所有文件中查找，包括close的文件，并分页输出。 Linux软件包管理 yum常用命令 yum -y install [软件包名]　安装 yum erase [软件包名] 卸载 yum clean all 清除缓存 yum makecache　加载缓存 本地yum配置 本机创建yum仓库 mkdir -p /root/test cd /root/test wget http://mirror.centos.org/centos-7/7/os/x86_64/Packages/dhclient-4.2.5-77.el7.centos.x86_64.rpm 生成repodata依赖文件 create /root/test 修改yum配置文件 cd /etc/yum.repos.d 进入yum配置目录 touch local.repo 创建配置文件 vim local.repo 编辑配置文件 网络yum配置 `cd /etc/yum.repos.d`` ``touch cenos.repo` vim cenos.repo 注意：baseurl路径，到repodate文件所在目录\n设置本地cache保存路径 vim /etc/yum.conf cachedir 表示cache保存路径 keepcache 1-表示保存；0-表示不保 vim使用 vim三种模式：命令模式、插入模式、编辑模式。使用ESC或i或：来切换模式。\n命令模式下：\n:q 退出 :q! 强制退出 :wq 保存并退出 :set number 显示行号 :set nonumber 隐藏行号 /apache 在文档中查找apache 按n跳到下一个，shift+n上一个 yyp 复制光标所在行，并粘贴 h(左移一个字符←)、j(下一行↓)、k(上一行↑)、l(右移一个字符→) 用户及用户组管理 /etc/passwd 存储用户账号 /etc/group 存储组账号 /etc/shadow 存储用户账号的密码 /etc/gshadow 存储用户组账号的密码 useradd 用户名 userdel 用户名 groupadd 组名 groupdel 组名 passwd root 给root设置密码 /etc/profile 系统环境变量 bash_profile 用户环境变量 .bashrc 用户环境变量 su user 切换用户，加载配置文件.bashrc su - user 切换用户，加载配置文件/etc/profile ，加载bash_profile 更改文件的用户及用户组 sudo chown [-R] owner[:group] {File|Directory}\n例如：还以jdk-7u21-linux-i586.tar.gz为例。属于用户hadoop，组hadoop 要想切换此文件所属的用户及组。可以使用命令。 sudo chown root:root jdk-7u21-linux-i586.tar.gz 文件权限管理 三种基本权限\nR 读 数值表示为4 W 写 数值表示为2 X 可执行 数值表示为1 如图所示，goblog.exe文件的权限为-rw-rw-rw-\n-rw-rw-r--一共十个字符，分成四段。\n第一个字符“-”表示普通文件；这个位置还可能会出现“l”链接；“d”表示目录 第二三四个字符“rw-”表示当前所属用户的权限。 所以用数值表示为4+2=6 第五六七个字符“rw-”表示当前所属组的权限。 所以用数值表示为4+2=6 第八九十个字符“r\u0026ndash;”表示其他用户权限。 所以用数值表示为4 所以操作此文件的权限用数值表示为664 更改权限 sudo chmod [u所属用户 g所属组 o其他用户 a所有用户] [+增加权限 -减少权限] [r w x] 目录名\n例如：有一个文件filename，权限为-rw-r----x1 ,将权限值改为-rwxrw-r-x，用数值表示为765\nsudo chmod u+x g+w o+r filename 上面的例子可以用数值表示\nsudo chmod 765 filename ","date":"2021-08-11","permalink":"https://daemon365.dev/2021/08/11/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","tags":["linux"],"title":"Linux常用命令"},{"content":"微服务架构全景图 服务注册和发现 Client side implement\n调用需要维护所有调用服务的地址 有一定的技术难度，需要rpc框架支持 Server side implement\n架构简单 有单点故障 注册中心 etcd注册中心\n分布式一致性系统 基于raft一致性协议 etcd使用场景\n服务注册和发现 共享配置 分布式锁 Leader选举 Raft协议详解 应用场景\n解决分布式系统一致性的问题 基于复制的 工作机制\nleader选举 日志复制 安全性 基本概念 raft协议演示：http://www.kailing.pub/raft/index.html\n角色\nFollower角色 Leader角色 Candicate角色 Term（任期）概念\n在raft协议中，将时间分成一个个term（任期） 复制状态机 在一个分布式系统数据库中,如果每个节点的状态一致,每个节点都执行相同的命令序列,那么最终他们会得到一个一致的状态。也就是和说,为了保证整个分布式系统的一致性,我们需要保证每个节点执行相同的命令序列,也就是说每个节点的日志要保持一样。所以说,保证日志复制一致就是Raf等一致性算法的工作了\n这里就涉及 Replicated State Machine(复制状态机),如上图所示。在一个节点上,一致性模块( Consensus Module,也就是分布式共识算法)接收到了来自客户端的命令。然后把接收到的命令写入到日志中,该节点和其他节点通过一致性模块进行通信确保每个日志最终包含相同的命令序列。一旦这些日志的命令被正确复制,每个节点的状态机( State Machine)都会按照相同的序列去执行他们,从而最终得到一致的状态。然后将达成共识的结果返回给客户端,如下图所示。\n心跳（heartbeats）和超时机制（timeout） 在Ra算法中,有两个 timeout机制来控制领导人选举一个是选举定时器( elation timeou):\n即 Follower等待成为 Candidate状态的等待时间,这个时间被随机设定为150ms-300ms之间\n另一个是 headrbeat timeout:在某个节点成为 Leader以后,它会发送 Append Entries消息给其他节点,这些消息就是通过 heartbeat timeout来传送, Follower接收到 Leader的心跳包的同时也重置选举定时器\nLeader选举 Raft 的选举过程 Raft 协议在集群初始状态下是没有 Leader 的, 集群中所有成员均是 Follower，在选举开始期间所有 Follower 均可参与选举，这时所有 Follower 的角色均转变为 Condidate, Leader 由集群中所有的 Condidate 投票选出，最后获得投票最多的 Condidate 获胜，其角色转变为 Leader 并开始其任期，其余落败的 Condidate 角色转变为 Follower 开始服从 Leader 领导。这里有一种意外的情况会选不出 Leader 就是所有 Condidate 均投票给自己，这样无法决出票数多的一方，Raft 算法为了解决这个问题引入了北洋时期袁世凯获选大总统的谋略，即选不出 Leader 不罢休，直到选出为止，一轮选不出 Leader，便令所有 Condidate 随机 sleap（Raft 论文称为 timeout）一段时间，然后马上开始新一轮的选举，这里的随机 sleep 就起了很关键的因素，第一个从 sleap 状态恢复过来的 Condidate 会向所有 Condidate 发出投票给我的申请，这时还没有苏醒的 Condidate 就只能投票给已经苏醒的 Condidate ，因此可以有效解决 Condiadte 均投票给自己的故障，便可快速的决出 Leader。\n选举出 Leader 后 Leader 会定期向所有 Follower 发送 heartbeat 来维护其 Leader 地位，如果 Follower 一段时间后未收到 Leader 的心跳则认为 Leader 已经挂掉，便转变自身角色为 Condidate，同时发起新一轮的选举，产生新的 Leader。\nRaft 的数据一致性策略 Raft 协议强依赖 Leader 节点来确保集群数据一致性。即 client 发送过来的数据均先到达 Leader 节点，Leader 接收到数据后，先将数据标记为 uncommitted 状态，随后 Leader 开始向所有 Follower 复制数据并等待响应，在获得集群中大于 N/2 个 Follower 的已成功接收数据完毕的响应后，Leader 将数据的状态标记为 committed，随后向 client 发送数据已接收确认，在向 client 发送出已数据接收后，再向所有 Follower 节点发送通知表明该数据状态为committed。\nRaft 如何处理 Leader 意外的？ client 发送数据到达 Leader 之前 Leader 就挂了，因为数据还没有到达集群内部，所以对集群内部数据的一致性没有影响，Leader 挂了之后，集群会进行新的选举产生新的 Leader，之前挂掉的 Leader 重启后作为 Follower 加入集群，并同步 Leader 上的数据。这里最好要求 client 有重试机制在一定时间没有收到 Leader 的数据已接收确认后进行一定次数的重试，并再次向新的 Leader 发送数据来确保业务的流畅性。 client 发送数据到 Leader，数据到达 Leader 后，Leader 还没有开始向 Folloers 复制数据，Leader就挂了，此时数据仍被标记为 uncommited 状态，这时集群会进行新的选举产生新的 Leader，之前挂掉的 Leader 重启后作为 Follower 加入集群，并同步 Leader 上的数据，来保证数据一致性，之前接收到 client 的数据由于是 uncommited 状态所以可能会被丢弃。这里同样最好要求 client 有重试机制通过在一定时间在没有收到 Leader 的数据已接收确认后进行一定次数的重试，再次向新的 Leader 发送数据来确保业务的流畅性。 client 发送数据到 Leader, Leader 接收数据完毕后标记为 uncommited，开始向 Follower复制数据，在复制完毕一小部分 Follower 后 Leader 挂了，此时数据在所有已接收到数据的 Follower 上仍被标记为 uncommitted，但国不可一日无君，此时集群将进行新的选举，而拥有最新数据的 Follower 变换角色为 Condidate，也就意味着 Leader 将在拥有最新数据的 Follower 中产生，新的 Leader 产生后所有节点开始从新 Leader 上同步数据确保数据的一致性，包括之前挂掉后恢复了状态的 老Leader，这时也以 Follower 的身份同步新 Leader 上的数据。 client 发送数据到 Leader，Leader 接收数据完毕后标记为 uncommitted，开始向 Follower 复制数据，在复制完毕所有 Follower 节点或者大部分节点（大于 N/2），并接收到大部分节点接收完毕的响应后，Leader 节点将数据标记为 committed，这时 Leader 挂了，此时已接收到数据的所有 Follower 节点上的数据状态由于还没有接收到 Leader 的 commited 通知，均处于 uncommited 状态。这时集群进行了新的选举，新的 Leader 将在拥有最新数据的节点中产生，新的 Leader 产生后，由于 client 端因老 Leader 挂掉前没有通知其数据已接收，所以会向新的 Leader 发送重试请求，而新的 Leader 上已经存在了这个之前从老 Leader 上同步过来的数据，因此 Raft 集群要求各节点自身实现去重的机制，保证数据的一致性。 集群脑裂的一致性处理，多发于双机房的跨机房模式的集群。假设一个 5 节点的 Raft 集群，其中三个节点在 A 机房，Leader 节点也在 A 机房，两个节点在 B 机房。突然 A、B 两个机房之间因其他故障无法通讯，那么此时 B 机房中的 2 个Follower 因为失去与 Leader 的联系，均转变自身角色为 Condidate。根据 Leader 选举机制，B 机房中产生了一个新的 Leader，这就发生了脑裂即存在 A 机房中的老 Leader 的集群与B机房新 Leader 的集群。Raft 针对这种情况的处理方式是老的 Leader 集群虽然剩下三个节点，但是 Leader 对数据的处理过程还是在按原来 5 个节点进行处理，所以老的 Leader 接收到的数据，在向其他 4 个节点复制数据，由于无法获取超过 N/2 个 Follower 节点的复制完毕数据响应（因为无法连接到 B 机房中的 2个节点），所以 client 在向老 Leader 发送的数据请求均无法成功写入，而 client 向B机房新 Leader 发送的数据，因为是新成立的集群，所以可以成功写入数据，在A、B两个机房恢复网络通讯后，A 机房中的所有节点包括老 Leader 再以 Follower 角色接入这个集群，并同步新 Leader 中的数据，完成数据一致性处理。 参考文章: - https://www.jianshu.com/p/aa77c8f4cb5c\n","date":"2021-05-30","permalink":"https://daemon365.dev/2021/05/30/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E5%8F%8Araft%E5%8D%8F%E8%AE%AE/","tags":["etcd"],"title":"微服务架构及raft协议"},{"content":"这篇文章主要讲 map 的赋值、删除、查询、扩容的具体执行过程，仍然是从底层的角度展开。结合源码，看完本文一定会彻底明白 map 底层原理。\n我要说明的是，这里对 map 的基本用法涉及比较少，我相信可以通过阅读其他入门书籍了解。本文的内容比较深入，但是由于我画了各种图，我相信很容易看懂。\n什么是 map 维基百科里这样定义 map：\nIn computer science, an associative array, map, symbol table, or dictionary is an abstract data type composed of a collection of (key, value) pairs, such that each possible key appears at most once in the collection.\n简单说明一下：在计算机科学里，被称为相关数组、map、符号表或者字典，是由一组 \u0026lt;key, value\u0026gt; 对组成的抽象数据结构，并且同一个 key 只会出现一次。\n有两个关键点：map 是由 key-value 对组成的；key 只会出现一次。\n和 map 相关的操作主要是：\n增加一个 k-v 对 —— Add or insert； 删除一个 k-v 对 —— Remove or delete； 修改某个 k 对应的 v —— Reassign； 查询某个 k 对应的 v —— Lookup； 简单说就是最基本的 增删查改。\nmap 的设计也被称为 “The dictionary problem”，它的任务是设计一种数据结构用来维护一个集合的数据，并且可以同时对集合进行增删查改的操作。最主要的数据结构有两种：哈希查找表（Hash table）、搜索树（Search tree）。\n哈希查找表用一个哈希函数将 key 分配到不同的桶（bucket，也就是数组的不同 index）。这样，开销主要在哈希函数的计算以及数组的常数访问时间。在很多场景下，哈希查找表的性能很高。\n哈希查找表一般会存在“碰撞”的问题，就是说不同的 key 被哈希到了同一个 bucket。一般有两种应对方法：链表法和开放地址法。链表法将一个 bucket 实现成一个链表，落在同一个 bucket 中的 key 都会插入这个链表。开放地址法则是碰撞发生后，通过一定的规律，在数组的后面挑选“空位”，用来放置新的 key。\n搜索树法一般采用自平衡搜索树，包括：AVL 树，红黑树。面试时经常会被问到，甚至被要求手写红黑树代码，很多时候，面试官自己都写不上来，非常过分。\n自平衡搜索树法的最差搜索效率是 O(logN)，而哈希查找表最差是 O(N)。当然，哈希查找表的平均查找效率是 O(1)，如果哈希函数设计的很好，最坏的情况基本不会出现。还有一点，遍历自平衡搜索树，返回的 key 序列，一般会按照从小到大的顺序；而哈希查找表则是乱序的。\n为什么要用 map 从 Go 语言官方博客摘录一段话：\nOne of the most useful data structures in computer science is the hash table. Many hash table implementations exist with varying properties, but in general they offer fast lookups, adds, and deletes. Go provides a built-in map type that implements a hash table.\nhash table 是计算机数据结构中一个最重要的设计。大部分 hash table 都实现了快速查找、添加、删除的功能。Go 语言内置的 map 实现了上述所有功能。\n很难想象写一个程序不使用 map，以至于在回答为什么要用 map 这个问题上犯了难。\n所以，到底为什么要用 map 呢？因为它太强大了，各种增删查改的操作效率非常高。\nmap 的底层如何实现 首先声明我用的 Go 版本：\ngo version go1.9.2 darwin/amd64 前面说了 map 实现的几种方案，Go 语言采用的是哈希查找表，并且使用链表解决哈希冲突。\n接下来我们要探索 map 的核心原理，一窥它的内部结构。\nmap 内存模型 在源码中，表示 map 的结构体是 hmap，它是 hashmap 的“缩写”：\n// A header for a Go map. type hmap struct { // 元素个数，调用 len(map) 时，直接返回此值 count int flags uint8 // buckets 的对数 log_2 B uint8 // overflow 的 bucket 近似数 noverflow uint16 // 计算 key 的哈希的时候会传入哈希函数 hash0 uint32 // 指向 buckets 数组，大小为 2^B // 如果元素个数为0，就为 nil buckets unsafe.Pointer // 扩容的时候，buckets 长度会是 oldbuckets 的两倍 oldbuckets unsafe.Pointer // 指示扩容进度，小于此地址的 buckets 迁移完成 nevacuate uintptr extra *mapextra // optional fields } 说明一下，B 是 buckets 数组的长度的对数，也就是说 buckets 数组的长度就是 2^B。bucket 里面存储了 key 和 value，后面会再讲。\nbuckets 是一个指针，最终它指向的是一个结构体：\ntype bmap struct { tophash [bucketCnt]uint8 } 但这只是表面(src/runtime/hashmap.go)的结构，编译期间会给它加料，动态地创建一个新的结构：\ntype bmap struct { topbits [8]uint8 keys [8]keytype values [8]valuetype pad uintptr overflow uintptr } bmap 就是我们常说的“桶”，桶里面会最多装 8 个 key，这些 key 之所以会落入同一个桶，是因为它们经过哈希计算后，哈希结果是“一类”的。在桶内，又会根据 key 计算出来的 hash 值的高 8 位来决定 key 到底落入桶内的哪个位置（一个桶内最多有8个位置）。\n来一个整体的图：\n当 map 的 key 和 value 都不是指针，并且 size 都小于 128 字节的情况下，会把 bmap 标记为不含指针，这样可以避免 gc 时扫描整个 hmap。但是，我们看 bmap 其实有一个 overflow 的字段，是指针类型的，破坏了 bmap 不含指针的设想，这时会把 overflow 移动到 extra 字段来。\ntype mapextra struct { // overflow[0] contains overflow buckets for hmap.buckets. // overflow[1] contains overflow buckets for hmap.oldbuckets. overflow [2]*[]*bmap // nextOverflow 包含空闲的 overflow bucket，这是预分配的 bucket nextOverflow *bmap } bmap 是存放 k-v 的地方，我们把视角拉近，仔细看 bmap 的内部组成。\n上图就是 bucket 的内存模型，HOB Hash 指的就是 top hash。 注意到 key 和 value 是各自放在一起的，并不是 key/value/key/value/... 这样的形式。源码里说明这样的好处是在某些情况下可以省略掉 padding 字段，节省内存空间。\n例如，有这样一个类型的 map：\nmap[int64]int8 如果按照 key/value/key/value/... 这样的模式存储，那在每一个 key/value 对之后都要额外 padding 7 个字节；而将所有的 key，value 分别绑定到一起，这种形式 key/key/.../value/value/...，则只需要在最后添加 padding。\n每个 bucket 设计成最多只能放 8 个 key-value 对，如果有第 9 个 key-value 落入当前的 bucket，那就需要再构建一个 bucket ，通过 overflow 指针连接起来。\n创建 map 从语法层面上来说，创建 map 很简单：\nageMp := make(map[string]int) // 指定 map 长度 ageMp := make(map[string]int, 8) // ageMp 为 nil，不能向其添加元素，会直接panic var ageMp map[string]int 通过汇编语言可以看到，实际上底层调用的是 makemap 函数，主要做的工作就是初始化 hmap 结构体的各种字段，例如计算 B 的大小，设置哈希种子 hash0 等等。\nfunc makemap(t *maptype, hint int64, h *hmap, bucket unsafe.Pointer) *hmap { // 省略各种条件检查... // 找到一个 B，使得 map 的装载因子在正常范围内 B := uint8(0) for ; overLoadFactor(hint, B); B++ { } // 初始化 hash table // 如果 B 等于 0，那么 buckets 就会在赋值的时候再分配 // 如果长度比较大，分配内存会花费长一点 buckets := bucket var extra *mapextra if B != 0 { var nextOverflow *bmap buckets, nextOverflow = makeBucketArray(t, B) if nextOverflow != nil { extra = new(mapextra) extra.nextOverflow = nextOverflow } } // 初始化 hamp if h == nil { h = (*hmap)(newobject(t.hmap)) } h.count = 0 h.B = B h.extra = extra h.flags = 0 h.hash0 = fastrand() h.buckets = buckets h.oldbuckets = nil h.nevacuate = 0 h.noverflow = 0 return h } 注意，这个函数返回的结果：*hmap，它是一个指针，而我们之前讲过的 makeslice 函数返回的是 Slice 结构体：\nfunc makeslice(et *_type, len, cap int) slice 回顾一下 slice 的结构体定义：\n// runtime/slice.go type slice struct { array unsafe.Pointer // 元素指针 len int // 长度 cap int // 容量 } 结构体内部包含底层的数据指针。\nmakemap 和 makeslice 的区别，带来一个不同点：当 map 和 slice 作为函数参数时，在函数参数内部对 map 的操作会影响 map 自身；而对 slice 却不会（之前讲 slice 的文章里有讲过）。\n主要原因：一个是指针（*hmap），一个是结构体（slice）。Go 语言中的函数传参都是值传递，在函数内部，参数会被 copy 到本地。*hmap指针 copy 完之后，仍然指向同一个 map，因此函数内部对 map 的操作会影响实参。而 slice 被 copy 后，会成为一个新的 slice，对它进行的操作不会影响到实参。\n哈希函数 map 的一个关键点在于，哈希函数的选择。在程序启动时，会检测 cpu 是否支持 aes，如果支持，则使用 aes hash，否则使用 memhash。这是在函数 alginit() 中完成，位于路径：src/runtime/alg.go 下。\nhash 函数，有加密型和非加密型。 加密型的一般用于加密数据、数字摘要等，典型代表就是 md5、sha1、sha256、aes256 这种； 非加密型的一般就是查找。在 map 的应用场景中，用的是查找。 选择 hash 函数主要考察的是两点：性能、碰撞概率。\n之前我们讲过，表示类型的结构体：\ntype _type struct { size uintptr ptrdata uintptr // size of memory prefix holding all pointers hash uint32 tflag tflag align uint8 fieldalign uint8 kind uint8 alg *typeAlg gcdata *byte str nameOff ptrToThis typeOff } 其中 alg 字段就和哈希相关，它是指向如下结构体的指针：\n// src/runtime/alg.go type typeAlg struct { // (ptr to object, seed) -\u0026gt; hash hash func(unsafe.Pointer, uintptr) uintptr // (ptr to object A, ptr to object B) -\u0026gt; ==? equal func(unsafe.Pointer, unsafe.Pointer) bool } typeAlg 包含两个函数，hash 函数计算类型的哈希值，而 equal 函数则计算两个类型是否“哈希相等”。\n对于 string 类型，它的 hash、equal 函数如下：\nfunc strhash(a unsafe.Pointer, h uintptr) uintptr { x := (*stringStruct)(a) return memhash(x.str, h, uintptr(x.len)) } func strequal(p, q unsafe.Pointer) bool { return *(*string)(p) == *(*string)(q) } 根据 key 的类型，_type 结构体的 alg 字段会被设置对应类型的 hash 和 equal 函数。\nkey 定位过程 key 经过哈希计算后得到哈希值，共 64 个 bit 位（64位机，32位机就不讨论了，现在主流都是64位机），计算它到底要落在哪个桶时，只会用到最后 B 个 bit 位。还记得前面提到过的 B 吗？如果 B = 5，那么桶的数量，也就是 buckets 数组的长度是 2^5 = 32。\n例如，现在有一个 key 经过哈希函数计算后，得到的哈希结果是：\n10010111 | 000011110110110010001111001010100010010110010101010 │ 01010 用最后的 5 个 bit 位，也就是 01010，值为 10，也就是 10 号桶。这个操作实际上就是取余操作，但是取余开销太大，所以代码实现上用的位操作代替。\n再用哈希值的高 8 位，找到此 key 在 bucket 中的位置，这是在寻找已有的 key。最开始桶内还没有 key，新加入的 key 会找到第一个空位，放入。\nbuckets 编号就是桶编号，当两个不同的 key 落在同一个桶中，也就是发生了哈希冲突。冲突的解决手段是用链表法：在 bucket 中，从前往后找到第一个空位。这样，在查找某个 key 时，先找到对应的桶，再去遍历 bucket 中的 key。\n这里参考曹大 github 博客里的一张图，原图是 ascii 图，geek 味十足，可以从参考资料找到曹大的博客，推荐大家去看看。\n上图中，假定 B = 5，所以 bucket 总数就是 2^5 = 32。首先计算出待查找 key 的哈希，使用低 5 位 00110，找到对应的 6 号 bucket，使用高 8 位 10010111，对应十进制 151，在 6 号 bucket 中寻找 tophash 值（HOB hash）为 151 的 key，找到了 2 号槽位，这样整个查找过程就结束了。\n如果在 bucket 中没找到，并且 overflow 不为空，还要继续去 overflow bucket 中寻找，直到找到或是所有的 key 槽位都找遍了，包括所有的 overflow bucket。\n我们来看下源码吧，哈哈！通过汇编语言可以看到，查找某个 key 的底层函数是 mapacess 系列函数，函数的作用类似，区别在下一节会讲到。这里我们直接看 mapacess1 函数：\nfunc mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer { // …… // 如果 h 什么都没有，返回零值 if h == nil || h.count == 0 { return unsafe.Pointer(\u0026amp;zeroVal[0]) } // 写和读冲突 if h.flags\u0026amp;hashWriting != 0 { throw(\u0026quot;concurrent map read and map write\u0026quot;) } // 不同类型 key 使用的 hash 算法在编译期确定 alg := t.key.alg // 计算哈希值，并且加入 hash0 引入随机性 hash := alg.hash(key, uintptr(h.hash0)) // 比如 B=5，那 m 就是31，二进制是全 1 // 求 bucket num 时，将 hash 与 m 相与， // 达到 bucket num 由 hash 的低 8 位决定的效果 m := uintptr(1)\u0026lt;\u0026lt;h.B - 1 // b 就是 bucket 的地址 b := (*bmap)(add(h.buckets, (hash\u0026amp;m)*uintptr(t.bucketsize))) // oldbuckets 不为 nil，说明发生了扩容 if c := h.oldbuckets; c != nil { // 如果不是同 size 扩容（看后面扩容的内容） // 对应条件 1 的解决方案 if !h.sameSizeGrow() { // 新 bucket 数量是老的 2 倍 m \u0026gt;\u0026gt;= 1 } // 求出 key 在老的 map 中的 bucket 位置 oldb := (*bmap)(add(c, (hash\u0026amp;m)*uintptr(t.bucketsize))) // 如果 oldb 没有搬迁到新的 bucket // 那就在老的 bucket 中寻找 if !evacuated(oldb) { b = oldb } } // 计算出高 8 位的 hash // 相当于右移 56 位，只取高8位 top := uint8(hash \u0026gt;\u0026gt; (sys.PtrSize*8 - 8)) // 增加一个 minTopHash if top \u0026lt; minTopHash { top += minTopHash } for { // 遍历 8 个 bucket for i := uintptr(0); i \u0026lt; bucketCnt; i++ { // tophash 不匹配，继续 if b.tophash[i] != top { continue } // tophash 匹配，定位到 key 的位置 k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) // key 是指针 if t.indirectkey { // 解引用 k = *((*unsafe.Pointer)(k)) } // 如果 key 相等 if alg.equal(key, k) { // 定位到 value 的位置 v := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.valuesize)) // value 解引用 if t.indirectvalue { v = *((*unsafe.Pointer)(v)) } return v } } // bucket 找完（还没找到），继续到 overflow bucket 里找 b = b.overflow(t) // overflow bucket 也找完了，说明没有目标 key // 返回零值 if b == nil { return unsafe.Pointer(\u0026amp;zeroVal[0]) } } } 函数返回 h[key] 的指针，如果 h 中没有此 key，那就会返回一个 key 相应类型的零值，不会返回 nil。\n代码整体比较直接，没什么难懂的地方。跟着上面的注释一步步理解就好了。\n这里，说一下定位 key 和 value 的方法以及整个循环的写法。\n// key 定位公式 k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) // value 定位公式 v := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.valuesize)) b 是 bmap 的地址，这里 bmap 还是源码里定义的结构体，只包含一个 tophash 数组，经编译器扩充之后的结构体才包含 key，value，overflow 这些字段。dataOffset 是 key 相对于 bmap 起始地址的偏移：\ndataOffset = unsafe.Offsetof(struct { b bmap v int64 }{}.v) 因此 bucket 里 key 的起始地址就是 unsafe.Pointer(b)+dataOffset。第 i 个 key 的地址就要在此基础上跨过 i 个 key 的大小；而我们又知道，value 的地址是在所有 key 之后，因此第 i 个 value 的地址还需要加上所有 key 的偏移。理解了这些，上面 key 和 value 的定位公式就很好理解了。\n再说整个大循环的写法，最外层是一个无限循环，通过\nb = b.overflow(t) 遍历所有的 bucket，这相当于是一个 bucket 链表。\n当定位到一个具体的 bucket 时，里层循环就是遍历这个 bucket 里所有的 cell，或者说所有的槽位，也就是 bucketCnt=8 个槽位。整个循环过程：\n再说一下 minTopHash，当一个 cell 的 tophash 值小于 minTopHash 时，标志这个 cell 的迁移状态。因为这个状态值是放在 tophash 数组里，为了和正常的哈希值区分开，会给 key 计算出来的哈希值一个增量：minTopHash。这样就能区分正常的 top hash 值和表示状态的哈希值。\n下面的这几种状态就表征了 bucket 的情况：\n// 空的 cell，也是初始时 bucket 的状态 empty = 0 // 空的 cell，表示 cell 已经被迁移到新的 bucket evacuatedEmpty = 1 // key,value 已经搬迁完毕，但是 key 都在新 bucket 前半部分， // 后面扩容部分会再讲到。 evacuatedX = 2 // 同上，key 在后半部分 evacuatedY = 3 // tophash 的最小正常值 minTopHash = 4 源码里判断这个 bucket 是否已经搬迁完毕，用到的函数：\nfunc evacuated(b *bmap) bool { h := b.tophash[0] return h \u0026gt; empty \u0026amp;\u0026amp; h \u0026lt; minTopHash } 只取了 tophash 数组的第一个值，判断它是否在 0-4 之间。对比上面的常量，当 top hash 是 evacuatedEmpty、evacuatedX、evacuatedY 这三个值之一，说明此 bucket 中的 key 全部被搬迁到了新 bucket。\nmap 的两种 get 操作 Go 语言中读取 map 有两种语法：带 comma 和 不带 comma。当要查询的 key 不在 map 里，带 comma 的用法会返回一个 bool 型变量提示 key 是否在 map 中；而不带 comma 的语句则会返回一个 value 类型的零值。如果 value 是 int 型就会返回 0，如果 value 是 string 类型，就会返回空字符串。\npackage main import \u0026quot;fmt\u0026quot; func main() { ageMap := make(map[string]int) ageMap[\u0026quot;qcrao\u0026quot;] = 18 // 不带 comma 用法 age1 := ageMap[\u0026quot;stefno\u0026quot;] fmt.Println(age1) // 带 comma 用法 age2, ok := ageMap[\u0026quot;stefno\u0026quot;] fmt.Println(age2, ok) } 运行结果：\n0 0 false 以前一直觉得好神奇，怎么实现的？这其实是编译器在背后做的工作：分析代码后，将两种语法对应到底层两个不同的函数。\n// src/runtime/hashmap.go func mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer func mapaccess2(t *maptype, h *hmap, key unsafe.Pointer) (unsafe.Pointer, bool) 源码里，函数命名不拘小节，直接带上后缀 1，2，完全不理会《代码大全》里的那一套命名的做法。从上面两个函数的声明也可以看出差别了，mapaccess2 函数返回值多了一个 bool 型变量，两者的代码也是完全一样的，只是在返回值后面多加了一个 false 或者 true。\n另外，根据 key 的不同类型，编译器还会将查找、插入、删除的函数用更具体的函数替换，以优化效率：\nkey 类型 查找 uint32 mapaccess1_fast32(t *maptype, h *hmap, key uint32) unsafe.Pointer uint32 mapaccess2_fast32(t *maptype, h *hmap, key uint32) (unsafe.Pointer, bool) uint64 mapaccess1_fast64(t *maptype, h *hmap, key uint64) unsafe.Pointer uint64 mapaccess2_fast64(t *maptype, h *hmap, key uint64) (unsafe.Pointer, bool) string mapaccess1_faststr(t *maptype, h *hmap, ky string) unsafe.Pointer string mapaccess2_faststr(t *maptype, h *hmap, ky string) (unsafe.Pointer, bool) 这些函数的参数类型直接是具体的 uint32、unt64、string，在函数内部由于提前知晓了 key 的类型，所以内存布局是很清楚的，因此能节省很多操作，提高效率。\n上面这些函数都是在文件 src/runtime/hashmap_fast.go 里。\n如何进行扩容 使用哈希表的目的就是要快速查找到目标 key，然而，随着向 map 中添加的 key 越来越多，key 发生碰撞的概率也越来越大。bucket 中的 8 个 cell 会被逐渐塞满，查找、插入、删除 key 的效率也会越来越低。最理想的情况是一个 bucket 只装一个 key，这样，就能达到 O(1) 的效率，但这样空间消耗太大，用空间换时间的代价太高。\nGo 语言采用一个 bucket 里装载 8 个 key，定位到某个 bucket 后，还需要再定位到具体的 key，这实际上又用了时间换空间。\n当然，这样做，要有一个度，不然所有的 key 都落在了同一个 bucket 里，直接退化成了链表，各种操作的效率直接降为 O(n)，是不行的。\n因此，需要有一个指标来衡量前面描述的情况，这就是装载因子。Go 源码里这样定义 装载因子：\nloadFactor := count / (2^B) count 就是 map 的元素个数，2^B 表示 bucket 数量。\n再来说触发 map 扩容的时机：在向 map 插入新 key 的时候，会进行条件检测，符合下面这 2 个条件，就会触发扩容：\n装载因子超过阈值，源码里定义的阈值是 6.5。 overflow 的 bucket 数量过多：当 B 小于 15，也就是 bucket 总数 2^B 小于 2^15 时，如果 overflow 的 bucket 数量超过 2^B；当 B \u0026gt;= 15，也就是 bucket 总数 2^B 大于等于 2^15，如果 overflow 的 bucket 数量超过 2^15。 通过汇编语言可以找到赋值操作对应源码中的函数是 mapassign，对应扩容条件的源码如下：\n// src/runtime/hashmap.go/mapassign // 触发扩容时机 if !h.growing() \u0026amp;\u0026amp; (overLoadFactor(int64(h.count), h.B) || tooManyOverflowBuckets(h.noverflow, h.B)) { hashGrow(t, h) } // 装载因子超过 6.5 func overLoadFactor(count int64, B uint8) bool { return count \u0026gt;= bucketCnt \u0026amp;\u0026amp; float32(count) \u0026gt;= loadFactor*float32((uint64(1)\u0026lt;\u0026lt;B)) } // overflow buckets 太多 func tooManyOverflowBuckets(noverflow uint16, B uint8) bool { if B \u0026lt; 16 { return noverflow \u0026gt;= uint16(1)\u0026lt;\u0026lt;B } return noverflow \u0026gt;= 1\u0026lt;\u0026lt;15 } 解释一下：\n第 1 点：我们知道，每个 bucket 有 8 个空位，在没有溢出，且所有的桶都装满了的情况下，装载因子算出来的结果是 8。因此当装载因子超过 6.5 时，表明很多 bucket 都快要装满了，查找效率和插入效率都变低了。在这个时候进行扩容是有必要的。\n第 2 点：是对第 1 点的补充。就是说在装载因子比较小的情况下，这时候 map 的查找和插入效率也很低，而第 1 点识别不出来这种情况。表面现象就是计算装载因子的分子比较小，即 map 里元素总数少，但是 bucket 数量多（真实分配的 bucket 数量多，包括大量的 overflow bucket）。\n不难想像造成这种情况的原因：不停地插入、删除元素。先插入很多元素，导致创建了很多 bucket，但是装载因子达不到第 1 点的临界值，未触发扩容来缓解这种情况。之后，删除元素降低元素总数量，再插入很多元素，导致创建很多的 overflow bucket，但就是不会触犯第 1 点的规定，你能拿我怎么办？overflow bucket 数量太多，导致 key 会很分散，查找插入效率低得吓人，因此出台第 2 点规定。这就像是一座空城，房子很多，但是住户很少，都分散了，找起人来很困难。\n对于命中条件 1，2 的限制，都会发生扩容。但是扩容的策略并不相同，毕竟两种条件应对的场景不同。\n对于条件 1，元素太多，而 bucket 数量太少，很简单：将 B 加 1，bucket 最大数量（2^B）直接变成原来 bucket 数量的 2 倍。于是，就有新老 bucket 了。注意，这时候元素都在老 bucket 里，还没迁移到新的 bucket 来。而且，新 bucket 只是最大数量变为原来最大数量（2^B）的 2 倍（2^B * 2）。\n对于条件 2，其实元素没那么多，但是 overflow bucket 数特别多，说明很多 bucket 都没装满。解决办法就是开辟一个新 bucket 空间，将老 bucket 中的元素移动到新 bucket，使得同一个 bucket 中的 key 排列地更紧密。这样，原来，在 overflow bucket 中的 key 可以移动到 bucket 中来。结果是节省空间，提高 bucket 利用率，map 的查找和插入效率自然就会提升。\n对于条件 2 的解决方案，曹大的博客里还提出了一个极端的情况：如果插入 map 的 key 哈希都一样，就会落到同一个 bucket 里，超过 8 个就会产生 overflow bucket，结果也会造成 overflow bucket 数过多。移动元素其实解决不了问题，因为这时整个哈希表已经退化成了一个链表，操作效率变成了 O(n)。\n再来看一下扩容具体是怎么做的。由于 map 扩容需要将原有的 key/value 重新搬迁到新的内存地址，如果有大量的 key/value 需要搬迁，会非常影响性能。因此 Go map 的扩容采取了一种称为“渐进式”地方式，原有的 key 并不会一次性搬迁完毕，每次最多只会搬迁 2 个 bucket。\n上面说的 hashGrow() 函数实际上并没有真正地“搬迁”，它只是分配好了新的 buckets，并将老的 buckets 挂到了 oldbuckets 字段上。真正搬迁 buckets 的动作在 growWork() 函数中，而调用 growWork() 函数的动作是在 mapassign 和 mapdelete 函数中。也就是插入或修改、删除 key 的时候，都会尝试进行搬迁 buckets 的工作。先检查 oldbuckets 是否搬迁完毕，具体来说就是检查 oldbuckets 是否为 nil。\n我们先看 hashGrow() 函数所做的工作，再来看具体的搬迁 buckets 是如何进行的。\nfunc hashGrow(t *maptype, h *hmap) { // B+1 相当于是原来 2 倍的空间 bigger := uint8(1) // 对应条件 2 if !overLoadFactor(int64(h.count), h.B) { // 进行等量的内存扩容，所以 B 不变 bigger = 0 h.flags |= sameSizeGrow } // 将老 buckets 挂到 buckets 上 oldbuckets := h.buckets // 申请新的 buckets 空间 newbuckets, nextOverflow := makeBucketArray(t, h.B+bigger) flags := h.flags \u0026amp;^ (iterator | oldIterator) if h.flags\u0026amp;iterator != 0 { flags |= oldIterator } // 提交 grow 的动作 h.B += bigger h.flags = flags h.oldbuckets = oldbuckets h.buckets = newbuckets // 搬迁进度为 0 h.nevacuate = 0 // overflow buckets 数为 0 h.noverflow = 0 // …… } 主要是申请到了新的 buckets 空间，把相关的标志位都进行了处理：例如标志 nevacuate 被置为 0， 表示当前搬迁进度为 0。\n值得一说的是对 h.flags 的处理：\nflags := h.flags \u0026amp;^ (iterator | oldIterator) if h.flags\u0026amp;iterator != 0 { flags |= oldIterator } 这里得先说下运算符：\u0026amp;^。这叫按位置 0运算符。例如：\nx = 01010011 y = 01010100 z = x \u0026amp;^ y = 00000011 如果 y bit 位为 1，那么结果 z 对应 bit 位就为 0，否则 z 对应 bit 位就和 x 对应 bit 位的值相同。\n所以上面那段对 flags 一顿操作的代码的意思是：先把 h.flags 中 iterator 和 oldIterator 对应位清 0，然后如果发现 iterator 位为 1，那就把它转接到 oldIterator 位，使得 oldIterator 标志位变成 1。潜台词就是：buckets 现在挂到了 oldBuckets 名下了，对应的标志位也转接过去吧。\n几个标志位如下：\n// 可能有迭代器使用 buckets iterator = 1 // 可能有迭代器使用 oldbuckets oldIterator = 2 // 有协程正在向 map 中写入 key hashWriting = 4 // 等量扩容（对应条件 2） sameSizeGrow = 8 再来看看真正执行搬迁工作的 growWork() 函数。\nfunc growWork(t *maptype, h *hmap, bucket uintptr) { // 确认搬迁老的 bucket 对应正在使用的 bucket evacuate(t, h, bucket\u0026amp;h.oldbucketmask()) // 再搬迁一个 bucket，以加快搬迁进程 if h.growing() { evacuate(t, h, h.nevacuate) } } h.growing() 函数非常简单：\nfunc (h *hmap) growing() bool { return h.oldbuckets != nil } 如果 oldbuckets 不为空，说明还没有搬迁完毕，还得继续搬。\nbucket\u0026amp;h.oldbucketmask() 这行代码，如源码注释里说的，是为了确认搬迁的 bucket 是我们正在使用的 bucket。oldbucketmask() 函数返回扩容前的 map 的 bucketmask。\n所谓的 bucketmask，作用就是将 key 计算出来的哈希值与 bucketmask 相与，得到的结果就是 key 应该落入的桶。比如 B = 5，那么 bucketmask 的低 5 位是 11111，其余位是 0，hash 值与其相与的意思是，只有 hash 值的低 5 位决策 key 到底落入哪个 bucket。\n接下来，我们集中所有的精力在搬迁的关键函数 evacuate。源码贴在下面，不要紧张，我会加上大面积的注释，通过注释绝对是能看懂的。之后，我会再对搬迁过程作详细说明。\n源码如下：\nfunc evacuate(t *maptype, h *hmap, oldbucket uintptr) { // 定位老的 bucket 地址 b := (*bmap)(add(h.oldbuckets, oldbucket*uintptr(t.bucketsize))) // 结果是 2^B，如 B = 5，结果为32 newbit := h.noldbuckets() // key 的哈希函数 alg := t.key.alg // 如果 b 没有被搬迁过 if !evacuated(b) { var ( // 表示bucket 移动的目标地址 x, y *bmap // 指向 x,y 中的 key/val xi, yi int // 指向 x，y 中的 key xk, yk unsafe.Pointer // 指向 x，y 中的 value xv, yv unsafe.Pointer ) // 默认是等 size 扩容，前后 bucket 序号不变 // 使用 x 来进行搬迁 x = (*bmap)(add(h.buckets, oldbucket*uintptr(t.bucketsize))) xi = 0 xk = add(unsafe.Pointer(x), dataOffset) xv = add(xk, bucketCnt*uintptr(t.keysize))、 // 如果不是等 size 扩容，前后 bucket 序号有变 // 使用 y 来进行搬迁 if !h.sameSizeGrow() { // y 代表的 bucket 序号增加了 2^B y = (*bmap)(add(h.buckets, (oldbucket+newbit)*uintptr(t.bucketsize))) yi = 0 yk = add(unsafe.Pointer(y), dataOffset) yv = add(yk, bucketCnt*uintptr(t.keysize)) } // 遍历所有的 bucket，包括 overflow buckets // b 是老的 bucket 地址 for ; b != nil; b = b.overflow(t) { k := add(unsafe.Pointer(b), dataOffset) v := add(k, bucketCnt*uintptr(t.keysize)) // 遍历 bucket 中的所有 cell for i := 0; i \u0026lt; bucketCnt; i, k, v = i+1, add(k, uintptr(t.keysize)), add(v, uintptr(t.valuesize)) { // 当前 cell 的 top hash 值 top := b.tophash[i] // 如果 cell 为空，即没有 key if top == empty { // 那就标志它被\u0026quot;搬迁\u0026quot;过 b.tophash[i] = evacuatedEmpty // 继续下个 cell continue } // 正常不会出现这种情况 // 未被搬迁的 cell 只可能是 empty 或是 // 正常的 top hash（大于 minTopHash） if top \u0026lt; minTopHash { throw(\u0026quot;bad map state\u0026quot;) } k2 := k // 如果 key 是指针，则解引用 if t.indirectkey { k2 = *((*unsafe.Pointer)(k2)) } // 默认使用 X，等量扩容 useX := true // 如果不是等量扩容 if !h.sameSizeGrow() { // 计算 hash 值，和 key 第一次写入时一样 hash := alg.hash(k2, uintptr(h.hash0)) // 如果有协程正在遍历 map if h.flags\u0026amp;iterator != 0 { // 如果出现 相同的 key 值，算出来的 hash 值不同 if !t.reflexivekey \u0026amp;\u0026amp; !alg.equal(k2, k2) { // 只有在 float 变量的 NaN() 情况下会出现 if top\u0026amp;1 != 0 { // 第 B 位置 1 hash |= newbit } else { // 第 B 位置 0 hash \u0026amp;^= newbit } // 取高 8 位作为 top hash 值 top = uint8(hash \u0026gt;\u0026gt; (sys.PtrSize*8 - 8)) if top \u0026lt; minTopHash { top += minTopHash } } } // 取决于新哈希值的 oldB+1 位是 0 还是 1 // 详细看后面的文章 useX = hash\u0026amp;newbit == 0 } // 如果 key 搬到 X 部分 if useX { // 标志老的 cell 的 top hash 值，表示搬移到 X 部分 b.tophash[i] = evacuatedX // 如果 xi 等于 8，说明要溢出了 if xi == bucketCnt { // 新建一个 bucket newx := h.newoverflow(t, x) x = newx // xi 从 0 开始计数 xi = 0 // xk 表示 key 要移动到的位置 xk = add(unsafe.Pointer(x), dataOffset) // xv 表示 value 要移动到的位置 xv = add(xk, bucketCnt*uintptr(t.keysize)) } // 设置 top hash 值 x.tophash[xi] = top // key 是指针 if t.indirectkey { // 将原 key（是指针）复制到新位置 *(*unsafe.Pointer)(xk) = k2 // copy pointer } else { // 将原 key（是值）复制到新位置 typedmemmove(t.key, xk, k) // copy value } // value 是指针，操作同 key if t.indirectvalue { *(*unsafe.Pointer)(xv) = *(*unsafe.Pointer)(v) } else { typedmemmove(t.elem, xv, v) } // 定位到下一个 cell xi++ xk = add(xk, uintptr(t.keysize)) xv = add(xv, uintptr(t.valuesize)) } else { // key 搬到 Y 部分，操作同 X 部分 // …… // 省略了这部分，操作和 X 部分相同 } } } // 如果没有协程在使用老的 buckets，就把老 buckets 清除掉，帮助gc if h.flags\u0026amp;oldIterator == 0 { b = (*bmap)(add(h.oldbuckets, oldbucket*uintptr(t.bucketsize))) // 只清除bucket 的 key,value 部分，保留 top hash 部分，指示搬迁状态 if t.bucket.kind\u0026amp;kindNoPointers == 0 { memclrHasPointers(add(unsafe.Pointer(b), dataOffset), uintptr(t.bucketsize)-dataOffset) } else { memclrNoHeapPointers(add(unsafe.Pointer(b), dataOffset), uintptr(t.bucketsize)-dataOffset) } } } // 更新搬迁进度 // 如果此次搬迁的 bucket 等于当前进度 if oldbucket == h.nevacuate { // 进度加 1 h.nevacuate = oldbucket + 1 // Experiments suggest that 1024 is overkill by at least an order of magnitude. // Put it in there as a safeguard anyway, to ensure O(1) behavior. // 尝试往后看 1024 个 bucket stop := h.nevacuate + 1024 if stop \u0026gt; newbit { stop = newbit } // 寻找没有搬迁的 bucket for h.nevacuate != stop \u0026amp;\u0026amp; bucketEvacuated(t, h, h.nevacuate) { h.nevacuate++ } // 现在 h.nevacuate 之前的 bucket 都被搬迁完毕 // 所有的 buckets 搬迁完毕 if h.nevacuate == newbit { // 清除老的 buckets h.oldbuckets = nil // 清除老的 overflow bucket // 回忆一下：[0] 表示当前 overflow bucket // [1] 表示 old overflow bucket if h.extra != nil { h.extra.overflow[1] = nil } // 清除正在扩容的标志位 h.flags \u0026amp;^= sameSizeGrow } } } evacuate 函数的代码注释非常清晰，对着代码和注释是很容易看懂整个的搬迁过程的，耐心点。\n搬迁的目的就是将老的 buckets 搬迁到新的 buckets。而通过前面的说明我们知道，应对条件 1，新的 buckets 数量是之前的一倍，应对条件 2，新的 buckets 数量和之前相等。\n对于条件 1，从老的 buckets 搬迁到新的 buckets，由于 bucktes 数量不变，因此可以按序号来搬，比如原来在 0 号 bucktes，到新的地方后，仍然放在 0 号 buckets。\n对于条件 2，就没这么简单了。要重新计算 key 的哈希，才能决定它到底落在哪个 bucket。例如，原来 B = 5，计算出 key 的哈希后，只用看它的低 5 位，就能决定它落在哪个 bucket。扩容后，B 变成了 6，因此需要多看一位，它的低 6 位决定 key 落在哪个 bucket。这称为 rehash。\n因此，某个 key 在搬迁前后 bucket 序号可能和原来相等，也可能是相比原来加上 2^B（原来的 B 值），取决于 hash 值 第 6 bit 位是 0 还是 1。\n理解了上面 bucket 序号的变化，我们就可以回答另一个问题了：为什么遍历 map 是无序的？\nmap 在扩容后，会发生 key 的搬迁，原来落在同一个 bucket 中的 key，搬迁后，有些 key 就要远走高飞了（bucket 序号加上了 2^B）。而遍历的过程，就是按顺序遍历 bucket，同时按顺序遍历 bucket 中的 key。搬迁后，key 的位置发生了重大的变化，有些 key 飞上高枝，有些 key 则原地不动。这样，遍历 map 的结果就不可能按原来的顺序了。\n当然，如果我就一个 hard code 的 map，我也不会向 map 进行插入删除的操作，按理说每次遍历这样的 map 都会返回一个固定顺序的 key/value 序列吧。的确是这样，但是 Go 杜绝了这种做法，因为这样会给新手程序员带来误解，以为这是一定会发生的事情，在某些情况下，可能会酿成大错。\n当然，Go 做得更绝，当我们在遍历 map 时，并不是固定地从 0 号 bucket 开始遍历，每次都是从一个随机值序号的 bucket 开始遍历，并且是从这个 bucket 的一个随机序号的 cell 开始遍历。这样，即使你是一个写死的 map，仅仅只是遍历它，也不太可能会返回一个固定序列的 key/value 对了。\n多说一句，“迭代 map 的结果是无序的”这个特性是从 go 1.0 开始加入的。\n再明确一个问题：如果扩容后，B 增加了 1，意味着 buckets 总数是原来的 2 倍，原来 1 号的桶“裂变”到两个桶。\n例如，原始 B = 2，1号 bucket 中有 2 个 key 的哈希值低 3 位分别为：010，110。由于原来 B = 2，所以低 2 位 10 决定它们落在 2 号桶，现在 B 变成 3，所以 010、110 分别落入 2、6 号桶。\n理解了这个，后面讲 map 迭代的时候会用到。\n再来讲搬迁函数中的几个关键点：\nevacuate 函数每次只完成一个 bucket 的搬迁工作，因此要遍历完此 bucket 的所有的 cell，将有值的 cell copy 到新的地方。bucket 还会链接 overflow bucket，它们同样需要搬迁。因此会有 2 层循环，外层遍历 bucket 和 overflow bucket，内层遍历 bucket 的所有 cell。这样的循环在 map 的源码里到处都是，要理解透了。\n源码里提到 X, Y part，其实就是我们说的如果是扩容到原来的 2 倍，桶的数量是原来的 2 倍，前一半桶被称为 X part，后一半桶被称为 Y part。一个 bucket 中的 key 可能会分裂落到 2 个桶，一个位于 X part，一个位于 Y part。所以在搬迁一个 cell 之前，需要知道这个 cell 中的 key 是落到哪个 Part。很简单，重新计算 cell 中 key 的 hash，并向前“多看”一位，决定落入哪个 Part，这个前面也说得很详细了。\n有一个特殊情况是：有一种 key，每次对它计算 hash，得到的结果都不一样。这个 key 就是 math.NaN() 的结果，它的含义是 not a number，类型是 float64。当它作为 map 的 key，在搬迁的时候，会遇到一个问题：再次计算它的哈希值和它当初插入 map 时的计算出来的哈希值不一样！\n你可能想到了，这样带来的一个后果是，这个 key 是永远不会被 Get 操作获取的！当我使用 m[math.NaN()] 语句的时候，是查不出来结果的。这个 key 只有在遍历整个 map 的时候，才有机会现身。所以，可以向一个 map 插入任意数量的 math.NaN() 作为 key。\n当搬迁碰到 math.NaN() 的 key 时，只通过 tophash 的最低位决定分配到 X part 还是 Y part（如果扩容后是原来 buckets 数量的 2 倍）。如果 tophash 的最低位是 0 ，分配到 X part；如果是 1 ，则分配到 Y part。\n这是通过 tophash 值与新算出来的哈希值进行运算得到的：\nif top\u0026amp;1 != 0 { // top hash 最低位为 1 // 新算出来的 hash 值的 B 位置 1 hash |= newbit } else { // 新算出来的 hash 值的 B 位置 0 hash \u0026amp;^= newbit } // hash 值的 B 位为 0，则搬迁到 x part // 当 B = 5时，newbit = 32，二进制低 6 位为 10 0000 useX = hash\u0026amp;newbit == 0 其实这样的 key 我随便搬迁到哪个 bucket 都行，当然，还是要搬迁到上面裂变那张图中的两个 bucket 中去。但这样做是有好处的，在后面讲 map 迭代的时候会再详细解释，暂时知道是这样分配的就行。\n确定了要搬迁到的目标 bucket 后，搬迁操作就比较好进行了。将源 key/value 值 copy 到目的地相应的位置。\n设置 key 在原始 buckets 的 tophash 为 evacuatedX 或是 evacuatedY，表示已经搬迁到了新 map 的 x part 或是 y part。新 map 的 tophash 则正常取 key 哈希值的高 8 位。\n下面通过图来宏观地看一下扩容前后的变化。\n扩容前，B = 2，共有 4 个 buckets，lowbits 表示 hash 值的低位。假设我们不关注其他 buckets 情况，专注在 2 号 bucket。并且假设 overflow 太多，触发了等量扩容（对应于前面的条件 2）。\n扩容完成后，overflow bucket 消失了，key 都集中到了一个 bucket，更为紧凑了，提高了查找的效率。\n假设触发了 2 倍的扩容，那么扩容完成后，老 buckets 中的 key 分裂到了 2 个 新的 bucket。一个在 x part，一个在 y 的 part。依据是 hash 的 lowbits。新 map 中 0-3 称为 x part，4-7 称为 y part。\n注意，上面的两张图忽略了其他 buckets 的搬迁情况，表示所有的 bucket 都搬迁完毕后的情形。实际上，我们知道，搬迁是一个“渐进”的过程，并不会一下子就全部搬迁完毕。所以在搬迁过程中，oldbuckets 指针还会指向原来老的 []bmap，并且已经搬迁完毕的 key 的 tophash 值会是一个状态值，表示 key 的搬迁去向。\nmap 的遍历 本来 map 的遍历过程比较简单：遍历所有的 bucket 以及它后面挂的 overflow bucket，然后挨个遍历 bucket 中的所有 cell。每个 bucket 中包含 8 个 cell，从有 key 的 cell 中取出 key 和 value，这个过程就完成了。\n但是，现实并没有这么简单。还记得前面讲过的扩容过程吗？扩容过程不是一个原子的操作，它每次最多只搬运 2 个 bucket，所以如果触发了扩容操作，那么在很长时间里，map 的状态都是处于一个中间态：有些 bucket 已经搬迁到新家，而有些 bucket 还待在老地方。\n因此，遍历如果发生在扩容的过程中，就会涉及到遍历新老 bucket 的过程，这是难点所在。\n我先写一个简单的代码样例，假装不知道遍历过程具体调用的是什么函数：\npackage main import \u0026quot;fmt\u0026quot; func main() { ageMp := make(map[string]int) ageMp[\u0026quot;qcrao\u0026quot;] = 18 for name, age := range ageMp { fmt.Println(name, age) } } 执行命令：\ngo tool compile -S main.go 得到汇编命令。这里就不逐行讲解了，可以去看之前的几篇文章，说得很详细。\n关键的几行汇编代码如下：\n// ...... 0x0124 00292 (test16.go:9) CALL runtime.mapiterinit(SB) // ...... 0x01fb 00507 (test16.go:9) CALL runtime.mapiternext(SB) 0x0200 00512 (test16.go:9) MOVQ \u0026quot;\u0026quot;..autotmp_4+160(SP), AX 0x0208 00520 (test16.go:9) TESTQ AX, AX 0x020b 00523 (test16.go:9) JNE 302 // ...... 这样，关于 map 迭代，底层的函数调用关系一目了然。先是调用 mapiterinit 函数初始化迭代器，然后循环调用 mapiternext 函数进行 map 迭代。\n迭代器的结构体定义：\ntype hiter struct { // key 指针 key unsafe.Pointer // value 指针 value unsafe.Pointer // map 类型，包含如 key size 大小等 t *maptype // map header h *hmap // 初始化时指向的 bucket buckets unsafe.Pointer // 当前遍历到的 bmap bptr *bmap overflow [2]*[]*bmap // 起始遍历的 bucet 编号 startBucket uintptr // 遍历开始时 cell 的编号（每个 bucket 中有 8 个 cell） offset uint8 // 是否从头遍历了 wrapped bool // B 的大小 B uint8 // 指示当前 cell 序号 i uint8 // 指向当前的 bucket bucket uintptr // 因为扩容，需要检查的 bucket checkBucket uintptr } mapiterinit 就是对 hiter 结构体里的字段进行初始化赋值操作。\n前面已经提到过，即使是对一个写死的 map 进行遍历，每次出来的结果也是无序的。下面我们就可以近距离地观察他们的实现了。\n// 生成随机数 r r := uintptr(fastrand()) if h.B \u0026gt; 31-bucketCntBits { r += uintptr(fastrand()) \u0026lt;\u0026lt; 31 } // 从哪个 bucket 开始遍历 it.startBucket = r \u0026amp; (uintptr(1)\u0026lt;\u0026lt;h.B - 1) // 从 bucket 的哪个 cell 开始遍历 it.offset = uint8(r \u0026gt;\u0026gt; h.B \u0026amp; (bucketCnt - 1)) 例如，B = 2，那 uintptr(1)\u0026lt;\u0026lt;h.B - 1 结果就是 3，低 8 位为 0000 0011，将 r 与之相与，就可以得到一个 0~3 的 bucket 序号；bucketCnt - 1 等于 7，低 8 位为 0000 0111，将 r 右移 2 位后，与 7 相与，就可以得到一个 0~7 号的 cell。\n于是，在 mapiternext 函数中就会从 it.startBucket 的 it.offset 号的 cell 开始遍历，取出其中的 key 和 value，直到又回到起点 bucket，完成遍历过程。\n源码部分比较好看懂，尤其是理解了前面注释的几段代码后，再看这部分代码就没什么压力了。所以，接下来，我将通过图形化的方式讲解整个遍历过程，希望能够清晰易懂。\n假设我们有下图所示的一个 map，起始时 B = 1，有两个 bucket，后来触发了扩容（这里不要深究扩容条件，只是一个设定），B 变成 2。并且， 1 号 bucket 中的内容搬迁到了新的 bucket，1 号裂变成 1 号和 3 号；0 号 bucket 暂未搬迁。老的 bucket 挂在在 *oldbuckets 指针上面，新的 bucket 则挂在 *buckets 指针上面。\n这时，我们对此 map 进行遍历。假设经过初始化后，startBucket = 3，offset = 2。于是，遍历的起点将是 3 号 bucket 的 2 号 cell，下面这张图就是开始遍历时的状态：\n标红的表示起始位置，bucket 遍历顺序为：3 -\u0026gt; 0 -\u0026gt; 1 -\u0026gt; 2。\n因为 3 号 bucket 对应老的 1 号 bucket，因此先检查老 1 号 bucket 是否已经被搬迁过。判断方法就是：\nfunc evacuated(b *bmap) bool { h := b.tophash[0] return h \u0026gt; empty \u0026amp;\u0026amp; h \u0026lt; minTopHash } 如果 b.tophash[0] 的值在标志值范围内，即在 (0,4) 区间里，说明已经被搬迁过了。\nempty = 0 evacuatedEmpty = 1 evacuatedX = 2 evacuatedY = 3 minTopHash = 4 在本例中，老 1 号 bucket 已经被搬迁过了。所以它的 tophash[0] 值在 (0,4) 范围内，因此只用遍历新的 3 号 bucket。\n依次遍历 3 号 bucket 的 cell，这时候会找到第一个非空的 key：元素 e。到这里，mapiternext 函数返回，这时我们的遍历结果仅有一个元素：\n由于返回的 key 不为空，所以会继续调用 mapiternext 函数。\n继续从上次遍历到的地方往后遍历，从新 3 号 overflow bucket 中找到了元素 f 和 元素 g。\n遍历结果集也因此壮大：\n新 3 号 bucket 遍历完之后，回到了新 0 号 bucket。0 号 bucket 对应老的 0 号 bucket，经检查，老 0 号 bucket 并未搬迁，因此对新 0 号 bucket 的遍历就改为遍历老 0 号 bucket。那是不是把老 0 号 bucket 中的所有 key 都取出来呢？\n并没有这么简单，回忆一下，老 0 号 bucket 在搬迁后将裂变成 2 个 bucket：新 0 号、新 2 号。而我们此时正在遍历的只是新 0 号 bucket（注意，遍历都是遍历的 *bucket 指针，也就是所谓的新 buckets）。所以，我们只会取出老 0 号 bucket 中那些在裂变之后，分配到新 0 号 bucket 中的那些 key。\n因此，lowbits == 00 的将进入遍历结果集：\n和之前的流程一样，继续遍历新 1 号 bucket，发现老 1 号 bucket 已经搬迁，只用遍历新 1 号 bucket 中现有的元素就可以了。结果集变成：\n继续遍历新 2 号 bucket，它来自老 0 号 bucket，因此需要在老 0 号 bucket 中那些会裂变到新 2 号 bucket 中的 key，也就是 lowbit == 10 的那些 key。\n这样，遍历结果集变成：\n最后，继续遍历到新 3 号 bucket 时，发现所有的 bucket 都已经遍历完毕，整个迭代过程执行完毕。\n顺便说一下，如果碰到 key 是 math.NaN() 这种的，处理方式类似。核心还是要看它被分裂后具体落入哪个 bucket。只不过只用看它 top hash 的最低位。如果 top hash 的最低位是 0 ，分配到 X part；如果是 1 ，则分配到 Y part。据此决定是否取出 key，放到遍历结果集里。\nmap 遍历的核心在于理解 2 倍扩容时，老 bucket 会分裂到 2 个新 bucket 中去。而遍历操作，会按照新 bucket 的序号顺序进行，碰到老 bucket 未搬迁的情况时，要在老 bucket 中找到将来要搬迁到新 bucket 来的 key。\nmap 的赋值 通过汇编语言可以看到，向 map 中插入或者修改 key，最终调用的是 mapassign 函数。\n实际上插入或修改 key 的语法是一样的，只不过前者操作的 key 在 map 中不存在，而后者操作的 key 存在 map 中。\nmapassign 有一个系列的函数，根据 key 类型的不同，编译器会将其优化为相应的“快速函数”。\nkey 类型 插入 uint32 mapassign_fast32(t *maptype, h *hmap, key uint32) unsafe.Pointer uint64 mapassign_fast64(t *maptype, h *hmap, key uint64) unsafe.Pointer string mapassign_faststr(t *maptype, h *hmap, ky string) unsafe.Pointer 我们只用研究最一般的赋值函数 mapassign。\n整体来看，流程非常得简单：对 key 计算 hash 值，根据 hash 值按照之前的流程，找到要赋值的位置（可能是插入新 key，也可能是更新老 key），对相应位置进行赋值。\n源码大体和之前讲的类似，核心还是一个双层循环，外层遍历 bucket 和它的 overflow bucket，内层遍历整个 bucket 的各个 cell。限于篇幅，这部分代码的注释我也不展示了，有兴趣的可以去看，保证理解了这篇文章内容后，能够看懂。\n我这里会针对这个过程提几点重要的。\n函数首先会检查 map 的标志位 flags。如果 flags 的写标志位此时被置 1 了，说明有其他协程在执行“写”操作，进而导致程序 panic。这也说明了 map 对协程是不安全的。\n通过前文我们知道扩容是渐进式的，如果 map 处在扩容的过程中，那么当 key 定位到了某个 bucket 后，需要确保这个 bucket 对应的老 bucket 完成了迁移过程。即老 bucket 里的 key 都要迁移到新的 bucket 中来（分裂到 2 个新 bucket），才能在新的 bucket 中进行插入或者更新的操作。\n上面说的操作是在函数靠前的位置进行的，只有进行完了这个搬迁操作后，我们才能放心地在新 bucket 里定位 key 要安置的地址，再进行之后的操作。\n现在到了定位 key 应该放置的位置了，所谓找准自己的位置很重要。准备两个指针，一个（inserti）指向 key 的 hash 值在 tophash 数组所处的位置，另一个(insertk)指向 cell 的位置（也就是 key 最终放置的地址），当然，对应 value 的位置就很容易定位出来了。这三者实际上都是关联的，在 tophash 数组中的索引位置决定了 key 在整个 bucket 中的位置（共 8 个 key），而 value 的位置需要“跨过” 8 个 key 的长度。\n在循环的过程中，inserti 和 insertk 分别指向第一个找到的空闲的 cell。如果之后在 map 没有找到 key 的存在，也就是说原来 map 中没有此 key，这意味着插入新 key。那最终 key 的安置地址就是第一次发现的“空位”（tophash 是 empty）。\n如果这个 bucket 的 8 个 key 都已经放置满了，那在跳出循环后，发现 inserti 和 insertk 都是空，这时候需要在 bucket 后面挂上 overflow bucket。当然，也有可能是在 overflow bucket 后面再挂上一个 overflow bucket。这就说明，太多 key hash 到了此 bucket。\n在正式安置 key 之前，还要检查 map 的状态，看它是否需要进行扩容。如果满足扩容的条件，就主动触发一次扩容操作。\n这之后，整个之前的查找定位 key 的过程，还得再重新走一次。因为扩容之后，key 的分布都发生了变化。\n最后，会更新 map 相关的值，如果是插入新 key，map 的元素数量字段 count 值会加 1；在函数之初设置的 hashWriting 写标志出会清零。\n另外，有一个重要的点要说一下。前面说的找到 key 的位置，进行赋值操作，实际上并不准确。我们看 mapassign 函数的原型就知道，函数并没有传入 value 值，所以赋值操作是什么时候执行的呢？\nfunc mapassign(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer 答案还得从汇编语言中寻找。我直接揭晓答案，有兴趣可以私下去研究一下。mapassign 函数返回的指针就是指向的 key 所对应的 value 值位置，有了地址，就很好操作赋值了。\nmap 的删除 写操作底层的执行函数是 mapdelete：\nfunc mapdelete(t *maptype, h *hmap, key unsafe.Pointer) 根据 key 类型的不同，删除操作会被优化成更具体的函数：\nkey 类型 删除 uint32 mapdelete_fast32(t *maptype, h *hmap, key uint32) uint64 mapdelete_fast64(t *maptype, h *hmap, key uint64) string mapdelete_faststr(t *maptype, h *hmap, ky string) 当然，我们只关心 mapdelete 函数。它首先会检查 h.flags 标志，如果发现写标位是 1，直接 panic，因为这表明有其他协程同时在进行写操作。\n计算 key 的哈希，找到落入的 bucket。检查此 map 如果正在扩容的过程中，直接触发一次搬迁操作。\n删除操作同样是两层循环，核心还是找到 key 的具体位置。寻找过程都是类似的，在 bucket 中挨个 cell 寻找。\n找到对应位置后，对 key 或者 value 进行“清零”操作：\n// 对 key 清零 if t.indirectkey { *(*unsafe.Pointer)(k) = nil } else { typedmemclr(t.key, k) } // 对 value 清零 if t.indirectvalue { *(*unsafe.Pointer)(v) = nil } else { typedmemclr(t.elem, v) } 最后，将 count 值减 1，将对应位置的 tophash 值置成 Empty。\n这块源码同样比较简单，感兴起直接去看代码。\nmap 进阶 可以边遍历边删除吗 map 并不是一个线程安全的数据结构。同时读写一个 map 是未定义的行为，如果被检测到，会直接 panic。\n一般而言，这可以通过读写锁来解决：sync.RWMutex。\n读之前调用 RLock() 函数，读完之后调用 RUnlock() 函数解锁；写之前调用 Lock() 函数，写完之后，调用 Unlock() 解锁。\n另外，sync.Map 是线程安全的 map，也可以使用。它的实现原理，这次先不说了。\nkey 可以是 float 型吗？ 从语法上看，是可以的。Go 语言中只要是可比较的类型都可以作为 key。除开 slice，map，functions 这几种类型，其他类型都是 OK 的。具体包括：布尔值、数字、字符串、指针、通道、接口类型、结构体、只包含上述类型的数组。这些类型的共同特征是支持 == 和 != 操作符，k1 == k2 时，可认为 k1 和 k2 是同一个 key。如果是结构体，则需要它们的字段值都相等，才被认为是相同的 key。\n顺便说一句，任何类型都可以作为 value，包括 map 类型。\n来看个例子：\nfunc main() { m := make(map[float64]int) m[1.4] = 1 m[2.4] = 2 m[math.NaN()] = 3 m[math.NaN()] = 3 for k, v := range m { fmt.Printf(\u0026quot;[%v, %d] \u0026quot;, k, v) } fmt.Printf(\u0026quot;\\nk: %v, v: %d\\n\u0026quot;, math.NaN(), m[math.NaN()]) fmt.Printf(\u0026quot;k: %v, v: %d\\n\u0026quot;, 2.400000000001, m[2.400000000001]) fmt.Printf(\u0026quot;k: %v, v: %d\\n\u0026quot;, 2.4000000000000000000000001, m[2.4000000000000000000000001]) fmt.Println(math.NaN() == math.NaN()) } 程序的输出：\n[2.4, 2] [NaN, 3] [NaN, 3] [1.4, 1] k: NaN, v: 0 k: 2.400000000001, v: 0 k: 2.4, v: 2 false 例子中定义了一个 key 类型是 float 型的 map，并向其中插入了 4 个 key：1.4， 2.4， NAN，NAN。\n打印的时候也打印出了 4 个 key，如果你知道 NAN != NAN，也就不奇怪了。因为他们比较的结果不相等，自然，在 map 看来就是两个不同的 key 了。\n接着，我们查询了几个 key，发现 NAN 不存在，2.400000000001 也不存在，而 2.4000000000000000000000001 却存在。\n有点诡异，不是吗？\n接着，我通过汇编发现了如下的事实：\n当用 float64 作为 key 的时候，先要将其转成 unit64 类型，再插入 key 中。\n具体是通过 Float64frombits 函数完成：\n// Float64frombits returns the floating point number corresponding // the IEEE 754 binary representation b. func Float64frombits(b uint64) float64 { return *(*float64)(unsafe.Pointer(\u0026amp;b)) } 也就是将浮点数表示成 IEEE 754 规定的格式。如赋值语句：\n0x00bd 00189 (test18.go:9) LEAQ \u0026quot;\u0026quot;.statictmp_0(SB), DX 0x00c4 00196 (test18.go:9) MOVQ DX, 16(SP) 0x00c9 00201 (test18.go:9) PCDATA $0, $2 0x00c9 00201 (test18.go:9) CALL runtime.mapassign(SB) \u0026quot;\u0026quot;.statictmp_0(SB) 变量是这样的：\n\u0026quot;\u0026quot;.statictmp_0 SRODATA size=8 0x0000 33 33 33 33 33 33 03 40 \u0026quot;\u0026quot;.statictmp_1 SRODATA size=8 0x0000 ff 3b 33 33 33 33 03 40 \u0026quot;\u0026quot;.statictmp_2 SRODATA size=8 0x0000 33 33 33 33 33 33 03 40 我们再来输出点东西：\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;math\u0026quot; ) func main() { m := make(map[float64]int) m[2.4] = 2 fmt.Println(math.Float64bits(2.4)) fmt.Println(math.Float64bits(2.400000000001)) fmt.Println(math.Float64bits(2.4000000000000000000000001)) } 4612586738352864255 4612586738352862003 4612586738352862003 转成十六进制为：\n0x4003333333333333 0x4003333333333BFF 0x4003333333333333 和前面的 \u0026quot;\u0026quot;.statictmp_0 比较一下，很清晰了吧。2.4 和 2.4000000000000000000000001 经过 math.Float64bits() 函数转换后的结果是一样的。自然，二者在 map 看来，就是同一个 key 了。\n再来看一下 NAN（not a number）：\n// NaN returns an IEEE 754 ``not-a-number'' value. func NaN() float64 { return Float64frombits(uvnan) } uvan 的定义为：\nuvnan = 0x7FF8000000000001 NAN() 直接调用 Float64frombits，传入写死的 const 型变量 0x7FF8000000000001，得到 NAN 型值。既然，NAN 是从一个常量解析得来的，为什么插入 map 时，会被认为是不同的 key？\n这是由类型的哈希函数决定的，例如，对于 64 位的浮点数，它的哈希函数如下：\nfunc f64hash(p unsafe.Pointer, h uintptr) uintptr { f := *(*float64)(p) switch { case f == 0: return c1 * (c0 ^ h) // +0, -0 case f != f: return c1 * (c0 ^ h ^ uintptr(fastrand())) // any kind of NaN default: return memhash(p, h, 8) } } 第二个 case，f != f 就是针对 NAN，这里会再加一个随机数。\n这样，所有的谜题都解开了。\n由于 NAN 的特性：\nNAN != NAN hash(NAN) != hash(NAN) 因此向 map 中查找的 key 为 NAN 时，什么也查不到；如果向其中增加了 4 次 NAN，遍历会得到 4 个 NAN。\n最后说结论：float 型可以作为 key，但是由于精度的问题，会导致一些诡异的问题，慎用之。\n文章转自 https://juejin.cn/post/6844903848587296781 ","date":"2021-05-22","permalink":"https://daemon365.dev/2021/05/22/golang-map%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","tags":["go","源码分析"],"title":"golang map实现原理"},{"content":"分布式id生成器 有时我们需要能够生成类似MySQL自增ID这样不断增大，同时又不会重复的id。以支持业务中的高并发场景。比较典型的，电商促销时，短时间内会有大量的订单涌入到系统，比如每秒10w+。明星出轨时，会有大量热情的粉丝发微博以表心意，同样会在短时间内产生大量的消息。\n在插入数据库之前，我们需要给这些消息、订单先打上一个ID，然后再插入到我们的数据库。对这个id的要求是希望其中能带有一些时间信息，这样即使我们后端的系统对消息进行了分库分表，也能够以时间顺序对这些消息进行排序。\nTwitter的snowflake算法是这种场景下的一个典型解法。先来看看snowflake是怎么一回事：\nsnowflake中的比特位分布\n首先确定我们的数值是64位，int64类型，被划分为四部分，不含开头的第一个bit，因为这个bit是符号位。用41位来表示收到请求时的时间戳，单位为毫秒，然后五位来表示数据中心的id，然后再五位来表示机器的实例id，最后是12位的循环自增id（到达1111,1111,1111后会归0）。\n这样的机制可以支持我们在同一台机器上，同一毫秒内产生2 ^ 12 = 4096条消息。一秒共409.6万条消息。从值域上来讲完全够用了。\n数据中心加上实例id共有10位，可以支持我们每数据中心部署32台机器，所有数据中心共1024台实例。\n表示timestamp的41位，可以支持我们使用69年。当然，我们的时间毫秒计数不会真的从1970年开始记，那样我们的系统跑到2039/9/7 23:47:35就不能用了，所以这里的timestamp实际上只是相对于某个时间的增量，比如我们的系统上线是2018-08-01，那么我们可以把这个timestamp当作是从2018-08-01 00:00:00.000的偏移量。\nworker_id分配 timestamp，datacenter_id，worker_id和sequence_id这四个字段中，timestamp和sequence_id是由程序在运行期生成的。但datacenter_id和worker_id需要我们在部署阶段就能够获取得到，并且一旦程序启动之后，就是不可更改的了（想想，如果可以随意更改，可能被不慎修改，造成最终生成的id有冲突）。\n一般不同数据中心的机器，会提供对应的获取数据中心id的API，所以datacenter_id我们可以在部署阶段轻松地获取到。而worker_id是我们逻辑上给机器分配的一个id，这个要怎么办呢？比较简单的想法是由能够提供这种自增id功能的工具来支持，比如MySQL:\nmysql\u0026gt; insert into a (ip) values(\u0026quot;10.1.2.101\u0026quot;); Query OK, 1 row affected (0.00 sec) mysql\u0026gt; select last_insert_id(); +------------------+ | last_insert_id() | +------------------+ | 2 | +------------------+ 1 row in set (0.00 sec) 从MySQL中获取到worker_id之后，就把这个worker_id直接持久化到本地，以避免每次上线时都需要获取新的worker_id。让单实例的worker_id可以始终保持不变。\n当然，使用MySQL相当于给我们简单的id生成服务增加了一个外部依赖。依赖越多，我们的服务的可运维性就越差。\n考虑到集群中即使有单个id生成服务的实例挂了，也就是损失一段时间的一部分id，所以我们也可以更简单暴力一些，把worker_id直接写在worker的配置中，上线时，由部署脚本完成worker_id字段替换。\n开源实例 标准snowflake实现 github.com/bwmarrin/snowflake 是一个相当轻量化的snowflake的Go实现。其文档对各位使用的定义见图 6-2所示。\n图 6-2 snowflake库\n和标准的snowflake完全一致。使用上比较简单：\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;os\u0026quot; \u0026quot;github.com/bwmarrin/snowflake\u0026quot; ) func main() { n, err := snowflake.NewNode(1) if err != nil { println(err) os.Exit(1) } for i := 0; i \u0026lt; 3; i++ { id := n.Generate() fmt.Println(\u0026quot;id\u0026quot;, id) fmt.Println( \u0026quot;node: \u0026quot;, id.Node(), \u0026quot;step: \u0026quot;, id.Step(), \u0026quot;time: \u0026quot;, id.Time(), \u0026quot;\\n\u0026quot;, ) } } 当然，这个库也给我们留好了定制的后路，其中预留了一些可定制字段：\n// Epoch is set to the twitter snowflake epoch of Nov 04 2010 01:42:54 UTC // You may customize this to set a different epoch for your application. Epoch int64 = 1288834974657 // Number of bits to use for Node // Remember, you have a total 22 bits to share between Node/Step NodeBits uint8 = 10 // Number of bits to use for Step // Remember, you have a total 22 bits to share between Node/Step StepBits uint8 = 12 Epoch就是本节开头讲的起始时间，NodeBits指的是机器编号的位长，StepBits指的是自增序列的位长。\nsonyflake sonyflake是Sony公司的一个开源项目，基本思路和snowflake差不多，不过位分配上稍有不同，见图 6-3：\n图 6-3 sonyflake\n这里的时间只用了39个bit，但时间的单位变成了10ms，所以理论上比41位表示的时间还要久(174年)。\nSequence ID和之前的定义一致，Machine ID其实就是节点id。sonyflake与众不同的地方在于其在启动阶段的配置参数：\nfunc NewSonyflake(st Settings) *Sonyflake Settings数据结构如下：\ntype Settings struct { StartTime time.Time MachineID func() (uint16, error) CheckMachineID func(uint16) bool } StartTime选项和我们之前的Epoch差不多，如果不设置的话，默认是从2014-09-01 00:00:00 +0000 UTC开始。\nMachineID可以由用户自定义的函数，如果用户不定义的话，会默认将本机IP的低16位作为machine id。\nCheckMachineID是由用户提供的检查MachineID是否冲突的函数。这里的设计还是比较巧妙的，如果有另外的中心化存储并支持检查重复的存储，那我们就可以按照自己的想法随意定制这个检查MachineID是否冲突的逻辑。如果公司有现成的Redis集群，那么我们可以很轻松地用Redis的集合类型来检查冲突。\nredis 127.0.0.1:6379\u0026gt; SADD base64_encoding_of_last16bits MzI0Mgo= (integer) 1 redis 127.0.0.1:6379\u0026gt; SADD base64_encoding_of_last16bits MzI0Mgo= (integer) 0 使用起来也比较简单，有一些逻辑简单的函数就略去实现了：\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;os\u0026quot; \u0026quot;time\u0026quot; \u0026quot;github.com/sony/sonyflake\u0026quot; ) func getMachineID() (uint16, error) { var machineID uint16 var err error machineID = readMachineIDFromLocalFile() if machineID == 0 { machineID, err = generateMachineID() if err != nil { return 0, err } } return machineID, nil } func checkMachineID(machineID uint16) bool { saddResult, err := saddMachineIDToRedisSet() if err != nil || saddResult == 0 { return true } err := saveMachineIDToLocalFile(machineID) if err != nil { return true } return false } func main() { t, _ := time.Parse(\u0026quot;2006-01-02\u0026quot;, \u0026quot;2018-01-01\u0026quot;) settings := sonyflake.Settings{ StartTime: t, MachineID: getMachineID, CheckMachineID: checkMachineID, } sf := sonyflake.NewSonyflake(settings) id, err := sf.NextID() if err != nil { fmt.Println(err) os.Exit(1) } fmt.Println(id) } 分布式锁 在单机程序并发或并行修改全局变量时，需要对修改行为加锁以创造临界区。为什么需要加锁呢？我们看看在不加锁的情况下并发计数会发生什么情况：\npackage main import ( \u0026quot;sync\u0026quot; ) // 全局变量 var counter int func main() { var wg sync.WaitGroup for i := 0; i \u0026lt; 1000; i++ { wg.Add(1) go func() { defer wg.Done() counter++ }() } wg.Wait() println(counter) } 多次运行会得到不同的结果：\n❯❯❯ go run local_lock.go 945 ❯❯❯ go run local_lock.go 937 ❯❯❯ go run local_lock.go 959 基于Redis的setnx 在分布式场景下，我们也需要这种“抢占”的逻辑，这时候怎么办呢？我们可以使用Redis提供的setnx命令：\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;time\u0026quot; \u0026quot;github.com/go-redis/redis\u0026quot; \u0026quot;github.com/gofrs/uuid\u0026quot; ) // 声明一个全局的rdb变量 var rdb *redis.Client // 初始化连接 func initClient() (err error) { rdb = redis.NewClient(\u0026amp;redis.Options{ Addr: \u0026quot;localhost:6379\u0026quot;, Password: \u0026quot;zhy1996\u0026quot;, // no password set DB: 0, // use default DB }) _, err = rdb.Ping().Result() if err != nil { return err } return nil } var unlock_lua = ` if redis.call(\u0026quot;get\u0026quot;,KEYS[1]) == ARGV[1] then return redis.call(\u0026quot;del\u0026quot;,KEYS[1]) else return 0 end ` func Lock(key, value string, expiration time.Duration) (bool, error) { is, err := rdb.SetNX(key, value, expiration).Result() if err != nil { return false, fmt.Errorf(\u0026quot;redis setnx failed\u0026quot;) } return is, nil } func UnLock(key, value string) (bool, error) { res, err := rdb.Eval(unlock_lua, []string{key}, value).Result() if err != nil { return false, err } v, ok := res.(int64) if !ok { return false, fmt.Errorf(\u0026quot;lua script return is not int\u0026quot;) } if v == 0 { return false, nil } return true, nil } func main() { err := initClient() if err != nil { fmt.Println(err) } ul, _ := uuid.NewV4() value := ul.String() for i := 0; i \u0026lt; 10; i++ { is, err := Lock(\u0026quot;lock_1\u0026quot;, value, time.Second) if err != nil { fmt.Println(err) return } fmt.Println(\u0026quot;是否拿到锁:\u0026quot;, is) } res, err := UnLock(\u0026quot;lock_1\u0026quot;, value) if err != nil { fmt.Println(err) return } fmt.Println(\u0026quot;解锁:\u0026quot;, res) } 看看运行结果：\n是否拿到锁: true 是否拿到锁: false 是否拿到锁: false 是否拿到锁: false 是否拿到锁: false 是否拿到锁: false 是否拿到锁: false 是否拿到锁: false 是否拿到锁: false 是否拿到锁: false 解锁: true 通过代码和执行结果可以看到，我们远程调用setnx实际上和单机的trylock非常相似，如果获取锁失败，那么相关的任务逻辑就不应该继续向前执行。\nsetnx很适合在高并发场景下，用来争抢一些“唯一”的资源。比如交易撮合系统中卖家发起订单，而多个买家会对其进行并发争抢。这种场景我们没有办法依赖具体的时间来判断先后，因为不管是用户设备的时间，还是分布式场景下的各台机器的时间，都是没有办法在合并后保证正确的时序的。哪怕是我们同一个机房的集群，不同的机器的系统时间可能也会有细微的差别。\n所以，我们需要依赖于这些请求到达Redis节点的顺序来做正确的抢锁操作。如果用户的网络环境比较差，那也只能自求多福了。\n基于etcd etcd是分布式系统中，功能上与ZooKeeper类似的组件，这两年越来越火了。上面基于ZooKeeper我们实现了分布式阻塞锁，基于etcd，也可以实现类似的功能：\npackage main import ( \u0026quot;log\u0026quot; \u0026quot;github.com/zieckey/etcdsync\u0026quot; ) func main() { m, err := etcdsync.New(\u0026quot;/lock\u0026quot;, 10, []string{\u0026quot;http://127.0.0.1:2379\u0026quot;}) if m == nil || err != nil { log.Printf(\u0026quot;etcdsync.New failed\u0026quot;) return } err = m.Lock() if err != nil { log.Printf(\u0026quot;etcdsync.Lock failed\u0026quot;) return } log.Printf(\u0026quot;etcdsync.Lock OK\u0026quot;) log.Printf(\u0026quot;Get the lock. Do something here.\u0026quot;) err = m.Unlock() if err != nil { log.Printf(\u0026quot;etcdsync.Unlock failed\u0026quot;) } else { log.Printf(\u0026quot;etcdsync.Unlock OK\u0026quot;) } } etcd中没有像ZooKeeper那样的Sequence节点。所以其锁实现和基于ZooKeeper实现的有所不同。在上述示例代码中使用的etcdsync的Lock流程是：\n先检查/lock路径下是否有值，如果有值，说明锁已经被别人抢了 如果没有值，那么写入自己的值。写入成功返回，说明加锁成功。写入时如果节点被其它节点写入过了，那么会导致加锁失败，这时候到 3 watch /lock下的事件，此时陷入阻塞 当/lock路径下发生事件时，当前进程被唤醒。检查发生的事件是否是删除事件（说明锁被持有者主动unlock），或者过期事件（说明锁过期失效）。如果是的话，那么回到 1，走抢锁流程。 值得一提的是，在etcdv3的API中官方已经提供了可以直接使用的锁API，读者可以查阅etcd的文档做进一步的学习。\n参考文章：《go语言高级编程》\n","date":"2021-04-30","permalink":"https://daemon365.dev/2021/04/30/%E5%88%86%E5%B8%83%E5%BC%8Fid%E7%94%9F%E6%88%90%E5%99%A8%E5%8F%8Aredisetcd%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/","tags":["redis","etcd"],"title":"分布式ID生成器及redis，etcd分布式锁"},{"content":"摘要 日志在整个工程实践中的重要性不言而喻，在选择日志组件的时候也有多方面的考量。详细、正确和及时的反馈是必不可少的，但是整个性能表现是否也是必要考虑的点呢？在长期的实践中发现有的日志组件对于计算资源的消耗十分巨大，这将导致整个服务成本的居高不下。此文从设计原理深度分析了 zap 的设计与实现上的权衡，也希望整个的选择、考量的过程能给其他的技术团队在开发高性能的 Go 组件时带来一定的借鉴意义。\n前言 日志作为整个代码行为的记录，是程序执行逻辑和异常最直接的反馈。对于整个系统来说，日志是至关重要的组成部分。通过分析日志我们不仅可以发现系统的问题，同时日志中也蕴含了大量有价值可以被挖掘的信息，因此合理地记录日志是十分必要的。\n我们的业务通常会记录大量的 Debug 日志，但在实际测试过程中，发现我们使用的日志库 seelog 性能存在严重的瓶颈，在我们的对比结果中发现：zap 表现非常突出，单线程 Qps 也是 logrus、seelog 的数倍。\n在分析源码后 zap 设计与实现上的考量让我感到受益颇多，在这里我们主要分享一下以下几个方面：\nzap 为何有这么高的性能 对于我们自己的开发有什么值得借鉴的地方 如何正确的使用 Go 开发高性能的组件 为什么选择使用ZAP 它同时提供了结构化日志记录和printf风格的日志记录 它非常的快 根据Uber-go Zap的文档，它的性能比类似的结构化日志包更好——也比标准库更快。 以下是Zap发布的基准测试信息\n记录一条消息和10个字段:\nPackage Time Time % to zap Objects Allocated ⚡️ zap 862 ns/op +0% 5 allocs/op ⚡️ zap (sugared) 1250 ns/op +45% 11 allocs/op zerolog 4021 ns/op +366% 76 allocs/op go-kit 4542 ns/op +427% 105 allocs/op apex/log 26785 ns/op +3007% 115 allocs/op logrus 29501 ns/op +3322% 125 allocs/op log15 29906 ns/op +3369% 122 allocs/op 记录一个静态字符串，没有任何上下文或printf风格的模板：\nPackage Time Time % to zap Objects Allocated ⚡️ zap 118 ns/op +0% 0 allocs/op ⚡️ zap (sugared) 191 ns/op +62% 2 allocs/op zerolog 93 ns/op -21% 0 allocs/op go-kit 280 ns/op +137% 11 allocs/op standard library 499 ns/op +323% 2 allocs/op apex/log 1990 ns/op +1586% 10 allocs/op logrus 3129 ns/op +2552% 24 allocs/op log15 3887 ns/op +3194% 23 allocs/op 安装 go get -u go.uber.org/zap 示例 简单示例 格式化输出 package main import ( \u0026quot;go.uber.org/zap\u0026quot; \u0026quot;time\u0026quot; ) func main() { // zap.NewDevelopment 格式化输出 logger, _ := zap.ewDevelopment() defer logger.Sync() logger.Info(\u0026quot;无法获取网址\u0026quot;, zap.String(\u0026quot;url\u0026quot;, \u0026quot;http://www.baidu.com\u0026quot;), zap.Int(\u0026quot;attempt\u0026quot;, 3), zap.Duration(\u0026quot;backoff\u0026quot;, time.Second), ) } 格式化输出打印结果：\n2019-01-02T15:01:13.923+0800 INFO spikeProxy/main.go:17 failed to fetch URL {\u0026quot;url\u0026quot;: \u0026quot;http://www.baidu.com\u0026quot;, \u0026quot;attempt\u0026quot;: 3, \u0026quot;backoff\u0026quot;: \u0026quot;1s\u0026quot;} json 序列化输出 package main import ( \u0026quot;go.uber.org/zap\u0026quot; \u0026quot;time\u0026quot; ) func main() { // zap.NewProduction json序列化输出 logger, _ := zap.NewProduction() defer logger.Sync() logger.Info(\u0026quot;无法获取网址\u0026quot;, zap.String(\u0026quot;url\u0026quot;, \u0026quot;http://www.baidu.com\u0026quot;), zap.Int(\u0026quot;attempt\u0026quot;, 3), zap.Duration(\u0026quot;backoff\u0026quot;, time.Second), ) } json序列化输出打印结果：\n{\u0026quot;level\u0026quot;:\u0026quot;info\u0026quot;,\u0026quot;ts\u0026quot;:1546413239.1466308,\u0026quot;caller\u0026quot;:\u0026quot;spikeProxy/main.go:16\u0026quot;,\u0026quot;msg\u0026quot;:\u0026quot;无法获取网址\u0026quot;,\u0026quot;url\u0026quot;:\u0026quot;http://www.baidu.com\u0026quot;,\u0026quot;attempt\u0026quot;:3,\u0026quot;backoff\u0026quot;:1} 自定义示例 选择一个日志库除了高性能是考量的一个标准，高扩展也非常重要，例如：json key 自定义、时间格式化、日志级别等。\npackage main import ( \u0026quot;go.uber.org/zap\u0026quot; \u0026quot;go.uber.org/zap/zapcore\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;time\u0026quot; ) func main() { encoderConfig := zapcore.EncoderConfig{ TimeKey: \u0026quot;time\u0026quot;, LevelKey: \u0026quot;level\u0026quot;, NameKey: \u0026quot;logger\u0026quot;, CallerKey: \u0026quot;caller\u0026quot;, MessageKey: \u0026quot;msg\u0026quot;, StacktraceKey: \u0026quot;stacktrace\u0026quot;, LineEnding: zapcore.DefaultLineEnding, EncodeLevel: zapcore.LowercaseLevelEncoder, // 小写编码器 EncodeTime: zapcore.ISO8601TimeEncoder, // ISO8601 UTC 时间格式 EncodeDuration: zapcore.SecondsDurationEncoder, EncodeCaller: zapcore.FullCallerEncoder, // 全路径编码器 } // 设置日志级别 atom := zap.NewAtomicLevelAt(zap.DebugLevel) config := zap.Config{ Level: atom, // 日志级别 Development: true, // 开发模式，堆栈跟踪 Encoding: \u0026quot;json\u0026quot;, // 输出格式 console 或 json EncoderConfig: encoderConfig, // 编码器配置 InitialFields: map[string]interface{}{\u0026quot;serviceName\u0026quot;: \u0026quot;spikeProxy\u0026quot;}, // 初始化字段，如：添加一个服务器名称 OutputPaths: []string{\u0026quot;stdout\u0026quot;, \u0026quot;./logs/spikeProxy.log\u0026quot;}, // 输出到指定文件 stdout（标准输出，正常颜色） stderr（错误输出，红色） ErrorOutputPaths: []string{\u0026quot;stderr\u0026quot;}, } // 构建日志 logger, err := config.Build() if err != nil { panic(fmt.Sprintf(\u0026quot;log 初始化失败: %v\u0026quot;, err)) } logger.Info(\u0026quot;log 初始化成功\u0026quot;) logger.Info(\u0026quot;无法获取网址\u0026quot;, zap.String(\u0026quot;url\u0026quot;, \u0026quot;http://www.baidu.com\u0026quot;), zap.Int(\u0026quot;attempt\u0026quot;, 3), zap.Duration(\u0026quot;backoff\u0026quot;, time.Second), ) } 打印结果：\n{\u0026quot;level\u0026quot;:\u0026quot;info\u0026quot;,\u0026quot;time\u0026quot;:\u0026quot;2019-01-02T15:38:33.778+0800\u0026quot;,\u0026quot;caller\u0026quot;:\u0026quot;/Users/lcl/go/src/spikeProxy/main.go:54\u0026quot;,\u0026quot;msg\u0026quot;:\u0026quot;log 初始化成功\u0026quot;,\u0026quot;serviceName\u0026quot;:\u0026quot;spikeProxy\u0026quot;} {\u0026quot;level\u0026quot;:\u0026quot;info\u0026quot;,\u0026quot;time\u0026quot;:\u0026quot;2019-01-02T15:38:33.778+0800\u0026quot;,\u0026quot;caller\u0026quot;:\u0026quot;/Users/lcl/go/src/spikeProxy/main.go:56\u0026quot;,\u0026quot;msg\u0026quot;:\u0026quot;无法获取网址\u0026quot;,\u0026quot;serviceName\u0026quot;:\u0026quot;spikeProxy\u0026quot;,\u0026quot;url\u0026quot;:\u0026quot;http://www.baidu.com\u0026quot;,\u0026quot;attempt\u0026quot;:3,\u0026quot;backoff\u0026quot;:1} 写入归档文件示例 安装 lumberjack go get gopkg.in/natefinch/lumberjack.v2 lumberjack介绍 Lumberjack是一个Go包，用于将日志写入滚动文件。 zap 不支持文件归档，如果要支持文件按大小或者时间归档，需要使用lumberjack，lumberjack也是zap官方推荐的。\n示例 package main import ( \u0026quot;go.uber.org/zap\u0026quot; \u0026quot;go.uber.org/zap/zapcore\u0026quot; \u0026quot;time\u0026quot; \u0026quot;gopkg.in/natefinch/lumberjack.v2\u0026quot; \u0026quot;os\u0026quot; ) func main() { hook := lumberjack.Logger{ Filename: \u0026quot;./logs/spikeProxy1.log\u0026quot;, // 日志文件路径 MaxSize: 128, // 每个日志文件保存的最大尺寸 单位：M MaxBackups: 30, // 日志文件最多保存多少个备份 MaxAge: 7, // 文件最多保存多少天 Compress: true, // 是否压缩 } encoderConfig := zapcore.EncoderConfig{ TimeKey: \u0026quot;time\u0026quot;, LevelKey: \u0026quot;level\u0026quot;, NameKey: \u0026quot;logger\u0026quot;, CallerKey: \u0026quot;linenum\u0026quot;, MessageKey: \u0026quot;msg\u0026quot;, StacktraceKey: \u0026quot;stacktrace\u0026quot;, LineEnding: zapcore.DefaultLineEnding, EncodeLevel: zapcore.LowercaseLevelEncoder, // 小写编码器 EncodeTime: zapcore.ISO8601TimeEncoder, // ISO8601 UTC 时间格式 EncodeDuration: zapcore.SecondsDurationEncoder, // EncodeCaller: zapcore.FullCallerEncoder, // 全路径编码器 EncodeName: zapcore.FullNameEncoder, } // 设置日志级别 atomicLevel := zap.NewAtomicLevel() atomicLevel.SetLevel(zap.InfoLevel) core := zapcore.NewCore( zapcore.NewJSONEncoder(encoderConfig), // 编码器配置 zapcore.NewMultiWriteSyncer(zapcore.AddSync(os.Stdout), zapcore.AddSync(\u0026amp;hook)), // 打印到控制台和文件 atomicLevel, // 日志级别 ) // 开启开发模式，堆栈跟踪 caller := zap.AddCaller() // 开启文件及行号 development := zap.Development() // 设置初始化字段 filed := zap.Fields(zap.String(\u0026quot;serviceName\u0026quot;, \u0026quot;serviceName\u0026quot;)) // 构造日志 logger := zap.New(core, caller, development, filed) logger.Info(\u0026quot;log 初始化成功\u0026quot;) logger.Info(\u0026quot;无法获取网址\u0026quot;, zap.String(\u0026quot;url\u0026quot;, \u0026quot;http://www.baidu.com\u0026quot;), zap.Int(\u0026quot;attempt\u0026quot;, 3), zap.Duration(\u0026quot;backoff\u0026quot;, time.Second)) } 控制台打印结果：\n{\u0026quot;level\u0026quot;:\u0026quot;info\u0026quot;,\u0026quot;time\u0026quot;:\u0026quot;2019-01-02T16:14:43.608+0800\u0026quot;,\u0026quot;linenum\u0026quot;:\u0026quot;/Users/lcl/go/src/spikeProxy/main.go:56\u0026quot;,\u0026quot;msg\u0026quot;:\u0026quot;log 初始化成功\u0026quot;,\u0026quot;serviceName\u0026quot;:\u0026quot;serviceName\u0026quot;} {\u0026quot;level\u0026quot;:\u0026quot;info\u0026quot;,\u0026quot;time\u0026quot;:\u0026quot;2019-01-02T16:14:43.608+0800\u0026quot;,\u0026quot;linenum\u0026quot;:\u0026quot;/Users/lcl/go/src/spikeProxy/main.go:57\u0026quot;,\u0026quot;msg\u0026quot;:\u0026quot;无法获取网址\u0026quot;,\u0026quot;serviceName\u0026quot;:\u0026quot;serviceName\u0026quot;,\u0026quot;url\u0026quot;:\u0026quot;http://www.baidu.com\u0026quot;,\u0026quot;attempt\u0026quot;:3,\u0026quot;backoff\u0026quot;:1} 文件打印结果：\n{\u0026quot;level\u0026quot;:\u0026quot;info\u0026quot;,\u0026quot;time\u0026quot;:\u0026quot;2019-01-02T16:14:43.608+0800\u0026quot;,\u0026quot;linenum\u0026quot;:\u0026quot;/Users/lcl/go/src/spikeProxy/main.go:56\u0026quot;,\u0026quot;msg\u0026quot;:\u0026quot;log 初始化成功\u0026quot;,\u0026quot;serviceName\u0026quot;:\u0026quot;serviceName\u0026quot;} {\u0026quot;level\u0026quot;:\u0026quot;info\u0026quot;,\u0026quot;time\u0026quot;:\u0026quot;2019-01-02T16:14:43.608+0800\u0026quot;,\u0026quot;linenum\u0026quot;:\u0026quot;/Users/lcl/go/src/spikeProxy/main.go:57\u0026quot;,\u0026quot;msg\u0026quot;:\u0026quot;无法获取网址\u0026quot;,\u0026quot;serviceName\u0026quot;:\u0026quot;serviceName\u0026quot;,\u0026quot;url\u0026quot;:\u0026quot;http://www.baidu.com\u0026quot;,\u0026quot;attempt\u0026quot;:3,\u0026quot;backoff\u0026quot;:1} gin框架使用zap+lumberjack 初始化zap,lumberjack // viperconfig type Log struct { FileName string `yaml:\u0026quot;filename\u0026quot;` MaxSize int `yaml:\u0026quot;maxsize\u0026quot;` MaxBackups int `yaml:\u0026quot;maxbackups\u0026quot;` MaxAges int `yaml:\u0026quot;maxages\u0026quot;` Compress bool `yaml:\u0026quot;compress\u0026quot;` Level string `yaml:\u0026quot;-\u0026quot;` } // init var ZapLog *zap.Logger // InitLogger 初始化Logger func InitLogger(cfg *viperConfig.Log) (err error) { writeSyncer := getLogWriter(cfg.FileName, cfg.MaxSize, cfg.MaxBackups, cfg.MaxAges) encoder := getEncoder() var l = new(zapcore.Level) err = l.UnmarshalText([]byte(cfg.Level)) if err != nil { return } core := zapcore.NewCore(encoder, writeSyncer, l) ZapLog = zap.New(core, zap.AddCaller()) zap.ReplaceGlobals(ZapLog) // 替换zap包中全局的logger实例，后续在其他包中只需使用zap.L()调用即可 return } func getEncoder() zapcore.Encoder { encoderConfig := zap.NewProductionEncoderConfig() encoderConfig.EncodeTime = zapcore.ISO8601TimeEncoder encoderConfig.TimeKey = \u0026quot;time\u0026quot; encoderConfig.EncodeLevel = zapcore.CapitalLevelEncoder encoderConfig.EncodeDuration = zapcore.SecondsDurationEncoder encoderConfig.EncodeCaller = zapcore.ShortCallerEncoder return zapcore.NewJSONEncoder(encoderConfig) } func getLogWriter(filename string, maxSize, maxBackup, maxAge int) zapcore.WriteSyncer { lumberJackLogger := \u0026amp;lumberjack.Logger{ Filename: filename, MaxSize: maxSize, MaxBackups: maxBackup, MaxAge: maxAge, } return zapcore.AddSync(lumberJackLogger) } gin日志中间件 模仿Logger()和Recovery()的实现，使用我们的日志库来接收gin框架默认输出的日志。\n// GinLogger 接收gin框架默认的日志 func GinLogger(logger *zap.Logger) gin.HandlerFunc { return func(c *gin.Context) { start := time.Now() path := c.Request.URL.Path query := c.Request.URL.RawQuery c.Next() cost := time.S***art) logger.Info(path, zap.Int(\u0026quot;status\u0026quot;, c.Writer.Status()), zap.String(\u0026quot;method\u0026quot;, c.Request.Method), zap.String(\u0026quot;path\u0026quot;, path), zap.String(\u0026quot;query\u0026quot;, query), zap.String(\u0026quot;ip\u0026quot;, c.ClientIP()), zap.String(\u0026quot;user-agent\u0026quot;, c.Request.UserAgent()), zap.String(\u0026quot;errors\u0026quot;, c.Errors.ByType(gin.ErrorTypePrivate).String()), zap.Duration(\u0026quot;cost\u0026quot;, cost), ) } } // GinRecovery recover掉项目可能出现的panic func GinRecovery(logger *zap.Logger, stack bool) gin.HandlerFunc { return func(c *gin.Context) { defer func() { if err := recover(); err != nil { // Check for a broken connection, as it is not really a // condition that warrants a panic stack trace. var brokenPipe bool if ne, ok := err.(*net.OpError); ok { if se, ok := ne.Err.(*os.SyscallError); ok { if strings.Contains(strings.ToLower(se.Error()), \u0026quot;broken pipe\u0026quot;) || strings.Contains(strings.ToLower(se.Error()), \u0026quot;connection reset by peer\u0026quot;) { brokenPipe = true } } } httpRequest, _ := httputil.DumpRequest(c.Request, false) if brokenPipe { logger.Error(c.Request.URL.Path, zap.Any(\u0026quot;error\u0026quot;, err), zap.String(\u0026quot;request\u0026quot;, string(httpRequest)), ) // If the connection is dead, we can't write a status to it. c.Error(err.(error)) // nolint: errcheck c.Abort() return } if stack { logger.Error(\u0026quot;[Recovery from panic]\u0026quot;, zap.Any(\u0026quot;error\u0026quot;, err), zap.String(\u0026quot;request\u0026quot;, string(httpRequest)), zap.String(\u0026quot;stack\u0026quot;, string(debug.Stack())), ) } else { logger.Error(\u0026quot;[Recovery from panic]\u0026quot;, zap.Any(\u0026quot;error\u0026quot;, err), zap.String(\u0026quot;request\u0026quot;, string(httpRequest)), ) } c.AbortWithStatus(http.StatusInternalServerError) } }() c.Next() } } 使用\n// 全局中间件 func InitMiddleWares(eng *gin.Engine) { eng.Use(GinLogger(log.ZapLog), GinRecovery(log.ZapLog, true)) } 参考文章:\nhttps://mp.weixin.qq.com/s/i0bMh_gLLrdnhAEWlF-xDw https://studygolang.com/articles/17394 ","date":"2021-03-23","permalink":"https://daemon365.dev/2021/03/23/zap%E9%AB%98%E6%80%A7%E8%83%BD%E6%97%A5%E5%BF%97/","tags":["go","zap"],"title":"zap高性能日志"},{"content":"channel介绍 channel一个类型管道，通过它可以在goroutine之间发送和接收消息。它是Golang在语言层面提供的goroutine间的通信方式。\n众所周知，Go依赖于称为CSP（Communicating Sequential Processes）的并发模型，通过Channel实现这种同步模式。Go并发的核心哲学是不要通过共享内存进行通信; 相反，通过沟通分享记忆。\n下面以简单的示例来演示Go如何通过channel来实现通信。\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;time\u0026quot; ) func goRoutineA(a \u0026lt;-chan int) { val := \u0026lt;-a fmt.Println(\u0026quot;goRoutineA received the data\u0026quot;, val) } func goRoutineB(b chan int) { val := \u0026lt;-b fmt.Println(\u0026quot;goRoutineB received the data\u0026quot;, val) } func main() { ch := make(chan int, 3) go goRoutineA(ch) go goRoutineB(ch) ch \u0026lt;- 3 time.Sleep(time.Second * 1) } 结果为：goRoutineA received the data 3\n上面只是个简单的例子，只输出goRoutineA ，没有执行goRoutineB，说明channel仅允许被一个goroutine读写。\ngo并发知识：链接\n说道channel这里不得不提通道的结构hchan。\nhchan 源代码在src/runtime/chan.go\ntype hchan struct { qcount uint // total data in the queue dataqsiz uint // size of the circular queue buf unsafe.Pointer // points to an array of dataqsiz elements elemsize uint16 closed uint32 elemtype *_type // element type sendx uint // send index recvx uint // receive index recvq waitq // list of recv waiters sendq waitq // list of send waiters // lock protects all fields in hchan, as well as several // fields in sudogs blocked on this channel. // // Do not change another G's status while holding this lock // (in particular, do not ready a G), as this can deadlock // with stack shrinking. lock mutex } type waitq struct { first *sudog last *sudog } 说明：\nqcount uint // 当前队列中剩余元素个数 dataqsiz uint // 环形队列长度，即缓冲区的大小，即make（chan T，N），N. buf unsafe.Pointer // 环形队列指针 elemsize uint16 // 每个元素的大小 closed uint32 // 表示当前通道是否处于关闭状态。创建通道后，该字段设置为0，即通道打开; 通过调用close将其设置为1，通道关闭。 elemtype *_type // 元素类型，用于数据传递过程中的赋值； sendx uint和recvx uint是环形缓冲区的状态字段，它指示缓冲区的当前索引 - 支持数组，它可以从中发送数据和接收数据。 recvq waitq // 等待读消息的goroutine队列 sendq waitq // 等待写消息的goroutine队列 lock mutex // 互斥锁，为每个读写操作锁定通道，因为发送和接收必须是互斥操作。 这里sudog代表goroutine。\nmake chan make函数在创建channel的时候会在该进程的heap区申请一块内存，创建一个hchan结构体，返回执行该内存的指针，所以获取的的ch变量本身就是一个指针，在函数之间传递的时候是同一个channel。\nhchan结构体使用一个环形队列来保存groutine之间传递的数据(如果是缓存channel的话)，使用两个list保存像该chan发送和从该chan接收数据的goroutine，还有一个mutex来保证操作这些结构的安全。\n创建channel 有两种，一种是带缓冲的channel，一种是不带缓冲的channel\n// 带缓冲 ch := make(chan Task, 3) // 不带缓冲 ch := make(chan int) 这里我们先讨论带缓冲\nch := make(chan int, 3) 创建通道后的缓冲通道结构\nhchan struct { qcount uint : 0 dataqsiz uint : 3 buf unsafe.Pointer : 0xc00007e0e0 elemsize uint16 : 8 closed uint32 : 0 elemtype *runtime._type : \u0026amp;{ size:8 ptrdata:0 hash:4149441018 tflag:7 align:8 fieldalign:8 kind:130 alg:0x55cdf0 gcdata:0x4d61b4 str:1055 ptrToThis:45152 } sendx uint : 0 recvx uint : 0 recvq runtime.waitq : {first:\u0026lt;nil\u0026gt; last:\u0026lt;nil\u0026gt;} sendq runtime.waitq : {first:\u0026lt;nil\u0026gt; last:\u0026lt;nil\u0026gt;} lock runtime.mutex : {key:0} } 源代码\nfunc makechan(t *chantype, size int) *hchan { elem := t.elem ... } 如果我们创建一个带buffer的channel，底层的数据模型如下图：\n发送和接受数据 向channel发送和从channel接收数据主要涉及hchan里的四个成员变量，借用Kavya ppt里的图示，来分析发送和接收的过程。\n向channel写入数据 ch \u0026lt;- 3 底层hchan数据流程如图\n发送操作概要\n1、锁定整个通道结构。\n2、确定写入。尝试recvq从等待队列中等待goroutine，然后将元素直接写入goroutine。\n3、如果recvq为Empty，则确定缓冲区是否可用。如果可用，从当前goroutine复制数据到缓冲区。\n4、如果缓冲区已满，则要写入的元素将保存在当前正在执行的goroutine的结构中，并且当前goroutine将在sendq中排队并从运行时挂起。\n5、写入完成释放锁。\n这里我们要注意几个属性buf、sendx、lock的变化。\n流程图\n从channel读取操作 几乎和写入操作相同\n代码\nfunc goRoutineA(a \u0026lt;-chan int) { val := \u0026lt;-a fmt.Println(\u0026quot;goRoutineA received the data\u0026quot;, val) } 底层hchan数据流程如图\n这里我们要注意几个属性buf、sendx、recvx、lock的变化。\n读取操作概要\n先获取channel全局锁 尝试sendq从等待队列中获取等待的goroutine， 如有等待的goroutine，没有缓冲区，取出goroutine并读取数据，然后唤醒这个goroutine，结束读取释放锁。 如有等待的goroutine，且有缓冲区（此时缓冲区已满），从缓冲区队首取出数据，再从sendq取出一个goroutine，将goroutine中的数据存入buf队尾，结束读取释放锁。 如没有等待的goroutine，且缓冲区有数据，直接读取缓冲区数据，结束读取释放锁。 如没有等待的goroutine，且没有缓冲区或缓冲区为空，将当前的goroutine加入recvq排队，进入睡眠，等待被写goroutine唤醒。结束读取释放锁。 流程图\nrecvq和sendq 结构 recvq和sendq基本上是链表，看起来基本如下\nGoroutine Pause/Resume goroutine是Golang实现的用户空间的轻量级的线程，有runtime调度器调度，与操作系统的thread有多对一的关系，相关的数据结构如下图:\n其中M是操作系统的线程，G是用户启动的goroutine，P是与调度相关的context，每个M都拥有一个P，P维护了一个能够运行的goutine队列，用于该线程执行。\n当G1向buf已经满了的ch发送数据的时候，当runtine检测到对应的hchan的buf已经满了，会通知调度器，调度器会将G1的状态设置为waiting, 移除与线程M的联系，然后从P的runqueue中选择一个goroutine在线程M中执行，此时G1就是阻塞状态，但是不是操作系统的线程阻塞，所以这个时候只用消耗少量的资源。\n那么G1设置为waiting状态后去哪了？怎们去resume呢？我们再回到hchan结构体，注意到hchan有个sendq的成员，其类型是waitq，查看源码如下：\ntype hchan struct { ... recvq waitq // list of recv waiters sendq waitq // list of send waiters ... } // type waitq struct { first *sudog last *sudog } 实际上，当G1变为waiting状态后，会创建一个代表自己的sudog的结构，然后放到sendq这个list中，sudog结构中保存了channel相关的变量的指针(如果该Goroutine是sender，那么保存的是待发送数据的变量的地址，如果是receiver则为接收数据的变量的地址，之所以是地址，前面我们提到在传输数据的时候使用的是copy的方式)\n当G2从ch中接收一个数据时，会通知调度器，设置G1的状态为runnable，然后将加入P的runqueue里，等待线程执行.\nwait empty channel 前面我们是假设G1先运行，如果G2先运行会怎么样呢？如果G2先运行，那么G2会从一个empty的channel里取数据，这个时候G2就会阻塞，和前面介绍的G1阻塞一样，G2也会创建一个sudog结构体，保存接收数据的变量的地址，但是该sudog结构体是放到了recvq列表里，当G1向ch发送数据的时候，runtime并没有对hchan结构体题的buf进行加锁，而是直接将G1里的发送到ch的数据copy到了G2 sudog里对应的elem指向的内存地址！\nselect select就是用来监听和channel有关的IO操作，当 IO 操作发生时，触发相应的动作。\n一个简单的示例如下\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;time\u0026quot; ) func goRoutineD(ch chan int, i int) { time.Sleep(time.Second * 3) ch \u0026lt;- i } func goRoutineE(chs chan string, i string) { time.Sleep(time.Second * 3) chs \u0026lt;- i } func main() { ch := make(chan int, 5) chs := make(chan string, 5) go goRoutineD(ch, 5) go goRoutineE(chs, \u0026quot;ok\u0026quot;) select { case msg := \u0026lt;-ch: fmt.Println(\u0026quot; received the data \u0026quot;, msg) case msgs := \u0026lt;-chs: fmt.Println(\u0026quot; received the data \u0026quot;, msgs) default: fmt.Println(\u0026quot;no data received \u0026quot;) time.Sleep(time.Second * 1) } } 运行程序，因为当前时间没有到3s，所以select 选择defult\nno data received\n修改程序，我们注释掉default，并多执行几次结果为\nreceived the data 5 received the data ok received the data ok received the data ok select语句会阻塞，直到监测到一个可以执行的IO操作为止，而这里goRoutineD和goRoutineE睡眠时间是相同的，都是3s，从输出可看出，从channel中读出数据的顺序是随机的。\n再修改代码，goRoutineD睡眠时间改成4s\nfunc goRoutineD(ch chan int, i int) { time.Sleep(time.Second * 4) ch \u0026lt;- i } 此时会先执行goRoutineE，select 选择case msgs := \u0026lt;-chs。\nrange 可以持续从channel读取数据，一直到channel被关闭，当channel中没有数据时会阻塞当前goroutine，与读channel时阻塞处理机制一样。\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;time\u0026quot; ) func goRoutineD(ch chan int, i int) { for i := 1; i \u0026lt;= 5; i++{ ch \u0026lt;- i } } func chanRange(chanName chan int) { for e := range chanName { fmt.Printf(\u0026quot;Get element from chan: %d\\n\u0026quot;, e) if len(chanName) \u0026lt;= 0 { // 如果现有数据量为0，跳出循环 break } } } func main() { ch := make(chan int, 5) go goRoutineD(ch, 5) chanRange(ch) } 结果：\nGet element from chan: 1 Get element from chan: 2 Get element from chan: 3 Get element from chan: 4 Get element from chan: 5 死锁（deadlock） 指两个或两个以上的协程的执行过程中，由于竞争资源或由于彼此通信而造成的一种阻塞的现象。\n在非缓冲信道若发生只流入不流出，或只流出不流入，就会发生死锁。\n下面是一些死锁的例子\n1、\npackage main func main() { ch := make(chan int) ch \u0026lt;- 3 } 上面情况，向非缓冲通道写数据会发生阻塞，导致死锁。解决办法创建缓冲区 ch := make(chan int，3)\n2、\npackage main import ( \u0026quot;fmt\u0026quot; ) func main() { ch := make(chan int) fmt.Println(\u0026lt;-ch) } 向非缓冲通道读取数据会发生阻塞，导致死锁。 解决办法开启缓冲区，先向channel写入数据。\n3、\npackage main func main() { ch := make(chan int, 3) ch \u0026lt;- 3 ch \u0026lt;- 4 ch \u0026lt;- 5 ch \u0026lt;- 6 } 写入数据超过缓冲区数量也会发生死锁。解决办法将写入数据取走。\n死锁的情况有很多这里不再赘述。 还有一种情况，向关闭的channel写入数据，不会产生死锁，产生panic。\npackage main func main() { ch := make(chan int, 3) ch \u0026lt;- 1 close(ch) ch \u0026lt;- 2 } 解决办法别向关闭的channel写入数据。\n参考文章 Go channel 实现原理分析 深入理解Golang Channel ","date":"2021-02-21","permalink":"https://daemon365.dev/2021/02/21/golang-channel%E5%8E%9F%E7%90%86/","tags":["go","源码分析"],"title":"golang channel原理"},{"content":"垃圾回收(Garbage Collection，简称GC)是编程语言中提供的自动的内存管理机制，自动释放不需要的对象，让出存储器资源，无需程序员手动执行。\nGolang中的垃圾回收主要应用三色标记法，GC过程和其他用户goroutine可并发运行，但需要一定时间的STW(stop the world)，STW的过程中，CPU不执行用户代码，全部用于垃圾回收，这个过程的影响很大，Golang进行了多次的迭代优化来解决这个问题。\nGo V1.3之前的标记-清除(mark and sweep)算法 此算法主要有两个主要的步骤：\n标记(Mark phase) 清除(Sweep phase) 第一步，暂停程序业务逻辑, 找出不可达的对象，然后做上标记。第二步，回收标记好的对象。\n操作非常简单，但是有一点需要额外注意：mark and sweep算法在执行的时候，需要程序暂停！即 STW(stop the world)。也就是说，这段时间程序会卡在哪儿。\n第二步, 开始标记，程序找出它所有可达的对象，并做上标记。如下图所示：\n第三步, 标记完了之后，然后开始清除未标记的对象. 结果如下.\n第四步, 停止暂停，让程序继续跑。然后循环重复这个过程，直到process程序生命周期结束。\n标记-清扫(mark and sweep)的缺点 STW，stop the world；让程序暂停，程序出现卡顿 (重要问题)。 标记需要扫描整个heap 清除数据会产生heap碎片 所以Go V1.3版本之前就是以上来实施的, 流程是\nGo V1.3 做了简单的优化,将STW提前, 减少STW暂停的时间范围.如下所示\n这里面最重要的问题就是：mark-and-sweep 算法会暂停整个程序 。\nGo是如何面对并这个问题的呢？接下来G V1.5版本 就用三色并发标记法来优化这个问题.\nGo V1.5的三色并发标记法 三色标记法 实际上就是通过三个阶段的标记来确定清楚的对象都有哪些. 我们来看一下具体的过程.\n第一步 , 就是只要是新创建的对象,默认的颜色都是标记为“白色”.\n这里面需要注意的是, 所谓“程序”, 则是一些对象的跟节点集合.\n所以上图,可以转换如下的方式来表示.\n第二步, 每次GC回收开始, 然后从根节点开始遍历所有对象，把遍历到的对象从白色集合放入“灰色”集合。\n第三步, 遍历灰色集合，将灰色对象引用的对象从白色集合放入灰色集合，之后将此灰色对象放入黑色集合\n第四步, 重复第三步, 直到灰色中无任何对象.\n第五步: 回收所有的白色标记表的对象. 也就是回收垃圾.\n以上便是三色并发标记法, 不难看出,我们上面已经清楚的体现三色的特性, 那么又是如何实现并行的呢?\nGo是如何解决标记-清除(mark and sweep)算法中的卡顿(stw，stop the world)问题的呢？\n没有STW的三色标记法 ​ 我们还是基于上述的三色并发标记法来说, 他是一定要依赖STW的. 因为如果不暂停程序, 程序的逻辑改变对象引用关系, 这种动作如果在标记阶段做了修改，会影响标记结果的正确性。我们举一个场景.\n如果三色标记法, 标记过程不使用STW将会发生什么事情?\n可以看出，有两个问题, 在三色标记法中,是不希望被发生的\n条件1: 一个白色对象被黑色对象引用**(白色被挂在黑色下)** 条件2: 灰色对象与它之间的可达关系的白色对象遭到破坏**(灰色同时丢了该白色)** 当以上两个条件同时满足时, 就会出现对象丢失现象!\n当然, 如果上述中的白色对象3, 如果他还有很多下游对象的话, 也会一并都清理掉.\n​ 为了防止这种现象的发生，最简单的方式就是STW，直接禁止掉其他用户程序对对象引用关系的干扰，但是STW的过程有明显的资源浪费，对所有的用户程序都有很大影响，如何能在保证对象不丢失的情况下合理的尽可能的提高GC效率，减少STW时间呢？\n​ 答案就是, 那么我们只要使用一个机制,来破坏上面的两个条件就可以了.\n屏障机制 我们让GC回收器,满足下面两种情况之一时,可保对象不丢失. 所以引出两种方式.\n“强-弱” 三色不变式 强三色不变式 不存在黑色对象引用到白色对象的指针。\n弱三色不变式 所有被黑色对象引用的白色对象都处于灰色保护状态.\n为了遵循上述的两个方式,Golang团队初步得到了如下具体的两种屏障方式“插入屏障”, “删除屏障”.\n插入屏障 具体操作: 在A对象引用B对象的时候，B对象被标记为灰色。(将B挂在A下游，B必须被标记为灰色)\n满足: 强三色不变式. (不存在黑色对象引用白色对象的情况了， 因为白色会强制变成灰色)\n伪码如下:\n添加下游对象(当前下游对象slot, 新下游对象ptr) { //1 标记灰色(新下游对象ptr) //2 当前下游对象slot = 新下游对象ptr } 场景：\nA.添加下游对象(nil, B) //A 之前没有下游， 新添加一个下游对象B， B被标记为灰色 A.添加下游对象(C, B) //A 将下游对象C 更换为B， B被标记为灰色 ​ 这段伪码逻辑就是写屏障,. 我们知道,黑色对象的内存槽有两种位置, 栈和堆. 栈空间的特点是容量小,但是要求相应速度快,因为函数调用弹出频繁使用, 所以“插入屏障”机制,在栈空间的对象操作中不使用. 而仅仅使用在堆空间对象的操作中.\n​ 接下来，我们用几张图，来模拟整个一个详细的过程， 希望您能够更可观的看清晰整体流程。\n但是如果栈不添加,当全部三色标记扫描之后,栈上有可能依然存在白色对象被引用的情况(如上图的对象9). 所以要对栈重新进行三色标记扫描, 但这次为了对象不丢失, 要对本次标记扫描启动STW暂停. 直到栈空间的三色标记结束.\n最后将栈和堆空间 扫描剩余的全部 白色节点清除. 这次STW大约的时间在10~100ms间.\n删除屏障 具体操作: 被删除的对象，如果自身为灰色或者白色，那么被标记为灰色。\n满足: 弱三色不变式. (保护灰色对象到白色对象的路径不会断)\n伪代码：\n添加下游对象(当前下游对象slot， 新下游对象ptr) { //1 if (当前下游对象slot是灰色 || 当前下游对象slot是白色) { 标记灰色(当前下游对象slot) //slot为被删除对象， 标记为灰色 } //2 当前下游对象slot = 新下游对象ptr } 场景：\nA.添加下游对象(B, nil) //A对象，删除B对象的引用。 B被A删除，被标记为灰(如果B之前为白) A.添加下游对象(B, C) //A对象，更换下游B变成C。 B被A删除，被标记为灰(如果B之前为白) 接下来，我们用几张图，来模拟整个一个详细的过程， 希望您能够更可观的看清晰整体流程。\n这种方式的回收精度低，一个对象即使被删除了最后一个指向它的指针也依旧可以活过这一轮，在下一轮GC中被清理掉。\nGo V1.8的混合写屏障(hybrid write barrier)机制 插入写屏障和删除写屏障的短板：\n插入写屏障：结束时需要STW来重新扫描栈，标记栈上引用的白色对象的存活； 删除写屏障：回收精度低，GC开始时STW扫描堆栈来记录初始快照，这个过程会保护开始时刻的所有存活对象。 Go V1.8版本引入了混合写屏障机制（hybrid write barrier），避免了对栈re-scan的过程，极大的减少了STW的时间。结合了两者的优点。\n混合写屏障规则 具体操作:\n1、GC开始将栈上的对象全部扫描并标记为黑色(之后不再进行第二次重复扫描，无需STW)，\n2、GC期间，任何在栈上创建的新对象，均为黑色。\n3、被删除的对象标记为灰色。\n4、被添加的对象标记为灰色。\n满足: 变形的弱三色不变式.\n伪代码：\n添加下游对象(当前下游对象slot, 新下游对象ptr) { //1 标记灰色(当前下游对象slot) //只要当前下游对象被移走，就标记灰色 //2 标记灰色(新下游对象ptr) //3 当前下游对象slot = 新下游对象ptr } 这里我们注意， 屏障技术是不在栈上应用的，因为要保证栈的运行效率。\n混合写屏障的具体场景分析 接下来，我们用几张图，来模拟整个一个详细的过程， 希望您能够更可观的看清晰整体流程。\n注意混合写屏障是Gc的一种屏障机制，所以只是当程序执行GC的时候，才会触发这种机制。\nGC开始：扫描栈区，将可达对象全部标记为黑 场景一： 对象被一个堆对象删除引用，成为栈对象的下游 伪代码\n//前提：堆对象4-\u0026gt;对象7 = 对象7； //对象7 被 对象4引用 栈对象1-\u0026gt;对象7 = 堆对象7； //将堆对象7 挂在 栈对象1 下游 堆对象4-\u0026gt;对象7 = null； //对象4 删除引用 对象7 场景二： 对象被一个栈对象删除引用，成为另一个栈对象的下游 伪代码\nnew 栈对象9； 对象8-\u0026gt;对象3 = 对象3； //将栈对象3 挂在 栈对象9 下游 对象2-\u0026gt;对象3 = null； //对象2 删除引用 对象3 场景三：对象被一个堆对象删除引用，成为另一个堆对象的下游 伪代码\n堆对象10-\u0026gt;对象7 = 堆对象7； //将堆对象7 挂在 堆对象10 下游 堆对象4-\u0026gt;对象7 = null； //对象4 删除引用 对象7 场景四：对象从一个栈对象删除引用，成为另一个堆对象的下游 伪代码\n堆对象10-\u0026gt;对象7 = 堆对象7； //将堆对象7 挂在 堆对象10 下游 堆对象4-\u0026gt;对象7 = null； //对象4 删除引用 对象7 ​ Golang中的混合写屏障满足弱三色不变式，结合了删除写屏障和插入写屏障的优点，只需要在开始时并发扫描各个goroutine的栈，使其变黑并一直保持，这个过程不需要STW，而标记结束后，因为栈在扫描后始终是黑色的，也无需再进行re-scan操作了，减少了STW的时间。\n总结 ​ 以上便是Golang的GC全部的标记-清除逻辑及场景演示全过程。\nGoV1.3- 普通标记清除法，整体过程需要启动STW，效率极低。\nGoV1.5- 三色标记法， 堆空间启动写屏障，栈空间不启动，全部扫描之后，需要重新扫描一次栈(需要STW)，效率普通\nGoV1.8-三色标记法，混合写屏障机制， 栈空间不启动，堆空间启动。整个过程几乎不需要STW，效率较高。\n文章转自 https://www.jianshu.com/p/4c5a303af470\n","date":"2021-02-20","permalink":"https://daemon365.dev/2021/02/20/golang-gc-%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E6%9C%BA%E5%88%B6/","tags":["go"],"title":"golang GC 垃圾回收机制"},{"content":"安装 go get github.com/spf13/viper viper支持的功能 1、可以设置默认值 2、可以加载多种格式的配置文件，如JSON，TOML，YAML，HCL和Java属性配置文件 3、应用程序运行过程中，保持监听和重新读取配置文件 4、可以从环境变量读取配置 5、可以从远程配置系统读取配置 6、可以读取命令行标志作为配置 7、可以从缓冲区中读取 8、设置显式的值\n在GitHub中，作者是这样描述viper对于开发人员的作用：在构建现代化应用程序的过程中，开发人员可以通过使用viper而不必考虑配置文件的格式问题。 viper具体的帮助 1、可以查找、加载和反序列化多种格式的配置文件，如JSON, TOML, YAML, HCL, Java属性配置格式。 2、提供一种为不同配置选项设置默认值的机制 3、提供一种通过命令行标志覆盖指定配置选项值的机制 4、提供了一种别名系统，可以在避免破坏现有代码的前提下，轻松地重命名参数 5、当用户提供的命令行或配置文件的配置选项与默认的配置选项相同时，可以很容易通过选项值结果看出优先级的差异。\nviper提供的配置方式的优先级顺序如下(由高到低)： 1.设置显示调用(explicit call to Set) 2.命令行标志(flag) 3.环境变量(env) 4.配置文件(config) 5.远程键/值存储(key/value store) 6.默认值(default)\nviper的简单使用 把值存入Viper 建立默认值 一个好的配置系统应该支持默认值。键不需要默认值，但如果没有通过配置文件、环境变量、远程配置或命令行标志（flag）设置键，则默认值非常有用。\n例如：\nviper.SetDefault(\u0026quot;ContentDir\u0026quot;, \u0026quot;content\u0026quot;) viper.SetDefault(\u0026quot;LayoutDir\u0026quot;, \u0026quot;layouts\u0026quot;) viper.SetDefault(\u0026quot;Taxonomies\u0026quot;, map[string]string{\u0026quot;tag\u0026quot;: \u0026quot;tags\u0026quot;, \u0026quot;category\u0026quot;: \u0026quot;categories\u0026quot;}) 读取配置文件 Viper需要最少知道在哪里查找配置文件的配置。Viper支持JSON、TOML、YAML、HCL、envfile和Java properties格式的配置文件。Viper可以搜索多个路径，但目前单个Viper实例只支持单个配置文件。Viper不默认任何配置搜索路径，将默认决策留给应用程序。\n下面是一个如何使用Viper搜索和读取配置文件的示例。不需要任何特定的路径，但是至少应该提供一个配置文件预期出现的路径。\nviper.SetConfigFile(\u0026quot;./config.yaml\u0026quot;) // 指定配置文件路径 viper.SetConfigName(\u0026quot;config\u0026quot;) // 配置文件名称(无扩展名) viper.SetConfigType(\u0026quot;yaml\u0026quot;) // 如果配置文件的名称中没有扩展名，则需要配置此项 viper.AddConfigPath(\u0026quot;/etc/appname/\u0026quot;) // 查找配置文件所在的路径 viper.AddConfigPath(\u0026quot;$HOME/.appname\u0026quot;) // 多次调用以添加多个搜索路径 viper.AddConfigPath(\u0026quot;.\u0026quot;) // 还可以在工作目录中查找配置 err := viper.ReadInConfig() // 查找并读取配置文件 if err != nil { // 处理读取配置文件的错误 panic(fmt.Errorf(\u0026quot;Fatal error config file: %s \\n\u0026quot;, err)) } 在加载配置文件出错时，你可以像下面这样处理找不到配置文件的特定情况：\nif err := viper.ReadInConfig(); err != nil { if _, ok := err.(viper.ConfigFileNotFoundError); ok { // 配置文件未找到错误；如果需要可以忽略 } else { // 配置文件被找到，但产生了另外的错误 } } // 配置文件找到并成功解析 注意[自1.6起]： 你也可以有不带扩展名的文件，并以编程方式指定其格式。对于位于用户$HOME目录中的配置文件没有任何扩展名，如.bashrc。\n这里补充两个问题供读者解答并自行验证\n当你使用如下方式读取配置时，viper会从./conf目录下查找任何以config为文件名的配置文件，如果同时存在./conf/config.json和./conf/config.yaml两个配置文件的话，viper会从哪个配置文件加载配置呢？\nviper.SetConfigName(\u0026quot;config\u0026quot;) viper.AddConfigPath(\u0026quot;./conf\u0026quot;) 在上面两个语句下搭配使用viper.SetConfigType(\u0026ldquo;yaml\u0026rdquo;)指定配置文件类型可以实现预期的效果吗？\n写入配置文件 从配置文件中读取配置文件是有用的，但是有时你想要存储在运行时所做的所有修改。为此，可以使用下面一组命令，每个命令都有自己的用途:\nWriteConfig - 将当前的viper配置写入预定义的路径并覆盖（如果存在的话）。如果没有预定义的路径，则报错。 SafeWriteConfig - 将当前的viper配置写入预定义的路径。如果没有预定义的路径，则报错。如果存在，将不会覆盖当前的配置文件。 WriteConfigAs - 将当前的viper配置写入给定的文件路径。将覆盖给定的文件(如果它存在的话)。 SafeWriteConfigAs - 将当前的viper配置写入给定的文件路径。不会覆盖给定的文件(如果它存在的话)。 根据经验，标记为safe的所有方法都不会覆盖任何文件，而是直接创建（如果不存在），而默认行为是创建或截断。\n一个小示例：\nviper.WriteConfig() // 将当前配置写入“viper.AddConfigPath()”和“viper.SetConfigName”设置的预定义路径 viper.SafeWriteConfig() viper.WriteConfigAs(\u0026quot;/path/to/my/.config\u0026quot;) viper.SafeWriteConfigAs(\u0026quot;/path/to/my/.config\u0026quot;) // 因为该配置文件写入过，所以会报错 viper.SafeWriteConfigAs(\u0026quot;/path/to/my/.other_config\u0026quot;) 监控并重新读取配置文件 Viper支持在运行时实时读取配置文件的功能。\n需要重新启动服务器以使配置生效的日子已经一去不复返了，viper驱动的应用程序可以在运行时读取配置文件的更新，而不会错过任何消息。\n只需告诉viper实例watchConfig。可选地，你可以为Viper提供一个回调函数，以便在每次发生更改时运行。\n确保在调用WatchConfig()之前添加了所有的配置路径。 viper.WatchConfig() viper.OnConfigChange(func(e fsnotify.Event) { // 配置文件发生变更之后会调用的回调函数 fmt.Println(\u0026quot;Config file changed:\u0026quot;, e.Name) }) 从io.Reader读取配置 Viper预先定义了许多配置源，如文件、环境变量、标志和远程K/V存储，但你不受其约束。你还可以实现自己所需的配置源并将其提供给viper。\nviper.SetConfigType(\u0026quot;yaml\u0026quot;) // 或者 viper.SetConfigType(\u0026quot;YAML\u0026quot;) // 任何需要将此配置添加到程序中的方法。 var yamlExample = []byte(` Hacker: true name: steve hobbies: - skateboarding - snowboarding - go clothing: jacket: leather trousers: denim age: 35 eyes : brown beard: true `) viper.ReadConfig(bytes.NewBuffer(yamlExample)) viper.Get(\u0026quot;name\u0026quot;) // 这里会得到 \u0026quot;steve\u0026quot; 覆盖设置 这些可能来自命令行标志，也可能来自你自己的应用程序逻辑。\nviper.Set(\u0026quot;Verbose\u0026quot;, true) viper.Set(\u0026quot;LogFile\u0026quot;, LogFile) 注册和使用别名 别名允许多个键引用单个值\nviper.RegisterAlias(\u0026quot;loud\u0026quot;, \u0026quot;Verbose\u0026quot;) // 注册别名（此处loud和Verbose建立了别名） viper.Set(\u0026quot;verbose\u0026quot;, true) // 结果与下一行相同 viper.Set(\u0026quot;loud\u0026quot;, true) // 结果与前一行相同 viper.GetBool(\u0026quot;loud\u0026quot;) // true viper.GetBool(\u0026quot;verbose\u0026quot;) // true 使用环境变量 Viper完全支持环境变量。这使Twelve-Factor App开箱即用。有五种方法可以帮助与ENV协作:\nAutomaticEnv() BindEnv(string\u0026hellip;) : error SetEnvPrefix(string) SetEnvKeyReplacer(string\u0026hellip;) *strings.Replacer AllowEmptyEnv(bool) 使用ENV变量时，务必要意识到Viper将ENV变量视为区分大小写。\nViper提供了一种机制来确保ENV变量是惟一的。通过使用SetEnvPrefix，你可以告诉Viper在读取环境变量时使用前缀。BindEnv和AutomaticEnv都将使用这个前缀。\nBindEnv使用一个或两个参数。第一个参数是键名称，第二个是环境变量的名称。环境变量的名称区分大小写。如果没有提供ENV变量名，那么Viper将自动假设ENV变量与以下格式匹配：前缀+ “_” +键名全部大写。当你显式提供ENV变量名（第二个参数）时，它 不会 自动添加前缀。例如，如果第二个参数是“id”，Viper将查找环境变量“ID”。\n在使用ENV变量时，需要注意的一件重要事情是，每次访问该值时都将读取它。Viper在调用BindEnv时不固定该值。\nAutomaticEnv是一个强大的助手，尤其是与SetEnvPrefix结合使用时。调用时，Viper会在发出viper.Get请求时随时检查环境变量。它将应用以下规则。它将检查环境变量的名称是否与键匹配（如果设置了EnvPrefix）。\nSetEnvKeyReplacer允许你使用strings.Replacer对象在一定程度上重写 Env 键。如果你希望在Get()调用中使用-或者其他什么符号，但是环境变量里使用_分隔符，那么这个功能是非常有用的。可以在viper_test.go中找到它的使用示例。\n或者，你可以使用带有NewWithOptions工厂函数的EnvKeyReplacer。与SetEnvKeyReplacer不同，它接受StringReplacer接口，允许你编写自定义字符串替换逻辑。\n默认情况下，空环境变量被认为是未设置的，并将返回到下一个配置源。若要将空环境变量视为已设置，请使用AllowEmptyEnv方法。\nEnv 示例： SetEnvPrefix(\u0026quot;spf\u0026quot;) // 将自动转为大写 BindEnv(\u0026quot;id\u0026quot;) os.Setenv(\u0026quot;SPF_ID\u0026quot;, \u0026quot;13\u0026quot;) // 通常是在应用程序之外完成的 id := Get(\u0026quot;id\u0026quot;) // 13 使用Flags Viper 具有绑定到标志的能力。具体来说，Viper支持Cobra库中使用的Pflag。\n与BindEnv类似，该值不是在调用绑定方法时设置的，而是在访问该方法时设置的。这意味着你可以根据需要尽早进行绑定，即使在init()函数中也是如此。\n对于单个标志，BindPFlag()方法提供此功能。\n例如：\nserverCmd.Flags().Int(\u0026quot;port\u0026quot;, 1138, \u0026quot;Port to run Application server on\u0026quot;) viper.BindPFlag(\u0026quot;port\u0026quot;, serverCmd.Flags().Lookup(\u0026quot;port\u0026quot;)) 你还可以绑定一组现有的pflags （pflag.FlagSet）：\n举个例子：\npflag.Int(\u0026quot;flagname\u0026quot;, 1234, \u0026quot;help message for flagname\u0026quot;) pflag.Parse() viper.BindPFlags(pflag.CommandLine) i := viper.GetInt(\u0026quot;flagname\u0026quot;) // 从viper而不是从pflag检索值 在 Viper 中使用 pflag 并不阻碍其他包中使用标准库中的 flag 包。pflag 包可以通过导入这些 flags 来处理flag包定义的flags。这是通过调用pflag包提供的便利函数AddGoFlagSet()来实现的。\n例如：\npackage main import ( \u0026quot;flag\u0026quot; \u0026quot;github.com/spf13/pflag\u0026quot; ) func main() { // 使用标准库 \u0026quot;flag\u0026quot; 包 flag.Int(\u0026quot;flagname\u0026quot;, 1234, \u0026quot;help message for flagname\u0026quot;) pflag.CommandLine.AddGoFlagSet(flag.CommandLine) pflag.Parse() viper.BindPFlags(pflag.CommandLine) i := viper.GetInt(\u0026quot;flagname\u0026quot;) // 从 viper 检索值 ... } flag接口 如果你不使用Pflag，Viper 提供了两个Go接口来绑定其他 flag 系统。\nFlagValue表示单个flag。这是一个关于如何实现这个接口的非常简单的例子：\ntype myFlag struct {} func (f myFlag) HasChanged() bool { return false } func (f myFlag) Name() string { return \u0026quot;my-flag-name\u0026quot; } func (f myFlag) ValueString() string { return \u0026quot;my-flag-value\u0026quot; } func (f myFlag) ValueType() string { return \u0026quot;string\u0026quot; } 一旦你的 flag 实现了这个接口，你可以很方便地告诉Viper绑定它：\nviper.BindFlagValue(\u0026quot;my-flag-name\u0026quot;, myFlag{}) FlagValueSet代表一组 flags 。这是一个关于如何实现这个接口的非常简单的例子:\ntype myFlagSet struct { flags []myFlag } func (f myFlagSet) VisitAll(fn func(FlagValue)) { for _, flag := range flags { fn(flag) } } 一旦你的flag set实现了这个接口，你就可以很方便地告诉Viper绑定它：\nfSet := myFlagSet{ flags: []myFlag{myFlag{}, myFlag{}}, } viper.BindFlagValues(\u0026quot;my-flags\u0026quot;, fSet) 远程Key/Value存储支持 在Viper中启用远程支持，需要在代码中匿名导入viper/remote这个包。\nimport _ \u0026quot;github.com/spf13/viper/remote\u0026quot; Viper将读取从Key/Value存储（例如etcd或Consul）中的路径检索到的配置字符串（如JSON、TOML、YAML、HCL、envfile和Java properties格式）。这些值的优先级高于默认值，但是会被从磁盘、flag或环境变量检索到的配置值覆盖。（译注：也就是说Viper加载配置值的优先级为：磁盘上的配置文件\u0026gt;命令行标志位\u0026gt;环境变量\u0026gt;远程Key/Value存储\u0026gt;默认值。）\nViper使用crypt从K/V存储中检索配置，这意味着如果你有正确的gpg密匙，你可以将配置值加密存储并自动解密。加密是可选的。\n你可以将远程配置与本地配置结合使用，也可以独立使用。\ncrypt有一个命令行助手，你可以使用它将配置放入K/V存储中。crypt默认使用在http://127.0.0.1:4001的etcd。\n$ go get github.com/bketelsen/crypt/bin/crypt $ crypt set -plaintext /config/hugo.json /Users/hugo/settings/config.json 确认值已经设置：\n$ crypt get -plaintext /config/hugo.json 有关如何设置加密值或如何使用Consul的示例，请参见crypt文档。\n远程Key/Value存储示例-未加密 etcd viper.AddRemoteProvider(\u0026quot;etcd\u0026quot;, \u0026quot;http://127.0.0.1:4001\u0026quot;,\u0026quot;/config/hugo.json\u0026quot;) viper.SetConfigType(\u0026quot;json\u0026quot;) // 因为在字节流中没有文件扩展名，所以这里需要设置下类型。支持的扩展名有 \u0026quot;json\u0026quot;, \u0026quot;toml\u0026quot;, \u0026quot;yaml\u0026quot;, \u0026quot;yml\u0026quot;, \u0026quot;properties\u0026quot;, \u0026quot;props\u0026quot;, \u0026quot;prop\u0026quot;, \u0026quot;env\u0026quot;, \u0026quot;dotenv\u0026quot; err := viper.ReadRemoteConfig() Consul 你需要 Consul Key/Value存储中设置一个Key保存包含所需配置的JSON值。例如，创建一个keyMY_CONSUL_KEY将下面的值存入Consul key/value 存储：\n{ \u0026quot;port\u0026quot;: 8080, \u0026quot;hostname\u0026quot;: \u0026quot;liwenzhou.com\u0026quot; } viper.AddRemoteProvider(\u0026quot;consul\u0026quot;, \u0026quot;localhost:8500\u0026quot;, \u0026quot;MY_CONSUL_KEY\u0026quot;) viper.SetConfigType(\u0026quot;json\u0026quot;) // 需要显示设置成json err := viper.ReadRemoteConfig() fmt.Println(viper.Get(\u0026quot;port\u0026quot;)) // 8080 fmt.Println(viper.Get(\u0026quot;hostname\u0026quot;)) // liwenzhou.com Firestore viper.AddRemoteProvider(\u0026quot;firestore\u0026quot;, \u0026quot;google-cloud-project-id\u0026quot;, \u0026quot;collection/document\u0026quot;) viper.SetConfigType(\u0026quot;json\u0026quot;) // 配置的格式: \u0026quot;json\u0026quot;, \u0026quot;toml\u0026quot;, \u0026quot;yaml\u0026quot;, \u0026quot;yml\u0026quot; err := viper.ReadRemoteConfig() 当然，你也可以使用SecureRemoteProvider。\n远程Key/Value存储示例-加密 viper.AddSecureRemoteProvider(\u0026quot;etcd\u0026quot;,\u0026quot;http://127.0.0.1:4001\u0026quot;,\u0026quot;/config/hugo.json\u0026quot;,\u0026quot;/etc/secrets/mykeyring.gpg\u0026quot;) viper.SetConfigType(\u0026quot;json\u0026quot;) // 因为在字节流中没有文件扩展名，所以这里需要设置下类型。支持的扩展名有 \u0026quot;json\u0026quot;, \u0026quot;toml\u0026quot;, \u0026quot;yaml\u0026quot;, \u0026quot;yml\u0026quot;, \u0026quot;properties\u0026quot;, \u0026quot;props\u0026quot;, \u0026quot;prop\u0026quot;, \u0026quot;env\u0026quot;, \u0026quot;dotenv\u0026quot; err := viper.ReadRemoteConfig() 监控etcd中的更改-未加密 // 或者你可以创建一个新的viper实例 var runtime_viper = viper.New() runtime_viper.AddRemoteProvider(\u0026quot;etcd\u0026quot;, \u0026quot;http://127.0.0.1:4001\u0026quot;, \u0026quot;/config/hugo.yml\u0026quot;) runtime_viper.SetConfigType(\u0026quot;yaml\u0026quot;) // 因为在字节流中没有文件扩展名，所以这里需要设置下类型。支持的扩展名有 \u0026quot;json\u0026quot;, \u0026quot;toml\u0026quot;, \u0026quot;yaml\u0026quot;, \u0026quot;yml\u0026quot;, \u0026quot;properties\u0026quot;, \u0026quot;props\u0026quot;, \u0026quot;prop\u0026quot;, \u0026quot;env\u0026quot;, \u0026quot;dotenv\u0026quot; // 第一次从远程读取配置 err := runtime_viper.ReadRemoteConfig() // 反序列化 runtime_viper.Unmarshal(\u0026amp;runtime_conf) // 开启一个单独的goroutine一直监控远端的变更 go func(){ for { time.Sleep(time.Second * 5) // 每次请求后延迟一下 // 目前只测试了etcd支持 err := runtime_viper.WatchRemoteConfig() if err != nil { log.Errorf(\u0026quot;unable to read remote config: %v\u0026quot;, err) continue } // 将新配置反序列化到我们运行时的配置结构体中。你还可以借助channel实现一个通知系统更改的信号 runtime_viper.Unmarshal(\u0026amp;runtime_conf) } }() 从Viper获取值 在Viper中，有几种方法可以根据值的类型获取值。存在以下功能和方法:\nGet(key string) : interface{} GetBool(key string) : bool GetFloat64(key string) : float64 GetInt(key string) : int GetIntSlice(key string) : []int GetString(key string) : string GetStringMap(key string) : map[string]interface{} GetStringMapString(key string) : map[string]string GetStringSlice(key string) : []string GetTime(key string) : time.Time GetDuration(key string) : time.Duration IsSet(key string) : bool AllSettings() : map[string]interface{} 需要认识到的一件重要事情是，每一个Get方法在找不到值的时候都会返回零值。为了检查给定的键是否存在，提供了IsSet()方法。\n例如：\nviper.GetString(\u0026quot;logfile\u0026quot;) // 不区分大小写的设置和获取 if viper.GetBool(\u0026quot;verbose\u0026quot;) { fmt.Println(\u0026quot;verbose enabled\u0026quot;) } 访问嵌套的键 访问器方法也接受深度嵌套键的格式化路径。例如，如果加载下面的JSON文件：\n{ \u0026quot;host\u0026quot;: { \u0026quot;address\u0026quot;: \u0026quot;localhost\u0026quot;, \u0026quot;port\u0026quot;: 5799 }, \u0026quot;datastore\u0026quot;: { \u0026quot;metric\u0026quot;: { \u0026quot;host\u0026quot;: \u0026quot;127.0.0.1\u0026quot;, \u0026quot;port\u0026quot;: 3099 }, \u0026quot;warehouse\u0026quot;: { \u0026quot;host\u0026quot;: \u0026quot;198.0.0.1\u0026quot;, \u0026quot;port\u0026quot;: 2112 } } } Viper可以通过传入.分隔的路径来访问嵌套字段：\nGetString(\u0026quot;datastore.metric.host\u0026quot;) // (返回 \u0026quot;127.0.0.1\u0026quot;) 这遵守上面建立的优先规则；搜索路径将遍历其余配置注册表，直到找到为止。(译注：因为Viper支持从多种配置来源，例如磁盘上的配置文件\u0026gt;命令行标志位\u0026gt;环境变量\u0026gt;远程Key/Value存储\u0026gt;默认值，我们在查找一个配置的时候如果在当前配置源中没找到，就会继续从后续的配置源查找，直到找到为止。)\n例如，在给定此配置文件的情况下，datastore.metric.host和datastore.metric.port均已定义（并且可以被覆盖）。如果另外在默认值中定义了datastore.metric.protocol，Viper也会找到它。\n然而，如果datastore.metric被直接赋值覆盖（被flag，环境变量，set()方法等等…），那么datastore.metric的所有子键都将变为未定义状态，它们被高优先级配置级别“遮蔽”（shadowed）了。\n最后，如果存在与分隔的键路径匹配的键，则返回其值。例如：\n{ \u0026quot;datastore.metric.host\u0026quot;: \u0026quot;0.0.0.0\u0026quot;, \u0026quot;host\u0026quot;: { \u0026quot;address\u0026quot;: \u0026quot;localhost\u0026quot;, \u0026quot;port\u0026quot;: 5799 }, \u0026quot;datastore\u0026quot;: { \u0026quot;metric\u0026quot;: { \u0026quot;host\u0026quot;: \u0026quot;127.0.0.1\u0026quot;, \u0026quot;port\u0026quot;: 3099 }, \u0026quot;warehouse\u0026quot;: { \u0026quot;host\u0026quot;: \u0026quot;198.0.0.1\u0026quot;, \u0026quot;port\u0026quot;: 2112 } } } GetString(\u0026quot;datastore.metric.host\u0026quot;) // 返回 \u0026quot;0.0.0.0\u0026quot; 提取子树 从Viper中提取子树。\n例如，viper实例现在代表了以下配置：\napp: cache1: max-items: 100 item-size: 64 cache2: max-items: 200 item-size: 80 执行后：\nsubv := viper.Sub(\u0026quot;app.cache1\u0026quot;) subv现在就代表：\nmax-items: 100 item-size: 64 假设我们现在有这么一个函数：\nfunc NewCache(cfg *Viper) *Cache {...} 它基于subv格式的配置信息创建缓存。现在，可以轻松地分别创建这两个缓存，如下所示：\ncfg1 := viper.Sub(\u0026quot;app.cache1\u0026quot;) cache1 := NewCache(cfg1) cfg2 := viper.Sub(\u0026quot;app.cache2\u0026quot;) cache2 := NewCache(cfg2) 反序列化 你还可以选择将所有或特定的值解析到结构体、map等。\n有两种方法可以做到这一点：\nUnmarshal(rawVal interface{}) : error UnmarshalKey(key string, rawVal interface{}) : error 举个例子：\ntype config struct { Port int Name string PathMap string `mapstructure:\u0026quot;path_map\u0026quot;` } var C config err := viper.Unmarshal(\u0026amp;C) if err != nil { t.Fatalf(\u0026quot;unable to decode into struct, %v\u0026quot;, err) } 如果你想要解析那些键本身就包含.(默认的键分隔符）的配置，你需要修改分隔符：\nv := viper.NewWithOptions(viper.KeyDelimiter(\u0026quot;::\u0026quot;)) v.SetDefault(\u0026quot;chart::values\u0026quot;, map[string]interface{}{ \u0026quot;ingress\u0026quot;: map[string]interface{}{ \u0026quot;annotations\u0026quot;: map[string]interface{}{ \u0026quot;traefik.frontend.rule.type\u0026quot;: \u0026quot;PathPrefix\u0026quot;, \u0026quot;traefik.ingress.kubernetes.io/ssl-redirect\u0026quot;: \u0026quot;true\u0026quot;, }, }, }) type config struct { Chart struct{ Values map[string]interface{} } } var C config v.Unmarshal(\u0026amp;C) Viper还支持解析到嵌入的结构体：\n/* Example config: module: enabled: true token: 89h3f98hbwf987h3f98wenf89ehf */ type config struct { Module struct { Enabled bool moduleConfig `mapstructure:\u0026quot;,squash\u0026quot;` } } // moduleConfig could be in a module specific package type moduleConfig struct { Token string } var C config err := viper.Unmarshal(\u0026amp;C) if err != nil { t.Fatalf(\u0026quot;unable to decode into struct, %v\u0026quot;, err) } Viper在后台使用github.com/mitchellh/mapstructure来解析值，其默认情况下使用mapstructuretag。\n注意 当我们需要将viper读取的配置反序列到我们定义的结构体变量中时，一定要使用mapstructuretag哦！\n序列化成字符串 你可能需要将viper中保存的所有设置序列化到一个字符串中，而不是将它们写入到一个文件中。你可以将自己喜欢的格式的序列化器与AllSettings()返回的配置一起使用。\nimport ( yaml \u0026quot;gopkg.in/yaml.v2\u0026quot; // ... ) func yamlStringSettings() string { c := viper.AllSettings() bs, err := yaml.Marshal(c) if err != nil { log.Fatalf(\u0026quot;unable to marshal config to YAML: %v\u0026quot;, err) } return string(bs) } 使用单个还是多个Viper实例? Viper是开箱即用的。你不需要配置或初始化即可开始使用Viper。由于大多数应用程序都希望使用单个中央存储库管理它们的配置信息，所以viper包提供了这个功能。它类似于单例模式。\n在上面的所有示例中，它们都以其单例风格的方法演示了如何使用viper。\n使用多个viper实例 你还可以在应用程序中创建许多不同的viper实例。每个都有自己独特的一组配置和值。每个人都可以从不同的配置文件，key value存储区等读取数据。每个都可以从不同的配置文件、键值存储等中读取。viper包支持的所有功能都被镜像为viper实例的方法。\n例如：\nx := viper.New() y := viper.New() x.SetDefault(\u0026quot;ContentDir\u0026quot;, \u0026quot;content\u0026quot;) y.SetDefault(\u0026quot;ContentDir\u0026quot;, \u0026quot;foobar\u0026quot;) //... 当使用多个viper实例时，由用户来管理不同的viper实例。\n使用Viper示例 假设我们的项目现在有一个./conf/config.yaml配置文件，内容如下：\nport: 8123 version: \u0026quot;v1.2.3\u0026quot; 接下来通过示例代码演示两种在项目中使用viper管理项目配置信息的方式。\n直接使用viper管理配置 这里用一个demo演示如何在gin框架搭建的web项目中使用viper，使用viper加载配置文件中的信息，并在代码中直接使用viper.GetXXX()方法获取对应的配置值。\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;net/http\u0026quot; \u0026quot;github.com/gin-gonic/gin\u0026quot; \u0026quot;github.com/spf13/viper\u0026quot; ) func main() { viper.SetConfigFile(\u0026quot;config.yaml\u0026quot;) // 指定配置文件 viper.AddConfigPath(\u0026quot;./conf/\u0026quot;) // 指定查找配置文件的路径 err := viper.ReadInConfig() // 读取配置信息 if err != nil { // 读取配置信息失败 panic(fmt.Errorf(\u0026quot;Fatal error config file: %s \\n\u0026quot;, err)) } // 监控配置文件变化 viper.WatchConfig() r := gin.Default() // 访问/version的返回值会随配置文件的变化而变化 r.GET(\u0026quot;/version\u0026quot;, func(c *gin.Context) { c.String(http.StatusOK, viper.GetString(\u0026quot;version\u0026quot;)) }) if err := r.Run( fmt.Sprintf(\u0026quot;:%d\u0026quot;, viper.GetInt(\u0026quot;port\u0026quot;))); err != nil { panic(err) } } 使用结构体变量保存配置信息 除了上面的用法外，我们还可以在项目中定义与配置文件对应的结构体，viper加载完配置信息后使用结构体变量保存配置信息。\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;net/http\u0026quot; \u0026quot;github.com/fsnotify/fsnotify\u0026quot; \u0026quot;github.com/gin-gonic/gin\u0026quot; \u0026quot;github.com/spf13/viper\u0026quot; ) type Config struct { Port int `mapstructure:\u0026quot;port\u0026quot;` Version string `mapstructure:\u0026quot;version\u0026quot;` } var Conf = new(Config) func main() { viper.SetConfigFile(\u0026quot;./conf/config.yaml\u0026quot;) // 指定配置文件路径 err := viper.ReadInConfig() // 读取配置信息 if err != nil { // 读取配置信息失败 panic(fmt.Errorf(\u0026quot;Fatal error config file: %s \\n\u0026quot;, err)) } // 将读取的配置信息保存至全局变量Conf if err := viper.Unmarshal(Conf); err != nil { panic(fmt.Errorf(\u0026quot;unmarshal conf failed, err:%s \\n\u0026quot;, err)) } // 监控配置文件变化 viper.WatchConfig() // 注意！！！配置文件发生变化后要同步到全局变量Conf viper.OnConfigChange(func(in fsnotify.Event) { fmt.Println(\u0026quot;夭寿啦~配置文件被人修改啦...\u0026quot;) if err := viper.Unmarshal(Conf); err != nil { panic(fmt.Errorf(\u0026quot;unmarshal conf failed, err:%s \\n\u0026quot;, err)) } }) r := gin.Default() // 访问/version的返回值会随配置文件的变化而变化 r.GET(\u0026quot;/version\u0026quot;, func(c *gin.Context) { c.String(http.StatusOK, Conf.Version) }) if err := r.Run(fmt.Sprintf(\u0026quot;:%d\u0026quot;, Conf.Port)); err != nil { panic(err) } } 参考文章 https://www.liwenzhou.com/posts/Go/viper_tutorial https://www.jianshu.com/p/7bb4f7f69280 ","date":"2021-01-23","permalink":"https://daemon365.dev/2021/01/23/viper%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86/","tags":["go","viper"],"title":"viper配置管理"},{"content":"docker的定义 Docker 最初是 dotCloud 公司创始人 Solomon Hykes 在法国期间发起的一个公司内部项目，于 2013 年 3 月以 Apache 2.0 授权协议开源，主要项目代码在 GitHub 上进行维护。 Docker 使用 Google 公司推出的 Go 语言 进行开发实现。 docker是linux容器的一种封装，提供简单易用的容器使用接口。它是最流行的Linux容器解决方案。 docker的接口相当简单，用户可以方便的创建、销毁容器。 docker将应用程序与程序的依赖，打包在一个文件里面。运行这个文件就会生成一个虚拟容器。 程序运行在虚拟容器里，如同在真实物理机上运行一样，有了docker，就不用担心环境问题了。\ndocker和虚拟机的区别 特性 容器 虚拟机 启动 秒级 分钟级 硬盘使用 一般为 MB 一般为 GB 性能 接近原生 弱 系统支持量 单机支持上千个容器 一般几十个 虚拟机也可以制作模板，基于模板创建虚拟机，保证环境问题一致\n虚拟机（virtual machine）就是带环境安装的一种解决方案。它可以在一种操作系统里面运行另一种操作系统，比如在 Windows 系统里面运行 Linux 系统。应用程序对此毫无感知，因为虚拟机看上去跟真实系统一模一样，而对于底层系统来说，虚拟机就是一个普通文件，不需要了就删掉，对其他部分毫无影响。\n虽然用户可以通过虚拟机还原软件的原始环境。但是，这个方案有几个缺点。\n资源占用多冗余 步骤多 启动慢 用上docker容器后，可以实现开发、测试和生产环境的统一化和标准化。\n镜像作为标准的交付件，可在开发、测试和生产环境上以容器来运行，最终实现三套环境上的应用以及运行所依赖内容的完全一致。\nLinux容器不是模拟一个完整的操作系统，而是对进程进行隔离。在正常进程的外面套了一个保护层，对于容器里面进程来说，它接触的资源都是虚拟的，从而实现和底层系统的隔离。\n启动快 资源占用少 体积小 docker容器的优势 更高效的利用系统资源 更快速的启动时间 持续交付和部署 更轻松的迁移 docker的三大概念 镜像 image Docker镜像就是一个只读的模板。 镜像可以用来创建Docker容器。 Docker提供了一个很简单的机制来创建镜像或者更新现有的镜像，用户甚至可以直接从其他人那里下载一个已经做好的镜像来直接使用。 镜像的分层存储\n因为镜像包含完整的root文件系统，体积是非常庞大的，因此docker在设计时按照Union FS的技术，将其设计为分层存储的架构。 镜像不是ISO那种完整的打包文件，镜像只是一个虚拟的概念，他不是一个完整的文件，而是由一组文件组成，或者多组文件系统联合组成。 容器 container 容器是镜像运行时的实体 容器可以被创建、启动、停止、删除、暂停 容器是从镜像创建的运行实例。它可以被启动、开始、停止、删除。每个容器都是相互隔离的，保证安全的平台。 Docker利用容器来运行应用。 仓库 repository 仓库是集中存放镜像文件的场所。有时候把仓库和仓库注册服务器（Registry）混为一谈，并不严格区分。实际上，仓库注册服务器上往往存放着多个仓库，每个仓库中又包含了多个镜像，每个镜像有不同的标签(tag)。 仓库分为公开仓库(Public)和私有仓库(Private)两种形式。 最大的公开仓库是Docker Hub，存放了数量庞大的镜像供用户下载。国内的公开仓库包括Docker Pool等，可以提供大陆用户更稳定快读的访问。 当用户创建了自己的镜像之后就可以使用push命令将它上传到公有或者私有仓库，这样下载在另外一台机器上使用这个镜像时候，只需需要从仓库上pull下来就可以了。 注意：Docker仓库的概念跟Git类似，注册服务器可以理解为GitHub这样的托管服务。 docker安装 卸载旧版本\nsudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine 设置存储库\nsudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo 安装docker社区版\nsudo yum install docker-ce 启动关闭docker\nsystemctl start docker docker镜像加速\nvim /etc/docker/daemon.json写以下内容\n{ \u0026quot;registry-mirrors\u0026quot;: [\u0026quot;https://kuamavit.mirror.aliyuncs.com\u0026quot;, \u0026quot;https://registry.docker-cn.com\u0026quot;,\u0026quot;https://docker.mirrors.ustc.edu.cn\u0026quot;], \u0026quot;max-concurrent-downloads\u0026quot;: 10, \u0026quot;storage-driver\u0026quot;: \u0026quot;overlay2\u0026quot;, \u0026quot;graph\u0026quot;: \u0026quot;/data/docker\u0026quot;, \u0026quot;log-driver\u0026quot;: \u0026quot;json-file\u0026quot;, \u0026quot;log-level\u0026quot;: \u0026quot;warn\u0026quot;, \u0026quot;log-opts\u0026quot;: { \u0026quot;max-size\u0026quot;: \u0026quot;10m\u0026quot;, \u0026quot;max-file\u0026quot;: \u0026quot;3\u0026quot; } } docker基本命令 选择参数:\n\u0026ndash;config=~/.dockerLocation of client config files 客户端配置文件的位置 -D, \u0026ndash;debug=falseEnable debug mode 启用Debug调试模式 -H, \u0026ndash;host=[]Daemon socket(s) to connect to 守护进程的套接字（Socket）连接 -h, \u0026ndash;help=falsePrint usage 打印使用 -l, \u0026ndash;log-level=infoSet the logging level 设置日志级别 \u0026ndash;tls=falseUse TLS; implied by\u0026ndash;tlsverify 证书 \u0026ndash;tlscacert=~/.docker/ca.pemTrust certs signed only by this CA 信任证书签名CA \u0026ndash;tlscert=~/.docker/cert.pemPath to TLS certificate file TLS证书文件路径 \u0026ndash;tlskey=~/.docker/key.pemPath to TLS key file TLS密钥文件路径 \u0026ndash;tlsverify=falseUse TLS and verify the remote 使用TLS验证远程 -v, \u0026ndash;version=falsePrint version information and quit 打印版本信息并退出 指令:\nattach Attach to a running container 当前shell下attach连接指定运行镜像 build Build an image from a Dockerfile 通过Dockerfile定制镜像 commit Create a new image from a container\u0026rsquo;s changes 提交当前容器为新的镜像 cp Copy files/folders from a container to a HOSTDIR or to STDOUT 从容器中拷贝指定文件或者目录到宿主机中 create Create a new container 创建一个新的容器，同run 但不启动容器 diff Inspect changes on a container\u0026rsquo;s filesystem 查看docker容器变化 events Get real time events from the server 从docker服务获取容器实时事件 exec Run a command in a running container 在已存在的容器上运行命令 export Export a container\u0026rsquo;s filesystem as a tar archive 导出容器的内容流作为一个tar归档文件(对应import) history Show the history of an image 展示一个镜像形成历史 images List images 列出系统当前镜像 import Import the contents from a tarball to create a filesystem image 从tar包中的内容创建一个新的文件系统映像(对应export) info Display system-wide information 显示系统相关信息 inspect Return low-level information on a container or image 查看容器详细信息 kill Kill a running container kill指定docker容器 load Load an image from a tar archive or STDIN 从一个tar包中加载一个镜像(对应save) login Register or log in to a Docker registry 注册或者登陆一个docker源服务器 logout Log out from a Docker registry 从当前Docker registry退出 logs Fetch the logs of a container 输出当前容器日志信息 pause Pause all processes within a container 暂停容器 port List port mappings or a specific mapping for the CONTAINER 查看映射端口对应的容器内部源端口 ps List containers 列出容器列表 pull Pull an image or a repository from a registry 从docker镜像源服务器拉取指定镜像或者库镜像 push Push an image or a repository to a registry 推送指定镜像或者库镜像至docker源服务器 rename Rename a container 重命名容器 restart Restart a running container 重启运行的容器 rm Remove one or more containers 移除一个或者多个容器 rmi Remove one or more images 移除一个或多个镜像(无容器使用该镜像才可以删除，否则需要删除相关容器才可以继续或者-f强制删除) run Run a command in a new container 创建一个新的容器并运行一个命令 save Save an image(s) to a tar archive 保存一个镜像为一个tar包(对应load) search Search the Docker Hub for images 在docker 镜像:\nstart Start one or more stopped containers 启动容器 stats Display a live stream of container(s) resource usage statistics 统计容器使用资源 stop Stop a running container 停止容器 tag Tag an image into a repository 给源中镜像打标签 top Display the running processes of a container 查看容器中运行的进程信息 unpause Unpause all processes within a container 取消暂停容器 version Show the Docker version information 查看容器版本号 wait Block until a container stops, then print its exit code 截取容器停止时的退出状态值 运行hello-world镜像 命令:docker run hello-world\n[root@VM-0-3-centos ~]## docker run hello-world Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world 0e03bdcc26d7: Pull complete Digest: sha256:7f0a9f93b4aa3022c3a4c147a449bf11e0941a1fd0bf4a8e6c9408b2600777c5 Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1\\. The Docker client contacted the Docker daemon. 2\\. The Docker daemon pulled the \u0026quot;hello-world\u0026quot; image from the Docker Hub. (amd64) 3\\. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4\\. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ 运行linux容器 下载imagedocker pull centos\n[root@VM-0-3-centos ~]## docker pull centos Using default tag: latest latest: Pulling from library/centos Digest: sha256:76d24f3ba3317fa945743bb3746fbaf3a0b752f10b10376960de01da70685fbd Status: Image is up to date for centos:latest docker.io/library/centos:latest 运行centosdocker run -it \u0026ndash;rm centos bash\n-i 是交互式操作，-t是终端 容器退出后将其删除 centos为镜像 指定用交互式的shell，因此需要bash命令 [root@VM-0-3-centos ~]## docker run -it --rm centos bash [root@33be613ee1eb /]## username -a bash: username: command not found [root@33be613ee1eb /]## cat /proc/version Linux version 3.10.0-1062.18.1.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) ) #1 SMP Tue Mar 17 23:49:17 UTC 2020 [root@33be613ee1eb /]## exit exit [root@VM-0-3-centos ~]# 后台模式启动docker-d后台运行容器，返回容器ID\n进入容器 使用-d参数时，容器启动后会进入后台,想进入容器操作\ndocker exec -it 容器id docker attach 容器id 提交创建自定义的镜像(docker container commit) 我们进入交互式的centos容器中，发现没有vim命令docker run -it centos\n在当前容器中，安装一个vimyum install -y vim\n安装好vim之后，exit退出容器exit\n查看刚才安装好vim的容器记录docker container ls -a\n提交这个容器，创建新的imagedocker commit 059fdea031ba zhy\n查看镜像文件\n外部访问容器 容器中可以运行网络应用，但是要让外部也可以访问这些应用，可以通过-p或-P参数指定端口映射。-P参数会随机映射端口到容器开放的网络端口 检查映射的端口docker ps -l 查看容器日志信息docker logs -f cfd不间断显示log 外部访问服务器的端口 查看指定容器的端口映射 docker port 76d\n查看容器内的进程 docker top 76d\ndocer数据卷 Docker volume使用 Docker中的数据可以存储在类似于虚拟机磁盘的介质中，在Docker中称为数据卷（Data Volume）。数据卷可以用来存储Docker应用的数据，也可以用来在Docker容器间进行数据共享。 数据卷呈现给Docker容器的形式就是一个目录，支持多个容器间共享，修改也不会影响镜像。使用Docker的数据卷，类似在系统中使用 mount 挂载一个文件系统。\n一个数据卷是一个特别指定的目录，该目录利用容器的UFS文件系统可以为容器提供一些稳定的特性或者数据共享。数据卷可以在多个容器之间共享。 创建数据卷，只要在docker run命令后面跟上-v参数即可创建一个数据卷，当然也可以跟多个-v参数来创建多个数据卷，当创建好带有数据卷的容器后， 就可以在其他容器中通过\u0026ndash;volumes-froms参数来挂载该数据卷了，而不管该容器是否运行。也可以在Dockerfile中通过VOLUME指令来增加一个或者多个数据卷。 如果有一些数据想在多个容器间共享，或者想在一些临时性的容器中使用该数据，那么最好的方案就是你创建一个数据卷容器，然后从该临时性的容器中挂载该数据卷容器的数据。这样，即使删除了刚开始的第一个数据卷容器或者中间层的数据卷容器，只要有其他容器使用数据卷，数据卷都不会被删除的。 不能使用docker export、save、cp等命令来备份数据卷的内容，因为数据卷是存在于镜像之外的。备份的方法可以是创建一个新容器，挂载数据卷容器，同时挂载一个本地目录，然后把远程数据卷容器的数据卷通过备份命令备份到映射的本地目录里面。如下：docker run -rm \u0026ndash;volumes-from DATA -v $(pwd):/backup busybox tar cvf /backup/backup.tar /data 也可以把一个本地主机的目录当做数据卷挂载在容器上，同样是在docker run后面跟-v参数，不过-v后面跟的不再是单独的目录了，它是[host-dir]:[container-dir]:[rw|ro]这样格式的，host-dir是一个绝对路径的地址，如果host-dir不存在，则docker会创建一个新的数据卷，如果host-dir存在，但是指向的是一个不存在的目录，则docker也会创建该目录，然后使用该目录做数据源。 Docker Volume数据卷可以实现：\n绕过“拷贝写”系统，以达到本地磁盘IO的性能，（比如运行一个容器，在容器中对数据卷修改内容，会直接改变宿主机上的数据卷中的内容，所以是本地磁盘IO的性能，而不是先在容器中写一份，最后还要将容器中的修改的内容拷贝出来进行同步。） 绕过“拷贝写”系统，有些文件不需要在docker commit打包进镜像文件。 数据卷可以在容器间共享和重用数据 数据卷可以在宿主和容器间共享数据 数据卷数据改变是直接修改的 数据卷是持续性的，直到没有容器使用它们。即便是初始的数据卷容器或中间层的数据卷容器删除了，只要还有其他的容器使用数据卷，那么里面的数据都不会丢失。 Docker数据持久化：\n容器在运行期间产生的数据是不会写在镜像里面的，重新用此镜像启动新的容器就会初始化镜像，会加一个全新的读写入层来保存数据。 如果想做到数据持久化，Docker提供数据卷（Data volume）或者数据容器卷来解决问题，另外还可以通过commit提交一个新的镜像来保存产生的数据。 创建一个数据卷 docker run -it \u0026ndash;name=myubuntu -v /home/flynngod/bin/MainDataVolume:/ContainerDataVolume /bin/hash\n\u0026ndash;name是给创建的容器取名 -v后的MainDataVolume是在宿主机上创建的文件夹名，用 : 将宿主机和容器的路径隔开；ContainerDataVolume是容器上的 创建完成后，在容器中touch文件test.txt并写入一行字符串： 在宿主机中可以看到同样的文件:\n同样的，如果在宿主机这端创建文件或者修改文件，容器中也会有相同的变化。如果说，容器中只能有读取文件权限，而无法修改的话，可以这么写\u0026hellip;. -v /home/flynngod/bin/MainDataVolume:/ContainerDataVolume:ro，在-v后添加:ro就行了。如果需要创建多个容器数据卷，那么久在后面再添加一个 -v + 宿主机和容器的目录路径。 数据卷的备份和还原 数据卷备份使用的命令： docker run \u0026ndash;volumes-from 存在的容器名 -v $(pwd):/backup \u0026ndash;name 新建的容器名 镜像名 tar cvf /backup/backup.tar 数据卷\nbackup.tar压缩文件只是对容器中的ContainerDataVolume文件夹的压缩。同时会生成容器的备份容器，使用docker ps -a可以查看到。\n数据卷还原的命令: docker run \u0026ndash;volumes-from 存在的容器名 -v $(pwd):/backup \u0026ndash;name 新建的容器名 镜像名 tar xvf /backup/backup.tar\n删除数据卷 Volume 只有在下列情况下才能被删除：\ndocker rm -v删除容器时添加了-v选项 docker run \u0026ndash;rm运行容器时添加了\u0026ndash;rm选项 否则，会在/var/lib/docker/volumes目录中遗留很多不明目录。\ndockerfile FROM FROM FROM指定构建镜像的基础源镜像，如果本地没有指定的镜像，则会自动从 Docker 的公共库 pull 镜像下来。 FROM必须是 Dockerfile 中非注释行的第一个指令，即一个 Dockerfile 从FROM语句开始。 如果FROM语句没有指定镜像标签，则默认使用latest标签。 FROM可以在一个 Dockerfile 中出现多次，如果有需求在一个 Dockerfile 中创建多个镜像。 MAINTAINER MAINTAINER 指定创建镜像的用户 RUN RUN \u0026quot;executable\u0026quot;, \u0026quot;param1\u0026quot;, \u0026quot;param2\u0026quot; 每条RUN指令将在当前镜像基础上执行指定命令，并提交为新的镜像，后续的RUN都在之前RUN提交后的镜像为基础，镜像是分层的，可以通过一个镜像的任何一个历史提交点来创建，类似源码的版本控制。 CMD CMD的目的是为了在启动容器时提供一个默认的命令执行选项。如果用户启动容器时指定了运行的命令，则会覆盖掉CMD指定的命令。 CMD指定在 Dockerfile 中只能使用一次，如果有多个，则只有最后一个会生效。 CMD [\u0026quot;executable\u0026quot;,\u0026quot;param1\u0026quot;,\u0026quot;param2\u0026quot;] CMD [\u0026quot;param1\u0026quot;,\u0026quot;param2\u0026quot;] CMD command param1 param2 (shell form) RUN 和CMD的区别： CMD会在启动容器的时候执行，build 时不执行。 RUN只是在构建镜像的时候执行\nEXPOSE EXPOSE [...] 告诉 Docker 服务端容器对外映射的本地端口，需要在 docker run 的时候使用-p或者-P选项生效 ENV ENV ## 只能设置一个变量 ENV = ... ## 允许一次设置多个变量 指定一个环境变量，会被后续RUN指令使用，可以在容器内被脚本或者程序调用。 ADD ADD ... ADD复制本地主机文件、目录到目标容器的文件系统中。 如果源是一个URL，该URL的内容将被下载并复制到目标容器中。 COPY COPY ... COPY复制新文件或者目录到目标容器指定路径中 。 用法和功能同ADD，区别在于不能用URL，ADD功能更强大些。 ENTRYPOINT ENTRYPOINT [\u0026quot;executable\u0026quot;, \u0026quot;param1\u0026quot;, \u0026quot;param2\u0026quot;] ENTRYPOINT command param1 param2 (shell form) 配置容器启动后执行的命令，并且不可被 docker run 提供的参数覆盖，而CMD是可以被覆盖的。如果需要覆盖，则可以使用docker run \u0026ndash;entrypoint选项。 每个 Dockerfile 中只能有一个ENTRYPOINT，当指定多个时，只有最后一个生效。 疑问: ENTRYPOINT 和 CMD 可同时存在吗？ 测试结果：可以的。 两者使用场景： ENTRYPOINT 用于稳定-不被修改的执行命令。 CMD 用于 可变的命令。\nVOLUME VOLUME [\u0026quot;/data\u0026quot;] 将本地主机目录挂载到目标容器中 将其他容器挂载的挂载点 挂载到目标容器中 USER USER mysql 指定运行容器时的用户名或 UID， 在这之后的命令如RUN、CMD、ENTRYPOINT也会使用指定用户 WORKDIR WORKDIR /path/to/workdir 切换目录，相当于cd ONBUILD ONBUILD [INSTRUCTION] 使用该dockerfile生成的镜像A，并不执行ONBUILD中命令 如再来个dockerfile 基础镜像为镜像A时，生成的镜像B时就会执行ONBUILD中的命令 docker-compose docker-compose是 docker 官方的开源项目，使用 python 编写，实现上调用了 Docker 服务的 API 进行容器管理。其官方定义为为 「定义和运行多个 Docker 容器的应用（Defining and running multi-container Docker applications）），其实就是上面所讲的功能。\n安装 sudo curl -L \u0026ldquo;https://github.com/docker/compose/releases/download/1.26.2/docker-compose-$(uname -s)-$(uname -m)\u0026rdquo; -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose docker-compose \u0026ndash;version查看 简介 类似 docker 的Dockerfile文件，docker-compose使用 YAML 文件对容器进行管理。\n对于 docker-compose 有两个基本的概念：\n服务(service)：一个应用容器，即 docker 容器，比如之前所说的mysql 容器、nginx 容器 项目(project)：由一组关联的应用容器组成的一个完整业务单元，比如上面所讲的由 mysql、web app、nginx 容器组成的网站。docker-compose 面向项目进行管理。 YAML 文件格式。\n1.大小写敏感，缩进表示表示层级关系\n2.缩进空格数不重要，相同层级左侧对齐即可。（不允许使用 tab 缩进！）\n3.由冒号分隔的键值对表示对象；一组连词线开头的行，构成一个数组；字符串默认不使用引号\ndocker-compose区域 services\n服务，在它下面可以定义应用需要的一些服务，每个服务都有自己的名字、使用的镜像、挂载的数据卷、所属的网络、依赖哪些其他服务等等。 volumes\n数据卷，在它下面可以定义的数据卷（名字等等），然后挂载到不同的服务下去使用。 networks\n应用的网络，在它下面可以定义应用的名字、使用的网络类型等等。 version: '2.0' services: nginx: restart: always image: nginx:1.11.6-alpine ports: - 8080:80 - 80:80 - 443:443 volumes: - ./conf.d:/etc/nginx/conf.d - ./log:/var/log/nginx - ./www:/var/www - /etc/letsencrypt:/etc/letsencrypt docker-compose命令 启动:docker-compose up -d-d为守护进程 查看服务进程 :docker-compose ps 停止服务:docker-compose stop [name] 启动服务:docker-compose start [name] 删除服务:docker-compose rm [name] 查看具体服务的日志:docker-compose logs -f [name] 可以进入容器内部:docker-compose exec [name] shell 说明 命令 build 构建项目中的服务容器 help 获得一个命令的帮助 kill 通过发送SIGKILL信号来强制停止服务容器 config 验证和查看compose文件配置 create 为服务创建容器。只是单纯的create，还需要使用start启动compose down 停止并删除容器，网络，镜像和数据卷 exec 在运行的容器中执行一个命令 logs 查看服务容器的输出 pause 暂停一个服务容器 port 打印某个容器端口所映射的公共端口 ps 列出项目中目前的所有容器 pull 拉取服务依赖的镜像 push 推送服务镜像 restart 重启项目中的服务 rm 删除所有（停止状态的）服务容器 run 在指定服务上执行一个命令 scale 设置指定服务运行的容器个数 start 启动已经存在的服务容器 stop 停止已经处于运行状态的容器，但不删除它 top 显示运行的进程 unpause 恢复处于暂停状态中的服务 up 自动完成包括构建镜像、创建服务、启动服务并关闭关联服务相关容器的一些列操作 参考文章:\nhttps://www.cnblogs.com/pyyu/p/9485268.html https://www.jianshu.com/p/b027c61346af https://zhuanlan.zhihu.com/p/51055141 https://www.jianshu.com/p/93a678d1bde6 ","date":"2021-01-10","permalink":"https://daemon365.dev/2021/01/10/docker/","tags":["docker"],"title":"docker"},{"content":"什么是soup 类似python中beatifulsoup，用于提取html标签提取，多用于爬虫。它可以很好的处理不规范标记并生成剖析树(parse tree)。 它提供简单又常用的导航，搜索以及修改剖析树的操作。利用它我们不在需要编写正则表达式就可以方便的实现网页信息的提取。soup是一个小型的网页提取包，其接口与beauthoulsoup非常相似。\n下载 go get github.com/anaskhan96/soup 接口 var Headers map[string]string 将头文件设置为键-值对的映射，这是单独调用Header()的替代方法 var Cookies map[string]string 将Cookie设置为键-值对的映射，这是单独调用Cookie()的另一种方法 func Get(string) (string,error) {} 将url作为参数，返回HTML字符串 func GetWithClient(string, *http.Client) {} 将url和自定义HTTP客户端作为参数，返回HTML字符串 func Post(string, string, interface{}) (string, error) {} 以url、bodyType和负载为参数，返回HTML字符串 func PostForm(string, url.Values) {} 接受url和正文。bodyType设置为“application/x-www-form-urlencoded func Header(string, string) {} 接受key，value对，将其设置为Get（）中的HTTP请求的头 func Cookie(string, string) {} 接受key，value对，将其设置为要与Get（）中的HTTP请求一起发送的Cookie func HTMLParse(string) Root {} 以HTML字符串为参数，返回一个指向构造的DOM的指针 func Find([]string) Root {} Element标记，（属性键值对）作为参数，返回指向第一个出现的指针 func FindAll([]string) []Root {} 与Find（）相同，但返回指向所有匹配项的指针 func FindStrict([]string) Root {} Element tag，（attribute key-value pair）作为参数，指向第一次出现的指针返回了完全匹配的值 func FindAllStrict([]string) []Root {} 与FindStrict（）相同，但指向返回的所有引用的指针 func FindNextSibling() Root {} find指向同一个functing}元素的下一个functing}指针 func FindNextElementSibling() Root {} 指向返回的DOM中元素的下一个同级元素的指针 func FindPrevSibling() Root {} 指向返回的DOM中元素的上一个同级的指针 func FindPrevElementSibling() Root {} 指向返回的DOM中元素的上一个同级元素的指针 func Children() []Root {} 查找此DOM元素的所有直接子级 func Attrs() map[string]string {} map返回元素的所有属性作为对其各自值的查找 func Text() string {} 返回非嵌套标记内的全文，在嵌套标记中返回前半部分e func FullText() string {} 返回嵌套/非嵌套标记内的全文 func SetDebug(bool) {} 将调试模式设置为true或false；默认为false func HTML() {} HTML返回特定元素的HTML代码 例子 package main import ( \u0026quot;fmt\u0026quot; \u0026quot;os\u0026quot; \u0026quot;github.com/anaskhan96/soup\u0026quot; ) func main() { resp, err := soup.Get(\u0026quot;http://zhaohaiyu.com\u0026quot;) if err != nil { os.Exit(1) } doc := soup.HTMLParse(resp) links := doc.Find(\u0026quot;div\u0026quot;, \u0026quot;class\u0026quot;, \u0026quot;res-cons\u0026quot;).FindAll(\u0026quot;article\u0026quot;,\u0026quot;class\u0026quot;,\u0026quot;post\u0026quot;) fmt.Println(links) for _, link := range links { l := link.Find(\u0026quot;a\u0026quot;) fmt.Println(l.Text(), \u0026quot;--------\u0026gt;\u0026quot;, l.Attrs()[\u0026quot;href\u0026quot;]) } } 结果\n【置顶】golang目录 --------\u0026gt; https://zhaohaiyu.com/post/go/go_catalog/ go语言文件系统 --------\u0026gt; https://zhaohaiyu.com/post/go/go_file/ Flex --------\u0026gt; https://zhaohaiyu.com/post/javascript/flex/ makefile --------\u0026gt; https://zhaohaiyu.com/post/go/makefile/ air热加载 --------\u0026gt; https://zhaohaiyu.com/post/go/air/ thrift的介绍及其使用 --------\u0026gt; https://zhaohaiyu.com/post/go/thrift/ golang中间件的实现 --------\u0026gt; https://zhaohaiyu.com/post/go/middleware/ zap高性能日志 --------\u0026gt; https://zhaohaiyu.com/post/go/zap/ viper配置管理 --------\u0026gt; https://zhaohaiyu.com/post/go/viper/ proto Prometheus --------\u0026gt; https://zhaohaiyu.com/post/go/promethues/ ","date":"2021-01-05","permalink":"https://daemon365.dev/2021/01/05/golanghtml%E6%A0%87%E7%AD%BE%E6%8F%90%E5%8F%96%E5%99%A8soup/","tags":["go"],"title":"golangHTML标签提取器soup"},{"content":"什么是thrift Thrift是Facebook于2007年开发的跨语言的rpc服框架，提供多语言的编译功能，并提供多种服务器工作模式；用户通过Thrift的IDL（接口定义语言）来描述接口函数及数据类型，然后通过Thrift的编译环境生成各种语言类型的接口文件，用户可以根据自己的需要采用不同的语言开发客户端代码和服务器端代码。\n例如，我想开发一个快速计算的RPC服务，它主要通过接口函数getInt对外提供服务，这个RPC服务的getInt函数使用用户传入的参数，经过复杂的计算，计算出一个整形值返回给用户；服务器端使用java语言开发，而调用客户端可以是java、c、python等语言开发的程序，在这种应用场景下，我们只需要使用Thrift的IDL描述一下getInt函数（以.thrift为后缀的文件），然后使用Thrift的多语言编译功能，将这个IDL文件编译成C、java、python几种语言对应的“特定语言接口文件”（每种语言只需要一条简单的命令即可编译完成），这样拿到对应语言的“特定语言接口文件”之后，就可以开发客户端和服务器端的代码了，开发过程中只要接口不变，客户端和服务器端的开发可以独立的进行。\nThrift为服务器端程序提供了很多的工作模式，例如：线程池模型、非阻塞模型等等，可以根据自己的实际应用场景选择一种工作模式高效地对外提供服务；\nThrift的官方网站：http://thrift.apache.org/\nthrift的协议结构 Thrift是一种c/s的架构体系。TServer主要任务是高效的接受客户端请求，并将请求转发给Processor处理。\n最上层是用户自行实现的业务逻辑代码； Processor是由thrift编译器自动生成的代码，它封装了从输入数据流中读数据和向数据流中写数据的操作，它的主要工作是：从连接中读取数据，把处理交给用户实现impl，最后把结果写到连接上。 TProtocol是用于数据类型解析的，将结构化数据转化为字节流给TTransport进行传输。从TProtocol以下部分是thirft的传输协议和底层I/O通信。 TTransport是与底层数据传输密切相关的传输层，负责以字节流方式接收和发送消息体，不关注是什么数据类型。 底层IO负责实际的数据传输，包括socket、文件和压缩数据流等。 下载 下载thrift编译软件:\nMacOs:brew install thrift Windows:Thrift官方下载地址：http://thrift.apache.org/download golang下载: go get github.com/apache/thrift/lib/go/thrift\npython下载: sudo pip install thrift\nIDL文件编写 thrift 采用IDL（Interface Definition Language）来定义通用的服务接口，并通过生成不同的语言代理实现来达到跨语言、平台的功能。在thrift的IDL中可以定义以下一些类型：基本数据类型，结构体，容器，异常、服务\n基本类型 bool: 布尔值 (true or false), one byte byte: 有符号字节 i16: 16位有符号整型 i32: 32位有符号整型 i64: 64位有符号整型 double: 64位浮点型 string: Encoding agnostic text or binary string 基本类型中基本都是有符号数，因为有些语言没有无符号数，所以Thrift不支持无符号整型。\n特殊类型 binary: Blob (byte array) a sequence of unencoded bytes\n这是string类型的一种变形，主要是为Java使用，目前我主要使用C++的语言，所以java的这个类型没有用过\nstruct thrift中struct是定义为一种对象，和面向对象语言的class差不多.,但是struct有以下一些约束：\nstruct不能继承，但是可以嵌套，不能嵌套自己。 其成员都是有明确类型 成员是被正整数编号过的，其中的编号使不能重复的，这个是为了在传输过程中编码使用。 成员分割符可以是逗号（,）或是分号（;），而且可以混用，但是为了清晰期间，建议在定义中只使用一种，比如C++学习者可以就使用分号（;）。 字段会有optional和required之分和protobuf一样，但是如果不指定则为无类型—可以不填充该值，但是在序列化传输的时候也会序列化进去，optional是不填充则部序列化，required是必须填充也必须序列化。 每个字段可以设置默认值 同一文件可以定义多个struct，也可以定义在不同的文件，进行include引入。 数字标签作用非常大，但是随着项目开发的不断发展，也许字段会有变化，但是建议不要轻易修改这些数字标签，修改之后如果没有同步客户端和服务器端会让一方解析出问题。\nstruct Report { 1: required string msg, //改字段必须填写 2: optional i32 type = 0; //默认值 3: i32 time //默认字段类型为optional } 规范的struct定义中的每个域均会使用required或者 optional关键字进行标识。如果required标识的域没有赋值，Thrift将给予提示；如果optional标识的域没有赋值，该域将不会被序列化传输；如果某个optional标识域有缺省值而用户没有重新赋值，则该域的值一直为缺省值；如果某个optional标识域有缺省值或者用户已经重新赋值，而不设置它的__isset为true，也不会被序列化传输。\n容器（Containers） Thrift容器与目前流行编程语言的容器类型相对应，有3种可用容器类型：\nlist: 元素类型为t的有序表，容许元素重复。对应c++的vector，java的ArrayList或者其他语言的数组（官方文档说是ordered list不知道如何理解？排序的？c++的vector不排序） set:元素类型为t的无序表，不容许元素重复。对应c++中的set，java中的HashSet,python中的set，php中没有set，则转换为list类型了 map\u0026lt;t,t\u0026gt;: 键类型为t，值类型为t的kv对，键不容许重复。对用c++中的map, Java的HashMap, PHP 对应 array, Python/Ruby 的dictionary。 容器中元素类型可以是除了service外的任何合法Thrift类型（包括结构体和异常）。为了最大的兼容性，map的key最好是thrift的基本类型，有些语言不支持复杂类型的key，JSON协议只支持那些基本类型的key。\n容器都是同构容器，不失异构容器。\n例子\nstruct Test { 1: map\u0026lt;Numberz, UserId\u0026gt; user_map, 2: set\u0026lt;Numberz\u0026gt; num_sets, 3: list\u0026lt;Stusers\u0026gt; users } 枚举（enmu） 很多语言都有枚举，意义都一样。比如，当定义一个消息类型时，它只能是预定义的值列表中的一个，可以用枚举实现。说明：\n编译器默认从0开始赋值 可以赋予某个常量某个整数 允许常量是十六进制整数 末尾没有分号 给常量赋缺省值时，使用常量的全称 注意，不同于protocal buffer，thrift不支持枚举类嵌套，枚举常量必须是32位的正整数\nenum EnOpType {CMD_OK = 0, // (0) CMD_NOT_EXIT = 2000, // (2000) CMD_EXIT = 2001, // (2001) CMD_ADD = 2002 // (2002) } struct StUser { 1: required i32 userId; 2: required string userName; 3: optional EnOpType cmd_code = EnOpType.CMD_OK; // (0) 4: optional string language = “english” } 常量定义和类型定义 Thrift允许定义跨语言使用的常量，复杂的类型和结构体可使用JSON形式表示。\nconst i32 INT_CONST = 1234; const EnOpType myEnOpType = EnOpType.CMD_EXIT; //2001 说明：分号可有可无。支持16进制。　Thrift支持C/C++类型定义。\ntypedef i32 MyInteger // a typedef StUser ReU // b typedef i64 UserId 说明：a.末尾没有逗号。b. Struct也可以使用typedef。\n异常（Exceptions） Thrift结构体在概念上类似于（similar to）C语言结构体类型—将相关属性封装在一起的简便方式。Thrift结构体将会被转换成面向对象语言的类。\n异常在语法和功能上类似于结构体，差别是异常使用关键字exception，而且异常是继承每种语言的基础异常类。\nexception Extest { 1: i32 errorCode, 2: string message, 3: StUser userinfo } 服务（Services） 服务的定义方法在语义(semantically)上等同于面向对象语言中的接口。Thrift编译器会产生执行这些接口的client和server stub。具体参见下一节。\n在流行的序列化/反序列化框架（如protocal buffer）中，Thrift是少有的提供多语言间RPC服务的框架。这是Thrift的一大特色。\nThrift编译器会根据选择的目标语言为server产生服务接口代码，为client产生stubs。\nservice SeTest { void ping(), bool postTweet(1: StUser user); StUser searchTweets(1:string name); oneway void zip() }　首先所有的方法都是纯虚汗数，也就是继承类必须实现这些方法 返回值不是基本类型的都把返回值放到了函数参数中第一个参数，命名_return 所有的参数（除返回值）都是const类型，意味这函数一般参数无法作为返回值携带者。只会有一个返回参数，如果返回值有多个，那只能封装复杂类型作为返回值参数。 oneway的返回值一定是void 当然服务是支持继承。 服务不支持重载 名字空间（Namespace） Thrift中的命名空间类似于C++中的namespace和java中的package，它们提供了一种组织（隔离）代码的简便方式。名字空间也可以用于解决类型定义中的名字冲突。\n由于每种语言均有自己的命名空间定义方式（如Python中有module）, thrift允许开发者针对特定语言定义namespace：\nnamespace go com.example.test namespace py com.example.test namespace php com.example.test 注释（Comment） Thrift支持C多行风格和Java/C++单行风格。\n/* * This is a multi-line comment. * Just like in C. */ // C++/Java style single-line comments work just as well. 11Includes 便于管理、重用和提高模块性/组织性，我们常常分割Thrift定义在不同的文件中。包含文件搜索方式与c++一样。Thrift允许文件包含其它thrift文件，用户需要使用thrift文件名作为前缀访问被包含的对象，如：\ninclude \u0026quot;test.thrift\u0026quot; ... struct StSearchResult { 1: in32 uid; ... } thrift文件名要用双引号包含，末尾没有逗号或者分号\nThrift支持的传输及服务模型 支持的传输格式： 参数: 描述: TBinaryProtocol 二进制格式 TCompactProtocol 压缩格式 TJSONProtocol JSON格式 TSimpleJSONProtocol 提供JSON只写协议, 生成的文件很容易通过脚本语言解析 TDebugProtocol 使用易懂的可读的文本格式，以便于debug 支持的数据传输方式： 参数: 描述: TSocket 阻塞式socker TFramedTransport 以frame为单位进行传输，非阻塞式服务中使用 TFileTransport 以文件形式进行传输 TMemoryTransport 将内存用于I/O. java实现时内部实际使用了简单的ByteArrayOutputStream TZlibTransport 使用zlib进行压缩， 与其他传输方式联合使用。当前无java实现 支持的服务模型： 参数: 描述: TSimpleServer 简单的单线程服务模型，常用于测试 TThreadPoolServer 多线程服务模型，使用标准的阻塞式IO TNonblockingServer 多线程服务模型，使用非阻塞式IO（需使用TFramedTransport数据传输方式） 代码实现 编写*.thrift文件 namespace go example struct Data { 1: string text } service format_data { Data do_format(1:Data data), } 解析成go代码:thrift --out . --gen go example.thrift\n解析成python代码:thrift --out . --gen py example.thrift\ngolang服务端 服务端的实现:\nHandler 服务端业务处理逻辑。这里就是业务代码，比如 计算两个字符串 相似度 Processor 从Thrift框架 转移到 业务处理逻辑。因此是RPC调用，客户端要把 参数发送给服务端，而这一切由Thrift封装起来了，由Processor将收到的“数据”转交给业务逻辑去处理 Protocol 数据的序列化与反序列化。客户端提供的是“字符串”，而数据传输是一个个的字节，因此会用到序列化与反序列化。 Transport 传输层的数据传输。 TServer 服务端的类型。服务器以何种方式来处理客户端请求 TSimpleServer —— 单线程服务器端使用标准的阻塞式 I/O TThreadPoolServer —— 多线程服务器端使用标准的阻塞式 I/O TNonblockingServer —— 多线程服务器端使用非阻塞式 I/O /* server.go */ package main import ( \u0026quot;context\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;log\u0026quot; \u0026quot;test/example\u0026quot; \u0026quot;github.com/apache/thrift/lib/go/thrift\u0026quot; ) type FormatDataImpl struct{} func (fdi *FormatDataImpl) DoFormat(ctx context.Context) (r string, err error) { return \u0026quot;不好\u0026quot;, nil } const ( HOST = \u0026quot;127.0.0.1\u0026quot; PORT = \u0026quot;8080\u0026quot; ) func main() { handler := \u0026amp;FormatDataImpl{} processor := example.NewFormatDataProcessor(handler) serverTransport, err := thrift.NewTServerSocket(HOST + \u0026quot;:\u0026quot; + PORT) if err != nil { log.Fatalln(\u0026quot;Error:\u0026quot;, err) } transportFactory := thrift.NewTBufferedTransportFactory(10000000) protocolFactory := thrift.NewTBinaryProtocolFactoryDefault() server := thrift.NewTSimpleServer4(processor, serverTransport, transportFactory, protocolFactory) fmt.Println(\u0026quot;Running at:\u0026quot;, HOST+\u0026quot;:\u0026quot;+PORT) err = server.Serve() if err != nil { log.Fatalln(\u0026quot;Error:\u0026quot;, err) } } python服务端 from thrift.protocol import TBinaryProtocol from thrift.server import TServer from thrift.transport import TSocket, TTransport from example import format_data __HOST = '127.0.0.1' __PORT = 8080 class FormatDataImpl: def do_format(self): return \u0026quot;你好\u0026quot; if __name__ == '__main__': handler = FormatDataImpl() processor = format_data.Processor(handler) transport = TSocket.TServerSocket('127.0.0.1', 8080) tfactory = TTransport.TBufferedTransportFactory() pfactory = TBinaryProtocol.TBinaryProtocolFactory() server = TServer.TSimpleServer(processor, transport, tfactory, pfactory) print(\u0026quot;Starting python server...\u0026quot;) server.serve() print(\u0026quot;done!\u0026quot;) golang客户端 package main import ( \u0026quot;context\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;log\u0026quot; \u0026quot;net\u0026quot; \u0026quot;test/example\u0026quot; \u0026quot;github.com/apache/thrift/lib/go/thrift\u0026quot; ) const ( HOST = \u0026quot;127.0.0.1\u0026quot; PORT = \u0026quot;8080\u0026quot; ) func main() { tSocket, err := thrift.NewTSocket(net.JoinHostPort(HOST, PORT)) if err != nil { log.Fatalln(\u0026quot;tSocket error:\u0026quot;, err) } transport := thrift.NewTBufferedTransport(tSocket,10000000) // transport, err := transportFactory.Gwt(tSocket) // if err != nil { // log.Fatalln(\u0026quot;GetTransport error:\u0026quot;, err) // } protocolFactory := thrift.NewTBinaryProtocolFactoryDefault() client := example.NewFormatDataClientFactory(transport, protocolFactory) if err := transport.Open(); err != nil { log.Fatalln(\u0026quot;Error opening:\u0026quot;, HOST+\u0026quot;:\u0026quot;+PORT) } defer transport.Close() // data := example.Data{Text: \u0026quot;hello,world!as赵海宇\u0026quot;} d, err := client.DoFormat(context.TODO() ) fmt.Println(d) } 5.python客户端\nfrom example import format_data from thrift import Thrift from thrift.transport import TSocket from thrift.transport import TTransport from thrift.protocol import TBinaryProtocol try: # Make socket transport = TSocket.TSocket('127.0.0.1', 8080) # Buffering is critical. Raw sockets are very slow transport = TTransport.TBufferedTransport(transport) # Wrap in a protocol protocol = TBinaryProtocol.TBinaryProtocol(transport) # Create a client to use the protocol encoder client = format_data.Client(protocol) # Connect! transport.open() # data = format_data.Data(text=\u0026quot;sada\u0026quot;) d = client.do_format() print(d) transport.close() except Thrift.TException as tx: print(tx.message) 常见的坑 golang中倒入包路径有github.com/apache/thrift/lib/go/thrift和git.apache.org/thrift.git/lib/go/thrift 要统一用一个,不然会出问题 golang中 thrift的新版实现的结构体多一个ctx context.Context参数.(老版本没有,网上很多的demo代码用新版的thrift会报错) 客户端以及服务端对应好传输及服务模型协议,不同会报错. 参考文章:\nhttps://www.cnblogs.com/hapjin/p/8075721.html https://blog.csdn.net/houjixin/article/details/42778335 https://www.jianshu.com/p/4723ce380b0e ","date":"2020-12-30","permalink":"https://daemon365.dev/2020/12/30/thrift%E7%9A%84%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%85%B6%E4%BD%BF%E7%94%A8/","tags":["thrift","go"],"title":"thrift的介绍及其使用"},{"content":"前言 在后台服务开发中，高可用性是构建中核心且重要的一环。服务发现（Service discovery）和负载均衡（Load Balance）一直都是我关注的话题。今天来谈一下我在实际中是如何理解及落地的。\n负载均衡 \u0026amp;\u0026amp; 服务发现 基础 负载均衡 ，顾名思义，是通过某种手段将流量 / 请求分配到不通的服务器上去，保证后台的每个服务收到的请求都尽可能保持平衡 服务发现 ，就是指客户端按照某种约定的方式主动去（注册中心）寻找服务，然后再连接相应的服务 关于负载均衡的构建与实现，可以看下这几篇文章：\ngRPC 服务发现 \u0026amp; 负载均衡 gRPC Load Balancing Load Balancing in gRPC 服务发现概念 我们说的服务发现，一般理解为客户端如何发现 (并连接到) 服务，这里一般包含三个组件：\n服务消费者：一般指客户端（可以是简单的 TCP-Client 或者是 RPC-Client ） 服务提供者：一般指服务提供方，如传统服务，微服务等 服务注册中心：用来存储（Key-Value）服务提供者的服务，一般以 DNS/HTTP/RPC 等方式对外暴露接口 负载均衡概念 我们把 LB 看作一个组件，根据组件位置的不同，大致上分为三种：\n集中式 LB（Proxy Model） 独立的 LB, 可以是硬件实现，如 F5，或者是 nginx 这种内置 Proxy-pass 或者 upstream 功能的网关，亦或是 LVS/HAPROXY，之前也使用 DPDK 开发过类似的专用网关。\n进程内 LB（Balancing-aware Client） 进程内 LB（集成到客户端），此方案将 LB 的功能集成到服务消费方进程里，也被称为软负载或者客户端负载方案。服务提供方启动时，首先将服务地址注册到服务注册表，同时定期报心跳到服务注册表以表明服务的存活状态，相当于健康检查，服务消费方要访问某个服务时，它通过内置的 LB 组件向服务注册表查询，同时缓存并定期刷新目标服务地址列表，然后以某种负载均衡策略选择一个目标服务地址，最后向目标服务发起请求。LB 和服务发现能力被分散到每一个服务消费者的进程内部，同时服务消费方和服务提供方之间是直接调用，没有额外开销，性能比较好。\n独立 LB 进程（External Load Balancing Service） 该方案是针对上一种方案的不足而提出的一种折中方案，原理和第二种方案基本类似。不同之处是将 LB 和服务发现功能从进程内移出来，变成主机上的一个独立进程。主机上的一个或者多个服务要访问目标服务时，他们都通过同一主机上的独立 LB 进程做服务发现和负载均衡。该方案也是一种分布式方案没有单点问题，一个 LB 进程挂了只影响该主机上的服务调用方，服务调用方和 LB 之间是进程内调用性能好，同时该方案还简化了服务调用方，不需要为不同语言开发客户库，LB 的升级不需要服务调用方改代码。 公司的 L5 是这种方式，每台机器上都安装了 L5 的 agent，供其他服务调用。该方案主要问题：部署较复杂，环节多，出错调试排查问题不方便。\ngRPC 内置的方案 gRPC 的内置方案如下图所示：\ngRPC 在官网文档中提供了实现 LB 的思路，并在不同语言的 gRPC 代码 API 中已提供了命名解析和负载均衡接口供扩展。默认提供了 DNS-resolver 的实现，接口相当规范，实现起来也不复杂，只需要实现服务注册（Registry）和服务监听 + 解析（Watcher+Resolver）的逻辑就行了，这里简单介绍其基本实现过程：\n构建注册中心，这里注册中心一般要求具备分布式一致性（满足 CAP 定理的 AP 或 CP）的高可用的组件集群，如 Zookeeper、Consul、Etcd 等 构建 gRPC 服务端的注册逻辑，服务启动后定时向注册中心注册自身的关键信息（一般开启新的 groutine 来完成），至少包含 IP 和端口，其他可选信息，如自身的负载信息（CPU 和 Memory）、当前实时连接数等，这些辅助信息有助于帮助系统更好的执行 LB 算法 gRPC 客户端向注册中心发出服务解析请求，注册中心将请求中关联的所有服务的信息返回给 gRPC 客户端，客户端与所有在线的服务建立起 HTTP2 长连接 gRPC 客户端发起 RPC 调用，根据 LB 均衡器中实现的负载均衡策略（gRPC 中默认提供的算法是 RoundRobin），选择其中一 HTTP2 长连接进行通信，即 LB 策略决定哪个子通道 - 即哪个 gRPC 服务器将接收请求 gRPC 负载均衡的运行机制 gRPC 提供了负载均衡实现的用户侧接口，我们可以非常方便的定制化业务的负载均衡策略，为了理解 gRPC 的负载均衡的实现机制，后续博客中我会分析下 gRPC 实现负载均衡的代码。\nResolver 解析器，用于从注册中心实时获取当前服务端的列表，同步发送给 Balancer Balancer 平衡器，一是接收从 Resolver 发送的服务端列表，建立并维护（长）连接状态；二是每次当 Client 发起 Rpc 调用时，按照一定算法从连接池中选择一个连接进行 Rpc 调用 Register 注册，用于服务端初始化和在线时，将自己信息上报到注册中心，主要信息有 Ip，端口等 负载均衡的算法及实现 在实践中，如何选取负载均衡策略是一个很有趣的话题，例如 Nginx 的 upstream 机制中就有很多经典的 LB 策略，如带权重的轮询 Weight-RoundRobin，一般常用的负载均衡方法有如下几种：\nRoundRobin（轮询） Weight-RoundRobin（加权轮询） 不同的后端服务器可能机器的配置和当前系统的负载并不相同，因此它们的抗压能力也不相同。给配置高、负载低的机器配置更高的权重，而配置低、负载高的机器，给其分配较低的权重，降低其系统负载，加权轮询能很好地处理这一问题，并将请求顺序且按照权重分配到后端。 Random（随机） Weight-Random（加权随机） 通过系统的随机算法，根据后端服务器的列表随机选取其中的一台服务器进行访问 源地址哈希法 源地址哈希的思想是根据获取客户端的 IP 地址，通过哈希函数计算得到的一个数值，用该数值对服务器列表的大小进行取模运算，得到的结果便是客服端要访问服务器的序号。采用源地址哈希法进行负载均衡，同一 IP 地址的客户端，当后端服务器列表不变时，它每次都会映射到同一台后端服务器进行访问 最小连接数法 最小连接数算法比较灵活和智能，由于后端服务器的配置不尽相同，对于请求的处理有快有慢，它是根据后端服务器当前的连接情况，动态地选取其中当前积压连接数最少的一台服务器来处理当前的请求，尽可能地提高后端服务的利用效率，将负责合理地分流到每一台服务器 一致性哈希算法 常见的是 Ketama 算法，该算法是用来解决 cache 失效导致的缓存穿透的问题的，当然也可以适用于 gRPC 长连接的场景 gRPC 服务治理的优势 在现网环境中，后端服务就是采用了 gRPC 与 Etcd 的服务治理方案，总结下有这么几个优点；\n采用了 gRPC 实现负载均衡策略，模块之间通信采用长连接方式，避免每次 RPC 调用时新建连接的开销，充分发挥 HTTP2 的优势 扩容和缩容都及其方便，例如扩容，只要部署上服务，运行后，服务成功注册到 Etcd 便大功告成 灵活的自定义的 LB 算法，使得后端压力更为均衡 客户端加入重试逻辑，使得网络抖动情况下，可以通过重试连接上另外一台服务 Resolver 暴露的三个接口 前文说过，gRPC 内置的服务治理功能，对开发者暴露了服务发现的 interface{}，resolver.Builder 和 resolver.ClientConn 和 resolver.Resolver，相关代码。开发者在实例化这三个接口之后，就可以实现从指定的 scheme 中获取服务列表，通知 balancer 并与这些服务端建立 RPC 长连接。\nresolver.Builder resolver.ClientConn resolver.Resolver resolver.Builder Builder 用于 gRPC 内部创建 Resolver 接口的实现，但注意内部声明的 Build() 方法将接口 ClientConn 作为参数传入了，在前文的分析中，我们了解到 ClientConn结库 是非常重要的结构，其成员 conns map[*addrConn]struct{} 中维护了所有从注册中心获取到的服务端列表。 // Builder creates a resolver that will be used to watch name resolution updates. type Builder interface { // Build creates a new resolver for the given target. // // gRPC dial calls Build synchronously, and fails if the returned error is // not nil. Build(target Target, cc ClientConn, opts BuildOption) (Resolver, error) // Scheme returns the scheme supported by this resolver. // Scheme is defined at https://github.com/grpc/grpc/blob/master/doc/naming.md. Scheme() string } resolver.ClientConn ClientConn 接口中，UpdateState 方法需要传入 State 结构，NewAddress 方法需要传入 Address 结构，看代码可以发现其中包含了 Addresses []Address // Resolved addresses for the target，可以看出是需要将服务发现得到的 Address 对象列表告诉 ClientConn 的对象。 // ClientConn contains the callbacks for resolver to notify any updates // to the gRPC ClientConn. // // This interface is to be implemented by gRPC. Users should not need a // brand new implementation of this interface. For the situations like // testing, the new implementation should embed this interface. This allows // gRPC to add new methods to this interface. type ClientConn interface { // UpdateState updates the state of the ClientConn appropriately. UpdateState(State) // NewAddress is called by resolver to notify ClientConn a new list // of resolved addresses. // The address list should be the complete list of resolved addresses. // // Deprecated: Use UpdateState instead. NewAddress(addresses []Address) // NewServiceConfig is called by resolver to notify ClientConn a new // service config. The service config should be provided as a json string. // // Deprecated: Use UpdateState instead. NewServiceConfig(serviceConfig string) } resolver.Resolver Resolver 提供了 ResolveNow 用于被 gRPC 尝试重新进行服务发现 // Resolver watches for the updates on the specified target. // Updates include address updates and service config updates. type Resolver interface { // ResolveNow will be called by gRPC to try to resolve the target name // again. It's just a hint, resolver can ignore this if it's not necessary. // // It could be called multiple times concurrently. ResolveNow(ResolveNowOption) // Close closes the resolver. Close() } 梳理 Resolver 过程 通过这三个接口，再次梳理下 gRPC 的服务发现实现逻辑\n通过 Builder.Build() 进行 Reslover 的创建，在 Build() 的过程中将服务发现的地址信息丢给 ClientConn 用于内部连接创建（通过 ClientConn.UpdateState() 实现）等逻辑； 当 client 在 Dial 时会根据 target 解析的 scheme 获取对应的 Builder，代码位置 func DialContext(ctx context.Context, target string, opts ...DialOption) (conn *ClientConn, err error) { ... ... // Determine the resolver to use. cc.parsedTarget = parseTarget(cc.target) grpclog.Infof(\u0026quot;parsed scheme: %q\u0026quot;, cc.parsedTarget.Scheme) resolverBuilder := cc.getResolver(cc.parsedTarget.Scheme)\t// 通过 scheme(名字) 获取对应的 resolver if resolverBuilder == nil { // If resolver builder is still nil, the parsed target's scheme is // not registered. Fallback to default resolver and set Endpoint to // the original target. grpclog.Infof(\u0026quot;scheme %q not registered, fallback to default scheme\u0026quot;, cc.parsedTarget.Scheme) cc.parsedTarget = resolver.Target{ Scheme: resolver.GetDefaultScheme(), Endpoint: target, } resolverBuilder = cc.getResolver(cc.parsedTarget.Scheme) if resolverBuilder == nil { return nil, fmt.Errorf(\u0026quot;could not get resolver for default scheme: %q\u0026quot;, cc.parsedTarget.Scheme) } } ... ... // Build the resolver. rWrapper, err := newCCResolverWrapper(cc, resolverBuilder)\t// 通过 gRPC 提供的 Wrapper，应用我们实现的 resolver 逻辑 if err != nil { return nil, fmt.Errorf(\u0026quot;failed to build resolver: %v\u0026quot;, err) } cc.mu.Lock() cc.resolverWrapper = rWrapper cc.mu.Unlock() ... ... } 当 Dial 成功会创建出结构体 ClientConn 的对象 官方代码位置(注意不是上面的 ClientConn 接口)，可以看到结构体 ClientConn 内的成员 resolverWrapper 又实现了接口 ClientConn 的方法 官方代码位置 func DialContext(ctx context.Context, target string, opts ...DialOption) (conn *ClientConn, err error) { // 初始化 CC cc := \u0026amp;ClientConn{ target: target, csMgr: \u0026amp;connectivityStateManager{}, conns: make(map[*addrConn]struct{}), dopts: defaultDialOptions(), blockingpicker: newPickerWrapper(), czData: new(channelzData), firstResolveEvent: grpcsync.NewEvent(), } ... ... ... ... return cc, nil } 当 resolverWrapper 被初始化时就会调用 Build 方法 官方代码位置，其中参数为接口 ClientConn 传入的是 ccResolverWrapper // newCCResolverWrapper uses the resolver.Builder to build a Resolver and // returns a ccResolverWrapper object which wraps the newly built resolver. func newCCResolverWrapper(cc *ClientConn, rb resolver.Builder) (*ccResolverWrapper, error) { ccr := \u0026amp;ccResolverWrapper{ cc: cc, done: grpcsync.NewEvent(), } var credsClone credentials.TransportCredentials if creds := cc.dopts.copts.TransportCredentials; creds != nil { credsClone = creds.Clone() } rbo := resolver.BuildOptions{ DisableServiceConfig: cc.dopts.disableServiceConfig, DialCreds: credsClone, CredsBundle: cc.dopts.copts.CredsBundle, Dialer: cc.dopts.copts.Dialer, } var err error // We need to hold the lock here while we assign to the ccr.resolver field // to guard against a data race caused by the following code path, // rb.Build--\u0026gt;ccr.ReportError--\u0026gt;ccr.poll--\u0026gt;ccr.resolveNow, would end up // accessing ccr.resolver which is being assigned here. ccr.resolverMu.Lock() defer ccr.resolverMu.Unlock() ccr.resolver, err = rb.Build(cc.parsedTarget, ccr, rbo) if err != nil { return nil, err } return ccr, nil } 当用户基于 Builder 的实现进行 UpdateState 调用时，则会触发结构体 ClientConn 的 updateResolverState 方法 官方代码位置，updateResolverState 则会对传入的 Address 进行初始化等逻辑 官方代码位置 func (cc *ClientConn) updateResolverState(s resolver.State, err error) error { defer cc.firstResolveEvent.Fire() cc.mu.Lock() // Check if the ClientConn is already closed. Some fields (e.g. // balancerWrapper) are set to nil when closing the ClientConn, and could // cause nil pointer panic if we don't have this check. if cc.conns == nil { cc.mu.Unlock() return nil } if err != nil { // May need to apply the initial service config in case the resolver // doesn't support service configs, or doesn't provide a service config // with the new addresses. cc.maybeApplyDefaultServiceConfig(nil) if cc.balancerWrapper != nil { cc.balancerWrapper.resolverError(err) } // No addresses are valid with err set; return early. cc.mu.Unlock() return balancer.ErrBadResolverState } var ret error if cc.dopts.disableServiceConfig || s.ServiceConfig == nil { cc.maybeApplyDefaultServiceConfig(s.Addresses) // TODO: do we need to apply a failing LB policy if there is no // default, per the error handling design? } else { if sc, ok := s.ServiceConfig.Config.(*ServiceConfig); s.ServiceConfig.Err == nil \u0026amp;\u0026amp; ok { cc.applyServiceConfigAndBalancer(sc, s.Addresses) } else { ret = balancer.ErrBadResolverState if cc.balancerWrapper == nil { var err error if s.ServiceConfig.Err != nil { err = status.Errorf(codes.Unavailable, \u0026quot;error parsing service config: %v\u0026quot;, s.ServiceConfig.Err) } else { err = status.Errorf(codes.Unavailable, \u0026quot;illegal service config type: %T\u0026quot;, s.ServiceConfig.Config) } cc.blockingpicker.updatePicker(base.NewErrPicker(err)) cc.csMgr.updateState(connectivity.TransientFailure) cc.mu.Unlock() return ret } } } var balCfg serviceconfig.LoadBalancingConfig if cc.dopts.balancerBuilder == nil \u0026amp;\u0026amp; cc.sc != nil \u0026amp;\u0026amp; cc.sc.lbConfig != nil { balCfg = cc.sc.lbConfig.cfg } cbn := cc.curBalancerName bw := cc.balancerWrapper cc.mu.Unlock() if cbn != grpclbName { // Filter any grpclb addresses since we don't have the grpclb balancer. for i := 0; i \u0026lt;len(s.Addresses); { if s.Addresses[i].Type == resolver.GRPCLB { copy(s.Addresses[i:], s.Addresses[i+1:]) s.Addresses = s.Addresses[:len(s.Addresses)-1] continue } i++ } } uccsErr := bw.updateClientConnState(\u0026amp;balancer.ClientConnState{ResolverState: s, BalancerConfig: balCfg}) if ret == nil { ret = uccsErr // prefer ErrBadResolver state since any other error is // currently meaningless to the caller. } return ret } 至此整个服务发现过程就结束了。从中也可以看出 gRPC 官方提供的三个接口还是很灵活的，但也正因为灵活要实现稍微麻烦一些，而 Address官方代码位置 如果直接被业务拿来用于服务节点信息的描述结构则显得有些过于简单。 所以 warden 包装了 gRPC 的整个服务发现实现逻辑，代码分别位于 pkg/naming/naming.go 和 warden/resolver/resolver.go，其中：\nnaming.go 内定义了用于描述业务实例的 Instance 结构、用于服务注册的 Registry 接口、用于服务发现的 Resolver 接口 resolver.go 内实现了 gRPC 官方的 resolver.Builder 和 resolver.Resolver 接口，但也暴露了 naming.go 内的 naming.Builder 和 naming.Resolver 接口 文章转自： 基于 gRPC 的服务发现与负载均衡（基础篇） - 熊喵君的博客 | PANDAYCHEN gRPC 应用篇之 Resolver 接口封装 - 熊喵君的博客 | PANDAYCHEN ","date":"2020-12-20","permalink":"https://daemon365.dev/2020/12/20/grpc%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0%E4%B8%8E%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/","tags":["grpc","go"],"title":"grpc服务发现与负载均衡"},{"content":"RPC 框架原理 RPC 框架的目标就是让远程服务调用更加简单、透明，RPC 框架负责屏蔽底层的传输方式（TCP 或者 UDP）、序列化方式（XML/Json/ 二进制）和通信细节。服务调用者可以像调用本地接口一样调用远程的服务提供者，而不需要关心底层通信细节和调用过程。\n业界主流的 RPC 框架整体上分为三类：\n支持多语言的 RPC 框架，比较成熟的有 Google 的 gRPC、facebook的Apache、Thrift； 只支持特定语言的 RPC 框架，例如新浪微博的 Motan； 支持服务治理等服务化特性的分布式服务框架，其底层内核仍然是 RPC 框架, 例如阿里的 Dubbo。 gRPC是什么 gRPC 是一个高性能、开源和通用的 RPC 框架，面向移动和 HTTP/2 设计。目前提供 C、Java 和 Go 语言版本，分别是：grpc, grpc-java, grpc-go. 其中 C 版本支持 C, C++, Node.js, Python, Ruby, Objective-C, PHP 和 C## 支持.\ngRPC 基于 HTTP/2 标准设计，带来诸如双向流、流控、头部压缩、单 TCP 连接上的多复用请求等特。这些特性使得其在移动设备上表现更好，更省电和节省空间占用。\ngrc优点 多语言：语言中立，支持多种语言。 轻量级、高性能：序列化支持 PB(Protocol Buffer)和 JSON，PB 是一种语言无关的高性能序列化框架。 可插拔 IDL：基于文件定义服务，通过 proto3 工具生成指定语言的数据结构、服务端接口以及客户端 Stub。 移动端：基于标准的 HTTP2 设计，支持双向流、消息头压缩、单 TCP 的多路复用、服务端推送等特性，这些特性使得 gRPC 在移动端设备上更加省电和节省网络流量。 安全 HTTP2 规范当使用 TLS 时强制使用 TLS 1.2 及以上的版本，并且在部署上对允许的密码施加一些额外的限制以避免已知的比如需要 SNI 支持的问题。并且期待 HTTP2 与专有的传输安全机制相结合，这些传输机制的规格说明不能提供有意义的建议。\ngRPC使用 使用gRPC， 我们可以一次性的在一个.proto文件中定义服务并使用任何支持它的语言去实现客户端和服务端，反过来，它们可以应用在各种场景中，从Google的服务器到你自己的平板电脑—— gRPC帮你解决了不同语言及环境间通信的复杂性。使用protocol buffers还能获得其他好处，包括高效的序列号，简单的IDL以及容易进行接口更新。总之一句话，使用gRPC能让我们更容易编写跨语言的分布式代码。\n通过一个 protocol buffers 模式，定义一个简单的带有 Hello World 方法的 RPC 服务。 用你最喜欢的语言(如果可用的话)来创建一个实现了这个接口的服务端。 用你最喜欢的(或者其他你愿意的)语言来访问你的服务端。 什么用grpc 服务而非对象、消息而非引用：促进微服务的系统间粗粒度消息交互设计理念。 负载无关的：不同的服务需要使用不同的消息类型和编码，例如 protocol buffers、JSON、XML 和 Thrift。 流：Streaming API。 阻塞式和非阻塞式：支持异步和同步处理在客户端和服务端间交互的消息序列。 元数据交换：常见的横切关注点，如认证或跟踪，依赖数据交换。 标准化状态码：客户端通常以有限的方式响应 API 调用返回的错误。 HealthCheck gRPC 有一个标准的健康检测协议，在 gRPC 的所有语言实现中基本都提供了生成代码和用于设置运行状态的功能。\n主动健康检查 health check，可以在服务提供者服务不稳定时，被消费者所感知，临时从负载均衡中摘除，减少错误请求。当服务提供者重新稳定后，health check 成功，重新加入到消费者的负载均衡，恢复请求。health check，同样也被用于外挂方式的容器健康检测，或者流量检测(k8s liveness \u0026amp; readiness)。\nprotubuf文件编写 syntax = \u0026quot;proto3\u0026quot;; package hello; // option go_package = \u0026quot;hello\u0026quot;; option go_package = \u0026quot;/hello\u0026quot;; // The greeting service definition. service Greeter { // Sends a greeting rpc SayHello (HelloRequest) returns (HelloReply) {} } // The request message containing the user's name. message HelloRequest { string name = 1; } // The response message containing the greetings message HelloReply { string message = 1; } golang创建grpc server 安装工具包:\n下载protoc 链接 go install google.golang.org/protobuf/cmd/protoc-gen-go@latest go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest 执行:\nprotoc --go_out=./ --go-grpc_out=./ hello.proto \u0026ndash;proto_path: 指定了要去哪个目录中搜索import中导入的和要编译为.go的proto文件 (在这没有使用,需要的话可以加上) \u0026ndash;go_out:指定了生成的go文件的目录，我在这里把go文件放到本目录中 \u0026ndash;go-grpc_out: 指定了生成的go grpc文件的目录，我在这里把go grpc文件放到本目录中 hello.proto， 定义了我要编译的文件是哪个文件。 go server代码 package main import ( \u0026quot;context\u0026quot; \u0026quot;errors\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;github.com/zhaohaiyu1996/akit/example/grpc/hello\u0026quot; \u0026quot;google.golang.org/grpc\u0026quot; \u0026quot;net\u0026quot; ) type Server struct { hello.UnimplementedGreeterServer } // SayHello implements helloworld.GreeterServer func (s *Server) SayHello(ctx context.Context, in *hello.HelloRequest) (*hello.HelloReply, error) { if in.Name == \u0026quot;error\u0026quot; { return nil, errors.New(\u0026quot;123\u0026quot;) } if in.Name == \u0026quot;panic\u0026quot; { panic(\u0026quot;grpc panic\u0026quot;) } return \u0026amp;hello.HelloReply{Message: fmt.Sprintf(\u0026quot;Hello %+v\u0026quot;, in.Name)}, nil } type Ss struct { *grpc.Server } func main() { // 监听本地的8848端口 s := Ss{grpc.NewServer()} hello.RegisterGreeterServer(s, \u0026amp;Server{}) // 在gRPC服务端注册服务 lis, err := net.Listen(\u0026quot;tcp\u0026quot;, \u0026quot;127.0.0.1:8808\u0026quot;) if err != nil { fmt.Printf(\u0026quot;listen failed: %v\u0026quot;, err) return } //reflection.Register(s.Server) //在给定的gRPC服务器上注册服务器反射服务 // Serve方法在lis上接受传入连接，为每个连接创建一个ServerTransport和server的goroutine。 // 该goroutine读取gRPC请求，然后调用已注册的处理程序来响应它们。 err = s.Serve(lis) if err != nil { fmt.Printf(\u0026quot;failed to serve: %v\u0026quot;, err) return } } golang创建grpc client 执行:\nprotoc --go_out=./ --go-grpc_out=./ hello.proto go client代码 package main import ( \u0026quot;context\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;github.com/zhaohaiyu1996/akit/example/grpc/hello\u0026quot; \u0026quot;google.golang.org/grpc\u0026quot; ) func main() { // 连接服务器 conn, err := grpc.Dial(\u0026quot;localhost:8808\u0026quot;, grpc.WithInsecure()) if err != nil { fmt.Printf(\u0026quot;connect faild: %v\u0026quot;, err) } defer conn.Close() c := hello.NewGreeterClient(conn) // 调用SayHello r, err := c.SayHello(context.Background(), \u0026amp;hello.HelloRequest{Name: \u0026quot;zhaohaiyu\u0026quot;}) if err != nil { fmt.Printf(\u0026quot;sayHello failed: %v\u0026quot;, err) } fmt.Println(r) } 结果:\nSayHello: hello ---\u0026gt; zhaohaiyu python创建grpc client 使用python客户端调用golang服务端的方法\n下载依赖:\npip install grpcio pip install protobuf pip install grpcio_tools 执行:\npython -m grpc_tools.protoc -I ./ --python_out=./ --grpc_python_out=./ hello.proto python client代码 import grpc import hello_pb2 import hello_pb2_grpc def run(): with grpc.insecure_channel('localhost:8848') as channel: stub = hello_pb2_grpc.HelloStub(channel) res = stub.SayHello(hello_pb2.HelloRequest(name=\u0026quot;赵海宇\u0026quot;)) print(res.message) if __name__ == '__main__': run() 结果:\npython ./main.go hello ---\u0026gt; 赵海宇 gprc的haeder grpc是基于http2.0的rpc框架 - grpc对于http头部传递数据进行了封装 metadata,单独抽象了一个包google.golang.org/grpc/metadata- type ***p[string][]string其实就是一个map 客户端发送方式一:\n// 创建md 并加入ctx md := metadata.Pairs(\u0026quot;key1\u0026quot;,\u0026quot;value1\u0026quot;,\u0026quot;key2\u0026quot;,\u0026quot;value2\u0026quot;) ctx := metadata.NewOutgoingContext(context.Background(),md) // 从ctx中拿出md md,_ = metadata.FromOutgoingContext(ctx) newMd := metadata.Pairs(\u0026quot;key3\u0026quot;,\u0026quot;value3\u0026quot;) ctx = metadata.NewOutgoingContext(ctx,metadata.Join(md,newMd)) 客户端发送方式二:\nctx := context.Background() ctx = metadata.AppendToOutgoingContext(ctx,\u0026quot;key1\u0026quot;,\u0026quot;value1\u0026quot;,\u0026quot;key2\u0026quot;,\u0026quot;value2\u0026quot;) ctx = metadata.AppendToOutgoingContext(ctx,\u0026quot;key3\u0026quot;,\u0026quot;value3\u0026quot;) 服务端接收:\nmd,ok := metadata.FromIncomingContext(ctx) 实例:\nserver: package main import ( \u0026quot;fmt\u0026quot; \u0026quot;net\u0026quot; pb \u0026quot;test/demo13/server/hello\u0026quot; \u0026quot;github.com/grpc-ecosystem/grpc-gateway/examples/clients/responsebody\u0026quot; \u0026quot;github.com/uber/jaeger-client-go/crossdock/client\u0026quot; \u0026quot;golang.org/x/net/context\u0026quot; \u0026quot;google.golang.org/grpc\u0026quot; \u0026quot;google.golang.org/grpc/metadata\u0026quot; \u0026quot;google.golang.org/grpc/reflection\u0026quot; ) type server struct{} func (s *server) SayHello(ctx context.Context, in *pb.HelloRequest) (*pb.HelloResponse, error) { md,ok := metadata.FromIncomingContext(ctx) if ok { fmt.Println(md) } return \u0026amp;pb.HelloResponse{Message: \u0026quot;hello ---\u0026gt; \u0026quot; + in.Name}, nil } func main() { // 监听本地的8848端口 lis, err := net.Listen(\u0026quot;tcp\u0026quot;, \u0026quot;localhost:8848\u0026quot;) if err != nil { fmt.Printf(\u0026quot;listen failed: %v\u0026quot;, err) return } s := grpc.NewServer() // 创建gRPC服务器 pb.RegisterHelloServer(s, \u0026amp;server{}) // 在gRPC服务端注册服务 reflection.Register(s) //在给定的gRPC服务器上注册服务器反射服务 // Serve方法在lis上接受传入连接，为每个连接创建一个ServerTransport和server的goroutine。 // 该goroutine读取gRPC请求，然后调用已注册的处理程序来响应它们。 err = s.Serve(lis) if err != nil { fmt.Printf(\u0026quot;failed to serve: %v\u0026quot;, err) return } } client package main import ( \u0026quot;context\u0026quot; \u0026quot;fmt\u0026quot; pb \u0026quot;test/demo13/client/hello\u0026quot; \u0026quot;google.golang.org/grpc\u0026quot; \u0026quot;google.golang.org/grpc/metadata\u0026quot; ) func main() { // 连接服务器 conn, err := grpc.Dial(\u0026quot;localhost:8848\u0026quot;, grpc.WithInsecure()) if err != nil { fmt.Printf(\u0026quot;faild to connect: %v\u0026quot;, err) } defer conn.Close() c := pb.NewHelloClient(conn) // 调用服务端的SayHello ctx := context.Background() ctx = metadata.AppendToOutgoingContext(ctx,\u0026quot;zhyyz\u0026quot;,\u0026quot;961119\u0026quot;) r, err := c.SayHello(ctx, \u0026amp;pb.HelloRequest{Name: \u0026quot;zhaohaiyu\u0026quot;}) if err != nil { fmt.Printf(\u0026quot;sayHello failed: %v\u0026quot;, err) } fmt.Printf(\u0026quot;SayHello: %s \\n\u0026quot;, r.Message) } ","date":"2020-12-10","permalink":"https://daemon365.dev/2020/12/10/grpc%E5%9F%BA%E7%A1%80/","tags":["grpc","go"],"title":"grpc基础"},{"content":"msyql执行流程 你有个最简单的表，表里只有一个 ID 字段，在执行下面这个查询语句时：：\nselect * from T where ID=10； 我们看到的只是输入一条语句，返回一个结果，却不知道这条语句在 MySQL 内部的执行过程。\n下面我给出的是 MySQL 的基本架构示意图，从中你可以清楚地看到 SQL 语句在 MySQL 的各个功能模块中的执行过程。\n大体上，MySQL 分为 Server 层和存储引擎层两部分。\nServer 层包括连接器、查询缓存、分析器、执行器等，以及所有的内置函数（如日期、时间、数学和加密函数等）和跨存储引擎的功能（如存储过程、触发器、视图）。\n存储引擎层负责数据的存储和提取，支持 InnoDB、MyISAM、Memory 等多个存储引擎。MySQL 5.5.5 版本后默认存储存储引擎是 InnoDB。\n连接器 验证账号密码是否正确 到权限表里面查出你拥有的权限，之后的执行语句，都会依赖这个权限数据。 查询缓存 在建立连接后，就开始执行 select 语句了，执行前首先会查询缓存。\nMySQL 拿到查询请求后，会先查询缓存，看是不是执行过这条语句。执行过的语句及其结果会以 key-value 对的形式保存在一定的内存区域中。key 是查询的语句，value 是查询的结果。如果你的查询能够直接在这个缓存中找到 key，那么这个value 就会被直接返回给客户端。\n如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存中。如果查询命中缓存，MySQL 不需要执行后面的复杂操作，就可以直接返回结果，会提升效率。\n但是查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。对于更新压力大的数据库来说，查询缓存的命中率会非常低。如果业务中需要有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。MySQL 提供了这种按需使用的方式。可以将参数 query_cache_type 设置成 DEMAND，对于默认的 SQL 语句都将不使用查询缓存。\nMySQL 8.0 版本将查询缓存的功能删除了。\n分析器 如果没有命中查询缓存，就要开始真正执行语句了。首先，MySQL 需要知道你要做什么，因此需要对 SQL 语句做解析。\n分析器先会做“词法分析”。你输入的是由多个字符串和空格组成的一条 SQL 语句，MySQL 需要识别出里面的字符串分别是什么，代表什么。MySQL 从你输入的\u0026quot;select\u0026quot;这个关键字识别出来，这是一个查询语句。它也要把字符串“T”识别成“表名 T”，把字符串“ID”识别成“列 ID”。做完了这些识别以后，就要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。\n如果你的语句不对，就会收到“You have an error in your SQL syntax”的错误提醒，比如下面这个语句 select 少打了开头的字母“s”。\nelect * from t where ID=1; /* ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'elect * from t where ID=1' at line 1 */ 优化器 经过了分析器，MySQL 就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。\n优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。比如你执行下面这样的语句，这个语句是执行两个表的 join：\nselect * from t1 join t2 using(ID) where t1.c=10 and t2.d=20; 既可以先从表 t1 里面取出 c=10 的记录的 ID 值，再根据 ID 值关联到表 t2，再判断 t2 里面 d 的值是否等于 20。 也可以先从表 t2 里面取出 d=20 的记录的 ID 值，再根据 ID 值关联到 t1，再判断 t1 里面 c 的值是否等于 10。 执行器 MySQL 通过分析器知道了你要做什么，通过优化器知道了该怎么做，于是就进入了执行器阶段，开始执行语句。\n开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限，如果没有，就会返回没有权限的错误，如下所示 (在工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。查询也会在优化器之前调用 precheck 验证权限)。\nselect * from T where ID=10; /* ERROR 1142 (42000): SELECT command denied to user 'b'@'localhost' for table 'T' */ 如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。\n比如我们这个例子中的表 T 中，ID 字段没有索引，那么执行器的执行流程是这样的：\n调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中； 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。 对于有索引的表，第一次调用的是取满足条件的第一行这个接口，之后循环取满足条件的下一行这个接口。\n数据库的慢查询日志中有 rows_examined 字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟 rows_examined 并不是完全相同的。\n总结 主要通过对一个 SQL 语句完整执行过程进行讲解，介绍 MySQL 的逻辑架构，MySQL 主要包括连接器、查询缓存、分析器、优化器、执行器这几个模块。\n参考文章 https://time.geekbang.org/column/article/68319 ","date":"2020-05-21","permalink":"https://daemon365.dev/2020/05/21/sql%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B/","tags":["mysql"],"title":"SQL查询语句执行流程"},{"content":"什么是JWT？ JWT全称JSON Web Token是一种跨域认证解决方案，属于一个开放的标准，它规定了一种Token实现方式，目前多用于前后端分离项目和OAuth2.0业务场景下。\nJWT作用？ JWT就是一种基于Token的轻量级认证模式，服务端认证通过后，会生成一个JSON对象，经过签名后得到一个Token（令牌）再发回给用户，用户后续请求只需要带上这个Token，服务端解密之后就能获取该用户的相关信息了。\n下载jwt go get -u github.com/golang-jwt/jwt 生成JWT和解析JWT 我们在这里直接使用jwt-go这个库来实现我们生成JWT和解析JWT的功能。\n定义需求\n我们需要定制自己的需求来决定JWT中保存哪些数据\ntype MyClaims struct { UserID uint64 `json:\u0026quot;user_id\u0026quot;` Username string `json:\u0026quot;username\u0026quot;` jwt.StandardClaims } 定义JWT的过期时间和Secret(盐)：\nconst TokenExpireDuration = time.Hour * 24 * 2 // 过期时间 -2天 var Secret = []byte(\u0026quot;i am zhaohaiyu\u0026quot;) // Secret(盐) 用来加密解密 生成JWT //生成 jwt token func genToken(userID uint64, username string) (string, error) { var claims = MyClaims{ userID, username, jwt.StandardClaims{ ExpiresAt: time.Now().Add(TokenExpireDuration).Unix(), // 过期时间 Issuer: \u0026quot;zhaohaiyu\u0026quot;, // 签发人 }, } token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims) signedToken, err := token.SignedString([]byte(Secret)) if err != nil { return \u0026quot;\u0026quot;, fmt.Errorf(\u0026quot;生成token失败:%v\u0026quot;, err) } return signedToken, nil } 解析JWT //验证jwt token func ParseToken(tokenStr string) (*MyClaims, error) { token, err := jwt.ParseWithClaims(tokenStr, \u0026amp;MyClaims{}, func(token *jwt.Token) (i interface{}, err error) { // 解析token return Secret, nil }) if err != nil { return nil, err } if claims, ok := token.Claims.(*MyClaims); ok \u0026amp;\u0026amp; token.Valid { // 校验token return claims, nil } return nil, errors.New(\u0026quot;invalid token\u0026quot;) } 在gin框架中使用JWT r.POST(\u0026quot;/auth/token\u0026quot;, GetTokenHandler) 登录生成token func GetTokenHandler(c *gin.Context) { // 接收参数 var user UserInfo err := c.ShouldBind(\u0026amp;user) if err != nil { c.JSON(http.StatusOK, gin.H{ \u0026quot;code\u0026quot;: 1001, \u0026quot;err_info\u0026quot;: \u0026quot;参数错误\u0026quot;, }) return } // 验证密码 if user.Username == \u0026quot;root\u0026quot; \u0026amp;\u0026amp; user.Password == \u0026quot;123\u0026quot; { // 生成Token tokenString, err := GenToken(user.Username) if err != nil { c.JSON(http.StatusOK, gin.H{ \u0026quot;code\u0026quot;: 1001, \u0026quot;err_info\u0026quot;: \u0026quot;生成token错误\u0026quot;, }) return } c.JSON(http.StatusOK, gin.H{ \u0026quot;code\u0026quot;: 2000, \u0026quot;msg\u0026quot;: \u0026quot;success\u0026quot;, \u0026quot;data\u0026quot;: gin.H{\u0026quot;token\u0026quot;: tokenString}, }) return } else { c.JSON(http.StatusOK, gin.H{ \u0026quot;code\u0026quot;: 2002, \u0026quot;msg\u0026quot;: \u0026quot;用户名或密码错误\u0026quot;, }) return } return } jwt中间件验证 // JWThMiddleware 中间件 func JWThMiddleware() func(c *gin.Context) { return func(c *gin.Context) { // 客户端携带Token有三种方式 1.放在请求头 2.放在请求体 3.放在URI // 这里假设Token放在Header的token中 token := c.Request.Header.Get(\u0026quot;token\u0026quot;) if token == \u0026quot;\u0026quot; { // 处理 没有token的时候 c.Abort() // 不会继续停止 return } // 解析 mc, err := ParseToken(token) if err != nil { // 处理 解析失败 c.Abort() return } // 将当前请求的userID信息保存到请求的上下文c上 c.Set(\u0026quot;userID\u0026quot;, mc.UserID) c.Next() } } ","date":"2020-05-20","permalink":"https://daemon365.dev/2020/05/20/golang-jwt/","tags":["go"],"title":"golang jwt"},{"content":"什么是索引 一般的应用系统，都是读多写少。而且插入操作和一般的更新操作很少出现性能问题（因为有redo log锁cache缓存）。在生产环境中，我们遇到最多的，也是最容易出问题的，还是一些复杂的查询操作，因此对查询语句的优化显然是重中之重。说起加速查询，就不得不提到索引了。索引的核心思想就是加速查询。\n索引的原理 索引原理 索引的目的在于提高查询效率，可以类比字典，如果要查“mysql”这个单词，我们肯定需要定位到m字母，然后从下往下找到y字母，再找到剩下的sql。如果没有索引，那么你可能需要把所有单词看一遍才能找到你想要的，如果我想找到m开头的单词呢？或者ze开头的单词呢？是不是觉得如果没有索引，这个事情根本无法完成？\n本质都是：通过不断地缩小想要获取数据的范围来筛选出最终想要的结果，同时把随机的事件变成顺序的事件，也就是说，有了这种索引机制，我们可以总是用同一种查找方式来锁定数据。\n数据库也是一样，但显然要复杂许多，因为不仅面临着等值查询，还有范围查询(\u0026gt;、\u0026lt;、between、in)、模糊查询(like)、并集查询(or)等等。数据库应该选择怎么样的方式来应对所有的问题呢？我们回想字典的例子，能不能把数据分成段，然后分段查询呢？最简单的如果1000条数据，1到100分成第一段，101到200分成第二段，201到300分成第三段……这样查第250条数据，只要找第三段就可以了，一下子去除了90%的无效数据。但如果是1千万的记录呢，分成几段比较好？稍有算法基础的同学会想到搜索树，其平均复杂度是lgN，具有不错的查询性能。但这里我们忽略了一个关键的问题，复杂度模型是基于每次相同的操作成本来考虑的，数据库实现比较复杂，数据保存在磁盘上，而为了提高性能，每次又可以把部分数据读入内存来计算，因为我们知道访问磁盘的成本大概是访问内存的十万倍左右，所以简单的搜索树难以满足复杂的应用场景。\n磁盘IO与预读 考虑到磁盘IO是非常高昂的操作，计算机操作系统做了一些优化，当一次IO时，不光把当前磁盘地址的数据，而是把相邻的数据也都读取到内存缓冲区内，因为局部预读性原理告诉我们，当计算机访问一个地址的数据的时候，与其相邻的数据也会很快被访问到。每一次IO读取的数据我们称之为一页(page)。具体一页有多大数据跟操作系统有关，一般为4k或8k，也就是我们读取一页内的数据时候，实际上才发生了一次IO，这个理论对于索引的数据结构设计非常有帮助。\n索引的数据结构 前面讲了生活中索引的例子，索引的基本原理，数据库的复杂性，又讲了操作系统的相关知识，目的就是让大家了解，任何一种数据结构都不是凭空产生的，一定会有它的背景和使用场景，我们现在总结一下，我们需要这种数据结构能够做些什么，其实很简单，那就是：每次查找数据时把磁盘IO次数控制在一个很小的数量级，最好是常数数量级。那么我们就想到如果一个高度可控的多路搜索树是否能满足需求呢？就这样，b+树应运而生。\n详解b+树 如上图，是一颗b+树，关于b+树的定义可以参见B+树，这里只说一些重点，浅蓝色的块我们称之为一个磁盘块，可以看到每个磁盘块包含几个数据项（深蓝色所示）和指针（黄色所示），如磁盘块1包含数据项17和35，包含指针P1、P2、P3，P1表示小于17的磁盘块，P2表示在17和35之间的磁盘块，P3表示大于35的磁盘块。真实的数据存在于叶子节点即3、5、9、10、13、15、28、29、36、60、75、79、90、99。非叶子节点只不存储真实的数据，只存储指引搜索方向的数据项，如17、35并不真实存在于数据表中。\nb+树的查找过程 如图所示，如果要查找数据项29，那么首先会把磁盘块1由磁盘加载到内存，此时发生一次IO，在内存中用二分查找确定29在17和35之间，锁定磁盘块1的P2指针，内存时间因为非常短（相比磁盘的IO）可以忽略不计，通过磁盘块1的P2指针的磁盘地址把磁盘块3由磁盘加载到内存，发生第二次IO，29在26和30之间，锁定磁盘块3的P2指针，通过指针加载磁盘块8到内存，发生第三次IO，同时内存中做二分查找找到29，结束查询，总计三次IO。真实的情况是，3层的b+树可以表示上百万的数据，如果上百万的数据查找只需要三次IO，性能提高将是巨大的，如果没有索引，每个数据项都要发生一次IO，那么总共需要百万次的IO，显然成本非常非常高。\nb+树性质 索引字段要尽量的小：通过上面的分析，我们知道IO次数取决于b+数的高度h，假设当前数据表的数据为N，每个磁盘块的数据项的数量是m，则有h=㏒(m+1)N，当数据量N一定的情况下，m越大，h越小；而m = 磁盘块的大小 / 数据项的大小，磁盘块的大小也就是一个数据页的大小，是固定的，如果数据项占的空间越小，数据项的数量越多，树的高度越低。这就是为什么每个数据项，即索引字段要尽量的小，比如int占4字节，要比bigint8字节少一半。这也是为什么b+树要求把真实的数据放到叶子节点而不是内层节点，一旦放到内层节点，磁盘块的数据项会大幅度下降，导致树增高。当数据项等于1时将会退化成线性表。 索引的最左匹配特性：当b+树的数据项是复合的数据结构，比如(name,age,sex)的时候，b+数是按照从左到右的顺序来建立搜索树的，比如当(张三,20,F)这样的数据来检索的时候，b+树会优先比较name来确定下一步的所搜方向，如果name相同再依次比较age和sex，最后得到检索的数据；但当(20,F)这样的没有name的数据来的时候，b+树就不知道下一步该查哪个节点，因为建立搜索树的时候name就是第一个比较因子，必须要先根据name来搜索才能知道下一步去哪里查询。比如当(张三,F)这样的数据来检索时，b+树可以用name来指定搜索方向，但下一个字段age的缺失，所以只能把名字等于张三的数据都找到，然后再匹配性别是F的数据了， 这个是非常重要的性质，即索引的最左匹配特性。 聚集索引与辅助索引 在数据库中，B+树的高度一般都在24层，这也就是说查找某一个键值的行记录时最多只需要24次IO，这倒不错。因为当前一般的机械硬盘每秒至少可以做100次IO，24次的IO意味着查询时间只需要0.02~0.04秒。\n数据库中的B+树索引可以分为聚集索引（clustered index）和辅助索引（secondary index），\n聚集索引与辅助索引相同的是：不管是聚集索引还是辅助索引，其内部都是B+树的形式，即高度是平衡的，叶子结点存放着所有的数据。\n聚集索引与辅助索引不同的是：叶子结点存放的是否是一整行的信息\n聚集索引 InnoDB存储引擎表示索引组织表，即表中数据按照主键顺序存放。而聚集索引（clustered index）就是按照每张表的主键构造一棵B+树，同时叶子结点存放的即为整张表的行记录数据，也将聚集索引的叶子结点称为数据页。聚集索引的这个特性决定了索引组织表中数据也是索引的一部分。同B+树数据结构一样，每个数据页都通过一个双向链表来进行链接。 如果未定义主键，MySQL取第一个唯一索引（unique）而且只含非空列（NOT NULL）作为主键，InnoDB使用它作为聚簇索引。 如果没有这样的列，InnoDB就自己产生一个这样的ID值，它有六个字节，而且是隐藏的，使其作为聚簇索引。 由于实际的数据页只能按照一棵B+树进行排序，因此每张表只能拥有一个聚集索引。在多少情况下，查询优化器倾向于采用聚集索引。因为聚集索引能够在B+树索引的叶子节点上直接找到数据。此外由于定义了数据的逻辑顺序，聚集索引能够特别快地访问针对范围值得查询。 聚集索引的好处 它对主键的排序查找和范围查找速度非常快，叶子节点的数据就是用户所要查询的数据。如用户需要查找一张表，查询最后的10位用户信息，由于B+树索引是双向链表，所以用户可以快速找到最后一个数据页，并取出10条记录 范围查询（range query），即如果要查找主键某一范围内的数据，通过叶子节点的上层中间节点就可以得到页的范围，之后直接读取数据页即可 辅助索引 表中除了聚集索引外其他索引都是辅助索引（Secondary Index，也称为非聚集索引），与聚集索引的区别是：辅助索引的叶子节点不包含行记录的全部数据。 叶子节点除了包含键值以外，每个叶子节点中的索引行中还包含一个书签（bookmark）。该书签用来告诉InnoDB存储引擎去哪里可以找到与索引相对应的行数据。 由于InnoDB存储引擎是索引组织表，因此InnoDB存储引擎的辅助索引的书签就是相应行数据的聚集索引键。如下图 辅助索引的存在并不影响数据在聚集索引中的组织，因此每张表上可以有多个辅助索引，但只能有一个聚集索引。当通过辅助索引来寻找数据时，InnoDB存储引擎会遍历辅助索引并通过叶子级别的指针获得只想主键索引的主键，然后再通过主键索引来找到一个完整的行记录。\n举例来说，如果在一棵高度为3的辅助索引树种查找数据，那需要对这个辅助索引树遍历3次找到指定主键，如果聚集索引树的高度同样为3，那么还需要对聚集索引树进行3次查找，最终找到一个完整的行数据所在的页，因此一共需要6次逻辑IO访问才能得到最终的一个数据页。\nMySQL索引管理 功能 索引的功能就是加速查找 mysql中的primary key，unique，联合唯一也都是索引，这些索引除了加速查找以外，还有约束的功能 MySQL常用的索引 聚簇索引（主键索引）：\n加速查询 存储所有数据 普通索引INDEX：\n加速查找 唯一索引：\n主键索引PRIMARY KEY：加速查找+约束（不为空、不能重复） 唯一索引UNIQUE:加速查找+约束（不能重复） 联合索引：\n加速查找 - PRIMARY KEY(id,name):联合主键索引 - UNIQUE(id,name):联合唯一索引 - INDEX(id,name):联合普通索引 索引建议使用bTree 不建议使用hash索引\n创建/删除索引的语法 创建表时 /* CREATE TABLE 表名 ( 字段名1 数据类型 [完整性约束条件…], 字段名2 数据类型 [完整性约束条件…], [UNIQUE | FULLTEXT | SPATIAL ] INDEX | KEY [索引名] (字段名[(长度)] [ASC |DESC]) ); */ create table t1( id int, name char, age int, sex enum('male','female'), unique key uni_id(id), index ix_name(name) # index没有key ); CREATE在已存在的表上创建索引 /* CREATE [UNIQUE | FULLTEXT | SPATIAL ] INDEX 索引名 ON 表名 (字段名[(长度)] [ASC |DESC]) ; */ create index ix_age on t1(age); ALTER TABLE在已存在的表上创建索引 /* ALTER TABLE 表名 ADD [UNIQUE | FULLTEXT | SPATIAL ] INDEX 索引名 (字段名[(长度)] [ASC |DESC]) ; */ alter table t1 add index ix_sex(sex); 查看索引 show create table t1; /* | t1 | CREATE TABLE `t1` ( `id` int(11) DEFAULT NULL, `name` char(1) DEFAULT NULL, `age` int(11) DEFAULT NULL, `sex` enum('male','female') DEFAULT NULL, UNIQUE KEY `uni_id` (`id`), KEY `ix_name` (`name`), KEY `ix_age` (`age`), KEY `ix_sex` (`sex`) ) ENGINE=InnoDB DEFAULT CHARSET=latin1 */ 删除索引 DROP INDEX 索引名 ON 表名字; 查询优化神器 - explain命令 关于explain命令相信大家并不陌生，具体用法和字段含义可以参考官网explain-output，这里需要强调rows是核心指标，绝大部分rows小的语句执行一定很快（有例外，下面会讲到）。所以优化语句基本上都是在优化rows。\n慢查询优化基本步骤 先运行看看是否真的很慢，注意设置SQL_NO_CACHE\nwhere条件单表查，锁定最小返回记录表。这句话的意思是把查询语句的where都应用到表中返回的记录数最小的表开始查起，单表每个字段分别查询，看哪个字段的区分度最高\nexplain查看执行计划，是否与1预期一致（从锁定记录较少的表开始查询）\norder by limit 形式的sql语句让排序的表优先查\n了解业务方使用场景\n加索引时参照建索引的几大原则\n观察结果，不符合预期继续从0分析\nexplain参数 select_typec 表示 SELECT 的类型。 常见的取值有\nSIMPLE（简单表，即不使用表连接或者子查询） PRIMARY（主查询，即外层的查询） UNION（UNION 中的第二个或者后面的查询语句） SUBQUERY（子查询中的第一个 SELECT）等。 table 输出结果集的表。\ntype 表示表的连接类型，性能由好到差的连接类型如下\nsystem（表中仅有一行，即常量表） const（单表中最多有一个匹配行，例如 primary key 或者 unique index） eq_ref（对于前面的每一行，在此表中只查询一条记录，简单来说，就是多表连接中使用primary key或者unique index） ref（与eq_ref类似，区别在于不是使用primary key 或者 unique index，而是使用普通的索引） ref_or_null（与 ref 类似，区别在于条件中包含对 NULL 的查询） index_merge (索引合并优化) unique_subquery（in的后面是一个查询主键字段的子查询） index_subquery（与 unique_subquery 类似，区别在于 in 的后面是查询非唯一索引字段的子查询） range（单表中的范围查询） index（对于前面的每一行，都通过查询索引来得到数据） all（对于前面的每一行，都通过全表扫描来得到数据）。 possible_keys 表示查询时，可能使用的索引。\nkey 表示实际使用的索引。\nkey_len 索引字段的长度。\nrows 扫描行的数量。\nExtra 执行情况的说明和描述。\n","date":"2020-04-20","permalink":"https://daemon365.dev/2020/04/20/mysql%E7%B4%A2%E5%BC%95/","tags":["mysql"],"title":"mysql索引"},{"content":"事务是什么 事务就是指逻辑上的一组SQL语句操作，组成这组操作的各个SQL语句，执行时要么全成功要么全失败。\n在 MySQL 中，事务支持是在引擎层实现的。MySQL 是一个支持多引擎的系统，但并不是所有的引擎都支持事务。\n比如 MySQL 原生的 MyISAM 引擎就不支持事务，这也是 MyISAM 被 InnoDB 取代的重要原因之一\n事务的四大特性 原子性(Atomicity) 事务是一个不可分割的单位，事务中的所有SQL等操作要么都发生，要么都不发生。 一致性(Consistency) 事务发生前和发生后，数据的完整性必须保持一致。 隔离性(Isolation) 当并发访问数据库时，一个正在执行的事务在执行完毕前，对于其他的会话是不可见的，多个并发事务之间的数据是相互隔离的。也就是其他人的操作在这个事务的执行过程中是看不到这个事务的执行结果的，也就是他们拿到的是这个事务执行之前的内容，等这个事务执行完才能拿到新的数据。 持久性(Durability) 一个事务一旦被提交，它对数据库中的数据改变就是永久性的。如果出了错误，事务也不允撤销，只能通过\u0026rsquo;补偿性事务\u0026rsquo;。 事务的开启 开启:\nbegin/start transaction 执行第一个语句是开启事务\nstart transaction with consistent snapshot 直接开启事务\n隐性事务：set autocommit=0，这个命令会将这个线程的自动提交关掉。\n意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。\n有些客户端连接框架会默认连接成功后先执行一个 set autocommit=0 的命令。这就导致接下来的查询都在事务中，如果是长连接，就导致了意外的长事务。\n提交:commit\n回滚:rollback\n在事务中混合使用存储引擎 MySQL服务器层不管理事务，事务是由下层的存储引擎实现的。所以在同一个事务中，使用多种存储引擎是不可靠的。\n如果在事务中混合使用了事务型和非事务型的表（例如innodb和myisam表），在正常提交的情况下不会有什么问题。\n但如果该事务需要回滚，非事务型的表上的变更就无法撤销，这会导致数据库处于不一致的状态，这种情况很难修复，事务的最终结果将无法确定。\n所以，为每张表选择合适的存储引擎非常重要。\n在非事务型的表上执行事务相关操作的时候，MySQL通常不会发出提醒，也不会报错。有时候只有回滚的时候才会发出一个警告：\u0026ldquo;某些非事务型的表上的变更不能被回滚\u0026rdquo;。\n但大多数情况下，对非事务型表的操作都不会有提示。\n脏读 幻读 不可重复读 脏读 所谓脏读是指一个事务中访问到了另外一个事务未提交的数据，如下图：\ns1 s2 begin begin update test set number = 100 where id = 1; select number from test where id = 1; commit commit 如果会话 2 更新 number 为 100，但是在 number 之前，会话 1 希望得到 number，那么会获得的值就是更新前的值。或者如果会话 2 更新了值但是执行了 rollback，而会话 1 拿到的仍是 100。这就是脏读。 不可重复读 一个事务查询同一条记录2次，得到的结果不一致：\nS1 S2 begin begin; select number from test where id = 1; update test set number = 200 where id = 1; commit select number from test where id = 1; commit; 由于在读取中间变更了数据，所以会话 1 事务查询期间的得到的结果就不一样了。 幻读 一个事务查询2次，得到的记录条数不一致：\nS1 S2 begin begin select number from test where id \u0026lt; 5; insert into test value(2,3); commit select number from test where id \u0026lt; 5; commit 幻读是不可重复读的一种特殊场景。 事务的隔离级别 MySQL 里有四个隔离级别：\nREAD UNCOMMITTED（读未提交） 在read uncommitted级别，事务中的修改 ，即使没有提交，对其他事务也都是可见的。事务可以读取未提交的数据，这也被称为脏读。\n这个级别会导致很多问题，从性能上来说，read uncommitted不会比其他的级别好太多，但却缺乏其他级别的很多好处，除非真个有非常必要的理由，在实际应用中一般很少使用。\nREAD COMMITTED（提已提交） 大多数数据库系统的默认隔离级别都是READ COMMITTED（但MySQL不是）。\nREAD COMMITTED满足前面提到的隔离性的简单定义：一个事务开始时，只能看见已经提交的事务所做的修改。\n换句话说，一个事务从开始直到提交之前，所做的任何修改对其他事务都是不可见的，这个级别有时候也叫做不可重复读。\n不可重复读现象：当事务内相同的记录被检索两次，且两次得到的结果不同时，此现象成为不可重复读。\nREPEATABLE READ（读可重复读） REPEATABLE READ解决了脏读的问题。该级别保证了在同一个事务中多次读取同样记录的结果是一致的。但是理论上，可重复读隔离级别还是无法解决另外一个幻读的问题。\n所谓幻读，指的是当某个事务在读取某个范围内的记录时，另外一个事务又在该范围内插入了新的记录，当之前的事务再次读取该范围的记录时，会产生幻行。\n可重复读是MySQL的默认事务隔离级别。\nSERIZLIZABLE（可串行化） SERIZLIZABLE是最高的隔离级别。它通过强制事务串行执行，避免了前面说的幻读的问题。\n简单来说，SERIZLIZABLE会在读取的每一行数据上都加锁，所以可能导致大量的超时和锁争用的问题。\n实际应用中也很少用到这个隔离级别，只有在非常需要确保数据的一致性而且可以接受没有并发的情况下，才考虑采用该级别。\n不同事务隔离级别的效果： 隔离级别 脏读 不可重复读 幻读 READ UNCOMMITTED（未提交读） ✅ ✅ ✅ READ COMMITTED（提已提交） ❎ ✅ ✅ REPEATABLE READ（可重复读） ❎ ❎ ✅ SERIZLIZABLE（可串行化） ❎ ❎ ✅ 在 InnoDB 中，默认为 Repeatable 级别，InnoDB 中使用一种被称为 next-key locking 的策略来避免幻读（phantom）现象的产生。\n隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大。\n事务隔离的实现 隔离级别为默认可重复读\n在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。 记录上的最新值，通过回滚操作，都可以得到前一个状态的值。 假设一个值从 1 被按顺序改成了 2、3、4，在回滚日志里面就会有类似下面的记录。\n当前值是 4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的 read-view。 如图中看到的，在视图 A、B、C 里面，这一个记录的值分别是 1、2、4， 同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）。 对于 read-view A，要得到 1，就必须将当前值依次执行图中所有的回滚操作得到。 同时你会发现， 即使现在有另外一个事务正在将 4 改成 5，这个事务跟 read-view A、B、C 对应的事务是不会冲突的\n参考文章 https://time.geekbang.org/column/article/70562 ","date":"2020-04-03","permalink":"https://daemon365.dev/2020/04/03/mysql%E4%BA%8B%E5%8A%A1/","tags":["mysql","事务"],"title":"mysql事务"},{"content":"MySQL中的锁 数据库锁设计的初衷是处理并发问题。作为多用户共享的资源，当出现并发访问的时候，数据库需要合理地控制资源的访问规则。而锁就是用来实现这些访问规则的重要数据结构。\n根据加锁的范围，MySQL 里面的锁大致可以分成全局锁、表级锁和行锁三类\n全局锁 全局锁就是对整个数据库实例加锁。\nMySQL提供了一个加全局读锁的方法，命令是Flush tables with read lock。当需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句\n全局锁的典型使用场景是，做全库逻辑备份。也就是把整库每个表都select出来存成文本\n但是让整个库都只读，可能出现以下问题：\n如果在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆 如果在从库上备份，那么在备份期间从库不能执行主库同步过来的binlog，会导致主从延迟 在可重复读隔离级别下开启一个事务能够拿到一致性视图\n官方自带的逻辑备份工具是mysqldump。当mysqldump使用参数–single-transaction的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于MVCC的支持，这个过程中数据是可以正常更新的。single-transaction只适用于所有的表使用事务引擎的库\n既然要全库只读，为什么不使用set global readonly=true的方式？\n在有些系统中，readonly的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此修改global变量的方式影响面更大 在异常处理机制上有差异。如果执行Flush tables with read lock命令之后由于客户端发生异常断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为readonly之后，如果客户端发生异常，则数据库会一直保持readonly状态，这样会导致整个库长时间处于不可写状态，风险较高\n表级锁 MySQL 里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。\n表锁的语法是 lock tables … read/write。与 FTWRL 类似，可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。需要注意，lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。\n另一类表级的锁是 MDL（metadata lock)。MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。\n在 MySQL 5.5 版本中引入了 MDL，当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。\n读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。 给一个表加字段，或者修改字段，或者加索引，需要扫描全表的数据。在对大表操作的时候，你肯定会特别小心，以免对线上服务造成影响。而实际上，即使是小表，操作不慎也会出问题。\n如果某个表上的查询语句频繁，而且客户端有重试机制，也就是说超时后会再起一个新 session 再请求的话，这个库的线程很快就会爆满。\n事务中的 MDL 锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。\n行锁 MySQL 的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持行锁。\n不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB 是支持行锁的，这也是 MyISAM 被 InnoDB 替代的重要原因之一。\n顾名思义，行锁就是针对数据表中行记录的锁。这很好理解，比如事务 A 更新了一行，而这时候事务 B 也要更新同一行，则必须等事务 A 的操作完成后才能进行更新。\n当然，数据库中还有一些没那么一目了然的概念和设计，这些概念如果理解和使用不当，容易导致程序出现非预期行为，比如两阶段锁。\n两阶段锁 这个问题的结论取决于事务 A 在执行完两条 update 语句后，持有哪些锁，以及在什么时候释放。\n事务 A 持有的两个记录的行锁，都是在 commit 的时候才释放的。也就是说，在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。\n死锁和死锁检测 当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁\n事务 A 和事务 B 在互相等待对方的资源释放，就是进入了死锁状态。\n当出现死锁以后，有两种策略：\n一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。 在 InnoDB 中，innodb_lock_wait_timeout 的默认值是 50s，意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的。但是，我们又不可能直接把这个时间设置成一个很小的值，比如 1s。这样当出现死锁的时候，确实很快就可以解开，但如果不是死锁，而是简单的锁等待呢？所以，超时时间设置太短的话，会出现很多误伤。所以，正常情况下我们还是要采用第二种策略，即：主动死锁检测，而且 innodb_deadlock_detect 的默认值本身就是 on。主动死锁检测在发生死锁的时候，是能够快速发现并进行处理的，但是它也是有额外负担的。\n你可以想象一下这个过程：每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁。那如果是我们上面说到的所有事务都要更新同一行的场景呢？\n每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是 O(n) 的操作。假设有 1000 个并发线程要同时更新同一行，那么死锁检测操作就是 100 万这个量级的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的 CPU 资源。因此，你就会看到 CPU 利用率很高，但是每秒却执行不了几个事务。\n间隙锁 为了解决幻读问题，InnoDB 只好引入新的锁，也就是间隙锁 (Gap Lock)。\n顾名思义，间隙锁，锁的就是两个值之间的空隙。比如文章开头的表 t，初始化插入了 6 个记录，这就产生了 7 个间隙。\n这样，当你执行 select * from t where d=5 for update 的时候，就不止是给数据库中已有的 6 个记录加上了行锁，还同时加了 7 个间隙锁。这样就确保了无法再插入新的记录。也就是说这时候，在一行行扫描的过程中，不仅将给行加上了行锁，还给行两边的空隙，也加上了间隙锁。现在你知道了，数据行是可以加上锁的实体，数据行之间的间隙，也是可以加上锁的实体。但是间隙锁跟我们之前碰到过的锁都不太一样。比如行锁，分成读锁和写锁。下图就是这两种类型行锁的冲突关系。\n也就是说，跟行锁有冲突关系的是“另外一个行锁”。但是间隙锁不一样，跟间隙锁存在冲突关系的，是“往这个间隙中插入一个记录”这个操作。间隙锁之间都不存在冲突关系。\n举个例子：\n这里 session B 并不会被堵住。因为表 t 里并没有 c=7 这个记录，因此 session A 加的是间隙锁 (5,10)。而 session B 也是在这个间隙加的间隙锁。它们有共同的目标，即：保护这个间隙，不允许插入值。但，它们之间是不冲突的。\nnext-key lock 间隙锁和行锁合称 next-key lock，每个 next-key lock 是前开后闭区间。也就是说，我们的表 t 初始化以后，如果用 select * from t for update 要把整个表所有记录锁起来，就形成了 7 个 next-key lock，分别是 (-∞,0]、(0,5]、(5,10]、(10,15]、(15,20]、(20, 25]、(25, +supremum]。\n备注：这篇文章中，如果没有特别说明，我们把间隙锁记为开区间，把 next-key lock 记为前开后闭区间。\n这个 supremum 从哪儿来的呢？这是因为 +∞是开区间。实现上，InnoDB 给每个索引加了一个不存在的最大值 supremum，这样才符合我们前面说的“都是前开后闭区间”。\n参考文章 https://time.geekbang.org/column/article/69862 ","date":"2020-04-02","permalink":"https://daemon365.dev/2020/04/02/mysql-%E9%94%81/","tags":["mysql"],"title":"mysql 锁"},{"content":"更新语句执行流程 下面是这个表的创建语句，这个表有一个主键 ID 和一个整型字段 c：\ncreate table T(ID int primary key, c int); 如果要将 ID=2 这一行的值加 1，SQL 语句就会这么写：\nupdate T set c=c+1 where ID=2; 前面我有跟你介绍过 SQL 语句基本的执行链路，这里我再把那张图拿过来，你也可以先简单看看这个图回顾下。首先，可以确定的说，查询语句的那一套流程，更新语句也是同样会走一遍。\n通过连接器，客户端与 MySQL 建立连接 update 语句会把 T 表上的所有查询缓存结果清空 分析器会通过词法分析和语法分析识别这是一条更新语句 优化器会决定使用 ID 这个索引（聚簇索引） 执行器负责具体执行，找到匹配的一行，然后更新 更新过程中还会涉及 redo log（重做日志）和 binlog（归档日志）的操作 其中，这两种日志默认在数据库的 data 目录下，redo log 是 ib_logfile0 格式的，binlog 是 xxx-bin.000001 格式的。\n接下来让我们分别去研究下日志模块中的 redo log 和 binlog。\n日志模块：redo log 在 MySQL 中，如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程 IO 成本、查找成本都很高。为了解决这个问题，MySQL 的设计者就采用了日志（redo log）来提升更新效率。\n而日志和磁盘配合的整个过程，其实就是 MySQL 里的 WAL 技术，WAL 的全称是 Write-Ahead Logging，它的关键点就是先写日志，再写磁盘。\n具体来说，当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log（redolog buffer）里面，并更新内存（buffer pool），这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候（如系统空闲时），将这个操作记录更新到磁盘里面（刷脏页）。\nredo log 是 InnoDB 存储引擎层的日志，又称重做日志文件，redo log 是循环写的，redo log 不是记录数据页更新之后的状态，而是记录这个页做了什么改动。\nredo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，那么日志总共就可以记录 4GB 的操作。从头开始写，写到末尾就又回到开头循环写，如下图所示。\n图中展示了一组 4 个文件的 redo log 日志，checkpoint 是当前要擦除的位置，擦除记录前需要先把对应的数据落盘（更新内存页，等待刷脏页）。write pos 到 checkpoint 之间的部分可以用来记录新的操作，如果 write pos 和 checkpoint 相遇，说明 redolog 已满，这个时候数据库停止进行数据库更新语句的执行，转而进行 redo log 日志同步到磁盘中。checkpoint 到 write pos 之间的部分等待落盘（先更新内存页，然后等待刷脏页）。\n有了 redo log 日志，那么在数据库进行异常重启的时候，可以根据 redo log 日志进行恢复，也就达到了 crash-safe。\nredo log 用于保证 crash-safe 能力。innodb_flush_log_at_trx_commit 这个参数设置成 1 的时候，表示每次事务的 redo log 都直接持久化到磁盘。这个参数建议设置成 1，这样可以保证 MySQL 异常重启之后数据不丢失。\n日志模块：binlog MySQL 整体来看，其实就有两块：一块是 Server 层，它主要做的是 MySQL 功能层面的事情；还有一块是引擎层，负责存储相关的具体事宜。 redo log 是 InnoDB 引擎特有的日志，而 Server 层也有自己的日志，称为 binlog（归档日志）。\n为什么要有两份日志系统？\n因为最开始 MySQL 里并没有 InnoDB 引擎。MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档。而InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用另外一套日志系统——也就是 redo log 来实现 crash-safe 能力。\nredo log 和 binlog 区别：\nredo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。 redo log 是物理日志，记录的是在某个数据页上做了什么修改；binlog 是逻辑日志，记录的是这个语句的原始逻辑。 redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。追加写是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 有了对这两个日志的概念性理解后，再来看执行器和 InnoDB 引擎在执行这个 update 语句时的内部流程。\n执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。 引擎将这行新数据更新到内存（InnoDB Buffer Pool）中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。 update 语句的执行流程图，图中浅色框表示是在 InnoDB 内部执行的，深色框表示是在执行器中执行的。\n两阶段提交 MySQL 使用两阶段提交主要解决 binlog 和 redo log 的数据一致性的问题。\n由于 redo log 和 binlog 是两个独立的逻辑，如果不用两阶段提交，要么就是先写完 redo log 再写 binlog，或者采用反过来的顺序。\n仍然用前面的 update 语句来做例子。假设当前 ID=2 的行，字段 c 的值是 0，再假设执行 update 语句过程中在写完第一个日志后，第二个日志还没有写完期间发生了 crash，会出现什么情况呢？\n**先写 redo log 后写 binlog。**假设在 redo log 写完，binlog 还没有写完的时候，MySQL 进程异常重启。由于我们前面说过的，redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1。但是由于 binlog 没写完就 crash 了，这时候 binlog 里面就没有记录这个语句。因此，之后备份日志的时候，存起来的 binlog 里面就没有这条语句。然后你会发现，如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0，与原库的值不同。 **先写 binlog 后写 redo log。**如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。但是 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来，恢复出来的这一行 c 的值就是 1，与原库的值不同。 简单说，redo log 和 binlog 都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。\n参考文章 https://time.geekbang.org/column/article/68633 ","date":"2020-04-01","permalink":"https://daemon365.dev/2020/04/01/msyql-redo-log%E5%92%8Cbinlog/","tags":["mysql"],"title":"msyql redo log和binlog"},{"content":"数值类型 MySQL支持所有标准SQL数值数据类型。\n这些类型包括严格数值数据类型(INTEGER、SMALLINT、DECIMAL和NUMERIC)，以及近似数值数据类型(FLOAT、REAL和DOUBLE PRECISION)。\n关键字INT是INTEGER的同义词，关键字DEC是DECIMAL的同义词。\nBIT数据类型保存位字段值，并且支持MyISAM、MEMORY、InnoDB和BDB表。\n作为SQL标准的扩展，MySQL也支持整数类型TINYINT、MEDIUMINT和BIGINT。下面的表显示了需要的每个整数类型的存储和范围。\n类型 大小 范围（有符号） 范围（无符号） 用途 TINYINT 1 字节 (-128，127) (0，255) 小整数值 SMALLINT 2 字节 (-32 768，32 767) (0，65 535) 大整数值 MEDIUMINT 3 字节 (-8 388 608，8 388 607) (0，16 777 215) 大整数值 INT或INTEGER 4 字节 (-2 147 483 648，2 147 483 647) (0，4 294 967 295) 大整数值 BIGINT 8 字节 (-9 233 372 036 854 775 808，9 223 372 036 854 775 807) (0，18 446 744 073 709 551 615) 极大整数值 FLOAT 4 字节 (-3.402 823 466 E+38，-1.175 494 351 E-38)，0，(1.175 494 351 E-38，3.402 823 466 351 E+38) 0，(1.175 494 351 E-38，3.402 823 466 E+38) 单精度 浮点数值 DOUBLE 8 字节 (-1.797 693 134 862 315 7 E+308，-2.225 073 858 507 201 4 E-308)，0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308) 0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308) 双精度 浮点数值 DECIMAL 对DECIMAL(M,D) ，如果M\u0026gt;D，为M+2否则为D+2 依赖于M和D的值 依赖于M和D的值 小数值 日期和时间类型 表示时间值的日期和时间类型为DATETIME、DATE、TIMESTAMP、TIME和YEAR。\n每个时间类型有一个有效值范围和一个\u0026quot;零\u0026quot;值，当指定不合法的MySQL不能表示的值时使用\u0026quot;零\u0026quot;值。\nTIMESTAMP类型有专有的自动更新特性，将在后面描述。\n类型 大小 (字节) 范围 格式 用途 DATE 3 1000-01-01/9999-12-31 YYYY-MM-DD 日期值 TIME 3 \u0026lsquo;-838:59:59\u0026rsquo;/\u0026lsquo;838:59:59\u0026rsquo; HH:MM:SS 时间值或持续时间 YEAR 1 1901/2155 YYYY 年份值 DATETIME 8 1000-01-01 00:00:00/9999-12-31 23:59:59 YYYY-MM-DD HH:MM:SS 混合日期和时间值 TIMESTAMP 4 1970-01-01 00:00:00/2038结束时间是第 2147483647 秒，北京时间 2038-1-19 11:14:07，格林尼治时间 2038年1月19日 凌晨 03:14:07 YYYYMMDD HHMMSS 混合日期和时间值，时间戳 字符串类型 字符串类型指CHAR、VARCHAR、BINARY、VARBINARY、BLOB、TEXT、ENUM和SET。该节描述了这些类型如何工作以及如何在查询中使用这些类型。\n类型 大小 用途 CHAR 0-255字节 定长字符串 VARCHAR 0-65535 字节 变长字符串 TINYBLOB 0-255字节 不超过 255 个字符的二进制字符串 TINYTEXT 0-255字节 短文本字符串 BLOB 0-65 535字节 二进制形式的长文本数据 TEXT 0-65 535字节 长文本数据 MEDIUMBLOB 0-16 777 215字节 二进制形式的中等长度文本数据 MEDIUMTEXT 0-16 777 215字节 中等长度文本数据 LONGBLOB 0-4 294 967 295字节 二进制形式的极大文本数据 LONGTEXT 0-4 294 967 295字节 极大文本数据 CHAR 和 VARCHAR 类型类似，但它们保存和检索的方式不同。它们的最大长度和是否尾部空格被保留等方面也不同。在存储或检索过程中不进行大小写转换。\nBINARY 和 VARBINARY 类似于 CHAR 和 VARCHAR，不同的是它们包含二进制字符串而不要非二进制字符串。也就是说，它们包含字节字符串而不是字符字符串。这说明它们没有字符集，并且排序和比较基于列值字节的数值值。\nBLOB 是一个二进制大对象，可以容纳可变数量的数据。有 4 种 BLOB 类型：TINYBLOB、BLOB、MEDIUMBLOB 和 LONGBLOB。它们区别在于可容纳存储范围不同。\n有 4 种 TEXT 类型：TINYTEXT、TEXT、MEDIUMTEXT 和 LONGTEXT。对应的这 4 种 BLOB 类型，可存储的最大长度不同，可根据实际情况选择。\n文章转自 http://www.runoob.com/mysql/mysql-data-types.html ","date":"2020-03-31","permalink":"https://daemon365.dev/2020/03/31/mysql%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/","tags":["mysql"],"title":"MySQL基础数据类型"},{"content":"主键约束 主键可以是表中的某一列，也可以是表中的多个列所构成的一个组合；其中，由多个列组合而成的主键也称为复合主键。在MySQL中，主键列必须遵守以下规则。\n（1）每一个表只能定义一个主键。\n（2）唯一性原则。主键的值，也称键值，必须能够唯一表示表中的每一条记录，且不能为NULL。\n（3）最小化规则。复合主键不能包含不必要的多余列。也就是说，当从一个复合主键中删除一列后，如果剩下的列构成的主键仍能满足唯一性原则，那么这个复合主键是不正确的。\n（4）一个列名在复合主键的列表中只能出现一次。\n示例：创建学生信息表tb_student时，将学号（stu_id）字段设置为主键。\nCREATE TABLE tb_student ( stu_id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(30) ); 示例：创建用户信息表tb_student时，将学号（stu_id）和所在班级号（class_id）字段设置为复合主键。\nCREATE TABLE tb_student ( stu_id INT AUTO_INCREMENT, name VARCHAR(30), class_id INT NOT NULL, PRIMARY KEY (stu_id,class_id) ); 示例：通过修改数据表结构，添加主键约束。\nALTER TABLE tb_student ADD CONSTRAINT PRIMARY KEY(stu_id); 唯一约束 唯一约束使用UNIQUE关键字来定义。唯一约束的值必须是唯一的，且不能为空（NULL）。\n在MySQL中，唯一约束与主键之间存在以下两点区别。\n（1）一个表只能创建一个主键，但可以定义多个唯一约束。\n（2）定义主键约束时，系统会自动创建PRIMARY KEY索引，而定义候选键约束时，系统会自动创建UNIQUE索引。\n示例：创建用户信息表tb_student时，将学号（stu_id）和姓名（name）设置为唯一约束。\nCREATE TABLE tb_student ( stu_id INT UNIQUE, name VARCHAR(30) UNIQUE ); 示例：创建用户信息表tb_student时，将学号（stu_id）和姓名（name）字段设置为复合唯一约束。\nCREATE TABLE tb_student ( stu_id INT, name VARCHAR(30), UNIQUE uniq_id_name (stu_id,name) ); 示例：通过修改数据表结构，添加唯一约束。\nALTER TABLE tb_student ADD CONSTRAINT uniq_id_name UNIQUE(stu_id,name); 外键约束 MySQL有两种常用的引擎类型（MyISAM和InnoDB），目前，只用InnoDB引擎类型支持外键约束。\n示例：创建班级信息表（tb_class）和学生信息表（tb_student），并设置学生信息表中班级编号（class_id）字段的外键约束。\n\u0026ndash; 创建班级信息表\nCREATE TABLE tb_class ( class_id INT AUTO_INCREMENT PRIMARY KEY, class_name VARCHAR(30) NOT NULL ); \u0026ndash; 创建学生信息表，并设置班级ID的外键约束\nCREATE TABLE tb_student ( stu_id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(30), class_id INT NOT NULL, FOREIGN KEY fk_class_id (class_id) REFERENCES tb_class(class_id) ); 示例：通过修改数据表结构，添加外键约束。\nALTER TABLE tb_student ADD CONSTRAINT FOREIGN KEY fk_class_id (class_id) REFERENCES tb_class(class_id); 非空约束 非空约约束就是限制必须为某个列提供值。空值（NULL）是不存在值，它既不是数字0，也不是空字符串，而是不存在、未知的情况。\n示例：创建学生信息表tb_student时，将姓名（name）字段添加为非空约束。\nCREATE TABLE tb_student ( stu_id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(30) NOT NULL ); 示例：通过修改数据表结构，将姓名（name）字段修改为非空。\nALTER TABLE tb_student MODIFY COLUMN name VARCHAR(30) NOT NULL; 检查约束 检查约束用来指定某列的可取值的范围，它通过限制输入到列中的值来强制域的完整性。\n示例：创建学生信息表tb_student时，将年龄（age）的值设置在7至18之间（不包括18）的数值。\nCREATE TABLE tb_student ( stu_id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(30), age INT NOT NULL CHECK(age\u0026gt;=7 AND age\u0026lt;18) ); 注意：目前的MySQL版本只是对CHECK约束进行了分析处理，但会被直接忽略，并不会报错。\n约束的删除 删除约束语法：\nALTER TABLE 表名 DROP [FOREIGN KEY| INDEX 约束名称]|[PRIMARY KEY] 示例：删除约束。\nCREATE TABLE tb_student ( stu_id INT, name VARCHAR(30) , class_id INT NOT NULL, -- 主键约束 PRIMARY KEY(stu_id), -- 外键约束 FOREIGN KEY fk_class_id (class_id) REFERENCES tb_class(class_id), -- 唯一性约束 UNIQUE uniq_name (name) ); -- 删除主键约束 ALTER TABLE tb_student DROP PRIMARY KEY; -- 删除外键约束 ALTER TABLE tb_student DROP FOREIGN KEY fk_class_id; -- 删除唯一性约束 ALTER TABLE tb_student DROP INDEX uniq_name; 文章转自： https://blog.csdn.net/pan_junbiao/article/details/86158117\n","date":"2020-03-31","permalink":"https://daemon365.dev/2020/03/31/mysql%E6%95%B0%E6%8D%AE%E5%AE%8C%E6%95%B4%E6%80%A7%E7%BA%A6%E6%9D%9F/","tags":["mysql"],"title":"MySQL数据完整性约束"},{"content":"Go的web工作原理 在Go中使用及其简单的代码即可开启一个web服务。如下：\n//开启web服务 func test(){ http.HandleFunc(\u0026quot;/\u0026quot;, sayHello) err := http.ListenAndServe(\u0026quot;:9090\u0026quot;,nil) if err!=nil { log.Fatal(\u0026quot;ListenAndServer:\u0026quot;,err) } } func sayHello(w http.ResponseWriter, r *http.Request){ r.ParseForm() fmt.Println(\u0026quot;path\u0026quot;,r.URL.Path) fmt.Println(\u0026quot;scheme\u0026quot;,r.URL.Scheme) fmt.Fprintf(w, \u0026quot;Hello Guest!\u0026quot;) } 在使用ListenAndServe这个方法时，系统就会给我们指派一个路由器，DefaultServeMux是系统默认使用的路由器，如果ListenAndServe这个方法的第2个参数传入nil，系统就会默认使用DefaultServeMux。当然，这里也可以传入自定义的路由器。\n先来看http.HandleFunc(\u0026quot;/\u0026quot;, sayHello)，从HandleFunc方法点进去，如下：\nfunc HandleFunc(pattern string, handler func(ResponseWriter, *Request)) { DefaultServeMux.HandleFunc(pattern, handler) } 在这里调用了DefaultServeMux的HandleFunc方法，这个方法有两个参数，pattern是匹配的路由规则，handler表示这个路由规则对应的处理方法，并且这个处理方法有两个参数。\n在我们书写的代码示例中，pattern对应/，handler对应sayHello，当我们在浏览器中输入http://localhost:9090时，就会触发sayHello方法。\n我们再顺着DefaultServeMux的HandleFunc方法继续点下去，如下：\nfunc (mux *ServeMux) HandleFunc(pattern string, handler func(ResponseWriter, *Request)) { mux.Handle(pattern, HandlerFunc(handler)) } 在这个方法中，路由器又调用了Handle方法，注意这个Handle方法的第2个参数，将之前传入的handler这个响应方法强制转换成了HandlerFunc类型。\n这个HandlerFunc类型到底是个什么呢？如下：\ntype HandlerFunc func(ResponseWriter, *Request) 看来和我们定义的SayHello方法的类型都差不多。但是！！！ 这个HandlerFunc默认实现了ServeHTTP接口！这样HandlerFunc对象就有了ServeHTTP方法！如下：\n// ServeHTTP calls f(w, r). func (f HandlerFunc) ServeHTTP(w ResponseWriter, r *Request) { f(w, r) } 这个细节是十分重要的，因为这一步关乎到当路由规则匹配时，相应的响应方法是否会被调用的问题！这个方法的调用时机会在下一小节中讲到。\n接下来，我们返回去继续看mux的Handle方法，也就是这段代码mux.Handle(pattern, HandlerFunc(handler))。这段代码做了哪些事呢？源码如下：\nfunc (mux *ServeMux) Handle(pattern string, handler Handler) { mux.mu.Lock() defer mux.mu.Unlock() if pattern == \u0026quot;\u0026quot; { panic(\u0026quot;http: invalid pattern \u0026quot; + pattern) } if handler == nil { panic(\u0026quot;http: nil handler\u0026quot;) } if mux.m[pattern].explicit { panic(\u0026quot;http: multiple registrations for \u0026quot; + pattern) } if mux.m == nil { mux.m = make(map[string]muxEntry) } mux.m[pattern] = muxEntry{explicit: true, h: handler, pattern: pattern} if pattern[0] != '/' { mux.hosts = true } // Helpful behavior: // If pattern is /tree/, insert an implicit permanent redirect for /tree. // It can be overridden by an explicit registration. n := len(pattern) if n \u0026gt; 0 \u0026amp;\u0026amp; pattern[n-1] == '/' \u0026amp;\u0026amp; !mux.m[pattern[0:n-1]].explicit { // If pattern contains a host name, strip it and use remaining // path for redirect. path := pattern if pattern[0] != '/' { // In pattern, at least the last character is a '/', so // strings.Index can't be -1. path = pattern[strings.Index(pattern, \u0026quot;/\u0026quot;):] } url := \u0026amp;url.URL{Path: path} mux.m[pattern[0:n-1]] = muxEntry{h: RedirectHandler(url.String(), StatusMovedPermanently), pattern: pattern} } } 代码挺多，其实主要就做了一件事，向DefaultServeMux的map[string]muxEntry中增加对应的路由规则和handler。\nmap[string]muxEntry是个什么鬼？\nmap是一个字典对象，它保存的是key-value。 [string]表示这个字典的key是string类型的，这个key值会保存我们的路由规则。 muxEntry是一个实例对象，这个对象内保存了路由规则对应的处理方法。 找到相应代码，如下：\n//路由器 type ServeMux struct { mu sync.RWMutex m map[string]muxEntry //路由规则，一个string对应一个mux实例对象，map的key就是注册的路由表达式(string类型的) hosts bool // whether any patterns contain hostnames } //muxEntry type muxEntry struct { explicit bool h Handler //这个路由表达式对应哪个handler pattern string } //路由响应方法 type Handler interface { ServeHTTP(ResponseWriter, *Request) //handler的路由实现器 } ServeMux就是这个系统默认的路由器。\n最后，总结一下这个部分：\n调用http.HandleFunc(\u0026quot;/\u0026quot;, sayHello) 调用DefaultServeMux的HandleFunc()，把我们定义的sayHello()包装成HandlerFunc类型 继续调用DefaultServeMux的Handle()，向DefaultServeMux的map[string]muxEntry中增加路由规则和对应的handler OK，这部分代码做的事就这么多，第一部分结束。\n第二部分主要就是研究这句代码err := http.ListenAndServe(\u0026quot;:9090\u0026quot;,nil)，也就是ListenAndServe这个方法。从这个方法点进去，如下：\nfunc ListenAndServe(addr string, handler Handler) error { server := \u0026amp;Server{Addr: addr, Handler: handler} return server.ListenAndServe() } 在这个方法中，初始化了一个server对象，然后调用这个server对象的ListenAndServe方法，在这个方法中，如下：\nfunc (srv *Server) ListenAndServe() error { addr := srv.Addr if addr == \u0026quot;\u0026quot; { addr = \u0026quot;:http\u0026quot; } ln, err := net.Listen(\u0026quot;tcp\u0026quot;, addr) if err != nil { return err } return srv.Serve(tcpKeepAliveListener{ln.(*net.TCPListener)}) } 在这个方法中，调用了net.Listen(\u0026quot;tcp\u0026quot;, addr)，也就是底层用TCP协议搭建了一个服务，然后监控我们设置的端口。\n代码的最后，调用了srv的Serve方法，如下：\nfunc (srv *Server) Serve(l net.Listener) error { defer l.Close() if fn := testHookServerServe; fn != nil { fn(srv, l) } var tempDelay time.Duration // how long to sleep on accept failure if err := srv.setupHTTP2_Serve(); err != nil { return err } srv.trackListener(l, true) defer srv.trackListener(l, false) baseCtx := context.Background() // base is always background, per Issue 16220 ctx := context.WithValue(baseCtx, ServerContextKey, srv) ctx = context.WithValue(ctx, LocalAddrContextKey, l.Addr()) for { rw, e := l.Accept() if e != nil { select { case \u0026lt;-srv.getDoneChan(): return ErrServerClosed default: } if ne, ok := e.(net.Error); ok \u0026amp;\u0026amp; ne.Temporary() { if tempDelay == 0 { tempDelay = 5 * time.Millisecond } else { tempDelay *= 2 } if max := 1 * time.Second; tempDelay \u0026gt; max { tempDelay = max } srv.logf(\u0026quot;http: Accept error: %v; retrying in %v\u0026quot;, e, tempDelay) time.Sleep(tempDelay) continue } return e } tempDelay = 0 c := srv.newConn(rw) c.setState(c.rwc, StateNew) // before Serve can return go c.serve(ctx) } } 最后3段代码比较重要，也是Go语言支持高并发的体现，如下：\nc := srv.newConn(rw) c.setState(c.rwc, StateNew) // before Serve can return go c.serve(ctx) 上面那一大坨代码，总体意思是进入方法后，首先开了一个for循环，在for循环内时刻Accept请求，请求来了之后，会为每个请求创建一个Conn，然后单独开启一个goroutine，把这个请求的数据当做参数扔给这个Conn去服务：go c.serve()。用户的每一次请求都是在一个新的goroutine去服务，每个请求间相互不影响。\n在conn的serve方法中，有一句代码很重要，如下：\nserverHandler{c.server}.ServeHTTP(w, w.req) 表示serverHandler也实现了ServeHTTP接口，ServeHTTP方法实现如下：\nfunc (sh serverHandler) ServeHTTP(rw ResponseWriter, req *Request) { handler := sh.srv.Handler if handler == nil { handler = DefaultServeMux } if req.RequestURI == \u0026quot;*\u0026quot; \u0026amp;\u0026amp; req.Method == \u0026quot;OPTIONS\u0026quot; { handler = globalOptionsHandler{} } handler.ServeHTTP(rw, req) } 在这里如果handler为空（这个handler就可以理解为是我们自定义的路由器），就会使用系统默认的DefaultServeMux，代码的最后调用了DefaultServeMux的ServeHTTP()\nfunc (mux *ServeMux) ServeHTTP(w ResponseWriter, r *Request) { if r.RequestURI == \u0026quot;*\u0026quot; { if r.ProtoAtLeast(1, 1) { w.Header().Set(\u0026quot;Connection\u0026quot;, \u0026quot;close\u0026quot;) } w.WriteHeader(StatusBadRequest) return } h, _ := mux.Handler(r) //这里返回的h是Handler接口对象 h.ServeHTTP(w, r) //调用Handler接口对象的ServeHTTP方法实际上就调用了我们定义的sayHello方法 } 路由器接收到请求之后，如果是*那么关闭链接，如果不是*就调用mux.Handler(r)返回该路由对应的处理Handler，然后执行该handler的ServeHTTP方法，也就是这句代码h.ServeHTTP(w, r)，mux.Handler(r)做了什么呢？如下：\nfunc (mux *ServeMux) Handler(r *Request) (h Handler, pattern string) { if r.Method != \u0026quot;CONNECT\u0026quot; { if p := cleanPath(r.URL.Path); p != r.URL.Path { _, pattern = mux.handler(r.Host, p) url := *r.URL url.Path = p return RedirectHandler(url.String(), StatusMovedPermanently), pattern } } return mux.handler(r.Host, r.URL.Path) } func (mux *ServeMux) handler(host, path string) (h Handler, pattern string) { mux.mu.RLock() defer mux.mu.RUnlock() // Host-specific pattern takes precedence over generic ones if mux.hosts { h, pattern = mux.match(host + path) } if h == nil { h, pattern = mux.match(path) } if h == nil { h, pattern = NotFoundHandler(), \u0026quot;\u0026quot; } return } func (mux *ServeMux) match(path string) (h Handler, pattern string) { var n = 0 for k, v := range mux.m { //mux.m就是系统默认路由的map if !pathMatch(k, path) { continue } if h == nil || len(k) \u0026gt; n { n = len(k) h = v.h pattern = v.pattern } } return } 它会根据用户请求的URL到路由器里面存储的map中匹配，匹配成功就会返回存储的handler，调用这个handler的ServeHTTP()就可以执行到相应的处理方法了，这个处理方法实际上就是我们刚开始定义的sayHello()，只不过这个sayHello()被HandlerFunc又包了一层，因为HandlerFunc实现了ServeHTTP接口，所以在调用HandlerFunc对象的ServeHTTP()时，实际上在ServeHTTP ()的内部调用了我们的sayHello()。\n总结一下：\n调用http.ListenAndServe(\u0026quot;:9090\u0026quot;,nil) 实例化server 调用server的ListenAndServe() 调用server的Serve方法，开启for循环，在循环中Accept请求 对每一个请求实例化一个Conn，并且开启一个goroutine为这个请求进行服务go c.serve() 读取每个请求的内容c.readRequest() 调用serverHandler的ServeHTTP()，如果handler为空，就把handler设置为系统默认的路由器DefaultServeMux 调用handler的ServeHTTP() =\u0026gt;实际上是调用了DefaultServeMux的ServeHTTP() 在ServeHTTP()中会调用路由对应处理handler 在路由对应处理handler中会执行sayHello() 有一个需要注意的点： DefaultServeMux和路由对应的处理方法handler都实现了ServeHTTP接口，他们俩都有ServeHTTP方法，但是方法要达到的目的不同，在DefaultServeMux的ServeHttp()里会执行路由对应的处理handler的ServeHttp()。\n文章转自 https://juejin.cn/post/6844903550993039374\n","date":"2020-03-21","permalink":"https://daemon365.dev/2020/03/21/golang-web%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/","tags":["go","源码分析"],"title":"golang web源码解析"},{"content":"Redis是一种内存型数据库，一旦服务器进程退出，数据库的数据就会丢失，为了解决这个问题，Redis提供了两种持久化的方案，将内存中的数据保存到磁盘中，避免数据的丢失。\nRDB持久化 redis提供了RDB持久化的功能，这个功能可以将redis在内存中的的状态保存到硬盘中，它可以手动执行。\n也可以再redis.conf中配置，定期执行。\nRDB持久化产生的RDB文件是一个经过压缩的二进制文件，这个文件被保存在硬盘中，redis可以通过这个文件还原数据库当时的状态。\n内存数据保存到磁盘\n在指定的时间间隔内生成数据集的时间点快照（point-in-time snapshot）\n优点：速度快，适合做备份，主从复制就是基于RDB持久化功能实现\nrdb通过再redis中使用save命令触发 rdb\nrdb配置参数： dir /data/6379/ dbfilename dbmp.rdb 每过900秒 有1个操作就进行持久化 save 900秒 1个修改类的操作 save 300秒 10个操作 save 60秒 10000个操作 save 900 1 save 300 10 save 60 10000 redis持久化之RDB实践 1.启动redis服务端,准备配置文件\ndaemonize yes port 6379 logfile /data/6379/redis.log dir /data/6379 #定义持久化文件存储位置 dbfilename dbmp.rdb #rdb持久化文件 bind 10.0.0.10 127.0.0.1 #redis绑定地址 requirepass redhat #redis登录密码 save 900 1 #rdb机制 每900秒 有1个修改记录 save 300 10 #每300秒 10个修改记录 save 60 10000 #每60秒内 10000修改记录 2.启动redis服务端\n3.登录redis设置一个key\nredis-cli -a redhat 4.此时检查目录，/data/6379底下没有dbmp.rdb文件\n5.通过save触发持久化，将数据写入RDB文件\n127.0.0.1:6379\u0026gt; set age 18 OK 127.0.0.1:6379\u0026gt; save OK AOF持久化 AOF（append-only log file）\n记录服务器执行的所有变更操作命令（例如set del等），并在服务器启动时，通过重新执行这些命令来还原数据集\nAOF 文件中的命令全部以redis协议的格式保存，新命令追加到文件末尾。\n优点：最大程序保证数据不丢\n缺点：日志记录非常大\nredis-client 写入数据 \u0026gt; redis-server 同步命令 \u0026gt; AOF文件 配置参数\nAOF持久化配置，两条参数 appendonly yes appendfsync always 总是修改类的操作 everysec 每秒做一次持久化 no 依赖于系统自带的缓存大小机制 redis持久化之AOF实践 1.准备aof配置文件 redis.conf\ndaemonize yes port 6379 logfile /data/6379/redis.log dir /data/6379 dbfilename dbmp.rdb requirepass redhat save 900 1 save 300 10 save 60 10000 appendonly yes appendfsync everysec 2.启动redis服务\nredis-server /etc/redis.conf 3.检查redis数据目录/data/6379/是否产生了aof文件\n[root@web02 6379]## ls appendonly.aof dbmp.rdb redis.log 4.登录redis-cli，写入数据，实时检查aof文件信息\n[root@web02 6379]## tail -f appendonly.aof 5.设置新key，检查aof信息，然后关闭redis，检查数据是否持久化\nredis-cli -a redhat shutdown redis-server /etc/redis.conf redis-cli -a redhat redis 持久化方式有哪些？有什么区别 rdb：基于快照的持久化，速度更快，一般用作备份，主从复制也是依赖于rdb持久化功能\naof：以追加的方式记录redis操作日志的文件。可以最大程度的保证redis数据安全，类似于mysql的binlog\nredis不重启，切换RDB备份到AOF备份 确保redis版本在2.2以上 [root@pyyuc /data 22:23:30]#redis-server -v Redis server v=4.0.10 sha=00000000:0 malloc=jemalloc-4.0.3 bits=64 build=64cb6afcf41664c 本文在redis4.0中，通过config set命令，达到不重启redis服务，从RDB持久化切换为AOF\n实验环境准备 redis.conf服务端配置文件\ndaemonize yes port 6379 logfile /data/6379/redis.log dir /data/6379 dbfilename dbmp.rdb save 900 1 #rdb机制 每900秒 有1个修改记录 save 300 10 #每300秒 10个修改记录 save 60 10000 #每60秒内 10000修改记录 启动redis服务端\nredis-server redis.conf 登录redis-cli插入数据，手动持久化\n127.0.0.1:6379\u0026gt; set name chaoge OK 127.0.0.1:6379\u0026gt; set age 18 OK 127.0.0.1:6379\u0026gt; set addr shahe OK 127.0.0.1:6379\u0026gt; save OK 检查RDB文件\n[root@pyyuc /data 22:34:16]#ls 6379/ dbmp.rdb redis.log 备份这个rdb文件，保证数据安全 [root@pyyuc /data/6379 22:35:38]#cp dbmp.rdb /opt/ 执行命令，开启AOF持久化 127.0.0.1:6379\u0026gt; CONFIG set appendonly yes #开启AOF功能 OK 127.0.0.1:6379\u0026gt; CONFIG SET save \u0026quot;\u0026quot; #关闭RDB功能 OK 确保数据库的key数量正确 127.0.0.1:6379\u0026gt; keys * 1) \u0026quot;addr\u0026quot; 2) \u0026quot;age\u0026quot; 3) \u0026quot;name\u0026quot; 确保插入新的key，AOF文件会记录 127.0.0.1:6379\u0026gt; set title golang OK 此时RDB已经正确切换AOF，注意还得修改redis.conf添加AOF设置，不然重启后，通过config set的配置将丢失\n","date":"2020-03-21","permalink":"https://daemon365.dev/2020/03/21/redis%E6%8C%81%E4%B9%85%E5%8C%96/","tags":["redis"],"title":"redis持久化"},{"content":"redis介绍 Redis 是完全开源的，遵守 BSD 协议，是一个高性能的 key-value 数据库。\nRedis 与其他 key - value 缓存产品有以下三个特点：\nRedis支持数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。 Redis不仅仅支持简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储。 Redis支持数据的备份，即master-slave模式的数据备份。 redis的安装 brew install redis(mac) yum install redis(centos) apt-get install redis(ubuntu) redis的命令网站 Redis 命令参考 — Redis 命令参考 (redisfans.com)\nredis的基本操作 redis的五大数据类型 redis的五大数据类型是: String(字符串)、Hash(哈希)、List(列表)、Set(集合)、和zset(sorted set:有序集合)\nredis键操作 keys *查看当前库所有key (匹配：keys *1) exists key判断某个key是否存在 type key 查看你的key是什么类型 del key 删除指定的key数据 unlink key 根据value选择非阻塞删除 仅将keys从keyspace元数据中删除，真正的删除会在后续异步操作。 expire key 10 10秒钟：为给定的key设置过期时间 ttl key 查看还有多少秒过期，-1表示永不过期，-2表示已过期 select命令切换数据库 dbsize查看当前数据库的key的数量 flushdb清空当前库 flushall通杀全部库 字符串(String) 简介 String是Redis最基本的类型，你可以理解成与Memcached一模一样的类型，一个key对应一个value。\nString类型是二进制安全的。意味着Redis的string可以包含任何数据。比如jpg图片或者序列化的对象。\nString类型是Redis最基本的数据类型，一个Redis中字符串value最多可以是512M\n常用命令 set \u0026lt;key\u0026gt;\u0026lt;value\u0026gt;添加键值对 get \u0026lt;key\u0026gt;查询对应键值 append \u0026lt;key\u0026gt;\u0026lt;value\u0026gt;将给定的 追加到原值的末尾 strlen \u0026lt;key\u0026gt;获得值的长度 setnx \u0026lt;key\u0026gt;\u0026lt;value\u0026gt;只有在 key 不存在时 设置 key 的值 incr \u0026lt;key\u0026gt; 将 key 中储存的数字值增1 只能对数字值操作，如果为空，新增值为1 decr \u0026lt;key\u0026gt; 将 key 中储存的数字值减1 只能对数字值操作，如果为空，新增值为-1 incrby / decrby \u0026lt;key\u0026gt;\u0026lt;步长\u0026gt;将 key 中储存的数字值增减。自定义步长。 数据结构 String的数据结构为简单动态字符串(Simple Dynamic String,缩写SDS)。是可以修改的字符串，内部结构实现上类似于Java的ArrayList，采用预分配冗余空间的方式来减少内存的频繁分配.\n如图中所示，内部为当前字符串实际分配的空间capacity一般要高于实际字符串长度len。当字符串长度小于1M时，扩容都是加倍现有的空间，如果超过1M，扩容时一次只会多扩1M的空间。需要注意的是字符串最大长度为512M。\n列表(List) 简介 单键多值\nRedis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。\n它的底层实际是个双向链表，对两端的操作性能很高，通过索引下标的操作中间的节点性能会较差。\n常用命令 lpush/rpush \u0026lt;key\u0026gt;\u0026lt;value1\u0026gt;\u0026lt;value2\u0026gt;\u0026lt;value3\u0026gt; .... 从左边/右边插入一个或多个值。 lpop/rpop \u0026lt;key\u0026gt;从左边/右边吐出一个值。值在键在，值光键亡。 rpoplpush \u0026lt;key1\u0026gt;\u0026lt;key2\u0026gt;从列表右边吐出一个值，插到列表左边。 lrange \u0026lt;key\u0026gt;\u0026lt;start\u0026gt;\u0026lt;stop\u0026gt; 按照索引下标获得元素(从左到右) lrange mylist 0 -1 0左边第一个，-1右边第一个，（0-1表示获取所有） lindex \u0026lt;key\u0026gt;\u0026lt;index\u0026gt;按照索引下标获得元素(从左到右) llen \u0026lt;key\u0026gt;获得列表长度 linsert \u0026lt;key\u0026gt; before \u0026lt;value\u0026gt;\u0026lt;newvalue\u0026gt;在的后面插入插入值 lrem \u0026lt;key\u0026gt;\u0026lt;n\u0026gt;\u0026lt;value\u0026gt;从左边删除n个value(从左到右) lset\u0026lt;key\u0026gt;\u0026lt;index\u0026gt;\u0026lt;value\u0026gt;将列表key下标为index的值替换成value 数据结构 List的数据结构为快速链表quickList。\n首先在列表元素较少的情况下会使用一块连续的内存存储，这个结构是ziplist，也即是压缩列表。它将所有的元素紧挨着一起存储，分配的是一块连续的内存。当数据量比较多的时候才会改成quicklist。因为普通的链表需要的附加指针空间太大，会比较浪费空间。比如这个列表里存的只是int类型的数据，结构上还需要两个额外的指针prev和next。\nRedis将链表和ziplist结合起来组成了quicklist。也就是将多个ziplist使用双向指针串起来使用。这样既满足了快速的插入删除性能，又不会出现太大的空间冗余。\n集合(Set) 简介 Redis set对外提供的功能与list类似是一个列表的功能，特殊之处在于set是可以****自动排重****的，当你需要存储一个列表数据，又不希望出现重复数据时，set是一个很好的选择，并且set提供了判断某个成员是否在一个set集合内的重要接口，这个也是list所不能提供的。\nRedis的Set是string类型的无序集合。它底层其实是一个value为null的hash表，所以添加，删除，查找的****复杂度都是O(1)****。\n一个算法，随着数据的增加，执行时间的长短，如果是O(1)，数据增加，查找数据的时间不变\n常用命令 sadd \u0026lt;key\u0026gt;\u0026lt;value1\u0026gt;\u0026lt;value2\u0026gt; ..... 将一个或多个 member 元素加入到集合 key 中，已经存在的 member 元素将被忽略 smembers \u0026lt;key\u0026gt;取出该集合的所有值。 sismember \u0026lt;key\u0026gt;\u0026lt;value\u0026gt;判断集合是否为含有该值，有1，没有0 scard\u0026lt;key\u0026gt;返回该集合的元素个数。 srem \u0026lt;key\u0026gt;\u0026lt;value1\u0026gt;\u0026lt;value2\u0026gt; .... 删除集合中的某个元素。 spop \u0026lt;key\u0026gt;*随机从该集合中吐出一个值。* srandmember \u0026lt;key\u0026gt;\u0026lt;n\u0026gt;随机从该集合中取出n个值。不会从集合中删除 。 smove \u0026lt;source\u0026gt;\u0026lt;destination\u0026gt;value把集合中一个值从一个集合移动到另一个集合 sinter \u0026lt;key1\u0026gt;\u0026lt;key2\u0026gt;返回两个集合的交集元素。 sunion \u0026lt;key1\u0026gt;\u0026lt;key2\u0026gt;返回两个集合的并集元素。 sdiff \u0026lt;key1\u0026gt;\u0026lt;key2\u0026gt;返回两个集合的****差集****元素(key1中的，不包含key2中的) 数据结构 Set数据结构是dict字典，字典是用哈希表实现的。 Java中HashSet的内部实现使用的是HashMap，只不过所有的value都指向同一个对象。Redis的set结构也是一样，它的内部也使用hash结构，所有的value都指向同一个内部值。\n哈希(Hash) 简介 Redis hash 是一个键值对集合。\nRedis hash是一个string类型的field和value的映射表，hash特别适合用于存储对象。\n常用命令 hset \u0026lt;key\u0026gt;\u0026lt;field\u0026gt;\u0026lt;value\u0026gt;给集合中的 键赋值 hget \u0026lt;key1\u0026gt;\u0026lt;field\u0026gt;从集合取出 value hmset \u0026lt;key1\u0026gt;\u0026lt;field1\u0026gt;\u0026lt;value1\u0026gt;\u0026lt;field2\u0026gt;\u0026lt;value2\u0026gt;... 批量设置hash的值 hexists\u0026lt;key1\u0026gt;\u0026lt;field\u0026gt;查看哈希表 key 中，给定域 field 是否存在。 hkeys \u0026lt;key\u0026gt;列出该hash集合的所有field hvals \u0026lt;key\u0026gt;列出该hash集合的所有value hincrby \u0026lt;key\u0026gt;\u0026lt;field\u0026gt;\u0026lt;increment\u0026gt;为哈希表 key 中的域 field 的值加上增量 1 -1 hsetnx \u0026lt;key\u0026gt;\u0026lt;field\u0026gt;\u0026lt;value\u0026gt;将哈希表 key 中的域 field 的值设置为 value ，当且仅当域 field 不存在 . 数据结构 Hash类型对应的数据结构是两种：ziplist（压缩列表），hashtable（哈希表）。当field-value长度较短且个数较少时，使用ziplist，否则使用hashtable。\n有序集合Zset(sorted set) 简介 Redis有序集合zset与普通集合set非常相似，是一个没有重复元素的字符串集合。\n不同之处是有序集合的每个成员都关联了一个****评分（score）****,这个评分（score）被用来按照从最低分到最高分的方式排序集合中的成员。集合的成员是唯一的，但是评分可以是重复了 。\n因为元素是有序的, 所以你也可以很快的根据评分（score）或者次序（position）来获取一个范围的元素。\n访问有序集合的中间元素也是非常快的,因此你能够使用有序集合作为一个没有重复成员的智能列表。\n常用命令 zadd \u0026lt;key\u0026gt;\u0026lt;score1\u0026gt;\u0026lt;value1\u0026gt;\u0026lt;score2\u0026gt;\u0026lt;value2\u0026gt;… 将一个或多个 member 元素及其 score 值加入到有序集 key 当中。 zrange \u0026lt;key\u0026gt;\u0026lt;start\u0026gt;\u0026lt;stop\u0026gt; [WITHSCORES] 返回有序集 key 中，下标在之间的元素带WITHSCORES，可以让分数一起和值返回到结果集。 zrangebyscore key minmax [withscores] [limit offset count] 返回有序集 key 中，所有 score 值介于 min 和 max 之间(包括等于 min 或 max )的成员。有序集成员按 score 值递增(从小到大)次序排列。 zrevrangebyscore key maxmin [withscores] [limit offset count] 同上，改为从大到小排列。 zincrby \u0026lt;key\u0026gt;\u0026lt;increment\u0026gt;\u0026lt;value\u0026gt; 为元素的score加上增量 zrem \u0026lt;key\u0026gt;\u0026lt;value\u0026gt;删除该集合下，指定值的元素 zcount \u0026lt;key\u0026gt;\u0026lt;min\u0026gt;\u0026lt;max\u0026gt;统计该集合，分数区间内的元素个数 zrank \u0026lt;key\u0026gt;\u0026lt;value\u0026gt;返回该值在集合中的排名，从0开始。 数据结构 SortedSet(zset)是Redis提供的一个非常特别的数据结构，一方面它等价于Java的数据结构Map\u0026lt;String, Double\u0026gt;，可以给每一个元素value赋予一个权重score，另一方面它又类似于TreeSet，内部的元素会按照权重score进行排序，可以得到每个元素的名次，还可以通过score的范围来获取元素的列表。\nzset底层使用了两个数据结构\nhash，hash的作用就是关联元素value和权重score，保障元素value的唯一性，可以通过元素value找到相应的score值。 跳跃表，跳跃表的目的在于给元素value排序，根据score的范围获取元素列表。 Redis新数据类型 Bitmaps 简介 现代计算机用二进制（位） 作为信息的基础单位， 1个字节等于8位， 例如“abc”字符串是由3个字节组成， 但实际在计算机存储时将其用二进制表示， “abc”分别对应的ASCII码分别是97、 98、 99， 对应的二进制分别是01100001、 01100010和01100011，如下图\n合理地使用操作位能够有效地提高内存使用率和开发效率。\nRedis提供了Bitmaps这个“数据类型”可以实现对位的操作：\nBitmaps本身不是一种数据类型， 实际上它就是字符串（key-value） ， 但是它可以对字符串的位进行操作。 Bitmaps单独提供了一套命令， 所以在Redis中使用Bitmaps和使用字符串的方法不太相同。 可以把Bitmaps想象成一个以位为单位的数组， 数组的每个单元只能存储0和1， 数组的下标在Bitmaps中叫做偏移量。 命令 setbit （1）格式\nsetbit\u0026lt;key\u0026gt;\u0026lt;offset\u0026gt;\u0026lt;value\u0026gt;设置Bitmaps中某个偏移量的值（0或1）\n*offset:偏移量从0开始\n（2）实例\n每个独立用户是否访问过网站存放在Bitmaps中， 将访问的用户记做1， 没有访问的用户记做0， 用偏移量作为用户的id。\n设置键的第offset个位的值（从0算起） ， 假设现在有20个用户，userid=1， 6， 11， 15， 19的用户对网站进行了访问， 那么当前Bitmaps初始化结果如图\nunique:users:20201106代表2020-11-06这天的独立访问用户的Bitmaps\n注：\n很多应用的用户id以一个指定数字（例如10000） 开头， 直接将用户id和Bitmaps的偏移量对应势必会造成一定的浪费， 通常的做法是每次做setbit操作时将用户id减去这个指定数字。\n在第一次初始化Bitmaps时， 假如偏移量非常大， 那么整个初始化过程执行会比较慢， 可能会造成Redis的阻塞。\ngetbit （1）格式 getbit获取Bitmaps中某个偏移量的值\n获取键的第offset位的值（从0开始算）\n（2）实例 获取id=8的用户是否在2020-11-06这天访问过， 返回0说明没有访问过\n注：因为100根本不存在，所以也是返回0\nbitcount 统计字符串被设置为1的bit数。一般情况下，给定的整个字符串都会被进行计数，通过指定额外的 start 或 end 参数，可以让计数只在特定的位上进行。start 和 end 参数的设置，都可以使用负数值：比如 -1 表示最后一个位，而 -2 表示倒数第二个位，start、end 是指bit组的字节的下标数，二者皆包含。 （1）格式 bitcount[start end] 统计字符串从start字节到end字节比特值为1的数量\n（2）实例 计算2022-11-06这天的独立访问用户数量\nstart和end代表起始和结束字节数， 下面操作计算用户id在第1个字节到第3个字节之间的独立访问用户数， 对应的用户id是11， 15， 19。\n举例： K1 【01000001 01000000 00000000 00100001】，对应【0，1，2，3】 bitcount K1 1 2 ： 统计下标1、2字节组中bit=1的个数，即01000000 00000000 --》bitcount K1 1 2 --》1 bitcount K1 1 3 ： 统计下标1、2字节组中bit=1的个数，即01000000 00000000 00100001 --》bitcount K1 1 3　--》3 bitcount K1 0 -2 ： 统计下标0到下标倒数第2，字节组中bit=1的个数，即01000001 01000000 00000000 --》bitcount K1 0 -2　--》3 注意：redis的setbit设置或清除的是bit位置，而bitcount计算的是byte位置。\nbitop (1)格式 bitop and(or/not/xor) [key…]\nbitop是一个复合操作， 它可以做多个Bitmaps的and（交集） 、 or（并集） 、 not（非） 、 xor（异或） 操作并将结果保存在destkey中。\n(2)实例\n2020-11-04 日访问网站的userid=1,2,5,9。\nsetbit unique:users:20201104 1 1\nsetbit unique:users:20201104 2 1\nsetbit unique:users:20201104 5 1\nsetbit unique:users:20201104 9 1\n2020-11-03 日访问网站的userid=0,1,4,9。\nsetbit unique:users:20201103 0 1\nsetbit unique:users:20201103 1 1\nsetbit unique:users:20201103 4 1\nsetbit unique:users:20201103 9 1\n计算出两天都访问过网站的用户数量\nbitop and unique:users:and:20201104_03\nunique:users:20201103unique:users:20201104\n计算出任意一天都访问过网站的用户数量（例如月活跃就是类似这种） ， 可以使用or求并集\nBitmaps与set对比 假设网站有1亿用户， 每天独立访问的用户有5千万， 如果每天用集合类型和Bitmaps分别存储活跃用户可以得到表\nset和Bitmaps存储一天活跃用户对比 数据类型 每个用户id占用空间 需要存储的用户量 全部内存量 集合类型 64位 50000000 64位*50000000 = 400MB Bitmaps 1位 100000000 1位*100000000 = 12.5MB 很明显， 这种情况下使用Bitmaps能节省很多的内存空间， 尤其是随着时间推移节省的内存还是非常可观的\nset和Bitmaps存储独立用户空间对比 数据类型 一天 一个月 一年 集合类型 400MB 12GB 144GB Bitmaps 12.5MB 375MB 4.5GB 但Bitmaps并不是万金油， 假如该网站每天的独立访问用户很少， 例如只有10万（大量的僵尸用户） ， 那么两者的对比如下表所示， 很显然， 这时候使用Bitmaps就不太合适了， 因为基本上大部分位都是0。\nset和Bitmaps存储一天活跃用户对比（独立用户比较少） 数据类型 每个userid占用空间 需要存储的用户量 全部内存量 集合类型 64位 100000 64位*100000 = 800KB Bitmaps 1位 100000000 1位*100000000 = 12.5MB HyperLogLog 简介 在工作当中，我们经常会遇到与统计相关的功能需求，比如统计网站PV（PageView页面访问量）,可以使用Redis的incr、incrby轻松实现。 但像UV（UniqueVisitor，独立访客）、独立IP数、搜索记录数等需要去重和计数的问题如何解决？这种求集合中不重复元素个数的问题称为基数问题。 解决基数问题有很多种方案： （1）数据存储在MySQL表中，使用distinct count计算不重复个数\n（2）使用Redis提供的hash、set、bitmaps等数据结构来处理\n以上的方案结果精确，但随着数据不断增加，导致占用空间越来越大，对于非常大的数据集是不切实际的。\n能否能够降低一定的精度来平衡存储空间？Redis推出了HyperLogLog\nRedis HyperLogLog 是用来做基数统计的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定的、并且是很小的。\n在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。\n但是，因为 HyperLogLog 只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog 不能像集合那样，返回输入的各个元素。\n什么是基数?\n比如数据集 {1, 3, 5, 7, 5, 7, 8}， 那么这个数据集的基数集为 {1, 3, 5 ,7, 8}, 基数(不重复元素)为5。 基数估计就是在误差可接受的范围内，快速计算基数。\n命令 1、pfadd\n（1）格式\npfadd \u0026lt; element\u0026gt; [element \u0026hellip;] 添加指定元素到 HyperLogLog 中\n（2）实例\n​\t将所有元素添加到指定HyperLogLog数据结构中。如果执行命令后HLL估计的近似基数发生变化，则返回1，否则返回0。\n2、pfcount\n（1）格式\npfcount [key \u0026hellip;] 计算HLL的近似基数，可以计算多个HLL，比如用HLL存储每天的UV，计算一周的UV可以使用7天的UV合并计算即可\n（2）实例\n3、pfmerge\n（1）格式\npfmerge [sourcekey \u0026hellip;] 将一个或多个HLL合并后的结果存储在另一个HLL中，比如每月活跃用户可以使用每天的活跃用户来合并计算可得\n（2）实例\nGeospatial 简介 Redis 3.2 中增加了对GEO类型的支持。GEO，Geographic，地理信息的缩写。该类型，就是元素的2维坐标，在地图上就是经纬度。redis基于该类型，提供了经纬度设置，查询，范围查询，距离查询，经纬度Hash等常见操作。\n命令 1、geoadd\n（1）格式\ngeoadd\u0026lt; longitude\u0026gt; [longitude latitude member\u0026hellip;] 添加地理位置（经度，纬度，名称）\n2）实例\ngeoadd china:city 121.47 31.23 shanghai\ngeoadd china:city 106.50 29.53 chongqing 114.05 22.52 shenzhen 116.38 39.90 beijing\n两极无法直接添加，一般会下载城市数据，直接通过 Java 程序一次性导入。\n有效的经度从 -180 度到 180 度。有效的纬度从 -85.05112878 度到 85.05112878 度。\n当坐标位置超出指定范围时，该命令将会返回一个错误。\n已经添加的数据，是无法再次往里面添加的。\n2、geopos\n（1）格式\ngeopos [member\u0026hellip;] 获得指定地区的坐标值\n（2）实例\n3、geodist\n（1）格式\ngeodist [m|km|ft|mi ] 获取两个位置之间的直线距离\n（2）实例\n获取两个位置之间的直线距离\n单位：\nm 表示单位为米[默认值]。\nkm 表示单位为千米。\nmi 表示单位为英里。\nft 表示单位为英尺。\n如果用户没有显式地指定单位参数， 那么 GEODIST 默认使用米作为单位\n4、georadius\n（1）格式\ngeoradius\u0026lt; longitude\u0026gt;radius m|km|ft|mi 以给定的经纬度为中心，找出某一半径内的元素\n经度 纬度 距离 单位\n（2）实例\n","date":"2020-03-20","permalink":"https://daemon365.dev/2020/03/20/redis%E5%9F%BA%E7%A1%80/","tags":["redis"],"title":"redis基础"},{"content":"nginx是什么 nginx是一个开源的，支持高性能，高并发的www服务和代理服务软件。\n支持高并发，能支持几万并发连接\n资源消耗少，在3万并发连接下开启10个nginx线程消耗的内存不到200M\n可以做http反向代理和负载均衡\n支持异步网络i/o事件模型epoll\nTengine是由淘宝网发起的Web服务器项目。它在Nginx的基础上，针对大访问量网站的需求，添加了很多高级功能和特性。T目标是打造一个高效、稳定、安全、易用的Web平台。\n安装环境依赖包 yum install gcc-c++ pcre pcre-devel zlib zlib-devel gcc patch libffi-devel python-devel zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel openssl openssl-devel -y 安装并启动nginx 下载源码包 http://tengine.taobao.org/download/tengine-2.3.3.tar.gz\n解压缩源码 tar -zxvf tengine-2.3.3.tar.gz配置\n编译安装 开启nginx状态监测功能\n./configure make \u0026amp;\u0026amp; make install nginx操作，进入nginx下的sbin目录（默认在/usr/local下）\n启动./nginx -c指定配置文件 关闭./nginx -s stop 重新加载./nginx -s reload 检查配置文件 ./nginx -t 部署一个web站点 nginx默认站点是Nginx目录下的html文件夹，这里可以从nginx.conf中查到\nlocation / { root html; # 这里是默认的站点html文件夹，也就是 /usr/local/nginx/html 文件夹下的内容 index index.html index.htm; # 站点首页文件名是index.html } 如果要部署网站业务数据，只需要把开发好的程序全放到html目录下即可\n[root@VM-0-3-centos html]# ls /usr/local/nginx/html/ 50x.html index.html 因此只需要通过域名/资源，即可访问\nhttp://a.zhaohaoyu.com Nginx的目录结构 client_body_temp conf fastcgi_temp html logs proxy_temp sbin scgi_temp static uwsgi_temp conf 存放nginx所有配置文件的目录,主要nginx.conf\nhtml 存放nginx默认站点的目录，如index.html、error.html等\nlogs 存放nginx默认日志的目录，如error.log access.log\nsbin 存放nginx主命令的目录,./nginx\nNginx主配置文件解析 Nginx主配置文件/etc/nginx/nginx.conf是一个纯文本类型的文件，整个配置文件是以区块的形式组织的。一般，每个区块以一对大括号{}来表示开始与结束。\nCoreModule核心模块 user www Nginx进程所使用的用户\nworker_processes 1; Nginx运行的work进程数量(建议与CPU数量一致或auto)\nerror_log logs/error.log Nginx错误日志存放路径\npid logs/nginx.pid Nginx服务运行后产生的pid进程号\nevents事件模块 events { worker_connections 1024; # 每个worker进程支持的最大连接数 use epool; # 事件驱动模型, epoll默认 } http内核模块 # 公共的配置定义在http http { # http层开始 ... include mime.types; default_type application/octet-stream; access_log logs/access.log main; # 访问日志 # 使用Server配置网站, 每个Server{}代表一个网站(简称虚拟主机) server { listen 80; # 监听端口, 默认80 server_name localhost; # 提供服务的域名或主机名 location / { root html; # 存放网站代码路径 index index.html index.htm; # 服务器返回的默认页面文件 } # 指定错误代码, 统一定义错误页面, 错误代码重定向到新的Locaiton error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } include 当存在多个域名时，如果所有配置都写在 nginx.conf 主配置文件中，难免会显得杂乱与臃肿。为了方便配置文件的维护，所以需要进行拆分配置。\n# 在nginx.conf中 加入 include a.zhaohaiyu.com/nginx.conf; # 在a.zhaohaiyu.com/nginx.conf中加入 server { listen 8000; server_name test2.com; location / { proxy_set_header Host $host:$server_port; proxy_set_header X-Real-Ip $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; ## proxy_pass http://xxx.xxx.xxx; echo \u0026quot;test2.com\u0026quot;; ## 输出测试 } } # 等于把zhaohaiyu.com/nginx.conf直接吸入nginx.conf中 Nginx虚拟主机 如果每台linux服务器只运行了一个小网站，那么人气低，流量小的草根站长需要承担高额的服务器租赁费，也造成了硬件资源浪费。\n虚拟主机就是将一台服务器分割成多个“虚拟服务器”，每个站点使用各自的硬盘空间，由于省资源，省钱，众多网站都使用虚拟主机来部署网站。\n虚拟主机的概念就是在web服务里的一个独立的网站站点，这个站点对应独立的域名（IP），具有独立的程序和资源目录，可以独立的对外提供服务。 这个独立的站点配置是在nginx.conf中使用server{}代码块标签来表示一个虚拟主机。 Nginx支持多个server{}标签，即支持多个虚拟主机站点。\nnginx status 启用nginx status配置 在默认主机里面加上location或者你希望能访问到的主机里面。\nserver { stub_status on; } 重启nginx 请依照你的环境重启你的nginx\n./ngnix -s reload\n打开status页面 nginx status active connections – 活跃的连接数量\nserver accepts handled requests — 总共处理了11989个连接 , 成功创建11989次握手, 总共处理了11991个请求\nreading — 读取客户端的连接数.\nwriting — 响应数据到客户端的数量\nwaiting — 开启 keep-alive 的情况下,这个值等于 active – (reading+writing), 意思就是 Nginx 已经处理完正在等候下一次请求指令的驻留连接.\nNginx代理 正向代理 正向代理，也就是传说中的代理,他的工作原理就像一个跳板（VPN），简单的说：\n我是一个用户，我访问不了某网站，但是我能访问一个代理服务器，这个代理服务器呢，他能访问那个我不能访问的网站，于是我先连上代理服务器，告诉他我需要那个无法访问网站的内容，代理服务器去取回来，然后返回给我。\n反向代理 对于客户端而言，代理服务器就像是原始服务器。\nnginx实现负载均衡的组件\nngx_http_proxy_module # proxy代理模块，用于把请求抛给服务器节点或者upstream服务器池 实现一个简单的反向代理 机器准备，两台服务器\nmaster 192.168.11.63　主负载 slave 192.168.11.64　web1 主负载均衡节点的配置文件\nworker_processes 1; error_log logs/error.log; pid logs/nginx.pid; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; log_format main '$remote_addr - $remote_user [$time_local] \u0026quot;$request\u0026quot; ' '$status $body_bytes_sent \u0026quot;$http_referer\u0026quot; ' '\u0026quot;$http_user_agent\u0026quot; \u0026quot;$http_x_forwarded_for\u0026quot;'; access_log logs/access.log main; sendfile on; keepalive_timeout 65; upstream slave_pools { server 192.168.11.64:80 weight=1; } server { listen 80; server_name localhost; location / { proxy_pass http://slave_pools; root html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } } 检查语法并启动nginx\n[root@master 192.168.11.63 /opt/nginx1-12]$./nginx -t nginx: the configuration file /opt/nginx1-12/conf/nginx.conf syntax is ok nginx: configuration file /opt/nginx1-12/conf/nginx.conf test is successful #启动nginx [root@master 192.168.11.63 /opt/nginx1-12]$/./nginx #检查端口 [root@master 192.168.11.63 /opt/nginx1-12]$netstat -tunlp|grep nginx tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 8921/nginx: master 此时访问master的服务器192.168.11.63:80地址，已经会将请求转发给slave的80端口\n除了页面效果的展示以外，还可以通过log(access.log)查看代理效果\nmaster端日志\nslave端日志\nLOCALTION Location语法优先级排列 匹配符 匹配规则 优先级\n= 精确匹配 1\n^~ 以某个字符串开头 2\n~ 区分大小写的正则匹配 3\n~* 不区分大小写的正则匹配 4\n!~ 区分大小写不匹配的正则 5\n!~* 不区分大小写不匹配的正则 6\n/ 通用匹配，任何请求都会匹配到 7\nnginx.conf配置文件实例 server { listen 80; server_name pythonav.cn; #优先级1,精确匹配，根路径 location =/ { return 400; } #优先级2,以某个字符串开头,以av开头的，优先匹配这里，区分大小写 location ^~ /av { root /data/av/; } #优先级3，区分大小写的正则匹配，匹配/media*****路径 location ~ /media { alias /data/static/; } #优先级4 ，不区分大小写的正则匹配，所有的****.jpg|gif|png 都走这里 location ~* .*\\.(jpg|gif|png|js|css)$ { root /data/av/; } #优先7，通用匹配 location / { return 403; } } nginx语法之root和alias区别实战 nginx指定文件路径有root和alias两种方法 区别在方法和作用域：\n方法：\nroot 语法 root 路径; 默认值 root html; 配置块 http{} server {} location{}\nalias 语法： alias 路径 配置块 location{}\nroot和alias区别在nginx如何解释location后面的url，这会使得两者分别以不同的方式讲请求映射到服务器文件上\nroot参数是root路径+location位置\nroot实例：\nlocation ^~ /av { root /data/av; 注意这里可有可无结尾的 / } 请求url是zhaohaiyu.com/av/index.html时 web服务器会返回服务器上的/data/av/av/index.html\nroot实例2：\nlocation ~* .*\\.(jpg|gif|png|js|css)$ { root /data/av/; } 请求url是zhaohaiyu.com/girl.gif时 web服务器会返回服务器上的/data/static/girl.gif\nalias实例：\nalias参数是使用alias路径替换location路径 alias是一个目录的别名 注意alias必须有 \u0026ldquo;/\u0026rdquo; 结束！ alias只能位于location块中 location ^~ /av { alias /data/static/; } 请求url是zhaohaiyu.com/av/index.html时 web服务器会返回服务器上的/data/static/index.html\nKeepalived高可用软件 什么是keepalived\nKeepalived是一个用C语言编写的路由软件。该项目的主要目标是为Linux系统和基于Linux的基础架构提供简单而强大的负载均衡和高可用性设施。 还可以作为其他服务（nginx，mysql）的高可用软件 keepalived主要通过vrrp协议实现高可用功能。vrrp叫（virtual router redundancy protocol）虚拟路由器冗余协议，目的为了解决单点故障问题，他可以保证个别节点宕机时。整个网络可以不间断的运行。 高可用故障切换原理\n在keepalived工作时，主master节点会不断的向备节点发送心跳消息，告诉备节点自己还活着， 当master节点故障时，就无法发送心跳消息，备节点就无法检测到来自master的心跳了，于是调用自身的接管程序，接管master节点的ip资源以及服务， 当master主节点恢复时，备backup节点又会释放接管的ip资源和服务，回复到原本的备节点角色。 硬件环境准备 实验环境应该最好是4台虚拟机，环境有限因此用2台机器\nmaster\nslave\nglobal_defs { notification_email { eric.k.zhang@ericsson.com } notification_email_from keepalived@node-10-210-149-21 smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id LVS_DEVEL } vrrp_script check_webproxy { script \u0026quot;killall -0 nginx\u0026quot; interval 1 weight 21 } vrrp_script check_mantaince_down { script \u0026quot;[[ -f /etc/keepalived/down ]] \u0026amp;\u0026amp; exit 1 || exit 0\u0026quot; interval 1 weight 2 } vrrp_instance vip_10_210_149_23 { state MASTER interface ens192 virtual_router_id 23 garp_master_delay 1 mcast_src_ip 10.210.149.21 lvs_sync_daemon_interface ens192 priority 110 advert_int 2 authentication { auth_type PASS auth_pass 1111 } track_interface { ens192 } virtual_ipaddress { 10.210.149.23/24 dev ens192 label ens192:0 } track_script { check_webproxy check_mantaince_down } } vrrp_instance nginx_vip_10_210.149_24 { state BACKUP interface ens192 virtual_router_id 24 garp_master_delay 1 mcast_src_ip 10.210.149.21 lvs_sync_daemon_interface ens192 priority 100 advert_int 2 authentication { auth_type PASS auth_pass 1111 } track_interface { ens192 } virtual_ipaddress { 10.210.149.24/24 dev ens192 label ens192:1 } track_script { check_webproxy check_mantaince_down } } global_defs { notification_email { eric.k.zhang@ericsson.com } notification_email_from keepalived@node-10-210-149-21 smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id LVS_DEVEL } vrrp_script check_webproxy { script \u0026quot;killall -0 nginx\u0026quot; interval 1 weight 21 } vrrp_script check_mantaince_down { script \u0026quot;[[ -f /etc/keepalived/down ]] \u0026amp;\u0026amp; exit 1 || exit 0\u0026quot; interval 1 weight 2 } vrrp_instance vip_10_210_149_23 { state BACKUP interface ens192 virtual_router_id 23 garp_master_delay 1 mcast_src_ip 10.210.149.22 lvs_sync_daemon_interface ens192 priority 100 advert_int 2 authentication { auth_type PASS auth_pass 1111 } track_interface { ens192 } virtual_ipaddress { 10.210.149.23/24 dev ens192 label ens192:0 } track_script { check_webproxy check_mantaince_down } } vrrp_instance nginx_vip_10_210.149_24 { state MASTER interface ens192 virtual_router_id 24 garp_master_delay 1 mcast_src_ip 10.210.149.22 lvs_sync_daemon_interface ens192 priority 110 advert_int 2 authentication { auth_type PASS auth_pass 1111 } track_interface { ens192 } virtual_ipaddress { 10.210.149.24/24 dev ens192 label ens192:1 } track_script { check_webproxy check_mantaince_down } } ","date":"2020-03-10","permalink":"https://daemon365.dev/2020/03/10/nginx%E7%AE%80%E4%BB%8B%E4%BD%BF%E7%94%A8/","tags":["nginx","linux"],"title":"nginx简介,使用"},{"content":"在项目中我们通常可能会使用database/sql连接MySQL数据库。本文借助使用sqlx实现批量插入数据的例子，介绍了sqlx中可能被你忽视了的sqlx.In和DB.NamedExec方法。\nsqlx介绍 在项目中我们通常可能会使用database/sql连接MySQL数据库。sqlx可以认为是Go语言内置database/sql的超集，它在优秀的内置database/sql基础上提供了一组扩展。这些扩展中除了大家常用来查询的Get(dest interface{}, ...) error和Select(dest interface{}, ...) error外还有很多其他强大的功能。\n安装sqlx go get github.com/jmoiron/sqlx 基本使用 连接数据库 var db *sqlx.DB func initDB() (err error) { dsn := \u0026quot;user:password@tcp(127.0.0.1:3306)/sql_test?charset=utf8mb4\u0026amp;parseTime=True\u0026quot; // 也可以使用MustConnect连接不成功就panic db, err = sqlx.Connect(\u0026quot;mysql\u0026quot;, dsn) if err != nil { fmt.Printf(\u0026quot;connect DB failed, err:%v\\n\u0026quot;, err) return } db.SetMaxOpenConns(20) db.SetMaxIdleConns(10) return } 查询 查询单行数据示例代码如下：\n// 查询单条数据示例 func queryRowDemo() { sqlStr := \u0026quot;select id, name, age from user where id=?\u0026quot; var u user err := db.Get(\u0026amp;u, sqlStr, 1) if err != nil { fmt.Printf(\u0026quot;get failed, err:%v\\n\u0026quot;, err) return } fmt.Printf(\u0026quot;id:%d name:%s age:%d\\n\u0026quot;, u.ID, u.Name, u.Age) } 查询多行数据示例代码如下：\n// 查询多条数据示例 func queryMultiRowDemo() { sqlStr := \u0026quot;select id, name, age from user where id \u0026gt; ?\u0026quot; var users []user err := db.Select(\u0026amp;users, sqlStr, 0) if err != nil { fmt.Printf(\u0026quot;query failed, err:%v\\n\u0026quot;, err) return } fmt.Printf(\u0026quot;users:%#v\\n\u0026quot;, users) } 插入、更新和删除 sqlx中的exec方法与原生sql中的exec使用基本一致：\n// 插入数据 func insertRowDemo() { sqlStr := \u0026quot;insert into user(name, age) values (?,?)\u0026quot; ret, err := db.Exec(sqlStr, \u0026quot;沙河小王子\u0026quot;, 19) if err != nil { fmt.Printf(\u0026quot;insert failed, err:%v\\n\u0026quot;, err) return } theID, err := ret.LastInsertId() // 新插入数据的id if err != nil { fmt.Printf(\u0026quot;get lastinsert ID failed, err:%v\\n\u0026quot;, err) return } fmt.Printf(\u0026quot;insert success, the id is %d.\\n\u0026quot;, theID) } // 更新数据 func updateRowDemo() { sqlStr := \u0026quot;update user set age=? where id = ?\u0026quot; ret, err := db.Exec(sqlStr, 39, 6) if err != nil { fmt.Printf(\u0026quot;update failed, err:%v\\n\u0026quot;, err) return } n, err := ret.RowsAffected() // 操作影响的行数 if err != nil { fmt.Printf(\u0026quot;get RowsAffected failed, err:%v\\n\u0026quot;, err) return } fmt.Printf(\u0026quot;update success, affected rows:%d\\n\u0026quot;, n) } // 删除数据 func deleteRowDemo() { sqlStr := \u0026quot;delete from user where id = ?\u0026quot; ret, err := db.Exec(sqlStr, 6) if err != nil { fmt.Printf(\u0026quot;delete failed, err:%v\\n\u0026quot;, err) return } n, err := ret.RowsAffected() // 操作影响的行数 if err != nil { fmt.Printf(\u0026quot;get RowsAffected failed, err:%v\\n\u0026quot;, err) return } fmt.Printf(\u0026quot;delete success, affected rows:%d\\n\u0026quot;, n) } NamedExec DB.NamedExec方法用来绑定SQL语句与结构体或map中的同名字段。\nfunc insertUserDemo()(err error){ sqlStr := \u0026quot;INSERT INTO user (name,age) VALUES (:name,:age)\u0026quot; _, err = db.NamedExec(sqlStr, map[string]interface{}{ \u0026quot;name\u0026quot;: \u0026quot;七米\u0026quot;, \u0026quot;age\u0026quot;: 28, }) return } NamedQuery 与DB.NamedExec同理，这里是支持查询。\nfunc namedQuery(){ sqlStr := \u0026quot;SELECT * FROM user WHERE name=:name\u0026quot; // 使用map做命名查询 rows, err := db.NamedQuery(sqlStr, map[string]interface{}{\u0026quot;name\u0026quot;: \u0026quot;七米\u0026quot;}) if err != nil { fmt.Printf(\u0026quot;db.NamedQuery failed, err:%v\\n\u0026quot;, err) return } defer rows.Close() for rows.Next(){ var u user err := rows.StructScan(\u0026amp;u) if err != nil { fmt.Printf(\u0026quot;scan failed, err:%v\\n\u0026quot;, err) continue } fmt.Printf(\u0026quot;user:%#v\\n\u0026quot;, u) } u := user{ Name: \u0026quot;七米\u0026quot;, } // 使用结构体命名查询，根据结构体字段的 db tag进行映射 rows, err = db.NamedQuery(sqlStr, u) if err != nil { fmt.Printf(\u0026quot;db.NamedQuery failed, err:%v\\n\u0026quot;, err) return } defer rows.Close() for rows.Next(){ var u user err := rows.StructScan(\u0026amp;u) if err != nil { fmt.Printf(\u0026quot;scan failed, err:%v\\n\u0026quot;, err) continue } fmt.Printf(\u0026quot;user:%#v\\n\u0026quot;, u) } } 事务操作 对于事务操作，我们可以使用sqlx中提供的db.Beginx()和tx.Exec()方法。示例代码如下：\nfunc transactionDemo2()(err error) { tx, err := db.Beginx() // 开启事务 if err != nil { fmt.Printf(\u0026quot;begin trans failed, err:%v\\n\u0026quot;, err) return err } defer func() { if p := recover(); p != nil { tx.Rollback() panic(p) // re-throw panic after Rollback } else if err != nil { fmt.Println(\u0026quot;rollback\u0026quot;) tx.Rollback() // err is non-nil; don't change it } else { err = tx.Commit() // err is nil; if Commit returns error update err fmt.Println(\u0026quot;commit\u0026quot;) } }() sqlStr1 := \u0026quot;Update user set age=20 where id=?\u0026quot; rs, err := tx.Exec(sqlStr1, 1) if err!= nil{ return err } n, err := rs.RowsAffected() if err != nil { return err } if n != 1 { return errors.New(\u0026quot;exec sqlStr1 failed\u0026quot;) } sqlStr2 := \u0026quot;Update user set age=50 where i=?\u0026quot; rs, err = tx.Exec(sqlStr2, 5) if err!=nil{ return err } n, err = rs.RowsAffected() if err != nil { return err } if n != 1 { return errors.New(\u0026quot;exec sqlStr1 failed\u0026quot;) } return err } sqlx.In sqlx.In是sqlx提供的一个非常方便的函数。\nsqlx.In的批量插入示例 表结构 为了方便演示插入数据操作，这里创建一个user表，表结构如下：\nCREATE TABLE `user` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT, `name` VARCHAR(20) DEFAULT '', `age` INT(11) DEFAULT '0', PRIMARY KEY(`id`) )ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4; 结构体 定义一个user结构体，字段通过tag与数据库中user表的列一致。\ntype User struct { Name string `db:\u0026quot;name\u0026quot;` Age int `db:\u0026quot;age\u0026quot;` } bindvars（绑定变量） 查询占位符?在内部称为bindvars（查询占位符）,它非常重要。你应该始终使用它们向数据库发送值，因为它们可以防止SQL注入攻击。database/sql不尝试对查询文本进行任何验证；它与编码的参数一起按原样发送到服务器。除非驱动程序实现一个特殊的接口，否则在执行之前，查询是在服务器上准备的。因此bindvars是特定于数据库的:\nMySQL中使用? PostgreSQL使用枚举的$1、$2等bindvar语法 SQLite中?和$1的语法都支持 Oracle中使用:name的语法 bindvars的一个常见误解是，它们用来在sql语句中插入值。它们其实仅用于参数化，不允许更改SQL语句的结构。例如，使用bindvars尝试参数化列或表名将不起作用：\n// ？不能用来插入表名（做SQL语句中表名的占位符） db.Query(\u0026quot;SELECT * FROM ?\u0026quot;, \u0026quot;mytable\u0026quot;) // ？也不能用来插入列名（做SQL语句中列名的占位符） db.Query(\u0026quot;SELECT ?, ? FROM people\u0026quot;, \u0026quot;name\u0026quot;, \u0026quot;location\u0026quot;) 自己拼接语句实现批量插入 比较笨，但是很好理解。就是有多少个User就拼接多少个(?, ?)。\n// BatchInsertUsers 自行构造批量插入的语句 func BatchInsertUsers(users []*User) error { // 存放 (?, ?) 的slice valueStrings := make([]string, 0, len(users)) // 存放values的slice valueArgs := make([]interface{}, 0, len(users) * 2) // 遍历users准备相关数据 for _, u := range users { // 此处占位符要与插入值的个数对应 valueStrings = append(valueStrings, \u0026quot;(?, ?)\u0026quot;) valueArgs = append(valueArgs, u.Name) valueArgs = append(valueArgs, u.Age) } // 自行拼接要执行的具体语句 stmt := fmt.Sprintf(\u0026quot;INSERT INTO user (name, age) VALUES %s\u0026quot;, strings.Join(valueStrings, \u0026quot;,\u0026quot;)) _, err := DB.Exec(stmt, valueArgs...) return err } 使用sqlx.In实现批量插入 前提是需要我们的结构体实现driver.Valuer接口：\nfunc (u User) Value() (driver.Value, error) { return []interface{}{u.Name, u.Age}, nil } 使用sqlx.In实现批量插入代码如下：\n// BatchInsertUsers2 使用sqlx.In帮我们拼接语句和参数, 注意传入的参数是[]interface{} func BatchInsertUsers2(users []interface{}) error { query, args, _ := sqlx.In( \u0026quot;INSERT INTO user (name, age) VALUES (?), (?), (?)\u0026quot;, users..., // 如果arg实现了 driver.Valuer, sqlx.In 会通过调用 Value()来展开它 ) fmt.Println(query) // 查看生成的querystring fmt.Println(args) // 查看生成的args _, err := DB.Exec(query, args...) return err } 使用NamedExec实现批量插入 注意 ：该功能需1.3.1版本以上，并且1.3.1版本目前还有点问题，sql语句最后不能有空格和;，详见issues/690。\n使用NamedExec实现批量插入的代码如下：\n// BatchInsertUsers3 使用NamedExec实现批量插入 func BatchInsertUsers3(users []*User) error { _, err := DB.NamedExec(\u0026quot;INSERT INTO user (name, age) VALUES (:name, :age)\u0026quot;, users) return err } 把上面三种方法综合起来试一下：\nfunc main() { err := initDB() if err != nil { panic(err) } defer DB.Close() u1 := User{Name: \u0026quot;七米\u0026quot;, Age: 18} u2 := User{Name: \u0026quot;q1mi\u0026quot;, Age: 28} u3 := User{Name: \u0026quot;小王子\u0026quot;, Age: 38} // 方法1 users := []*User{\u0026amp;u1, \u0026amp;u2, \u0026amp;u3} err = BatchInsertUsers(users) if err != nil { fmt.Printf(\u0026quot;BatchInsertUsers failed, err:%v\\n\u0026quot;, err) } // 方法2 users2 := []interface{}{u1, u2, u3} err = BatchInsertUsers2(users2) if err != nil { fmt.Printf(\u0026quot;BatchInsertUsers2 failed, err:%v\\n\u0026quot;, err) } // 方法3 users3 := []*User{\u0026amp;u1, \u0026amp;u2, \u0026amp;u3} err = BatchInsertUsers3(users3) if err != nil { fmt.Printf(\u0026quot;BatchInsertUsers3 failed, err:%v\\n\u0026quot;, err) } } sqlx.In的查询示例 关于sqlx.In这里再补充一个用法，在sqlx查询语句中实现In查询和FIND_IN_SET函数。即实现SELECT * FROM user WHERE id in (3, 2, 1);和SELECT * FROM user WHERE id in (3, 2, 1) ORDER BY FIND_IN_SET(id, '3,2,1');。\nin查询 查询id在给定id集合中的数据。\n// QueryByIDs 根据给定ID查询 func QueryByIDs(ids []int)(users []User, err error){ // 动态填充id query, args, err := sqlx.In(\u0026quot;SELECT name, age FROM user WHERE id IN (?)\u0026quot;, ids) if err != nil { return } // sqlx.In 返回带 `?` bindvar的查询语句, 我们使用Rebind()重新绑定它 query = DB.Rebind(query) err = DB.Select(\u0026amp;users, query, args...) return } in查询和FIND_IN_SET函数 查询id在给定id集合的数据并维持给定id集合的顺序。\n// QueryAndOrderByIDs 按照指定id查询并维护顺序 func QueryAndOrderByIDs(ids []int)(users []User, err error){ // 动态填充id strIDs := make([]string, 0, len(ids)) for _, id := range ids { strIDs = append(strIDs, fmt.Sprintf(\u0026quot;%d\u0026quot;, id)) } query, args, err := sqlx.In(\u0026quot;SELECT name, age FROM user WHERE id IN (?) ORDER BY FIND_IN_SET(id, ?)\u0026quot;, ids, strings.Join(strIDs, \u0026quot;,\u0026quot;)) if err != nil { return } // sqlx.In 返回带 `?` bindvar的查询语句, 我们使用Rebind()重新绑定它 query = DB.Rebind(query) err = DB.Select(\u0026amp;users, query, args...) return } 当然，在这个例子里面你也可以先使用IN查询，然后通过代码按给定的ids对查询结果进行排序。\n文章转自 https://www.liwenzhou.com/posts/Go/sqlx/ ","date":"2020-01-13","permalink":"https://daemon365.dev/2020/01/13/golang-sqlx/","tags":["golang","sqlx"],"title":"golang sqlx"},{"content":"安装 下载第三方包:\ngo get -u github.com/go-redis/redis/v9 连接 // 定义一个rdis客户端 var redisdb *redis.Client // 初始化 func initClient() (err error) { redisdb = redis.NewClient(\u0026amp;redis.Options{ Addr: \u0026quot;localhost:6379\u0026quot;, // post端口 Password: \u0026quot;\u0026quot;, // 密码 DB: 0, // 使用redis的库 }) _, err = redisdb.Ping(context.Background()).Result() if err != nil { fmt.Println(\u0026quot;连接失败\u0026quot;) return } return } 使用 set/get示例 func redisExample() { ctx := context.Background() err := redisdb.Set(ctx, \u0026quot;score\u0026quot;, 100, 0).Err() if err != nil { fmt.Printf(\u0026quot;set score failed, err:%v\\n\u0026quot;, err) return } val, err := redisdb.Get(ctx, \u0026quot;score\u0026quot;).Result() if err != nil { fmt.Printf(\u0026quot;get score failed, err:%v\\n\u0026quot;, err) return } fmt.Println(\u0026quot;score\u0026quot;, val) val2, err := redisdb.Get(ctx, \u0026quot;name\u0026quot;).Result() if err == redis.Nil { fmt.Println(\u0026quot;name does not exist\u0026quot;) } else if err != nil { fmt.Printf(\u0026quot;get name failed, err:%v\\n\u0026quot;, err) return } else { fmt.Println(\u0026quot;name\u0026quot;, val2) } } zset示例 func redisExample2() { ctx := context.Background() zsetKey := \u0026quot;language_rank\u0026quot; languages := []*redis.Z{ \u0026amp;redis.Z{Score: 90.0, Member: \u0026quot;Golang\u0026quot;}, \u0026amp;redis.Z{Score: 98.0, Member: \u0026quot;Java\u0026quot;}, \u0026amp;redis.Z{Score: 95.0, Member: \u0026quot;Python\u0026quot;}, \u0026amp;redis.Z{Score: 97.0, Member: \u0026quot;JavaScript\u0026quot;}, \u0026amp;redis.Z{Score: 99.0, Member: \u0026quot;C/C++\u0026quot;}, } // ZADD num, err := redisdb.ZAdd(ctx, zsetKey, languages...).Result() if err != nil { fmt.Printf(\u0026quot;zadd failed, err:%v\\n\u0026quot;, err) return } fmt.Printf(\u0026quot;zadd %d succ.\\n\u0026quot;, num) // 把Golang的分数加10 newScore, err := redisdb.ZIncrBy(ctx, zsetKey, 10.0, \u0026quot;Golang\u0026quot;).Result() if err != nil { fmt.Printf(\u0026quot;zincrby failed, err:%v\\n\u0026quot;, err) return } fmt.Printf(\u0026quot;Golang's score is %f now.\\n\u0026quot;, newScore) // 取分数最高的3个 ret, err := redisdb.ZRevRangeWithScores(ctx, zsetKey, 0, 2).Result() if err != nil { fmt.Printf(\u0026quot;zrevrange failed, err:%v\\n\u0026quot;, err) return } for _, z := range ret { fmt.Println(z.Member, z.Score) } // 取95~100分的 op := \u0026amp;redis.ZRangeBy{ Min: \u0026quot;95\u0026quot;, Max: \u0026quot;100\u0026quot;, } ret, err = redisdb.ZRangeByScoreWithScores(ctx, zsetKey, op).Result() if err != nil { fmt.Printf(\u0026quot;zrangebyscore failed, err:%v\\n\u0026quot;, err) return } for _, z := range ret { fmt.Println(z.Member, z.Score) } } 输出结果如下：\n$ ./06redis_demo zadd 0 succ. Golang's score is 100.000000 now. Golang 100 C/C++ 99 Java 98 JavaScript 97 Java 98 C/C++ 99 Golang 100 ","date":"2020-01-12","permalink":"https://daemon365.dev/2020/01/12/golang-redis/","tags":["golang","redis"],"title":"golang redis"},{"content":"http协议 超文本传输协议（HTTP，HyperText Transfer Protocol)是互联网上应用最为广泛的一种网络传输协议，所有的WWW文件都必须遵守这个标准。设计HTTP最初的目的是为了提供一种发布和接收HTML页面的方法。\n关于http(https)协议: https://www.cnblogs.com/yuemoxi/p/15162601.html\nhttp包中重要的类型和接口 server：HTTP服务器，定义监听的地址、端口，处理器等信息。 conn：用户每次请求的链接。 response：返回给用户的信息。 request：用户请求的信息。 Handler: 处理器，用户请求到来时，服务器的处理逻辑，它是一个包含ServeHTTP方法的接口。 http包的运行流程 http包如何实现高并发 http包中，server每接收一个用户请求，都会生成一个conn链接，并生成一个goroutines来处理对应的conn。所以每个请求都是独立的，相互不阻塞的。\nc := srv.newConn(rw) c.setState(c.rwc, StateNew) go c.serve(ctx) 处理器(Handler)和多路复用器(ServeMux) func (sh serverHandler) ServeHTTP(rw ResponseWriter, req *Request) { handler := sh.srv.Handler if handler == nil { handler = DefaultServeMux } if req.RequestURI == \u0026quot;*\u0026quot; \u0026amp;\u0026amp; req.Method == \u0026quot;OPTIONS\u0026quot; { handler = globalOptionsHandler{} } handler.ServeHTTP(rw, req) } 所以当我们的处理器为空时，http包会默认帮我们生成一个DefaultServeMux处理器。 不是说第二个参数是处理器吗，但是DefaultServeMux是一个ServeMux结构的实例, 是一个多路复用器。这是怎么回事？ 其实处理器是一个拥有ServeHTTP方法的接口，只要实现了这个方法，它就是一个处理器。所以DefaultServeMux不仅是一个多路复用器，而且还是一个处理器。只是这个处理器比较特殊，它只是根据请求的URL将请求分配到对应的处理器。\nfunc (mux *ServeMux) ServeHTTP(w ResponseWriter, r *Request) { if r.RequestURI == \u0026quot;*\u0026quot; { if r.ProtoAtLeast(1, 1) { w.Header().Set(\u0026quot;Connection\u0026quot;, \u0026quot;close\u0026quot;) } w.WriteHeader(StatusBadRequest) return } h, _ := mux.Handler(r) h.ServeHTTP(w, r) } 我们可以自定义自己的处理器\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;net/http\u0026quot; ) // 定一个自定义的Handler的结构 type MyHanlder struct{} // 为MyHanlder结构实现Hanlder接口的ServeHTTP的方法，此时MyHandler将是一个处理器(Handler) func (h MyHanlder) ServeHTTP(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \u0026quot;This my handler\u0026quot;) } func main() { // 实例化MyHandler hanlder := MyHanlder{} //启动服务器并监听8000/tcp端口 err := http.ListenAndServe(\u0026quot;:8000\u0026quot;, hanlder) if err != nil { fmt.Println(err) } } ServeMux和DefaultServeMux 前面我们说DefaultServeMux是一个ServeMux的实例，它不仅是一个多路复用器，而且是一个处理器。多路复用器具有路由功能，可以根据请求的URL将请求传递给对应的处理器处理。看下http包中默认的多路复用器是怎么实现的：\ntype ServeMux struct { mu sync.RWMutex //锁，应为请求是并发处理的，所以这里需要锁机制 m map[string]muxEntry //路由规则的map，一个string对应一个muxEntry es []muxEntry hosts bool } type muxEntry struct { h Handler //string对应的处理器 pattern string //匹配的字符串 } 默认的多路复用器是根据请求的URL与存储的map做匹配，匹配到了就返回对应的handler，然后调用该handler的ServeHTTP方法来处理请求。\nfunc (mux *ServeMux) ServeHTTP(w ResponseWriter, r *Request) { if r.RequestURI == \u0026quot;*\u0026quot; { if r.ProtoAtLeast(1, 1) { w.Header().Set(\u0026quot;Connection\u0026quot;, \u0026quot;close\u0026quot;) } w.WriteHeader(StatusBadRequest) return } h, _ := mux.Handler(r) h.ServeHTTP(w, r) } func (mux *ServeMux) Handler(r *Request) (h Handler, pattern string) { if r.Method == \u0026quot;CONNECT\u0026quot; { if u, ok := mux.redirectToPathSlash(r.URL.Host, r.URL.Path, r.URL); ok { return RedirectHandler(u.String(), StatusMovedPermanently), u.Path } return mux.handler(r.Host, r.URL.Path) } host := stripHostPort(r.Host) path := cleanPath(r.URL.Path) if u, ok := mux.redirectToPathSlash(host, path, r.URL); ok { return RedirectHandler(u.String(), StatusMovedPermanently), u.Path } if path != r.URL.Path { _, pattern = mux.handler(host, path) url := *r.URL url.Path = path return RedirectHandler(url.String(), StatusMovedPermanently), pattern } return mux.handler(host, r.URL.Path) } func (mux *ServeMux) handler(host, path string) (h Handler, pattern string) { mux.mu.RLock() defer mux.mu.RUnlock() if mux.hosts { h, pattern = mux.match(host + path) } if h == nil { h, pattern = mux.match(path) } if h == nil { h, pattern = NotFoundHandler(), \u0026quot;\u0026quot; } return } 我们也可以根据接口定义实现自己简单的路由器\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;net/http\u0026quot; ) // 自定义一个多路复用器的结构 type MyMux struct{} // 实现ServeHTTP方法 func (m MyMux) ServeHTTP(w http.ResponseWriter, r *http.Request) { // 判断URL并转到对应的handler处理 if r.URL.Path == \u0026quot;/index\u0026quot; { IndexHandler(w, r) return } http.NotFound(w, r) return } func IndexHandler(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \u0026quot;This is index page\u0026quot;) } func main() { // 实例化一个自定义的路由器 mux := MyMux{} err := http.ListenAndServe(\u0026quot;:8000\u0026quot;, mux) if err != nil { fmt.Println(err) } } HTTP客户端 基本的HTTP/HTTPS请求 Get、Head、Post和PostForm函数发出HTTP/HTTPS请求。\nresp, err := http.Get(\u0026quot;http://example.com/\u0026quot;) ... resp, err := http.Post(\u0026quot;http://example.com/upload\u0026quot;, \u0026quot;image/jpeg\u0026quot;, \u0026amp;buf) ... resp, err := http.PostForm(\u0026quot;http://example.com/form\u0026quot;, url.Values{\u0026quot;key\u0026quot;: {\u0026quot;Value\u0026quot;}, \u0026quot;id\u0026quot;: {\u0026quot;123\u0026quot;}}) 程序在使用完response后必须关闭回复的主体。\nresp, err := http.Get(\u0026quot;http://example.com/\u0026quot;) if err != nil { // handle error } defer resp.Body.Close() body, err := ioutil.ReadAll(resp.Body) // ... GET请求示例 使用net/http包编写一个简单的发送HTTP请求的Client端，代码如下：\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;io/ioutil\u0026quot; \u0026quot;net/http\u0026quot; ) func main() { resp, err := http.Get(\u0026quot;https://www.liwenzhou.com/\u0026quot;) if err != nil { fmt.Printf(\u0026quot;get failed, err:%v\\n\u0026quot;, err) return } defer resp.Body.Close() body, err := ioutil.ReadAll(resp.Body) if err != nil { fmt.Printf(\u0026quot;read from resp.Body failed, err:%v\\n\u0026quot;, err) return } fmt.Print(string(body)) } 将上面的代码保存之后编译成可执行文件，执行之后就能在终端打印liwenzhou.com网站首页的内容了，我们的浏览器其实就是一个发送和接收HTTP协议数据的客户端，我们平时通过浏览器访问网页其实就是从网站的服务器接收HTTP数据，然后浏览器会按照HTML、CSS等规则将网页渲染展示出来。\n带参数的GET请求示例 关于GET请求的参数需要使用Go语言内置的net/url这个标准库来处理。\nfunc main() { apiUrl := \u0026quot;http://127.0.0.1:9090/get\u0026quot; // URL param data := url.Values{} data.Set(\u0026quot;name\u0026quot;, \u0026quot;小王子\u0026quot;) data.Set(\u0026quot;age\u0026quot;, \u0026quot;18\u0026quot;) u, err := url.ParseRequestURI(apiUrl) if err != nil { fmt.Printf(\u0026quot;parse url requestUrl failed, err:%v\\n\u0026quot;, err) } u.RawQuery = data.Encode() // URL encode fmt.Println(u.String()) resp, err := http.Get(u.String()) if err != nil { fmt.Printf(\u0026quot;post failed, err:%v\\n\u0026quot;, err) return } defer resp.Body.Close() b, err := ioutil.ReadAll(resp.Body) if err != nil { fmt.Printf(\u0026quot;get resp failed, err:%v\\n\u0026quot;, err) return } fmt.Println(string(b)) } 对应的Server端HandlerFunc如下：\nfunc getHandler(w http.ResponseWriter, r *http.Request) { defer r.Body.Close() data := r.URL.Query() fmt.Println(data.Get(\u0026quot;name\u0026quot;)) fmt.Println(data.Get(\u0026quot;age\u0026quot;)) answer := `{\u0026quot;status\u0026quot;: \u0026quot;ok\u0026quot;}` w.Write([]byte(answer)) } Post请求 上面演示了使用net/http包发送GET请求的示例，发送POST请求的示例代码如下：\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;io/ioutil\u0026quot; \u0026quot;net/http\u0026quot; \u0026quot;strings\u0026quot; ) // net/http post demo func main() { url := \u0026quot;http://127.0.0.1:9090/post\u0026quot; // 表单数据 //contentType := \u0026quot;application/x-www-form-urlencoded\u0026quot; //data := \u0026quot;name=小王子\u0026amp;age=18\u0026quot; // json contentType := \u0026quot;application/json\u0026quot; data := `{\u0026quot;name\u0026quot;:\u0026quot;小王子\u0026quot;,\u0026quot;age\u0026quot;:18}` resp, err := http.Post(url, contentType, strings.NewReader(data)) if err != nil { fmt.Printf(\u0026quot;post failed, err:%v\\n\u0026quot;, err) return } defer resp.Body.Close() b, err := ioutil.ReadAll(resp.Body) if err != nil { fmt.Printf(\u0026quot;get resp failed, err:%v\\n\u0026quot;, err) return } fmt.Println(string(b)) } 对应的Server端HandlerFunc如下：\nfunc postHandler(w http.ResponseWriter, r *http.Request) { defer r.Body.Close() // 1\\. 请求类型是application/x-www-form-urlencoded时解析form数据 r.ParseForm() fmt.Println(r.PostForm) // 打印form数据 fmt.Println(r.PostForm.Get(\u0026quot;name\u0026quot;), r.PostForm.Get(\u0026quot;age\u0026quot;)) // 2\\. 请求类型是application/json时从r.Body读取数据 b, err := ioutil.ReadAll(r.Body) if err != nil { fmt.Printf(\u0026quot;read request.Body failed, err:%v\\n\u0026quot;, err) return } fmt.Println(string(b)) answer := `{\u0026quot;status\u0026quot;: \u0026quot;ok\u0026quot;}` w.Write([]byte(answer)) } Client 要管理HTTP客户端的头域、重定向策略和其他设置，创建一个Client：\nclient := \u0026amp;http.Client{ CheckRedirect: redirectPolicyFunc, } resp, err := client.Get(\u0026quot;http://example.com\u0026quot;) // ... req, err := http.NewRequest(\u0026quot;GET\u0026quot;, \u0026quot;http://example.com\u0026quot;, nil) // ... req.Header.Add(\u0026quot;If-None-Match\u0026quot;, `W/\u0026quot;wyzzy\u0026quot;`) resp, err := client.Do(req) // ... 自定义Transport 要管理代理、TLS配置、keep-alive、压缩和其他设置，创建一个Transport：\ntr := \u0026amp;http.Transport{ TLSClientConfig: \u0026amp;tls.Config{RootCAs: pool}, DisableCompression: true, } client := \u0026amp;http.Client{Transport: tr} resp, err := client.Get(\u0026quot;https://example.com\u0026quot;) Client和Transport类型都可以安全的被多个goroutine同时使用。出于效率考虑，应该一次建立、尽量重用。\n服务端 默认的Server ListenAndServe使用指定的监听地址和处理器启动一个HTTP服务端。处理器参数通常是nil，这表示采用包变量DefaultServeMux作为处理器。\nHandle和HandleFunc函数可以向DefaultServeMux添加处理器。\nhttp.Handle(\u0026quot;/foo\u0026quot;, fooHandler) http.HandleFunc(\u0026quot;/bar\u0026quot;, func(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \u0026quot;Hello, %q\u0026quot;, html.EscapeString(r.URL.Path)) }) log.Fatal(http.ListenAndServe(\u0026quot;:8080\u0026quot;, nil)) 默认的Server示例 使用Go语言中的net/http包来编写一个简单的接收HTTP请求的Server端示例，net/http包是对net包的进一步封装，专门用来处理HTTP协议的数据。具体的代码如下：\n// http server func sayHello(w http.ResponseWriter, r *http.Request) { fmt.Fprintln(w, \u0026quot;Hello 沙河！\u0026quot;) } func main() { http.HandleFunc(\u0026quot;/\u0026quot;, sayHello) err := http.ListenAndServe(\u0026quot;:9090\u0026quot;, nil) if err != nil { fmt.Printf(\u0026quot;http server failed, err:%v\\n\u0026quot;, err) return } } 将上面的代码编译之后执行，打开你电脑上的浏览器在地址栏输入127.0.0.1:9090回车，此时就能够看到如下页面了。 自定义Server 要管理服务端的行为，可以创建一个自定义的Server：\ns := \u0026amp;http.Server{ Addr: \u0026quot;:8080\u0026quot;, Handler: myHandler, ReadTimeout: 10 * time.Second, WriteTimeout: 10 * time.Second, MaxHeaderBytes: 1 \u0026lt;\u0026lt; 20, } log.Fatal(s.ListenAndServe()) 参考文章 https://www.jianshu.com/p/c90ebdd5d1a1 https://www.liwenzhou.com/posts/Go/go_http/ ","date":"2020-01-11","permalink":"https://daemon365.dev/2020/01/11/golang-nethttp%E5%8C%85/","tags":["go"],"title":"golang nethttp包"},{"content":"net/http 路由注册 func test1() { http.HandleFunc(\u0026quot;/\u0026quot;, func(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \u0026quot;Hello world!\u0026quot;) }) err := http.ListenAndServe(\u0026quot;:9001\u0026quot;, nil) if err != nil { log.Fatal(\u0026quot;ListenAndServer:\u0026quot;, err) } } 在使用ListenAndServe这个方法时，系统就会给我们指派一个路由器，DefaultServeMux是系统默认使用的路由器，如果ListenAndServe这个方法的第2个参数传入nil，系统就会默认使用DefaultServeMux。当然，这里也可以传入自定义的路由器。\n先看http.HandleFunc(\u0026quot;/\u0026quot;, ...)，从HandleFunc方法点进去，如下：\nfunc HandleFunc(pattern string, handler func(ResponseWriter, *Request)) { DefaultServeMux.HandleFunc(pattern, handler) } 在这里调用了DefaultServeMux的HandleFunc方法，这个方法有两个参数，pattern是匹配的路由规则，handler表示这个路由规则对应的处理方法，并且这个处理方法有两个参数。\n在我们书写的代码示例中，pattern对应/，handler对应sayHello，当我们在浏览器中输入http://localhost:9001时，就会触发匿名函数。\n我们再顺着DefaultServeMux的HandleFunc方法继续点下去，如下：\nfunc (mux *ServeMux) HandleFunc(pattern string, handler func(ResponseWriter, *Request)) { if handler == nil { panic(\u0026quot;http: nil handler\u0026quot;) } mux.Handle(pattern, HandlerFunc(handler)) } 在这个方法中，路由器又调用了Handle方法，注意这个Handle方法的第2个参数，将之前传入的handler这个响应方法强制转换成了HandlerFunc类型。\n这个HandlerFunc类型到底是个什么呢？如下：\ntype HandlerFunc func(ResponseWriter, *Request) 看来和我们定义的\u0026quot;/\u0026ldquo;的匿名函数的类型都差不多。但是！！！ 这个HandlerFunc默认实现了ServeHTTP接口！这样HandlerFunc对象就有了ServeHTTP方法！如下：\n// ServeHTTP calls f(w, r). func (f HandlerFunc) ServeHTTP(w ResponseWriter, r *Request) { f(w, r) } 接下来，我们返回去继续看mux的Handle方法，也就是这段代码mux.Handle(pattern, HandlerFunc(handler))。这段代码做了哪些事呢？源码如下\n// Handle registers the handler for the given pattern. // If a handler already exists for pattern, Handle panics. func (mux *ServeMux) Handle(pattern string, handler Handler) { mux.mu.Lock() defer mux.mu.Unlock() if pattern == \u0026quot;\u0026quot; { panic(\u0026quot;http: invalid pattern\u0026quot;) } if handler == nil { panic(\u0026quot;http: nil handler\u0026quot;) } if _, exist := mux.m[pattern]; exist { panic(\u0026quot;http: multiple registrations for \u0026quot; + pattern) } if mux.m == nil { mux.m = make(map[string]muxEntry) } e := muxEntry{h: handler, pattern: pattern} mux.m[pattern] = e if pattern[len(pattern)-1] == '/' { mux.es = appendSorted(mux.es, e) } if pattern[0] != '/' { mux.hosts = true } } 主要就做了一件事，向DefaultServeMux的map[string]muxEntry中增加对应的路由规则和handler。\nmap[string]muxEntry是个什么鬼？\nmap是一个字典对象，它保存的是key-value。 [string]表示这个字典的key是string类型的，这个key值会保存我们的路由规则。 muxEntry是一个实例对象，这个对象内保存了路由规则对应的处理方法。 mux.es 为模糊匹配 有长倒短排序 比如有路由/hello/ 访问/hello/world 时没有路由 会落到/hello/上 找到相应代码，如下：\n// 路由器 type ServeMux struct { mu sync.RWMutex m map[string]muxEntry es []muxEntry // slice of entries sorted from longest to shortest. hosts bool // whether any patterns contain hostnames } type muxEntry struct { h Handler pattern string } // 路由响应方法 type Handler interface { ServeHTTP(ResponseWriter, *Request) } net/http 运行 第二部分主要就是研究这句代码err := http.ListenAndServe(\u0026quot;:9001\u0026quot;,nil)，也就是ListenAndServe这个方法。从这个方法点进去，如下：\nfunc ListenAndServe(addr string, handler Handler) error { server := \u0026amp;Server{Addr: addr, Handler: handler} return server.ListenAndServe() } 在这个方法中，初始化了一个server对象，然后调用这个server对象的ListenAndServe方法，在这个方法中，如下：\nfunc (srv *Server) ListenAndServe() error { if srv.shuttingDown() { return ErrServerClosed } addr := srv.Addr if addr == \u0026quot;\u0026quot; { addr = \u0026quot;:http\u0026quot; } ln, err := net.Listen(\u0026quot;tcp\u0026quot;, addr) if err != nil { return err } return srv.Serve(ln) } 在这个方法中，调用了net.Listen(\u0026quot;tcp\u0026quot;, addr)，也就是底层用TCP协议搭建了一个服务，然后监控我们设置的端口。\n代码的最后，调用了srv的Serve方法，如下：\nfunc (srv *Server) Serve(l net.Listener) error { if fn := testHookServerServe; fn != nil { fn(srv, l) // call hook with unwrapped listener } origListener := l l = \u0026amp;onceCloseListener{Listener: l} defer l.Close() if err := srv.setupHTTP2_Serve(); err != nil { return err } if !srv.trackListener(\u0026amp;l, true) { return ErrServerClosed } defer srv.trackListener(\u0026amp;l, false) baseCtx := context.Background() if srv.BaseContext != nil { baseCtx = srv.BaseContext(origListener) if baseCtx == nil { panic(\u0026quot;BaseContext returned a nil context\u0026quot;) } } var tempDelay time.Duration // how long to sleep on accept failure ctx := context.WithValue(baseCtx, ServerContextKey, srv) for { rw, err := l.Accept() if err != nil { select { case \u0026lt;-srv.getDoneChan(): return ErrServerClosed default: } if ne, ok := err.(net.Error); ok \u0026amp;\u0026amp; ne.Temporary() { if tempDelay == 0 { tempDelay = 5 * time.Millisecond } else { tempDelay *= 2 } if max := 1 * time.Second; tempDelay \u0026gt; max { tempDelay = max } srv.logf(\u0026quot;http: Accept error: %v; retrying in %v\u0026quot;, err, tempDelay) time.Sleep(tempDelay) continue } return err } connCtx := ctx if cc := srv.ConnContext; cc != nil { connCtx = cc(connCtx, rw) if connCtx == nil { panic(\u0026quot;ConnContext returned nil\u0026quot;) } } tempDelay = 0 c := srv.newConn(rw) c.setState(c.rwc, StateNew, runHooks) // before Serve can return go c.serve(connCtx) } } 最后3段代码比较重要，也是Go语言支持高并发的体现，如下：\nc := srv.newConn(rw) c.setState(c.rwc, StateNew, runHooks) // before Serve can return go c.serve(connCtx) 上面那一大坨代码，总体意思是进入方法后，首先开了一个for循环，在for循环内时刻Accept请求，请求来了之后，会为每个请求创建一个Conn，然后单独开启一个goroutine，把这个请求的数据当做参数扔给这个Conn去服务：go c.serve()。用户的每一次请求都是在一个新的goroutine去服务，每个请求间相互不影响。\n在conn的serve方法中，有一句代码很重要，如下：\nserverHandler{c.server}.ServeHTTP(w, w.req) 表示serverHandler也实现了ServeHTTP接口，ServeHTTP方法实现如下：\nfunc (sh serverHandler) ServeHTTP(rw ResponseWriter, req *Request) { handler := sh.srv.Handler if handler == nil { handler = DefaultServeMux } if req.RequestURI == \u0026quot;*\u0026quot; \u0026amp;\u0026amp; req.Method == \u0026quot;OPTIONS\u0026quot; { handler = globalOptionsHandler{} } if req.URL != nil \u0026amp;\u0026amp; strings.Contains(req.URL.RawQuery, \u0026quot;;\u0026quot;) { var allowQuerySemicolonsInUse int32 req = req.WithContext(context.WithValue(req.Context(), silenceSemWarnContextKey, func() { atomic.StoreInt32(\u0026amp;allowQuerySemicolonsInUse, 1) })) defer func() { if atomic.LoadInt32(\u0026amp;allowQuerySemicolonsInUse) == 0 { sh.srv.logf(\u0026quot;http: URL query contains semicolon, which is no longer a supported separator; parts of the query may be stripped when parsed; see golang.org/issue/25192\u0026quot;) } }() } handler.ServeHTTP(rw, req) } 在这里如果handler为空（这个handler就可以理解为是我们自定义的路由器），就会使用系统默认的DefaultServeMux，代码的最后调用了DefaultServeMux的ServeHTTP()\nfunc (mux *ServeMux) ServeHTTP(w ResponseWriter, r *Request) { if r.RequestURI == \u0026quot;*\u0026quot; { if r.ProtoAtLeast(1, 1) { w.Header().Set(\u0026quot;Connection\u0026quot;, \u0026quot;close\u0026quot;) } w.WriteHeader(StatusBadRequest) return } h, _ := mux.Handler(r) //这里返回的h是Handler接口对象 h.ServeHTTP(w, r) //调用Handler接口对象的ServeHTTP方法实际上就调用了我们定义的sayHello方法 } func (mux *ServeMux) Handler(r *Request) (h Handler, pattern string) { // CONNECT requests are not canonicalized. if r.Method == \u0026quot;CONNECT\u0026quot; { // If r.URL.Path is /tree and its handler is not registered, // the /tree -\u0026gt; /tree/ redirect applies to CONNECT requests // but the path canonicalization does not. if u, ok := mux.redirectToPathSlash(r.URL.Host, r.URL.Path, r.URL); ok { return RedirectHandler(u.String(), StatusMovedPermanently), u.Path } return mux.handler(r.Host, r.URL.Path) } // All other requests have any port stripped and path cleaned // before passing to mux.handler. host := stripHostPort(r.Host) path := cleanPath(r.URL.Path) // If the given path is /tree and its handler is not registered, // redirect for /tree/. if u, ok := mux.redirectToPathSlash(host, path, r.URL); ok { return RedirectHandler(u.String(), StatusMovedPermanently), u.Path } if path != r.URL.Path { _, pattern = mux.handler(host, path) u := \u0026amp;url.URL{Path: path, RawQuery: r.URL.RawQuery} return RedirectHandler(u.String(), StatusMovedPermanently), pattern } return mux.handler(host, r.URL.Path) } func (mux *ServeMux) handler(host, path string) (h Handler, pattern string) { mux.mu.RLock() defer mux.mu.RUnlock() // Host-specific pattern takes precedence over generic ones if mux.hosts { h, pattern = mux.match(host + path) } if h == nil { h, pattern = mux.match(path) } if h == nil { h, pattern = NotFoundHandler(), \u0026quot;\u0026quot; } return } func (mux *ServeMux) match(path string) (h Handler, pattern string) { // Check for exact match first. v, ok := mux.m[path] if ok { return v.h, v.pattern } // Check for longest valid match. mux.es contains all patterns // that end in / sorted from longest to shortest. for _, e := range mux.es { if strings.HasPrefix(path, e.pattern) { return e.h, e.pattern } } return nil, \u0026quot;\u0026quot; } 它会根据用户请求的URL到路由器里面存储的map中匹配，匹配成功就会返回存储的handler，调用这个handler的ServeHTTP()就可以执行到相应的处理方法了，这个处理方法实际上就是我们刚开始定义的sayHello()，只不过这个sayHello()被HandlerFunc又包了一层，因为HandlerFunc实现了ServeHTTP接口，所以在调用HandlerFunc对象的ServeHTTP()时，实际上在ServeHTTP ()的内部调用了我们的sayHello()。\n总结 调用http.ListenAndServe(\u0026quot;:9090\u0026quot;,nil) 实例化server 调用server的ListenAndServe() 调用server的Serve方法，开启for循环，在循环中Accept请求 对每一个请求实例化一个Conn，并且开启一个goroutine为这个请求进行服务go c.serve() 读取每个请求的内容c.readRequest() 调用serverHandler的ServeHTTP()，如果handler为空，就把handler设置为系统默认的路由器DefaultServeMux 调用handler的ServeHTTP() =\u0026gt;实际上是调用了DefaultServeMux的ServeHTTP() 在ServeHTTP()中会调用路由对应处理handler 在路由对应处理handler中会执行sayHello() 有一个需要注意的点： DefaultServeMux和路由对应的处理方法handler都实现了ServeHTTP接口，他们俩都有ServeHTTP方法，但是方法要达到的目的不同，在DefaultServeMux的ServeHttp()里会执行路由对应的处理handler的ServeHttp()。\n自定义个简单的路由 package mux import ( \u0026quot;net/http\u0026quot; \u0026quot;strings\u0026quot; ) type muxEntry struct { h TesthandleFunc } type TesthandleFunc func(http.ResponseWriter, *http.Request) type TestHandler struct { routes map[string]map[string]muxEntry } func (h *TestHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) { method := strings.ToUpper(r.Method) path := r.URL.Path if route, ok := h.routes[method]; ok { if entry, ok := route[path]; ok { entry.h(w, r) return } } w.WriteHeader(http.StatusNotFound) } func Newhandler() *TestHandler { return \u0026amp;TestHandler{routes: make(map[string]map[string]muxEntry)} } func (h *TestHandler) Handle(method, path string, handler TesthandleFunc) { method = strings.ToUpper(method) if _, ok := h.routes[method]; !ok { h.routes[method] = make(map[string]muxEntry) } h.routes[method][path] = muxEntry{handler} } package main import ( \u0026quot;fmt\u0026quot; \u0026quot;net/http\u0026quot; \u0026quot;study/mux\u0026quot; ) func main() { handler := mux.Newhandler() handler.Handle(\u0026quot;GET\u0026quot;, \u0026quot;/hello\u0026quot;, func(rw http.ResponseWriter, r *http.Request) { rw.Write([]byte(\u0026quot;Hello World\u0026quot;)) }) handler.Handle(\u0026quot;Post\u0026quot;, \u0026quot;/hello/world\u0026quot;, func(rw http.ResponseWriter, r *http.Request) { fmt.Fprintln(rw, \u0026quot;你好\u0026quot;) }) http.ListenAndServe(\u0026quot;:9002\u0026quot;, handler) } 自定义context\npackage router import ( \u0026quot;encoding/json\u0026quot; \u0026quot;net/http\u0026quot; \u0026quot;strings\u0026quot; ) type Context struct { w http.ResponseWriter r *http.Request } func (c *Context) Json(code int, v interface{}) { c.w.Header().Set(\u0026quot;Content-Type\u0026quot;, \u0026quot;application/json\u0026quot;) c.w.WriteHeader(code) s, _ := json.Marshal(v) c.w.Write(s) } type Routerfunc func(c *Context) type RouterHandler struct { routes map[string]map[string]Routerfunc } func NewRouterHandler() *RouterHandler { return \u0026amp;RouterHandler{routes: make(map[string]map[string]Routerfunc)} } func (h *RouterHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) { method := strings.ToUpper(r.Method) path := r.URL.Path c := \u0026amp;Context{w: w, r: r} if route, ok := h.routes[method]; ok { if h, ok := route[path]; ok { h(c) return } } w.WriteHeader(http.StatusNotFound) } func (h *RouterHandler) Handle(method, path string, handler Routerfunc) { method = strings.ToUpper(method) if _, ok := h.routes[method]; !ok { h.routes[method] = make(map[string]Routerfunc) } h.routes[method][path] = handler } func (r *RouterHandler) Run(addr string) error { return http.ListenAndServe(addr, r) } Gin type Engine struct { RouterGroup pool sync.Pool trees methodTrees }// trie type RouterGroup struct { basePath string engine *Engine } func (engine *Engine) ServeHTTP(w http.ResponseWriter, req *http.Request) { c := engine.pool.Get().(*Context) // 从pool 拿出一个context c.writermem.reset(w) // 记录http.ResponseWriter 及 *http.Request c.Request = req c.reset() // 重置上一个留下的值 engine.handleHTTPRequest(c) engine.pool.Put(c) // 把用完的context放回池子 } // get: /bac 添加路由\nfunc (group *RouterGroup) handle(httpMethod, relativePath string, handlers HandlersChain) IRoutes { absolutePath := group.calculateAbsolutePath(relativePath) handlers = group.combineHandlers(handlers) group.engine.addRoute(httpMethod, absolutePath, handlers) return group.returnObj() } Context\ntype Context struct { Request *http.Request Writer ResponseWriter Params Params handlers HandlersChain index int8 fullPath string engine *Engine params *Params skippedNodes *[]skippedNode // This mutex protect Keys map mu sync.RWMutex // Keys is a key/value pair exclusively for the context of each request. Keys map[string]interface{} // Errors is a list of errors attached to all the handlers/middlewares who used this context. Errors errorMsgs // Accepted defines a list of manually accepted formats for content negotiation. Accepted []string // queryCache use url.ParseQuery cached the param query result from c.Request.URL.Query() queryCache url.Values // formCache use url.ParseQuery cached PostForm contains the parsed form data from POST, PATCH, // or PUT body parameters. formCache url.Values // SameSite allows a server to define a cookie attribute making it impossible for // the browser to send this cookie along with cross-site requests. sameSite http.SameSite } func (c *Context) Next() { c.index++ for c.index \u0026lt; int8(len(c.handlers)) { c.handlers[c.index](c) c.index++ } } func (engine *Engine) handleHTTPRequest(c *Context) { httpMethod := c.Request.Method rPath := c.Request.URL.Path unescape := false if engine.UseRawPath \u0026amp;\u0026amp; len(c.Request.URL.RawPath) \u0026gt; 0 { rPath = c.Request.URL.RawPath unescape = engine.UnescapePathValues } if engine.RemoveExtraSlash { rPath = cleanPath(rPath) } // Find root of the tree for the given HTTP method t := engine.trees for i, tl := 0, len(t); i \u0026lt; tl; i++ { if t[i].method != httpMethod { continue } root := t[i].root // Find route in tree value := root.getValue(rPath, c.params, c.skippedNodes, unescape) if value.params != nil { c.Params = *value.params } if value.handlers != nil { c.handlers = value.handlers c.fullPath = value.fullPath c.Next() c.writermem.WriteHeaderNow() return } if httpMethod != http.MethodConnect \u0026amp;\u0026amp; rPath != \u0026quot;/\u0026quot; { if value.tsr \u0026amp;\u0026amp; engine.RedirectTrailingSlash { redirectTrailingSlash(c) return } if engine.RedirectFixedPath \u0026amp;\u0026amp; redirectFixedPath(c, root, engine.RedirectFixedPath) { return } } break } if engine.HandleMethodNotAllowed { for _, tree := range engine.trees { if tree.method == httpMethod { continue } if value := tree.root.getValue(rPath, nil, c.skippedNodes, unescape); value.handlers != nil { c.handlers = engine.allNoMethod serveError(c, http.StatusMethodNotAllowed, default405Body) return } } } c.handlers = engine.allNoRoute serveError(c, http.StatusNotFound, default404Body) } ","date":"2019-12-23","permalink":"https://daemon365.dev/2019/12/23/nethttp%E5%92%8Cgin-%E8%B7%AF%E7%94%B1/","tags":["go","gin"],"title":"nethttp和gin 路由"},{"content":"网络层次划分 为了使不同计算机厂家生产的计算机能够相互通信，以便在更大的范围内建立计算机网络，国际标准化组织（ISO）在1978年提出了\u0026quot;开放系统互联参考模型\u0026quot;，即著名的OSI/RM模型（Open System Interconnection/Reference Model）。它将计算机网络体系结构的通信协议划分为七层，自下而上依次为：物理层（Physics Layer）、数据链路层（Data Link Layer）、网络层（Network Layer）、传输层（Transport Layer）、会话层（Session Layer）、表示层（Presentation Layer）、应用层（Application Layer）。其中第四层完成数据传送服务，上面三层面向用户。\nosi七层协议 互联网协议按照功能不同分为osi七层或tcp/ip五层 物理层 物理层由来：上面提到，孤立的计算机之间要通信，就必须接入internet\n物理层功能：主要是基于电器特性发送高低电压(电信号)，高电压对应数字1，低电压对应数字0\n光纤： 双绞线：\n数据链路层 数据链路层由来：单纯的电信号0和1没有任何意义，必须规定电信号多少位一组，每组什么意思\n数据链路层的功能：定义了电信号的分组方式\n以太网协议：\n早期的时候各个公司都有自己的分组方式，后来形成了统一的标准，即以太网协议ethernet ethernet规定\n一组电信号构成一个数据豹，叫做‘帧’ 每一数据帧分成：报头head和数据data两部分 head包含：(固定18个字节) 发送者／源地址，6个字节 接收者／目标地址，6个字节 数据类型，6个字节 data包含：(最短46字节，最长1500字节) 数据包的具体内容 head长度＋data长度＝最短64字节，最长1518字节，超过最大限制就分片发送 mac地址：\n每块网卡出厂时都被烧制上一个世界唯一的mac地址，长度为48位2进制，通常由12位16进制数表示（前六位是厂商编号，后六位是流水线号）\n广播：\n有了mac地址，同一网络内的两台主机就可以通信了（一台主机通过arp协议获取另外一台主机的mac地址）\nethernet采用最原始的方式，广播的方式进行通信，即计算机通信***\n网络层 网络层由来：有了ethernet、mac地址、广播的发送方式，世界上的计算机就可以彼此通信了，问题是世界范围的互联网是由\n一个个彼此隔离的小的局域网组成的，那么如果所有的通信都采用以太网的广播方式，那么一台机器发送的包全世界都会收到，\n这就不仅仅是效率低的问题了，这会是一种灾难\n上图结论：必须找出一种方法来区分哪些计算机属于同一广播域，哪些不是，如果是就采用广播的方式发送，如果不是，\n就采用路由的方式（向不同广播域／子网分发数据包），mac地址是无法区分的，它只跟厂商有关\n网络层功能：引入一套新的地址用来区分不同的广播域／子网，这套地址即网络地址\nIP协议：\n规定网络地址的协议叫ip协议，它定义的地址称之为ip地址，广泛采用的v4版本即ipv4，它规定网络地址由32位2进制表示 范围0.0.0.0-255.255.255.255 一个ip地址通常写成四段十进制数，例：172.16.10.1 ip地址分成两部分\n网络部分：标识子网 主机部分：标识主机 注意：单纯的ip地址段只是标识了ip地址的种类，从网络部分或主机部分都无法辨识一个ip所处的子网\n例：172.16.10.1与172.16.10.2并不能确定二者处于同一子网\n子网掩码\n所谓”子网掩码”，就是表示子网络特征的一个参数。它在形式上等同于IP地址，也是一个32位二进制数字，它的网络部分全部为1，主机部分全部为0。比如，IP地址172.16.10.1，如果已知网络部分是前24位，主机部分是后8位，那么子网络掩码就是11111111.11111111.11111111.00000000，写成十进制就是255.255.255.0。\n知道”子网掩码”，我们就能判断，任意两个IP地址是否处在同一个子网络。方法是将两个IP地址与子网掩码分别进行AND运算（两个数位都为1，运算结果为1，否则为0），然后比较结果是否相同，如果是的话，就表明它们在同一个子网络中，否则就不是。\n比如，已知IP地址172.16.10.1和172.16.10.2的子网掩码都是255.255.255.0，请问它们是否在同一个子网络？两者与子网掩码分别进行AND运算，\n172.16.10.1：10101100.00010000.00001010.000000001\n255255.255.255.0:11111111.11111111.11111111.00000000\nAND运算得网络地址结果：10101100.00010000.00001010.000000001-\u0026gt;172.16.10.0\n172.16.10.2：10101100.00010000.00001010.000000010\n255255.255.255.0:11111111.11111111.11111111.00000000\nAND运算得网络地址结果：10101100.00010000.00001010.000000001-\u0026gt;172.16.10.0\n结果都是172.16.10.0，因此它们在同一个子网络。\n总结一下，IP协议的作用主要有两个，一个是为每一台计算机分配IP地址，另一个是确定哪些地址在同一个子网络。\nip数据包\nip数据包也分为head和data部分，无须为ip包定义单独的栏位，直接放入以太网包的data部分\nhead：长度为20到60字节\ndata：最长为65,515字节。\n而以太网数据包的”数据”部分，最长只有1500字节。因此，如果IP数据包超过了1500字节，它就需要分割成几个以太网数据包，分开发送了。\nARP协议\narp协议由来：计算机通信***，即广播的方式，所有上层的包到最后都要封装上以太网头，然后通过以太网协议发送，在谈及以太网协议时候，我门了解到\n通信是基于mac的广播方式实现，计算机在发包时，获取自身的mac是容易的，如何获取目标主机的mac，就需要通过arp协议\narp协议功能：广播的方式发送数据包，获取目标主机的mac地址\n协议工作方式：每台主机ip都是已知的\n例如：主机172.16.10.10/24访问172.16.10.11/24\n一：首先通过ip地址和子网掩码区分出自己所处的子网\n场景 数据包地址 同一子网 目标主机mac，目标主机ip 不同子网 网关mac，目标主机ip 二：分析172.16.10.10/24与172.16.10.11/24处于同一网络(如果不是同一网络，那么下表中目标ip为172.16.10.1,通过arp获取的是网关的mac)\n源mac 目标mac 源ip 目标ip 数据部分 发送端主机 发送端mac FF:FF:FF:FF:FF:FF 172.16.10.10/24 172.16.10.11/24 数据 三：这个包会以广播的方式在发送端所处的自网内传输，所有主机接收后拆开包，发现目标ip为自己的，就响应，返回自己的mac\n传输层 传输层的由来：网络层的ip帮我们区分子网，以太网层的mac帮我们找到主机，然后大家使用的都是应用程序，你的电脑上可能同时开启qq，暴风影音，等多个应用程序，\n那么我们通过ip和mac找到了一台特定的主机，如何标识这台主机上的应用程序，答案就是端口，端口即应用程序与网卡关联的编号。\n传输层功能：建立端口到端口的通信\n补充：端口范围0-65535，0-1023为系统占用端口\ntcp协议：\n可靠传输，TCP数据包没有长度限制，理论上可以无限长，但是为了保证网络的效率，通常TCP数据包的长度不会超过IP数据包的长度，以确保单个TCP数据包不必再分割。\n以太网头 ip 头 tcp头 数据 udp协议： 不可靠传输，”报头”部分一共只有8个字节，总长度不超过65,535字节，正好放进一个IP数据包。\n以太网头 ip头 udp头 数据 tcp报文\ntcp三次握手和四次挥手\n应用层 应用层由来：用户使用的都是应用程序，均工作于应用层，互联网是开发的，大家都可以开发自己的应用程序，数据多种多样，必须规定好数据的组织形式\n应用层功能：规定应用程序的数据格式。\n例：TCP协议可以为各种各样的程序传递数据，比如Email、WWW、FTP等等。那么，必须有不同协议规定电子邮件、网页、FTP数据的格式，这些应用程序协议就构成了”应用层”。\n网络通信实现 想实现网络通信，每台主机需具备四要素\n本机的IP地址 子网掩码 网关的IP地址 DNS的IP地址 获取这四要素分两种方式\n1.静态获取\n即手动配置\n2.动态获取\n通过dhcp获取\n以太网头 ip头 udp头 dhcp数据包 （1）最前面的”以太网标头”，设置发出方（本机）的MAC地址和接收方（DHCP服务器）的MAC地址。前者就是本机网卡的MAC地址，后者这时不知道，就填入一个广播地址：FF-FF-FF-FF-FF-FF。\n（2）后面的”IP标头”，设置发出方的IP地址和接收方的IP地址。这时，对于这两者，本机都不知道。于是，发出方的IP地址就设为0.0.0.0，接收方的IP地址设为255.255.255.255。\n（3）最后的”UDP标头”，设置发出方的端口和接收方的端口。这一部分是DHCP协议规定好的，发出方是68端口，接收方是67端口。\n这个据包构造完成后，就可以发出了。以太网是广播发送，同一个子网络的每台计算机都收到了这个包。因为接收方的MAC地址是FF-FF-FF-FF-FF-FF，看不出是发给谁的，所以每台收到这个包的计算机，还必须分析这个包的IP地址，才能确定是不是发给自己的。当看到发出方IP地址是0.0.0.0，接收方是255.255.255.255，于是DHCP服务器知道”这个包是发给我的”，而其他计算机就可以丢弃这个包。\n接下来，DHCP服务器读出这个包的数据内容，分配好IP地址，发送回去一个”DHCP响应”数据包。这个响应包的结构也是类似的，以太网标头的MAC地址是双方的网卡地址，IP标头的IP地址是DHCP服务器的IP地址（发出方）和255.255.255.255（接收方），UDP标头的端口是67（发出方）和68（接收方），分配给请求端的IP地址和本网络的具体参数则包含在Data部分。新加入的计算机收到这个响应包，于是就知道了自己的IP地址、子网掩码、网关地址、DNS服务器等等参数\n网络通信流程 本机获取 本机的IP地址：192.168.1.100 子网掩码：255.255.255.0 网关的IP地址：192.168.1.1 DNS的IP地址：8.8.8.8 打开浏览器，访问 想要访问Google，在地址栏输入了网址：www.google.com。 dns协议(基于udp协议) 13台根dns：\nA.root-servers.net198.41.0.4美国 B.root-servers.net192.228.79.201美国（另支持IPv6） C.root-servers.net192.33.4.12法国 D.root-servers.net128.8.10.90美国 E.root-servers.net192.203.230.10美国 F.root-servers.net192.5.5.241美国（另支持IPv6） G.root-servers.net192.112.36.4美国 H.root-servers.net128.63.2.53美国（另支持IPv6） I.root-servers.net192.36.148.17瑞典 J.root-servers.net192.58.128.30美国 K.root-servers.net193.0.14.129英国（另支持IPv6） L.root-servers.net198.32.64.12美国 M.root-servers.net202.12.27.33日本（另支持IPv6）\n域名定义：http://jingyan.baidu.com/article/1974b289a649daf4b1f774cb.html\n顶级域名：以.com,.net,.org,.cn等等属于国际顶级域名，根据目前的国际互联网域名体系，国际顶级域名分为两类：类别顶级域名(gTLD)和地理顶级域名(ccTLD)两种。类别顶级域名是　以\u0026quot;COM\u0026quot;、\u0026ldquo;NET\u0026rdquo;、\u0026ldquo;ORG\u0026rdquo;、\u0026ldquo;BIZ\u0026rdquo;、\u0026ldquo;INFO\u0026quot;等结尾的域名，均由国外公司负责管理。地理顶级域名是以国家或地区代码为结尾的域名，如\u0026quot;CN\u0026quot;代表中国，\u0026ldquo;UK\u0026quot;代表英国。地理顶级域名一般由各个国家或地区负责管理。\n二级域名：二级域名是以顶级域名为基础的地理域名，比喻中国的二级域有，.com.cn,.net.cn,.org.cn,.gd.cn等.子域名是其父域名的子域名，比喻父域名是abc.com,子域名就是www.abc.com或者*.abc.com. 一般来说，二级域名是域名的一条记录，比如alidiedie.com是一个域名，www.alidiedie.com是其中比较常用的记录，一般默认是用这个，但是类似*.alidiedie.com的域名全部称作是alidiedie.com的二级\nHTTP部分的内容 类似于这样的：\nGET / HTTP/1.1 Host: www.google.com Connection: keep-alive User-Agent: Mozilla/5.0 (Windows NT 6.1) …… Accept: text/html,application/xhtml+xml,application/xml;q=0.9,/;q=0.8 Accept-Encoding: gzip,deflate,sdch Accept-Language: zh-CN,zh;q=0.8 Accept-Charset: GBK,utf-8;q=0.7,*;q=0.3 Cookie: … …\n我们假定这个部分的长度为4960字节，它会被嵌在TCP数据包之中。\nTCP协议 TCP数据包需要设置端口，接收方（Google）的HTTP端口默认是80，发送方（本机）的端口是一个随机生成的1024-65535之间的整数，假定为51775。\nTCP数据包的标头长度为20字节，加上嵌入HTTP的数据包，总长度变为4980字节。\nIP协议 然后，TCP数据包再嵌入IP数据包。IP数据包需要设置双方的IP地址，这是已知的，发送方是192.168.1.100（本机），接收方是172.194.72.105（Google）。\nIP数据包的标头长度为20字节，加上嵌入的TCP数据包，总长度变为5000字节。\n以太网协议 最后，IP数据包嵌入以太网数据包。以太网数据包需要设置双方的MAC地址，发送方为本机的网卡MAC地址，接收方为网关192.168.1.1的MAC地址（通过ARP协议得到）。\n以太网数据包的数据部分，最大长度为1500字节，而现在的IP数据包长度为5000字节。因此，IP数据包必须分割成四个包。因为每个包都有自己的IP标头（20字节），所以四个包的IP数据包的长度分别为1500、1500、1500、560。\n服务器端响应 经过多个网关的转发，Google的服务器172.194.72.105，收到了这四个以太网数据包。\n根据IP标头的序号，Google将四个包拼起来，取出完整的TCP数据包，然后读出里面的”HTTP请求”，接着做出”HTTP响应”，再用TCP协议发回来。\n本机收到HTTP响应以后，就可以将网页显示出来，完成一次网络通信。\nsocket 五层通讯流程：\n但实际上从传输层开始以及以下，都是操作系统帮咱们完成的，下面的各种包头封装的过程，用咱们去一个一个做么？NO！\nSocket又称为套接字，它是应用层与TCP/IP协议族通信的中间软件抽象层，它是一组接口。在设计模式中，Socket其实就是一个门面模式，它把复杂的TCP/IP协议族隐藏在Socket接口后面，对用户来说，一组简单的接口就是全部，让Socket去组织数据，以符合指定的协议。当我们使用不同的协议进行通信时就得使用不同的接口，还得处理不同协议的各种细节，这就增加了开发的难度，软件也不易于扩展(就像我们开发一套公司管理系统一样，报账、会议预定、请假等功能不需要单独写系统，而是一个系统上多个功能接口，不需要知道每个功能如何去实现的)。于是UNIX BSD就发明了socket这种东西，socket屏蔽了各个协议的通信细节，使得程序员无需关注协议本身，直接使用socket提供的接口来进行互联的不同主机间的进程的通信。这就好比操作系统给我们提供了使用底层硬件功能的系统调用，通过系统调用我们可以方便的使用磁盘（文件操作），使用内存，而无需自己去进行磁盘读写，内存管理。socket其实也是一样的东西，就是提供了tcp/ip协议的抽象，对外提供了一套接口，同过这个接口就可以统一、方便的使用tcp/ip协议的功能了。\n其实站在你的角度上看，socket就是一个模块。我们通过调用模块中已经实现的方法建立两个进程之间的连接和通信。也有人将socket说成ip+port，因为ip是用来标识互联网中的一台主机的位置，而port是用来标识这台机器上的一个应用程序。 所以我们只要确立了ip和port就能找到一个应用程序，并且使用socket模块来与之通信。\n套接字发展史及分类 套接字起源于 20 世纪 70 年代加利福尼亚大学伯克利分校版本的 Unix,即人们所说的 BSD Unix。 因此,有时人们也把套接字称为“伯克利套接字”或“BSD 套接字”。一开始,套接字被设计用在同 一台主机上多个应用程序之间的通讯。这也被称进程间通讯,或 IPC。套接字有两种（或者称为有两个种族）,分别是基于文件型的和基于网络型的。\n基于文件类型的套接字家族\n套接字家族的名字：AF_UNIX\nunix一切皆文件，基于文件的套接字调用的就是底层的文件系统来取数据，两个套接字进程运行在同一机器，可以通过访问同一个文件系统间接完成通信\n基于网络类型的套接字家族\n套接字家族的名字：AF_INET\n(还有AF_INET6被用于ipv6，还有一些其他的地址家族，不过，他们要么是只用于某个平台，要么就是已经被废弃，或者是很少被使用，或者是根本没有实现，所有地址家族中，AF_INET是使用最广泛的一个，python支持很多种地址家族，但是由于我们只关心网络编程，所以大部分时候我么只使用AF_INET)\n套接字的工作流程（基于TCP和 UDP两个协议） TCP和UDP对比 TCP（Transmission Control Protocol）可靠的、面向连接的协议（eg:打电话）、传输效率低全双工通信（发送缓存\u0026amp;接收缓存）、面向字节流。使用TCP的应用：Web浏览器；文件传输程序。\nUDP（User Datagram Protocol）不可靠的、无连接的服务，传输效率高（发送前时延小），一对一、一对多、多对一、多对多、面向报文(数据包)，尽最大努力服务，无拥塞控制。使用UDP的应用：域名系统 (DNS)；视频流；IP语音(VoIP)。\nTCP协议下的socket 个生活中的场景。你要打电话给一个朋友，先拨号，朋友听到电话铃声后提起电话，这时你和你的朋友就建立起了连接，就可以讲话了。等交流结束，挂断电话结束此次交谈。 生活中的场景就解释了这工作原理。\n先从服务器端说起。服务器端先初始化Socket，然后与端口绑定(bind)，对端口进行监听(listen)，调用accept阻塞，等待客户端连接。在这时如果有个客户端初始化一个Socket，然后连接服务器(connect)，如果连接成功，这时客户端与服务器端的连接就建立了。客户端发送数据请求，服务器端接收请求并处理请求，然后把回应数据发送给客户端，客户端读取数据，最后关闭连接，一次交互结束 细说socket()模块函数用法\nimport socket socket.socket(socket_family,socket_type,protocal=0) socket_family 可以是 AF_UNIX 或 AF_INET。socket_type 可以是 SOCK_STREAM 或 SOCK_DGRAM。protocol 一般不填,默认值为 0。 获取tcp/ip套接字 tcpSock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) 获取udp/ip套接字 udpSock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) 由于 socket 模块中有太多的属性。我们在这里破例使用了'from module import *'语句。使用 'from socket import *',我们就把 socket 模块里的所有属性都带到我们的命名空间里了,这样能 大幅减短我们的代码。 例如tcpSock = socket(AF_INET, SOCK_STREAM) 服务端套接字函数 s.bind() 绑定(主机,端口号)到套接字 s.listen() 开始TCP监听 s.accept() 被动接受TCP客户的连接,(阻塞式)等待连接的到来 客户端套接字函数 s.connect() 主动初始化TCP服务器连接 s.connect_ex() connect()函数的扩展版本,出错时返回出错码,而不是抛出异常 公共用途的套接字函数 s.recv() 接收TCP数据 s.send() 发送TCP数据(send在待发送数据量大于己端缓存区剩余空间时,数据丢失,不会发完) s.sendall() 发送完整的TCP数据(本质就是循环调用send,sendall在待发送数据量大于己端缓存区剩余空间时,数据不丢失,循环调用send直到发完) s.recvfrom() 接收UDP数据 s.sendto() 发送UDP数据 s.getpeername() 连接到当前套接字的远端的地址 s.getsockname() 当前套接字的地址 s.getsockopt() 返回指定套接字的参数 s.setsockopt() 设置指定套接字的参数 s.close() 关闭套接字 面向锁的套接字方法 s.setblocking() 设置套接字的阻塞与非阻塞模式 s.settimeout() 设置阻塞套接字操作的超时时间 s.gettimeout() 得到阻塞套接字操作的超时时间 面向文件的套接字的函数 s.fileno() 套接字的文件描述符 s.makefile() 创建一个与该套接字相关的文件 第一版，单个客户端与服务端通信（low版）\nimport socket phone = socket.socket(socket.AF_INET,socket.SOCK_STREAM) # 买电话 phone.connect(('127.0.0.1',8080)) # 与客户端建立连接， 拨号 phone.send('hello'.encode('utf-8')) from_server_data = phone.recv(1024) print(from_server_data) phone.close() # 挂电话 # 网络通信与打电话（诺基亚）是一样的。 import socket phone = socket.socket(socket.AF_INET,socket.SOCK_STREAM) # 买电话 phone.bind(('127.0.0.1',8080)) # 0 ~ 65535 1024之前系统分配好的端口 绑定电话卡 phone.listen(5) # 同一时刻有5个请求，但是可以有N多个链接。 开机。 conn, client_addr = phone.accept() # 接电话 print(conn, client_addr, sep='\\n') from_client_data = conn.recv(1024) # 一次接收的最大限制 bytes print(from_client_data.decode('utf-8')) conn.send(from_client_data.upper()) conn.close() # 挂电话 phone.close() # 关机 服务端 第二版，通信循环\nimport socket phone = socket.socket(socket.AF_INET,socket.SOCK_STREAM) phone.bind(('127.0.0.1',8080)) phone.listen(5) conn, client_addr = phone.accept() print(conn, client_addr, sep='\\n') while 1: # 循环收发消息 try: from_client_data = conn.recv(1024) print(from_client_data.decode('utf-8')) conn.send(from_client_data + b'SB') except ConnectionResetError: break conn.close() phone.close() import socket phone = socket.socket(socket.AF_INET,socket.SOCK_STREAM) # 买电话 phone.connect(('127.0.0.1',8080)) # 与客户端建立连接， 拨号 while 1: # 循环收发消息 client_data = input('\u0026gt;\u0026gt;\u0026gt;') phone.send(client_data.encode('utf-8')) from_server_data = phone.recv(1024) print(from_server_data.decode('utf-8')) phone.close() # 挂电话 第三版， 通信，连接循环\nimport socket phone = socket.socket(socket.AF_INET,socket.SOCK_STREAM) phone.bind(('127.0.0.1',8080)) phone.listen(5) while 1 : # 循环连接客户端 conn, client_addr = phone.accept() print(client_addr) while 1: try: from_client_data = conn.recv(1024) print(from_client_data.decode('utf-8')) conn.send(from_client_data + b'SB') except ConnectionResetError: break conn.close() phone.close() import socket phone = socket.socket(socket.AF_INET,socket.SOCK_STREAM) # 买电话 phone.connect(('127.0.0.1',8080)) # 与客户端建立连接， 拨号 while 1: client_data = input('\u0026gt;\u0026gt;\u0026gt;') phone.send(client_data.encode('utf-8')) from_server_data = phone.recv(1024) print(from_server_data.decode('utf-8')) phone.close() # 挂电话 详解recv的工作原理\n''' 源码解释： Receive up to buffersize bytes from the socket. 接收来自socket缓冲区的字节数据， For the optional flags argument, see the Unix manual. 对于这些设置的参数，可以查看Unix手册。 When no data is available, block untilat least one byte is available or until the remote end is closed. 当缓冲区没有数据可取时，recv会一直处于阻塞状态，直到缓冲区至少有一个字节数据可取，或者远程端关闭。 When the remote end is closed and all data is read, return the empty string. 关闭远程端并读取所有数据后，返回空字符串。 ''' ----------服务端------------： # 1，验证服务端缓冲区数据没有取完，又执行了recv执行，recv会继续取值。 import socket phone =socket.socket(socket.AF_INET,socket.SOCK_STREAM) phone.bind(('127.0.0.1',8080)) phone.listen(5) conn, client_addr = phone.accept() from_client_data1 = conn.recv(2) print(from_client_data1) from_client_data2 = conn.recv(2) print(from_client_data2) from_client_data3 = conn.recv(1) print(from_client_data3) conn.close() phone.close() # 2，验证服务端缓冲区取完了，又执行了recv执行，此时客户端20秒内不关闭的前提下，recv处于阻塞状态。 import socket phone =socket.socket(socket.AF_INET,socket.SOCK_STREAM) phone.bind(('127.0.0.1',8080)) phone.listen(5) conn, client_addr = phone.accept() from_client_data = conn.recv(1024) print(from_client_data) print(111) conn.recv(1024) # 此时程序阻塞20秒左右，因为缓冲区的数据取完了，并且20秒内，客户端没有关闭。 print(222) conn.close() phone.close() # 3 验证服务端缓冲区取完了，又执行了recv执行，此时客户端处于关闭状态，则recv会取到空字符串。 import socket phone =socket.socket(socket.AF_INET,socket.SOCK_STREAM) phone.bind(('127.0.0.1',8080)) phone.listen(5) conn, client_addr = phone.accept() from_client_data1 = conn.recv(1024) print(from_client_data1) from_client_data2 = conn.recv(1024) print(from_client_data2) from_client_data3 = conn.recv(1024) print(from_client_data3) conn.close() phone.close() ------------客户端------------ # 1，验证服务端缓冲区数据没有取完，又执行了recv执行，recv会继续取值。 import socket import time phone = socket.socket(socket.AF_INET,socket.SOCK_STREAM) phone.connect(('127.0.0.1',8080)) phone.send('hello'.encode('utf-8')) time.sleep(20) phone.close() # 2，验证服务端缓冲区取完了，又执行了recv执行，此时客户端20秒内不关闭的前提下，recv处于阻塞状态。 import socket import time phone = socket.socket(socket.AF_INET,socket.SOCK_STREAM) phone.connect(('127.0.0.1',8080)) phone.send('hello'.encode('utf-8')) time.sleep(20) phone.close() # 3，验证服务端缓冲区取完了，又执行了recv执行，此时客户端处于关闭状态，则recv会取到空字符串。 import socket import time phone = socket.socket(socket.AF_INET,socket.SOCK_STREAM) phone.connect(('127.0.0.1',8080)) phone.send('hello'.encode('utf-8')) phone.close() 远程执行命令的示例：\nimport socket import subprocess phone = socket.socket(socket.AF_INET,socket.SOCK_STREAM) phone.bind(('127.0.0.1',8080)) phone.listen(5) while 1 : # 循环连接客户端 conn, client_addr = phone.accept() print(client_addr) while 1: try: cmd = conn.recv(1024) ret = subprocess.Popen(cmd.decode('utf-8'),shell=True,stdout=subprocess.PIPE,stderr=subprocess.PIPE) correct_msg = ret.stdout.read() error_msg = ret.stderr.read() conn.send(correct_msg + error_msg) except ConnectionResetError: break conn.close() phone.close() import socket phone = socket.socket(socket.AF_INET,socket.SOCK_STREAM) # 买电话 phone.connect(('127.0.0.1',8080)) # 与客户端建立连接， 拨号 while 1: cmd = input('\u0026gt;\u0026gt;\u0026gt;') phone.send(cmd.encode('utf-8')) from_server_data = phone.recv(1024) print(from_server_data.decode('gbk')) phone.close() # 挂电话 远程执行命令 import socket import subprocess phone = socket.socket() phone.bind((\u0026quot;127.0.0.1\u0026quot;,8848)) phone.listen(2) while 1: conn,addr = phone.accept() print(f\u0026quot;连接来了:{coon,addr}\u0026quot;) while 1: try: from_client_data = conn.recv(1024) if from_client_data.upper() == b\u0026quot;Q\u0026quot;: print(\u0026quot;客户端正常退出聊天了\u0026quot;) break obj = subprocess.Pepen(from_client_data.decode('utf-8'), shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE,) result = obj.stdout.read() + obj.stderr.read() conn.send(result) except ConnectionResetError: print('客户端链接中断了') break conn.close() phone.close() import socket phone = socket.socket() phone.connect((\u0026quot;127.0.0.1\u0026quot;,8848)) while 1: to_server_data = input(\u0026quot;\u0026gt;\u0026gt;\u0026gt;\u0026quot;).strip().encode('utf-8') if not to_server_data: # 服务端如果接受到了空的内容，服务端就会一直阻塞中，所以无论哪一端发送内容时，都不能为空发送 print('发送内容不能为空') continue phone.send(to_server_data) if to_server_data.upper() == b'Q': break from_server_data = phone.recv(1024) # 最多接受1024字节 print(f'{from_server_data.decode(\u0026quot;gbk\u0026quot;)}') phone.close() 粘包 每个 socket 被创建后，都会分配两个缓冲区，输入缓冲区和输出缓冲区。\nwrite()/send() 并不立即向网络中传输数据，而是先将数据写入缓冲区中，再由TCP协议将数据从缓冲区发送到目标机器。一旦将数据写入到缓冲区，函数就可以成功返回，不管它们有没有到达目标机器，也不管它们何时被发送到网络，这些都是TCP协议负责的事情。\nTCP协议独立于 write()/send() 函数，数据有可能刚被写入缓冲区就发送到网络，也可能在缓冲区中不断积压，多次写入的数据被一次性发送到网络，这取决于当时的网络情况、当前线程是否空闲等诸多因素，不由程序员控制。\nread()/recv() 函数也是如此，也从输入缓冲区中读取数据，而不是直接从网络中读取。\n这些I/O缓冲区特性可整理如下：\n1.I/O缓冲区在每个TCP套接字中单独存在； 2.I/O缓冲区在创建套接字时自动生成； 3.即使关闭套接字也会继续传送输出缓冲区中遗留的数据； 4.关闭套接字将丢失输入缓冲区中的数据。\n输入输出缓冲区的默认大小一般都是 8K，可以通过 getsockopt() 函数获取：\nunsigned optVal; int optLen = sizeof(int); getsockopt(servSock, SOL_SOCKET, SO_SNDBUF,(char*)\u0026amp;optVal, \u0026amp;optLen); printf(\u0026ldquo;Buffer length: %d\\n\u0026rdquo;, optVal); socket缓冲区解释\nsocket缓存区的详细解释\n哪些情况会发生粘包 接收方没有及时接收缓冲区的包，造成多个包接收（客户端发送了一段数据，服务端只收了一小部分，服务端下次再收的时候还是从缓冲区拿上次遗留的数据，产生粘包）\nimport socket import subprocess phone = socket.socket(socket.AF_INET, socket.SOCK_STREAM) phone.bind(('127.0.0.1', 8080)) phone.listen(5) while 1: # 循环连接客户端 conn, client_addr = phone.accept() print(client_addr) while 1: try: cmd = conn.recv(1024) ret = subprocess.Popen(cmd.decode('utf-8'), shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE) correct_msg = ret.stdout.read() error_msg = ret.stderr.read() conn.send(correct_msg + error_msg) except ConnectionResetError: break conn.close() phone.close() # 服务端 import socket phone = socket.socket(socket.AF_INET,socket.SOCK_STREAM) # 买电话 phone.connect(('127.0.0.1',8080)) # 与客户端建立连接， 拨号 while 1: cmd = input('\u0026gt;\u0026gt;\u0026gt;') phone.send(cmd.encode('utf-8')) from_server_data = phone.recv(1024) print(from_server_data.decode('gbk')) phone.close() # 由于客户端发的命令获取的结果大小已经超过1024，那么下次在输入命令，会继续取上次残留到缓存区的数据。 客户端 发送端需要等缓冲区满才发送出去，造成粘包（发送数据时间间隔很短，数据也很小，会合到一起，产生粘包）\nimport socket phone = socket.socket(socket.AF_INET, socket.SOCK_STREAM) phone.bind(('127.0.0.1', 8080)) phone.listen(5) conn, client_addr = phone.accept() frist_data = conn.recv(1024) print('1:',frist_data.decode('utf-8')) # 1: helloworld second_data = conn.recv(1024) print('2:',second_data.decode('utf-8')) conn.close() phone.close() # 服务端 import socket phone = socket.socket(socket.AF_INET, socket.SOCK_STREAM) phone.connect(('127.0.0.1', 8080)) phone.send(b'hello') phone.send(b'world') phone.close() # 两次返送信息时间间隔太短，数据小，造成服务端一次收取 # 客户端 粘包的解决方案 先介绍一下struct模块：\n该模块可以把一个类型，如数字，转成固定长度的bytes\nimport struct # 将一个数字转化成等长度的bytes类型。 ret = struct.pack('i', 183346) print(ret, type(ret), len(ret)) # 通过unpack反解回来 ret1 = struct.unpack('i',ret)[0] print(ret1, type(ret1), len(ret1)) # 但是通过struct 处理不能处理太大 ret = struct.pack('l', 4323241232132324) print(ret, type(ret), len(ret)) # 报错 low版\n问题的根源在于，接收端不知道发送端将要传送的字节流的长度，所以解决粘包的方法就是围绕，如何让发送端在发送数据前，把自己将要发送的字节流总数按照固定字节发送给接收端后面跟上总数据，然后接收端先接收固定字节的总字节流，再来一个死循环接收完所有数据。 import socket import subprocess import struct phone = socket.socket(socket.AF_INET, socket.SOCK_STREAM) phone.bind(('127.0.0.1', 8080)) phone.listen(5) while 1: conn, client_addr = phone.accept() print(client_addr) while 1: try: cmd = conn.recv(1024) ret = subprocess.Popen(cmd.decode('utf-8'), shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE) correct_msg = ret.stdout.read() error_msg = ret.stderr.read() # 1 制作固定报头 total_size = len(correct_msg) + len(error_msg) header = struct.pack('i', total_size) # 2 发送报头 conn.send(header) # 发送真实数据： conn.send(correct_msg) conn.send(error_msg) except ConnectionResetError: break conn.close() phone.close() # 但是low版本有问题： # 1，报头不只有总数据大小，而是还应该有MD5数据，文件名等等一些数据。 # 2，通过struct模块直接数据处理，不能处理太大。 # 服务端 import socket import struct phone = socket.socket(socket.AF_INET,socket.SOCK_STREAM) phone.connect(('127.0.0.1',8080)) while 1: cmd = input('\u0026gt;\u0026gt;\u0026gt;').strip() if not cmd: continue phone.send(cmd.encode('utf-8')) # 1，接收固定报头 header = phone.recv(4) # 2，解析报头 total_size = struct.unpack('i', header)[0] # 3，根据报头信息，接收真实数据 recv_size = 0 res = b'' while recv_size \u0026lt; total_size: recv_data = phone.recv(1024) res += recv_data recv_size += len(recv_data) print(res.decode('gbk')) phone.close() # 客户端 可自定制报头版\n整个流程的大致解释： 我们可以把报头做成字典，字典里包含将要发送的真实数据的描述信息(大小啊之类的)，然后json序列化，然后用struck将序列化后的数据长度打包成4个字节。 我们在网络上传输的所有数据 都叫做数据包，数据包里的所有数据都叫做报文，报文里面不止有你的数据，还有ip地址、mac地址、端口号等等，其实所有的报文都有报头，这个报头是协议规定的，看一下\n发送时： 先发报头长度 再编码报头内容然后发送 最后发真实内容\n接收时： 先手报头长度，用struct取出来 根据取出的长度收取报头内容，然后解码，反序列化 从反序列化的结果中取出待取数据的描述信息，然后去取真实的数据内容\n整体的流程解释\nimport socket import subprocess import struct import json phone = socket.socket(socket.AF_INET, socket.SOCK_STREAM) phone.bind(('127.0.0.1', 8080)) phone.listen(5) while 1: conn, client_addr = phone.accept() print(client_addr) while 1: try: cmd = conn.recv(1024) ret = subprocess.Popen(cmd.decode('utf-8'), shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE) correct_msg = ret.stdout.read() error_msg = ret.stderr.read() # 1 制作固定报头 total_size = len(correct_msg) + len(error_msg) header_dict = { 'md5': 'fdsaf2143254f', 'file_name': 'f1.txt', 'total_size':total_size, } header_dict_json = json.dumps(header_dict) # str bytes_headers = header_dict_json.encode('utf-8') header_size = len(bytes_headers) header = struct.pack('i', header_size) # 2 发送报头长度 conn.send(header) # 3 发送报头 conn.send(bytes_headers) # 4 发送真实数据： conn.send(correct_msg) conn.send(error_msg) except ConnectionResetError: break conn.close() phone.close() # 服务端 import socket import struct import json phone = socket.socket(socket.AF_INET,socket.SOCK_STREAM) phone.connect(('127.0.0.1',8080)) while 1: cmd = input('\u0026gt;\u0026gt;\u0026gt;').strip() if not cmd: continue phone.send(cmd.encode('utf-8')) # 1，接收固定报头 header_size = struct.unpack('i', phone.recv(4))[0] # 2，解析报头长度 header_bytes = phone.recv(header_size) header_dict = json.loads(header_bytes.decode('utf-8')) # 3,收取报头 total_size = header_dict['total_size'] # 3，根据报头信息，接收真实数据 recv_size = 0 res = b'' while recv_size \u0026lt; total_size: recv_data = phone.recv(1024) res += recv_data recv_size += len(recv_data) print(res.decode('gbk')) phone.close() # 客户端 udp下的socket udp是无链接的，先启动哪一端都不会报错 UDP下的socket通讯流程\n先从服务器端说起。服务器端先初始化Socket，然后与端口绑定(bind)，recvform接收消息，这个消息有两项，消息内容和对方客户端的地址，然后回复消息时也要带着你收到的这个客户端的地址，发送回去，最后关闭连接，一次交互结束\n上代码感受一下，需要创建两个文件，文件名称随便起，为了方便看，我的两个文件名称为udp_server.py(服务端)和udp_client.py(客户端)，将下面的server端的代码拷贝到udp_server.py文件中，将下面cliet端的代码拷贝到udp_client.py的文件中，然后先运行udp_server.py文件中的代码，再运行udp_client.py文件中的代码，然后在pycharm下面的输出窗口看一下效果。\nimport socket udp_sk = socket.socket(type=socket.SOCK_DGRAM) #创建一个服务器的套接字 udp_sk.bind(('127.0.0.1',9000)) #绑定服务器套接字 msg,addr = udp_sk.recvfrom(1024) print(msg) udp_sk.sendto(b'hi',addr) # 对话(接收与发送) udp_sk.close() # 关闭服务器套接字 # server端 import socket ip_port=('127.0.0.1',9000) udp_sk=socket.socket(type=socket.SOCK_DGRAM) udp_sk.sendto(b'hello',ip_port) back_msg,addr=udp_sk.recvfrom(1024) print(back_msg.decode('utf-8'),addr) # client端 类似于qq聊天的代码示例： #_*_coding:utf-8_*_ import socket ip_port=('127.0.0.1',8081) udp_server_sock=socket.socket(socket.AF_INET,socket.SOCK_DGRAM) #DGRAM:datagram 数据报文的意思，象征着UDP协议的通信方式 udp_server_sock.bind(ip_port)#你对外提供服务的端口就是这一个，所有的客户端都是通过这个端口和你进行通信的 while True: qq_msg,addr=udp_server_sock.recvfrom(1024)# 阻塞状态，等待接收消息 print('来自[%s:%s]的一条消息:\\033[1;44m%s\\033[0m' %(addr[0],addr[1],qq_msg.decode('utf-8'))) back_msg=input('回复消息: ').strip() udp_server_sock.sendto(back_msg.encode('utf-8'),addr) #server端 import socket BUFSIZE=1024 udp_client_socket=socket.socket(socket.AF_INET,socket.SOCK_DGRAM) qq_name_dic={ 'taibai':('127.0.0.1',8081), 'Jedan':('127.0.0.1',8081), 'Jack':('127.0.0.1',8081), 'John':('127.0.0.1',8081), } while True: qq_name=input('请选择聊天对象: ').strip() while True: msg=input('请输入消息,回车发送,输入q结束和他的聊天: ').strip() if msg == 'q':break if not msg or not qq_name or qq_name not in qq_name_dic:continue udp_client_socket.sendto(msg.encode('utf-8'),qq_name_dic[qq_name])# 必须带着自己的地址，这就是UDP不一样的地方，不需要建立连接，但是要带着自己的地址给服务端，否则服务端无法判断是谁给我发的消息，并且不知道该把消息回复到什么地方，因为我们之间没有建立连接通道 back_msg,addr=udp_client_socket.recvfrom(BUFSIZE)# 同样也是阻塞状态，等待接收消息 print('来自[%s:%s]的一条消息:\\033[1;44m%s\\033[0m' %(addr[0],addr[1],back_msg.decode('utf-8'))) udp_client_socket.close() # client端 ","date":"2019-12-21","permalink":"https://daemon365.dev/2019/12/21/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/","tags":["network"],"title":"网络编程"},{"content":"protobuf是一种高效的数据格式，平台无关、语言无关、可扩展，可用于 RPC 系统和持续数据存储系统。\nprotobuf介绍 Protobuf是Protocol Buffer的简称，它是Google公司于2008年开源的一种高效的平台无关、语言无关、可扩展的数据格式，目前Protobuf作为接口规范的描述语言，可以作为Go语言RPC接口的基础工具。\nprotobuf使用 protobuf是一个与语言无关的一个数据协议，所以我们需要先编写IDL文件然后借助专用工具生成指定语言的代码，从而实现数据的序列化与反序列化过程。\n大致开发流程如下： 1. IDL编写 2. 生成指定语言的代码 3. 序列化和反序列化\nprotobuf语法 官网：https://developers.google.cn/protocol-buffers/docs/proto3 (英文)\n三方：https://colobu.com/2017/03/16/Protobuf3-language-guide (中文)\n编译器安装 ptotoc mac安装：\nbrew info protobuf cdn下载：(下载需要的版本)\ncdn下载链接\nlinux/mac 编译安装\n教程\nprotoc-gen-go 安装生成Go语言代码的工具\n两个版本：\ngithub版本 github.com/golang/protobuf/protoc-gen-go google版本 google.golang.org/protobuf/cmd/protoc-gen-go 区别在于前者是旧版本，后者是google接管后的新版本，他们之间的API是不同的，也就是说用于生成的命令，以及生成的文件都是不一样的。\n因为目前的gRPC-go源码中的example用的是后者的生成方式，为了与时俱进，本文也采取最新的方式。\n安装：\ngo install google.golang.org/protobuf/cmd/protoc-gen-go@latest go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest 编写IDL代码 新建一个名为person.proto的文件具体内容如下：\n// 指定使用protobuf版本 // 此处使用v3版本 syntax = \u0026quot;proto3\u0026quot;; // 包名，通过protoc生成go文件 package address; // go的包名 新版protoc-gen-go 必须带\u0026quot;/\u0026quot; option go_package = \u0026quot;../hello\u0026quot;; // 性别类型 // 枚举类型第一个字段必须为0 enum GenderType { SECRET = 0; FEMALE = 1; MALE = 2; } // 人 message Person { int64 id = 1; string name = 2; GenderType gender = 3; string number = 4; } // 联系簿 message ContactBook { repeated Person persons = 1; } 生成go语言代码 在protobuf_demo/address目录下执行以下命令。\nprotoc --proto_path=./ --go_out=./ ./person.proto \u0026ndash;proto_path: 指定了要去哪个目录中搜索import中导入的和要编译为.go的proto文件 \u0026ndash;go_out:指定了生成的go文件的目录，我在这里把go文件放到本目录中 person.proto， 定义了我要编译的文件是哪个文件。 此时在当前目录下会生成一个person.pb.go文件，我们的Go语言代码里就是使用这个文件。 在protobuf_demo/main.go文件中：\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;io/ioutil\u0026quot; \u0026quot;z/test3/person\u0026quot; \u0026quot;github.com/golang/protobuf/proto\u0026quot; ) func main() { var cb person.ContactBook p1 := person.Person{ Name: \u0026quot;zhao\u0026quot;, Gender: person.GenderType_MALE, Number: \u0026quot;12345678910\u0026quot;, } fmt.Println(p1) cb.Persons = append(cb.Persons, \u0026amp;p1) data, err := proto.Marshal(\u0026amp;p1) if err != nil { fmt.Printf(\u0026quot;marshal failed,err:%v\\n\u0026quot;, err) return } ioutil.WriteFile(\u0026quot;./proto.dat\u0026quot;, data, 0644) data2, err := ioutil.ReadFile(\u0026quot;./proto.dat\u0026quot;) if err != nil { fmt.Printf(\u0026quot;read file failed, err:%v\\n\u0026quot;, err) return } var p2 person.Person proto.Unmarshal(data2, \u0026amp;p2) fmt.Println(p2) } 参考文章:https://www.liwenzhou.com/posts/Go/protobuf/\n","date":"2019-12-01","permalink":"https://daemon365.dev/2019/12/01/proto-buffer/","tags":["go","protobuf"],"title":"proto buffer"},{"content":"go module是 Go1.11版本之后官方推出的版本管理工具，并且从Go1.13版本开始，go module将是Go语言默认的依赖管理工具。\nGO111MODULE 要启用go module支持首先要设置环境变量GO111MODULE，通过它可以开启或关闭模块支持，它有三个可选值：off、on、auto，默认值是auto。\nGO111MODULE=off禁用模块支持，编译时会从GOPATH和vendor文件夹中查找包。\nGO111MODULE=on启用模块支持，编译时会忽略GOPATH和vendor文件夹，只根据 go.mod下载依赖。\nGO111MODULE=auto，当项目在$GOPATH/src外且项目根目录有go.mod文件时，开启模块支持。\n简单来说，设置GO111MODULE=on之后就可以使用go module了，以后就没有必要在GOPATH中创建项目了，并且还能够很好的管理项目依赖的第三方包信息。\n使用 go module 管理依赖后会在项目根目录下生成两个文件go.mod和go.sum。\nGOPROXY Go1.11之后设置GOPROXY命令为：\n1 export GOPROXY=https://goproxy.cn Go1.13之后GOPROXY默认值为https://proxy.golang.org，在国内是无法访问的，所以十分建议大家设置GOPROXY，这里我推荐使用goproxy.cn。\ngo env -w GOPROXY=https://goproxy.cn,direct go mod命令 常用的go mod命令如下：\ngo mod download 下载依赖的module到本地cache（默认为$GOPATH/pkg/mod目录） go mod edit 编辑go.mod文件 go mod graph 打印模块依赖图 go mod init 初始化当前文件夹, 创建go.mod文件 go mod tidy 增加缺少的module，删除无用的module go mod vendor 将依赖复制到vendor下 go mod verify 校验依赖 go mod why 解释为什么需要依赖 go.mod go.mod文件记录了项目所有的依赖信息，其结构大致如下：\nmodule test go 1.15 require ( github.com/DeanThompson/ginpprof v0.0.0-20190408063150-3be636683586 github.com/gin-gonic/gin v1.4.0 github.com/go-sql-driver/mysql v1.4.1 github.com/jmoiron/sqlx v1.2.0 10 github.com/satori/go.uuid v1.2.0 google.golang.org/appengine v1.6.1 // indirect 12 ) 其中，\nmodule用来定义包名\nrequire用来定义依赖包及版本\nindirect表示间接引用\n依赖的版本 go mod支持语义化版本号，比如go get foo@v1.2.3，也可以跟git的分支或tag，比如go get foo@master，当然也可以跟git提交哈希，比如go get foo@e3702bed2。关于依赖的版本支持以下几种格式：\ngopkg.in/tomb.v1 v1.0.0-20141024135613-dd632973f1e7 gopkg.in/vmihailenco/msgpack.v2 v2.9.1 gopkg.in/yaml.v2 v2.2.1 github.com/tatsushid/go-fastping v0.0.0-20160109021039-d7bb493dee3e latest replace 在国内访问golang.org/x的各个包都需要***，你可以在go.mod中使用replace替换成github上对应的库。\nreplace ( golang.org/x/crypto v0.0.0-20180820150726-614d502a4dac =\u0026gt; github.com/golang/crypto v0.0.0-20180820150726-614d502a4dac golang.org/x/net v0.0.0-20180821023952-922f4815f713 =\u0026gt; github.com/golang/net v0.0.0-20180826012351-8a410e7b638d golang.org/x/text v0.3.0 =\u0026gt; github.com/golang/text v0.3.0 ) go get 在项目中执行go get命令可以下载依赖包，并且还可以指定下载的版本。\n运行go get -u将会升级到最新的次要版本或者修订版本(x.y.z, z是修订版本号， y是次要版本号)\n运行go get -u=patch将会升级到最新的修订版本\n运行go get package@version将会升级到指定的版本号version\n如果下载所有依赖可以使用go mod download命令。\n整理依赖 我们在代码中删除依赖代码后，相关的依赖库并不会在go.mod文件中自动移除。这种情况下我们可以使用go mod tidy命令更新go.mod中的依赖关系。\ngo mod edit 格式化 因为我们可以手动修改go.mod文件，所以有些时候需要格式化该文件。Go提供了一下命令：\ngo mod edit -fmt 添加依赖项 go mod edit -require=golang.org/x/text 移除依赖项 如果只是想修改go.mod文件中的内容，那么可以运行go mod edit -droprequire=package path，比如要在go.mod中移除golang.org/x/text包，可以使用如下命令：\ngo mod edit -droprequire=golang.org/x/text 关于go mod edit的更多用法可以通过go help mod edit查看。\n在项目中使用go module 既有项目 如果需要对一个已经存在的项目启用go module，可以按照以下步骤操作：\n在项目目录下执行go mod init，生成一个go.mod文件。\n执行go get，查找并记录当前项目的依赖，同时生成一个go.sum记录每个依赖库的版本和哈希值。\n新项目 对于一个新创建的项目，我们可以在项目文件夹下按照以下步骤操作：\n执行go mod init 项目名命令，在当前项目文件夹下创建一个go.mod文件。\n手动编辑go.mod中的require依赖项或执行go get自动发现、维护依赖。\n文章转自 https://www.liwenzhou.com/posts/Go/go_dependency/ ","date":"2019-10-01","permalink":"https://daemon365.dev/2019/10/01/go-mod/","tags":["go"],"title":"go mod"},{"content":"html模板生成: html/template包实现了数据驱动的模板，用于生成可对抗代码注入的安全HTML输出。它提供了和text/template包相同的接口，Go语言中输出HTML的场景都应使用text/template包。 模板语法 {{.}} 模板语法都包含在{{和}}中间，其中{{.}}中的点表示当前对象。\n当我们传入一个结构体对象时，我们可以根据.来访问结构体的对应字段。例如：\n// main.go func sayHello(w http.ResponseWriter, r *http.Request) { // 解析指定文件生成模板对象 tmpl, err := template.ParseFiles(\u0026quot;./hello.html\u0026quot;) if err != nil { fmt.Println(\u0026quot;create template failed, err:\u0026quot;, err) return } var user = struct { name string age int }{ name:\u0026quot;zhaohaiyu\u0026quot;, age:18, } // 利用给定数据渲染模板，并将结果写入w tmpl.Execute(w, user) } func main() { http.HandleFunc(\u0026quot;/\u0026quot;, sayHello) err := http.ListenAndServe(\u0026quot;:9090\u0026quot;, nil) if err != nil { fmt.Println(\u0026quot;HTTP server failed,err:\u0026quot;, err) return } } Hello 姓名 {{.Name}} 年龄：{{.Age}} 同理，当我们传入的变量是map时，也可以在模板文件中通过.根据key来取值。 注释 {{/* a comment */}}\npipeline pipeline是指产生数据的操作。比如{{.}}、{{.Name}}等。Go的模板语法中支持使用管道符号|链接多个命令，用法和unix下的管道类似：|前面的命令会将运算结果(或返回值)传递给后一个命令的最后一个位置。 **注意：**并不是只有使用了|才是pipeline。Go的模板语法中，pipeline的概念是传递数据，只要能产生数据的，都是pipeline。\n变量 我们还可以在模板中声明变量，用来保存传入模板的数据或其他语句生成的结果。具体语法如下： $obj := {{.}} 其中$obj是变量的名字，在后续的代码中就可以使用该变量了。 移除空格 有时候我们在使用模板语法的时候会不可避免的引入一下空格或者换行符，这样模板最终渲染出来的内容可能就和我们想的不一样，这个时候可以使用{{-语法去除模板内容左侧的所有空白符号， 使用-}}去除模板内容右侧的所有空白符号。 例如：{{- .Name -}}\n注意：-要紧挨{{和}}，同时与模板值之间需要使用空格分隔。\n条件判断 Go模板语法中的条件判断有以下几种: {{if pipeline}} T1 {{end}} {{if pipeline}} T1 {{else}} T0 {{end}} {{if pipeline}} T1 {{else if pipeline}} T0 {{end}} range Go的模板语法中使用range关键字进行遍历，有以下两种写法，其中pipeline的值必须是数组、切片、字典或者通道。\n{{range pipeline}} T1 {{end}} {{range pipeline}} T1 {{else}} T0 {{end}} with {{with pipeline}} T1 {{end}} {{with pipeline}} T1 {{else}} T0 {{end}} 预定义函数 执行模板时，函数从两个函数字典中查找：首先是模板函数字典，然后是全局函数字典。一般不在模板内定义函数，而是使用Funcs方法添加函数到模板里。\n预定义的全局函数如下：\nand 函数返回它的第一个empty参数或者最后一个参数； 就是说\u0026quot;and x y\u0026quot;等价于\u0026quot;if x then y else x\u0026quot;；所有参数都会执行； or 返回第一个非empty参数或者最后一个参数； 亦即\u0026quot;or x y\u0026quot;等价于\u0026quot;if x then x else y\u0026quot;；所有参数都会执行； not 返回它的单个参数的布尔值的否定 len 返回它的参数的整数类型长度 index 执行结果为第一个参数以剩下的参数为索引/键指向的值； 如\u0026quot;index x 1 2 3\u0026quot;返回x[1][2][3]的值；每个被索引的主体必须是数组、切片或者字典。 print 即fmt.Sprint printf 即fmt.Sprintf println 即fmt.Sprintln html 返回与其参数的文本表示形式等效的转义HTML。 这个函数在html/template中不可用。 urlquery 以适合嵌入到网址查询中的形式返回其参数的文本表示的转义值。 这个函数在html/template中不可用。 js 返回与其参数的文本表示形式等效的转义JavaScript。 call 执行结果是调用第一个参数的返回值，该参数必须是函数类型，其余参数作为调用该函数的参数；\n如\u0026quot;call .X.Y 1 2\u0026quot;等价于go语言里的dot.X.Y(1, 2)； 其中Y是函数类型的字段或者字典的值，或者其他类似情况； call的第一个参数的执行结果必须是函数类型的值（和预定义函数如print明显不同）； 该函数类型值必须有1到2个返回值，如果有2个则后一个必须是error接口类型； 如果有2个返回值的方法返回的error非nil，模板执行会中断并返回给调用模板执行者该错误；\n比较函数 布尔函数会将任何类型的零值视为假，其余视为真。 下面是定义为函数的二元比较运算的集合：\neq 如果arg1 == arg2则返回真 ne 如果arg1 != arg2则返回真 lt 如果arg1 \u0026lt; arg2则返回真 le 如果arg1 \u0026lt;= arg2则返回真 gt 如果arg1 \u0026gt; arg2则返回真 ge 如果arg1 \u0026gt;= arg2则返回真\n为了简化多参数相等检测，eq（只有eq）可以接受2个或更多个参数，它会将第一个参数和其余参数依次比较，返回下式的结果：\n{{eq arg1 arg2 arg3}}\n比较函数只适用于基本类型（或重定义的基本类型，如”type Celsius float32”）。但是，整数和浮点数不能互相比较。\n自定义函数 Go的模板支持自定义函数。\nfunc sayHello(w http.ResponseWriter, r *http.Request) { htmlByte, err := ioutil.ReadFile(\u0026quot;./hello.tmpl\u0026quot;) if err != nil { fmt.Println(\u0026quot;read html failed, err:\u0026quot;, err) return } // 自定义一个夸人的模板函数 kua := func(arg string) (string, error) { return arg + \u0026quot;真帅\u0026quot;, nil } // 采用链式操作在Parse之前调用Funcs添加自定义的kua函数 tmpl, err := template.New(\u0026quot;hello\u0026quot;).Funcs(template.FuncMap{\u0026quot;kua\u0026quot;: kua}).Parse(string(htmlByte)) if err != nil { fmt.Println(\u0026quot;create template failed, err:\u0026quot;, err) return } user := UserInfo{ Name: \u0026quot;小王子\u0026quot;, Gender: \u0026quot;男\u0026quot;, Age: 18, } // 使用user渲染模板，并将结果写入w tmpl.Execute(w, user) } 我们可以在模板文件hello.tmpl中按照如下方式使用我们自定义的kua函数了。\n{{kua .Name}}\n嵌套template 我们可以在template中嵌套其他的template。这个template可以是单独的文件，也可以是通过define定义的template。\n举个例子：t.tmpl文件内容如下：tmpl test\n测试嵌套template语法 {{template \u0026ldquo;ul.tmpl\u0026rdquo;}}\n{{template \u0026quot;ol.tmpl\u0026quot;}} {{ define \u0026quot;ol.tmpl\u0026quot;}} 吃饭 睡觉 打豆豆 {{end}} ul.tmpl文件内容如下： 注释 日志 测试 我们注册一个templDemo路由处理函数. http.HandleFunc(\u0026quot;/tmpl\u0026quot;, tmplDemo) tmplDemo函数的具体内容如下：\nfunc tmplDemo(w http.ResponseWriter, r *http.Request) { tmpl, err := template.ParseFiles(\u0026quot;./t.tmpl\u0026quot;, \u0026quot;./ul.tmpl\u0026quot;) if err != nil { fmt.Println(\u0026quot;create template failed, err:\u0026quot;, err) return } user := UserInfo{ Name: \u0026quot;小王子\u0026quot;, Gender: \u0026quot;男\u0026quot;, Age: 18, } tmpl.Execute(w, user) } 注意：在解析模板时，被嵌套的模板一定要在后面解析，例如上面的示例中t.tmpl模板中嵌套了ul.tmpl，所以ul.tmpl要在t.tmpl后进行解析。\nblock {{block \u0026ldquo;name\u0026rdquo; pipeline}} T1 {{end}}\nblock是定义模板{{define \u0026ldquo;name\u0026rdquo;}} T1 {{end}}和执行{{template \u0026ldquo;name\u0026rdquo; pipeline}}缩写，典型的用法是定义一组根模板，然后通过在其中重新定义块模板进行自定义。\n定义一个根模板templates/base.tmpl，内容如下：Go Templates\n{{block \u0026ldquo;content\u0026rdquo; . }}{{end}}\n然后定义一个templates/index.tmpl，”继承”base.tmpl：\n{{template \u0026quot;base.tmpl\u0026quot;}} {{define \u0026quot;content\u0026quot;}} Hello world! {{end}} 然后使用template.ParseGlob按照正则匹配规则解析模板文件，然后通过ExecuteTemplate渲染指定的模板：\nfunc index(w http.ResponseWriter, r *http.Request){ tmpl, err := template.ParseGlob(\u0026quot;templates/*.tmpl\u0026quot;) if err != nil { fmt.Println(\u0026quot;create template failed, err:\u0026quot;, err) return } err = tmpl.ExecuteTemplate(w, \u0026quot;index.tmpl\u0026quot;, nil) if err != nil { fmt.Println(\u0026quot;render template failed, err:\u0026quot;, err) return } } 如果我们的模板名称冲突了，例如不同业务线下都定义了一个index.tmpl模板，我们可以通过下面两种方法来解决。\n在模板文件开头使用{{define 模板名}}语句显式的为模板命名。 可以把模板文件存放在templates文件夹下面的不同目录中，然后使用template.ParseGlob(\u0026ldquo;templates/**/*.tmpl\u0026rdquo;)解析模板。 修改默认的标识符 Go标准库的模板引擎使用的花括号{{和}}作为标识，而许多前端框架（如Vue和AngularJS）也使用{{和}}作为标识符，所以当我们同时使用Go语言模板引擎和以上前端框架时就会出现冲突，这个时候我们需要修改标识符，修改前端的或者修改Go语言的。这里演示如何修改Go语言模板引擎默认的标识符：\ntemplate.New(\u0026ldquo;test\u0026rdquo;).Delims(\u0026quot;{[\u0026quot;, \u0026ldquo;]}\u0026rdquo;).ParseFiles(\u0026quot;./t.tmpl\u0026quot;)\ntext/template与html/tempalte的区别 html/template针对的是需要返回HTML内容的场景，在模板渲染过程中会对一些有风险的内容进行转义，以此来防范跨站脚本攻击。\n例如，我定义下面的模板文件：\nHello {{.}} 这个时候传入一段JS代码并使用html/template去渲染该文件，会在页面上显示出转义后的JS内容。alert(\u0026lsquo;嘿嘿嘿\u0026rsquo;)这就是html/template为我们做的事。\n但是在某些场景下，我们如果相信用户输入的内容，不想转义的话，可以自行编写一个safe函数，手动返回一个template.HTML类型的内容。示例如下：\nfunc xss(w http.ResponseWriter, r *http.Request){ tmpl,err := template.New(\u0026quot;xss.tmpl\u0026quot;).Funcs(template.FuncMap{ \u0026quot;safe\u0026quot;: func(s string)template.HTML { return template.HTML(s) }, }).ParseFiles(\u0026quot;./xss.tmpl\u0026quot;) if err != nil { fmt.Println(\u0026quot;create template failed, err:\u0026quot;, err) return } jsStr := `alert('嘿嘿嘿')` err = tmpl.Execute(w, jsStr) if err != nil { fmt.Println(err) } } 这样我们只需要在模板文件不需要转义的内容后面使用我们定义好的safe函数就可以了。{{ . | safe }}\n代码生成 生成代码用text/template包,模板语法和html/template一样\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;html/template\u0026quot; \u0026quot;os\u0026quot; ) var data = ` package main import \u0026quot;fmt\u0026quot; func main() { str := \u0026quot;{{ . }}\u0026quot; for _, v := range str { fmt.Println(v) } } ` func main() { t := template.New(\u0026quot;main\u0026quot;) t, err := t.Parse(data) if err != nil { fmt.Println(\u0026quot;解析失败\u0026quot;) return } var str = \u0026quot;zhaohaiyu\u0026quot; file, err := os.OpenFile(\u0026quot;./print/main.go\u0026quot;, os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0755) if err != nil { fmt.Printf(\u0026quot;open file:%s failed, err:%v\\n\u0026quot;, \u0026quot;./print/main.go\u0026quot;, err) return } err = t.Execute(file, str) if err != nil { fmt.Println(\u0026quot;execute failed err:\u0026quot;,err) return } return } 生成的代码:\npackage main import \u0026quot;fmt\u0026quot; func main() { str := \u0026quot;zhaohaiyu\u0026quot; for _, v := range str { fmt.Println(v) } } 参考文章:\nhttps://www.liwenzhou.com/posts/Go/go_template/#autoid-1-3-0 ","date":"2019-07-13","permalink":"https://daemon365.dev/2019/07/13/golang-%E6%A8%A1%E6%9D%BF-htmltemplate%E4%B8%8Etexttemplate/","tags":["go"],"title":"golang 模板 htmltemplate与texttemplate"},{"content":"变量的内在机制 Go语言中的变量是分为两部分的:\n类型信息：预先定义好的元信息。 值信息：程序运行过程中可动态变化的。 反射介绍 反射是指在程序运行期对程序本身进行访问和修改的能力。程序在编译时，变量被转换为内存地址，变量名不会被编译器写入到可执行部分。在运行程序时，程序无法获取自身的信息。\n支持反射的语言可以在程序编译期将变量的反射信息，如字段名称、类型信息、结构体信息等整合到可执行文件中，并给程序提供接口访问反射信息，这样就可以在程序运行期获取类型的反射信息，并且有能力修改它们。\nGo程序在运行期使用reflect包访问程序的反射信息。\n在上一篇博客中我们介绍了空接口。 空接口可以存储任意类型的变量，那我们如何知道这个空接口保存的数据是什么呢？ 反射就是在运行时动态的获取一个变量的类型信息和值信息。\nreflect包 在Go语言的反射机制中，任何接口值都由是一个具体类型和具体类型的值两部分组成的(我们在上一篇接口的博客中有介绍相关概念)。 在Go语言中反射的相关功能由内置的reflect包提供，任意接口值在反射中都可以理解为由reflect.Type和reflect.Value两部分组成，并且reflect包提供了reflect.TypeOf和reflect.ValueOf两个函数来获取任意对象的Value和Type。\nreflect.Type 和 reflect.Value 反射是由 reflect 包提供的。它定义了两个重要的类型，Type 和 Value。一个 Type 表示一个Go类型。它是一个接口，有许多方法来区分类型以及检查它们的组成部分，例如一个结构体的成员或一个函数的参数等。唯一能反映 reflect.Type 实现的是接口的类型描述信息（§7.5），也正是这个实体标识了接口值的动态类型。\n函数 reflect.TypeOf 接受任意的 interface{} 类型，并以 reflect.Type 形式返回其动态类型：\nt := reflect.TypeOf(3) // a reflect.Type fmt.Println(t.String()) // \u0026quot;int\u0026quot; fmt.Println(t) // \u0026quot;int\u0026quot; 其中 TypeOf(3) 调用将值 3 传给 interface{} 参数。回到 7.5节 的将一个具体的值转为接口类型会有一个隐式的接口转换操作，它会创建一个包含两个信息的接口值：操作数的动态类型（这里是 int）和它的动态的值（这里是 3）。\n因为 reflect.TypeOf 返回的是一个动态类型的接口值，它总是返回具体的类型。因此，下面的代码将打印 \u0026ldquo;*os.File\u0026rdquo; 而不是 \u0026ldquo;io.Writer\u0026rdquo;。稍后，我们将看到能够表达接口类型的 reflect.Type。\nvar w io.Writer = os.Stdout fmt.Println(reflect.TypeOf(w)) // \u0026quot;*os.File\u0026quot; 要注意的是 reflect.Type 接口是满足 fmt.Stringer 接口的。因为打印一个接口的动态类型对于调试和日志是有帮助的， fmt.Printf 提供了一个缩写 %T 参数，内部使用 reflect.TypeOf 来输出：\nfmt.Printf(\u0026quot;%T\\n\u0026quot;, 3) // \u0026quot;int\u0026quot; reflect 包中另一个重要的类型是 Value。一个 reflect.Value 可以装载任意类型的值。函数 reflect.ValueOf 接受任意的 interface{} 类型，并返回一个装载着其动态值的 reflect.Value。和 reflect.TypeOf 类似，reflect.ValueOf 返回的结果也是具体的类型，但是 reflect.Value 也可以持有一个接口值。\nv := reflect.ValueOf(3) // a reflect.Value fmt.Println(v) // \u0026quot;3\u0026quot; fmt.Printf(\u0026quot;%v\\n\u0026quot;, v) // \u0026quot;3\u0026quot; fmt.Println(v.String()) // NOTE: \u0026quot;\u0026lt;int Value\u0026gt;\u0026quot; 和 reflect.Type 类似，reflect.Value 也满足 fmt.Stringer 接口，但是除非 Value 持有的是字符串，否则 String 方法只返回其类型。而使用 fmt 包的 %v 标志参数会对 reflect.Values 特殊处理。\n对 Value 调用 Type 方法将返回具体类型所对应的 reflect.Type：\nt := v.Type() // a reflect.Type fmt.Println(t.String()) // \u0026quot;int\u0026quot; reflect.ValueOf 的逆操作是 reflect.Value.Interface 方法。它返回一个 interface{} 类型，装载着与 reflect.Value 相同的具体值：\nv := reflect.ValueOf(3) // a reflect.Value x := v.Interface() // an interface{} i := x.(int) // an int fmt.Printf(\u0026quot;%d\\n\u0026quot;, i) // \u0026quot;3\u0026quot; reflect.Value 和 interface{} 都能装载任意的值。所不同的是，一个空的接口隐藏了值内部的表示方式和所有方法，因此只有我们知道具体的动态类型才能使用类型断言来访问内部的值（就像上面那样），内部值我们没法访问。相比之下，一个 Value 则有很多方法来检查其内容，无论它的具体类型是什么。让我们再次尝试实现我们的格式化函数 format.Any。\n我们使用 reflect.Value 的 Kind 方法来替代之前的类型 switch。虽然还是有无穷多的类型，但是它们的 kinds 类型却是有限的：Bool、String 和 所有数字类型的基础类型；Array 和 Struct 对应的聚合类型；Chan、Func、Ptr、Slice 和 Map 对应的引用类型；interface 类型；还有表示空值的 Invalid 类型。（空的 reflect.Value 的 kind 即为 Invalid。）\ngopl.io/ch12/format\npackage format import ( \u0026quot;reflect\u0026quot; \u0026quot;strconv\u0026quot; ) // Any formats any value as a string. func Any(value interface{}) string { return formatAtom(reflect.ValueOf(value)) } // formatAtom formats a value without inspecting its internal structure. func formatAtom(v reflect.Value) string { switch v.Kind() { case reflect.Invalid: return \u0026quot;invalid\u0026quot; case reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64: return strconv.FormatInt(v.Int(), 10) case reflect.Uint, reflect.Uint8, reflect.Uint16, reflect.Uint32, reflect.Uint64, reflect.Uintptr: return strconv.FormatUint(v.Uint(), 10) // ...floating-point and complex cases omitted for brevity... case reflect.Bool: return strconv.FormatBool(v.Bool()) case reflect.String: return strconv.Quote(v.String()) case reflect.Chan, reflect.Func, reflect.Ptr, reflect.Slice, reflect.Map: return v.Type().String() + \u0026quot; 0x\u0026quot; + strconv.FormatUint(uint64(v.Pointer()), 16) default: // reflect.Array, reflect.Struct, reflect.Interface return v.Type().String() + \u0026quot; value\u0026quot; } } 到目前为止，我们的函数将每个值视作一个不可分割没有内部结构的物品，因此它叫 formatAtom。对于聚合类型（结构体和数组）和接口，只是打印值的类型，对于引用类型（channels、functions、pointers、slices 和 maps），打印类型和十六进制的引用地址。虽然还不够理想，但是依然是一个重大的进步，并且 Kind 只关心底层表示，format.Any 也支持具名类型。例如：\nvar x int64 = 1 var d time.Duration = 1 * time.Nanosecond fmt.Println(format.Any(x)) // \u0026quot;1\u0026quot; fmt.Println(format.Any(d)) // \u0026quot;1\u0026quot; fmt.Println(format.Any([]int64{x})) // \u0026quot;[]int64 0x8202b87b0\u0026quot; fmt.Println(format.Any([]time.Duration{d})) // \u0026quot;[]time.Duration 0x8202b87e0\u0026quot; 通过reflect.Value修改值 到目前为止，反射还只是程序中变量的另一种读取方式。然而，在本节中我们将重点讨论如何通过反射机制来修改变量。\n回想一下，Go语言中类似x、x.f[1]和*p形式的表达式都可以表示变量，但是其它如x + 1和f(2)则不是变量。一个变量就是一个可寻址的内存空间，里面存储了一个值，并且存储的值可以通过内存地址来更新。\n对于reflect.Values也有类似的区别。有一些reflect.Values是可取地址的；其它一些则不可以。考虑以下的声明语句：\nx := 2 // value type variable? a := reflect.ValueOf(2) // 2 int no b := reflect.ValueOf(x) // 2 int no c := reflect.ValueOf(\u0026amp;x) // \u0026amp;x *int no d := c.Elem() // 2 int yes (x) 其中a对应的变量不可取地址。因为a中的值仅仅是整数2的拷贝副本。b中的值也同样不可取地址。c中的值还是不可取地址，它只是一个指针\u0026amp;x的拷贝。实际上，所有通过reflect.ValueOf(x)返回的reflect.Value都是不可取地址的。但是对于d，它是c的解引用方式生成的，指向另一个变量，因此是可取地址的。我们可以通过调用reflect.ValueOf(\u0026amp;x).Elem()，来获取任意变量x对应的可取地址的Value。\n我们可以通过调用reflect.Value的CanAddr方法来判断其是否可以被取地址：\nfmt.Println(a.CanAddr()) // \u0026quot;false\u0026quot; fmt.Println(b.CanAddr()) // \u0026quot;false\u0026quot; fmt.Println(c.CanAddr()) // \u0026quot;false\u0026quot; fmt.Println(d.CanAddr()) // \u0026quot;true\u0026quot; 每当我们通过指针间接地获取的reflect.Value都是可取地址的，即使开始的是一个不可取地址的Value。在反射机制中，所有关于是否支持取地址的规则都是类似的。例如，slice的索引表达式e[i]将隐式地包含一个指针，它就是可取地址的，即使开始的e表达式不支持也没有关系。以此类推，reflect.ValueOf(e).Index(i)对应的值也是可取地址的，即使原始的reflect.ValueOf(e)不支持也没有关系。\n要从变量对应的可取地址的reflect.Value来访问变量需要三个步骤。第一步是调用Addr()方法，它返回一个Value，里面保存了指向变量的指针。然后是在Value上调用Interface()方法，也就是返回一个interface{}，里面包含指向变量的指针。最后，如果我们知道变量的类型，我们可以使用类型的断言机制将得到的interface{}类型的接口强制转为普通的类型指针。这样我们就可以通过这个普通指针来更新变量了：\nx := 2 d := reflect.ValueOf(\u0026amp;x).Elem() // d refers to the variable x px := d.Addr().Interface().(*int) // px := \u0026amp;x *px = 3 // x = 3 fmt.Println(x) // \u0026quot;3\u0026quot; 或者，不使用指针，而是通过调用可取地址的reflect.Value的reflect.Value.Set方法来更新对应的值：\nd.Set(reflect.ValueOf(4)) fmt.Println(x) // \u0026quot;4\u0026quot; Set方法将在运行时执行和编译时进行类似的可赋值性约束的检查。以上代码，变量和值都是int类型，但是如果变量是int64类型，那么程序将抛出一个panic异常，所以关键问题是要确保改类型的变量可以接受对应的值：\nd.Set(reflect.ValueOf(int64(5))) // panic: int64 is not assignable to int 同样，对一个不可取地址的reflect.Value调用Set方法也会导致panic异常：\nx := 2 b := reflect.ValueOf(x) b.Set(reflect.ValueOf(3)) // panic: Set using unaddressable value 这里有很多用于基本数据类型的Set方法：SetInt、SetUint、SetString和SetFloat等。\nd := reflect.ValueOf(\u0026amp;x).Elem() d.SetInt(3) fmt.Println(x) // \u0026quot;3\u0026quot; 从某种程度上说，这些Set方法总是尽可能地完成任务。以SetInt为例，只要变量是某种类型的有符号整数就可以工作，即使是一些命名的类型、甚至只要底层数据类型是有符号整数就可以，而且如果对于变量类型值太大的话会被自动截断。但需要谨慎的是：对于一个引用interface{}类型的reflect.Value调用SetInt会导致panic异常，即使那个interface{}变量对于整数类型也不行。\nx := 1 rx := reflect.ValueOf(\u0026amp;x).Elem() rx.SetInt(2) // OK, x = 2 rx.Set(reflect.ValueOf(3)) // OK, x = 3 rx.SetString(\u0026quot;hello\u0026quot;) // panic: string is not assignable to int rx.Set(reflect.ValueOf(\u0026quot;hello\u0026quot;)) // panic: string is not assignable to int var y interface{} ry := reflect.ValueOf(\u0026amp;y).Elem() ry.SetInt(2) // panic: SetInt called on interface Value ry.Set(reflect.ValueOf(3)) // OK, y = int(3) ry.SetString(\u0026quot;hello\u0026quot;) // panic: SetString called on interface Value ry.Set(reflect.ValueOf(\u0026quot;hello\u0026quot;)) // OK, y = \u0026quot;hello\u0026quot; 当我们用Display显示os.Stdout结构时，我们发现反射可以越过Go语言的导出规则的限制读取结构体中未导出的成员，比如在类Unix系统上os.File结构体中的fd int成员。然而，利用反射机制并不能修改这些未导出的成员：\nstdout := reflect.ValueOf(os.Stdout).Elem() // *os.Stdout, an os.File var fmt.Println(stdout.Type()) // \u0026quot;os.File\u0026quot; fd := stdout.FieldByName(\u0026quot;fd\u0026quot;) fmt.Println(fd.Int()) // \u0026quot;1\u0026quot; fd.SetInt(2) // panic: unexported field 一个可取地址的reflect.Value会记录一个结构体成员是否是未导出成员，如果是的话则拒绝修改操作。因此，CanAddr方法并不能正确反映一个变量是否是可以被修改的。另一个相关的方法CanSet是用于检查对应的reflect.Value是否是可取地址并可被修改的：\nfmt.Println(fd.CanAddr(), fd.CanSet()) // \u0026quot;true false\u0026quot; 获取结构体字段标签 在4.5节我们使用构体成员标签用于设置对应JSON对应的名字。其中json成员标签让我们可以选择成员的名字和抑制零值成员的输出。在本节，我们将看到如何通过反射机制类获取成员标签。\n对于一个web服务，大部分HTTP处理函数要做的第一件事情就是展开请求中的参数到本地变量中。我们定义了一个工具函数，叫params.Unpack，通过使用结构体成员标签机制来让HTTP处理函数解析请求参数更方便。\n首先，我们看看如何使用它。下面的search函数是一个HTTP请求处理函数。它定义了一个匿名结构体类型的变量，用结构体的每个成员表示HTTP请求的参数。其中结构体成员标签指明了对于请求参数的名字，为了减少URL的长度这些参数名通常都是神秘的缩略词。Unpack将请求参数填充到合适的结构体成员中，这样我们可以方便地通过合适的类型类来访问这些参数。\ngopl.io/ch12/search\nimport \u0026quot;gopl.io/ch12/params\u0026quot; // search implements the /search URL endpoint. func search(resp http.ResponseWriter, req *http.Request) { var data struct { Labels []string `http:\u0026quot;l\u0026quot;` MaxResults int `http:\u0026quot;max\u0026quot;` Exact bool `http:\u0026quot;x\u0026quot;` } data.MaxResults = 10 // set default if err := params.Unpack(req, \u0026amp;data); err != nil { http.Error(resp, err.Error(), http.StatusBadRequest) // 400 return } // ...rest of handler... fmt.Fprintf(resp, \u0026quot;Search: %+v\\n\u0026quot;, data) } 下面的Unpack函数主要完成三件事情。第一，它调用req.ParseForm()来解析HTTP请求。然后，req.Form将包含所有的请求参数，不管HTTP客户端使用的是GET还是POST请求方法。\n下一步，Unpack函数将构建每个结构体成员有效参数名字到成员变量的映射。如果结构体成员有成员标签的话，有效参数名字可能和实际的成员名字不相同。reflect.Type的Field方法将返回一个reflect.StructField，里面含有每个成员的名字、类型和可选的成员标签等信息。其中成员标签信息对应reflect.StructTag类型的字符串，并且提供了Get方法用于解析和根据特定key提取的子串，例如这里的http:\u0026quot;\u0026hellip;\u0026ldquo;形式的子串。\ngopl.io/ch12/params\n// Unpack populates the fields of the struct pointed to by ptr // from the HTTP request parameters in req. func Unpack(req *http.Request, ptr interface{}) error { if err := req.ParseForm(); err != nil { return err } // Build map of fields keyed by effective name. fields := make(map[string]reflect.Value) v := reflect.ValueOf(ptr).Elem() // the struct variable for i := 0; i \u0026lt; v.NumField(); i++ { fieldInfo := v.Type().Field(i) // a reflect.StructField tag := fieldInfo.Tag // a reflect.StructTag name := tag.Get(\u0026quot;http\u0026quot;) if name == \u0026quot;\u0026quot; { name = strings.ToLower(fieldInfo.Name) } fields[name] = v.Field(i) } // Update struct field for each parameter in the request. for name, values := range req.Form { f := fields[name] if !f.IsValid() { continue // ignore unrecognized HTTP parameters } for _, value := range values { if f.Kind() == reflect.Slice { elem := reflect.New(f.Type().Elem()).Elem() if err := populate(elem, value); err != nil { return fmt.Errorf(\u0026quot;%s: %v\u0026quot;, name, err) } f.Set(reflect.Append(f, elem)) } else { if err := populate(f, value); err != nil { return fmt.Errorf(\u0026quot;%s: %v\u0026quot;, name, err) } } } } return nil } 最后，Unpack遍历HTTP请求的name/valu参数键值对，并且根据更新相应的结构体成员。回想一下，同一个名字的参数可能出现多次。如果发生这种情况，并且对应的结构体成员是一个slice，那么就将所有的参数添加到slice中。其它情况，对应的成员值将被覆盖，只有最后一次出现的参数值才是起作用的。\npopulate函数小心用请求的字符串类型参数值来填充单一的成员v（或者是slice类型成员中的单一的元素）。目前，它仅支持字符串、有符号整数和布尔型。其中其它的类型将留做练习任务。\nfunc populate(v reflect.Value, value string) error { switch v.Kind() { case reflect.String: v.SetString(value) case reflect.Int: i, err := strconv.ParseInt(value, 10, 64) if err != nil { return err } v.SetInt(i) case reflect.Bool: b, err := strconv.ParseBool(value) if err != nil { return err } v.SetBool(b) default: return fmt.Errorf(\u0026quot;unsupported kind %s\u0026quot;, v.Type()) } return nil } 如果我们上上面的处理程序添加到一个web服务器，则可以产生以下的会话：\n$ go build gopl.io/ch12/search $ ./search \u0026amp; $ ./fetch 'http://localhost:12345/search' Search: {Labels:[] MaxResults:10 Exact:false} $ ./fetch 'http://localhost:12345/search?l=golang\u0026amp;l=programming' Search: {Labels:[golang programming] MaxResults:10 Exact:false} $ ./fetch 'http://localhost:12345/search?l=golang\u0026amp;l=programming\u0026amp;max=100' Search: {Labels:[golang programming] MaxResults:100 Exact:false} $ ./fetch 'http://localhost:12345/search?x=true\u0026amp;l=golang\u0026amp;l=programming' Search: {Labels:[golang programming] MaxResults:10 Exact:true} $ ./fetch 'http://localhost:12345/search?q=hello\u0026amp;x=123' x: strconv.ParseBool: parsing \u0026quot;123\u0026quot;: invalid syntax $ ./fetch 'http://localhost:12345/search?q=hello\u0026amp;max=lots' max: strconv.ParseInt: parsing \u0026quot;lots\u0026quot;: invalid syntax 显示一个类型的方法集 我们的最后一个例子是使用reflect.Type来打印任意值的类型和枚举它的方法：\ngopl.io/ch12/methods\n// Print prints the method set of the value x. func Print(x interface{}) { v := reflect.ValueOf(x) t := v.Type() fmt.Printf(\u0026quot;type %s\\n\u0026quot;, t) for i := 0; i \u0026lt; v.NumMethod(); i++ { methType := v.Method(i).Type() fmt.Printf(\u0026quot;func (%s) %s%s\\n\u0026quot;, t, t.Method(i).Name, strings.TrimPrefix(methType.String(), \u0026quot;func\u0026quot;)) } } reflect.Type和reflect.Value都提供了一个Method方法。每次t.Method(i)调用将一个reflect.Method的实例，对应一个用于描述一个方法的名称和类型的结构体。每次v.Method(i)方法调用都返回一个reflect.Value以表示对应的值（§6.4），也就是一个方法是帮到它的接收者的。使用reflect.Value.Call方法（我们这里没有演示），将可以调用一个Func类型的Value，但是这个例子中只用到了它的类型。\n这是属于time.Duration和*strings.Replacer两个类型的方法：\nmethods.Print(time.Hour) // Output: // type time.Duration // func (time.Duration) Hours() float64 // func (time.Duration) Minutes() float64 // func (time.Duration) Nanoseconds() int64 // func (time.Duration) Seconds() float64 // func (time.Duration) String() string methods.Print(new(strings.Replacer)) // Output: // type *strings.Replacer // func (*strings.Replacer) Replace(string) string // func (*strings.Replacer) WriteString(io.Writer, string) (int, error) 文章转自：\n《go语言圣经》 https://www.liwenzhou.com/posts/Go/13_reflect ","date":"2019-07-12","permalink":"https://daemon365.dev/2019/07/12/golang-%E5%8F%8D%E5%B0%84/","tags":["go"],"title":"golang 反射"},{"content":"Gin框架介绍 基于httprouter开发的Web框架。 中文文档，齐全。 简单易用的轻量级框架。 Gin框架安装 go get -u github.com/gin-gonic/gin 实例:\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;github.com/gin-gonic/gin\u0026quot; ) func main() { r := gin.Default() // 创建一个默认的路由引擎 // 也可以用gin.New() gin.Default()多用了日志和panic的recover中间件 r.GET(\u0026quot;/helloworld\u0026quot;, func(c *gin.Context) { c.JSON(200, gin.H{ // c.JSON：返回JSON格式的数据 \u0026quot;msg\u0026quot;: \u0026quot;Hello world!\u0026quot;, }) }) err := r.Run(\u0026quot;127.0.0.1:8001\u0026quot;) // 启动HTTP服务，默认在127.0.0.1:8001启动服务 if err != nil { fmt.Println(\u0026quot;run gin field\u0026quot;) return } } RESTful API REST与技术无关，代表的是一种软件架构风格，REST是Representational State Transfer的简称，中文翻译为“表征状态转移”或“表现层状态转化”。\n简单来说，REST的含义就是客户端与Web服务器之间进行交互的时候，使用HTTP协议中的4个请求方法代表不同的动作。\nGET用来获取资源 POST用来新建资源 PUT用来更新资源 DELETE用来删除资源。 只要API程序遵循了REST风格，那就可以称其为RESTful API。目前在前后端分离的架构中，前后端基本都是通过RESTful API来进行交互。\n例如，我们现在要编写一个管理书籍的系统，我们可以查询对一本书进行查询、创建、更新和删除等操作，我们在编写程序的时候就要设计客户端浏览器与我们Web服务端交互的方式和路径。按照经验我们通常会设计成如下模式：\n请求方法 URL 含义 GET /book 查询书籍信息 POST /create_book 创建书籍记录 POST /update_book 更新书籍信息 POST /delete_book 删除书籍信息 同样的需求我们按照RESTful API设计如下：\n请求方法 URL 含义 GET /book 查询书籍信息 POST /book 创建书籍记录 PUT /book 更新书籍信息 DELETE /book 删除书籍信息 Gin框架支持开发RESTful API的开发。\nfunc main() { r := gin.Default() r.GET(\u0026quot;/book\u0026quot;, func(c *gin.Context) { c.JSON(200, gin.H{ \u0026quot;message\u0026quot;: \u0026quot;GET\u0026quot;, }) }) r.POST(\u0026quot;/book\u0026quot;, func(c *gin.Context) { c.JSON(200, gin.H{ \u0026quot;message\u0026quot;: \u0026quot;POST\u0026quot;, }) }) r.PUT(\u0026quot;/book\u0026quot;, func(c *gin.Context) { c.JSON(200, gin.H{ \u0026quot;message\u0026quot;: \u0026quot;PUT\u0026quot;, }) }) r.DELETE(\u0026quot;/book\u0026quot;, func(c *gin.Context) { c.JSON(200, gin.H{ \u0026quot;message\u0026quot;: \u0026quot;DELETE\u0026quot;, }) }) } 开发RESTful API的时候我们通常使用Postman来作为客户端的测试工具。\nGin渲染 HTML渲染 我们首先定义一个存放模板文件的templates文件夹，然后在其内部按照业务分别定义一个posts文件夹和一个users文件夹。posts/index.html文件的内容如下：\n{{define \u0026quot;posts/index.html\u0026quot;}} posts/index {{.title}} {{end}} users/index.html文件的内容如下：\n{{define \u0026quot;users/index.html\u0026quot;}} users/index {{.title}} {{end}} Gin框架中使用LoadHTMLGlob()或者LoadHTMLFiles()方法进行HTML模板渲染。\nfunc main() { r := gin.Default() r.LoadHTMLGlob(\u0026quot;templates/**/*\u0026quot;) //r.LoadHTMLFiles(\u0026quot;templates/posts/index.html\u0026quot;, \u0026quot;templates/users/index.html\u0026quot;) r.GET(\u0026quot;/posts/index\u0026quot;, func(c *gin.Context) { c.HTML(http.StatusOK, \u0026quot;posts/index.html\u0026quot;, gin.H{ \u0026quot;title\u0026quot;: \u0026quot;posts/index\u0026quot;, }) }) r.GET(\u0026quot;users/index\u0026quot;, func(c *gin.Context) { c.HTML(http.StatusOK, \u0026quot;users/index.html\u0026quot;, gin.H{ \u0026quot;title\u0026quot;: \u0026quot;users/index\u0026quot;, }) }) r.Run(\u0026quot;:8080\u0026quot;) } 静态文件处理 当我们渲染的HTML文件中引用了静态文件时，我们只需要按照以下方式在渲染页面前调用gin.Static方法即可。\nfunc main() { r := gin.Default() r.Static(\u0026quot;/static\u0026quot;, \u0026quot;./static\u0026quot;) r.LoadHTMLGlob(\u0026quot;templates/**/*\u0026quot;) ... r.Run(\u0026quot;:8080\u0026quot;) } 补充文件路径处理 关于模板文件和静态文件的路径，我们需要根据公司/项目的要求进行设置。可以使用下面的函数获取当前执行程序的路径。\nfunc getCurrentPath() string { if ex, err := os.Executable(); err == nil { return filepath.Dir(ex) } return \u0026quot;./\u0026quot; } JSON渲染 func main() { r := gin.Default() // gin.H 是map[string]interface{}的缩写 r.GET(\u0026quot;/someJSON\u0026quot;, func(c *gin.Context) { // 方式一：自己拼接JSON c.JSON(http.StatusOK, gin.H{\u0026quot;message\u0026quot;: \u0026quot;Hello world!\u0026quot;}) }) r.GET(\u0026quot;/moreJSON\u0026quot;, func(c *gin.Context) { // 方法二：使用结构体 var msg struct { Name string `json:\u0026quot;user\u0026quot;` Message string Age int } msg.Name = \u0026quot;zhy\u0026quot; msg.Message = \u0026quot;Hello world!\u0026quot; msg.Age = 18 c.JSON(http.StatusOK, msg) }) r.Run(\u0026quot;:8080\u0026quot;) } XML渲染 注意需要使用具名的结构体类型。\nfunc main() { r := gin.Default() // gin.H 是map[string]interface{}的缩写 r.GET(\u0026quot;/someXML\u0026quot;, func(c *gin.Context) { // 方式一：自己拼接JSON c.XML(http.StatusOK, gin.H{\u0026quot;message\u0026quot;: \u0026quot;Hello world!\u0026quot;}) }) r.GET(\u0026quot;/moreXML\u0026quot;, func(c *gin.Context) { // 方法二：使用结构体 type MessageRecord struct { Name string Message string Age int } var msg MessageRecord msg.Name = \u0026quot;小王子\u0026quot; msg.Message = \u0026quot;Hello world!\u0026quot; msg.Age = 18 c.XML(http.StatusOK, msg) }) r.Run(\u0026quot;:8080\u0026quot;) } YMAL渲染 r.GET(\u0026quot;/someYAML\u0026quot;, func(c *gin.Context) { c.YAML(http.StatusOK, gin.H{\u0026quot;message\u0026quot;: \u0026quot;ok\u0026quot;, \u0026quot;status\u0026quot;: http.StatusOK}) }) protobuf渲染 // protobuf文件 syntax = \u0026quot;proto3\u0026quot;; package models; message hello { string content = 1; } package main import ( \u0026quot;net/http\u0026quot; \u0026quot;test/models\u0026quot; \u0026quot;github.com/gin-gonic/gin\u0026quot; ) func main() { r := gin.Default() r.GET(\u0026quot;/hello\u0026quot;,func (c *gin.Context) { res := \u0026amp;models.Hello{ Content: \u0026quot;你好\u0026quot;, } c.ProtoBuf(http.StatusOK,res) }) _ = r.Run(\u0026quot;127.0.0.1:8001\u0026quot;) } 获取参数 获取querystring参数 querystring指的是URL中?后面携带的参数，例如：/user?username=赵海宇\u0026amp;address=地球一角。\nc.DefaultQuery有默认值 如果没有传去默认值 c.Query没有默认值 如果没传 为空 package main import ( \u0026quot;net/http\u0026quot; \u0026quot;github.com/gin-gonic/gin\u0026quot; ) func main() { r := gin.Default() r.GET(\u0026quot;/user\u0026quot;, func(c *gin.Context) { username := c.DefaultQuery(\u0026quot;username\u0026quot;, \u0026quot;zhy\u0026quot;) address := c.Query(\u0026quot;address\u0026quot;) c.JSON(http.StatusOK, gin.H{ \u0026quot;username\u0026quot;: username, \u0026quot;address\u0026quot;: address, }) }) _ = r.Run(\u0026quot;127.0.0.1:8001\u0026quot;) } 获取form参数 请求的数据通过form表单来提交，例如向/user发送一个POST请求，获取请求数据的方式如下：c.PostForm\npackage main import ( \u0026quot;net/http\u0026quot; \u0026quot;github.com/gin-gonic/gin\u0026quot; ) func main() { r := gin.Default() r.POST(\u0026quot;/user\u0026quot;, func(c *gin.Context) { username := c.PostForm(\u0026quot;username\u0026quot;) address := c.PostForm(\u0026quot;address\u0026quot;) c.JSON(http.StatusOK, gin.H{ \u0026quot;username\u0026quot;: username, \u0026quot;address\u0026quot;: address, }) }) _ = r.Run(\u0026quot;127.0.0.1:8001\u0026quot;) } 获取path参数 请求的参数通过URL路径传递，例如：/user/zhaohaiyu/地球一角/路由:/user/:username/:address方法:c.Param\npackage main import ( \u0026quot;net/http\u0026quot; \u0026quot;github.com/gin-gonic/gin\u0026quot; ) func main() { r := gin.Default() r.GET(\u0026quot;/user/:username/:address\u0026quot;, func(c *gin.Context) { username := c.Param(\u0026quot;username\u0026quot;) address := c.Param(\u0026quot;address\u0026quot;) c.JSON(http.StatusOK, gin.H{ \u0026quot;username\u0026quot;: username, \u0026quot;address\u0026quot;: address, }) }) _ = r.Run(\u0026quot;127.0.0.1:8001\u0026quot;) } 参数绑定 为了能够更方便的获取请求相关参数，提高开发效率，我们可以基于请求的content-type识别请求数据类型并利用反射机制自动提取请求中querystring、form表单、JSON、XML等参数到结构体中。\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;net/http\u0026quot; \u0026quot;github.com/gin-gonic/gin\u0026quot; ) // Binding from JSON type Login struct { User string `form:\u0026quot;user\u0026quot; json:\u0026quot;user\u0026quot; binding:\u0026quot;required\u0026quot;` Password string `form:\u0026quot;password\u0026quot; json:\u0026quot;password\u0026quot; binding:\u0026quot;required\u0026quot;` } func main() { r := gin.Default() // 绑定JSON的示例 ({\u0026quot;user\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;123\u0026quot;}) r.POST(\u0026quot;/loginJSON\u0026quot;, func(c *gin.Context) { var login Login if err := c.ShouldBindJSON(\u0026amp;login); err == nil { fmt.Printf(\u0026quot;login info:%#v\\n\u0026quot;, login) c.JSON(http.StatusOK, login) } else { c.JSON(http.StatusBadRequest, gin.H{\u0026quot;error\u0026quot;: err.Error()}) } }) // 绑定form表单示例 (user=root\u0026amp;password=123) r.POST(\u0026quot;/loginForm\u0026quot;, func(c *gin.Context) { var login Login // ShouldBind()会根据请求的Content-Type自行选择绑定器 if err := c.ShouldBind(\u0026amp;login); err == nil { c.JSON(http.StatusOK, login) } else { c.JSON(http.StatusBadRequest, gin.H{\u0026quot;error\u0026quot;: err.Error()}) } }) // 绑定querystring示例 (user=root\u0026amp;password=123) r.GET(\u0026quot;/loginForm\u0026quot;, func(c *gin.Context) { var login Login // ShouldBind()会根据请求的Content-Type自行选择绑定器 if err := c.ShouldBind(\u0026amp;login); err == nil { c.JSON(http.StatusOK, login) } else { c.JSON(http.StatusBadRequest, gin.H{\u0026quot;error\u0026quot;: err.Error()}) } }) _ = r.Run(\u0026quot;127.0.0.1:8001\u0026quot;) } 文件上传 func main() { router := gin.Default() // 处理multipart forms提交文件时默认的内存限制是32 MiB // 可以通过下面的方式修改 // router.MaxMultipartMemory = 8 \u0026lt;\u0026lt; 20 // 8 MiB router.POST(\u0026quot;/upload\u0026quot;, func(c *gin.Context) { // 单个文件 file, err := c.FormFile(\u0026quot;file\u0026quot;) if err != nil { c.JSON(http.StatusInternalServerError, gin.H{\u0026quot;message\u0026quot;: err.Error()}) return } log.Println(file.Filename) dst := fmt.Sprintf(\u0026quot;C:/tmp/%s\u0026quot;, file.Filename) // 上传文件到指定的目录 c.SaveUploadedFile(file, dst) c.JSON(http.StatusOK, gin.H{\u0026quot;message\u0026quot;: fmt.Sprintf(\u0026quot;'%s' uploaded!\u0026quot;, file.Filename)}) }) router.Run() } 多个文件上传 func main() { router := gin.Default() // 处理multipart forms提交文件时默认的内存限制是32 MiB // 可以通过下面的方式修改 // router.MaxMultipartMemory = 8 \u0026lt;\u0026lt; 20 // 8 MiB router.POST(\u0026quot;/upload\u0026quot;, func(c *gin.Context) { // Multipart form form, _ := c.MultipartForm() files := form.File[\u0026quot;file\u0026quot;] for index, file := range files { log.Println(file.Filename) dst := fmt.Sprintf(\u0026quot;C:/tmp/%s_%d\u0026quot;, file.Filename, index) // 上传文件到指定的目录 c.SaveUploadedFile(file, dst) } c.JSON(http.StatusOK, gin.H{ \u0026quot;message\u0026quot;: fmt.Sprintf(\u0026quot;%d files uploaded!\u0026quot;, len(files)), }) }) router.Run() } Gin中间件 Gin框架允许开发者在处理请求的过程中，加入用户自己的钩子（Hook）函数。这个钩子函数就叫中间件，中间件适合处理一些公共的业务逻辑，比如登录校验、日志打印、耗时统计等。\nGin中的中间件必须是一个gin.HandlerFunc类型。例如我们像下面的代码一样定义一个中间件。\n// StatCost 是一个统计耗时请求耗时的中间件 func StatCost() gin.HandlerFunc { return func(c *gin.Context) { start := time.Now() c.Set(\u0026quot;name\u0026quot;, \u0026quot;小王子\u0026quot;) // 执行其他中间件 c.Next() // 计算耗时 cost := time.S***art) log.Println(cost) } } 然后注册中间件的时候，可以在全局注册。\nfunc main() { // 新建一个没有任何默认中间件的路由 r := gin.New() // 注册一个全局中间件 r.Use(StatCost()) r.GET(\u0026quot;/test\u0026quot;, func(c *gin.Context) { name := c.MustGet(\u0026quot;name\u0026quot;).(string) log.Println(name) c.JSON(http.StatusOK, gin.H{ \u0026quot;message\u0026quot;: \u0026quot;Hello world!\u0026quot;, }) }) r.Run() } 也可以给某个路由单独注册中间件。\n// 给/test2路由单独注册中间件（可注册多个） r.GET(\u0026quot;/test2\u0026quot;, StatCost(), func(c *gin.Context) { name := c.MustGet(\u0026quot;name\u0026quot;).(string) log.Println(name) c.JSON(http.StatusOK, gin.H{ \u0026quot;message\u0026quot;: \u0026quot;Hello world!\u0026quot;, }) }) 重定向 HTTP重定向 HTTP 重定向很容易。 内部、外部重定向均支持。\nr.GET(\u0026quot;/test\u0026quot;, func(c *gin.Context) { c.Redirect(http.StatusMovedPermanently, \u0026quot;http://www.google.com/\u0026quot;) }) 路由重定向 路由重定向，使用HandleContext：\nr.GET(\u0026quot;/test\u0026quot;, func(c *gin.Context) { // 指定重定向的URL c.Request.URL.Path = \u0026quot;/test2\u0026quot; r.HandleContext(c) }) r.GET(\u0026quot;/test2\u0026quot;, func(c *gin.Context) { c.JSON(http.StatusOK, gin.H{\u0026quot;hello\u0026quot;: \u0026quot;world\u0026quot;}) }) Gin路由 普通路由 r.GET(\u0026quot;/index\u0026quot;, func(c *gin.Context) {...}) r.GET(\u0026quot;/login\u0026quot;, func(c *gin.Context) {...}) r.POST(\u0026quot;/login\u0026quot;, func(c *gin.Context) {...}) 此外，还有一个可以匹配所有请求方法的Any方法如下：\nr.Any(\u0026quot;/test\u0026quot;, func(c *gin.Context) {...}) 为没有配置处理函数的路由添加处理程序。默认情况下它返回404代码。\nr.NoRoute(func(c *gin.Context) { c.HTML(http.StatusNotFound, \u0026quot;views/404.html\u0026quot;, nil) }) 路由组 我们可以将拥有共同URL前缀的路由划分为一个路由组。\nfunc main() { r := gin.Default() userGroup := r.Group(\u0026quot;/user\u0026quot;) { userGroup.GET(\u0026quot;/index\u0026quot;, func(c *gin.Context) {...}) userGroup.GET(\u0026quot;/login\u0026quot;, func(c *gin.Context) {...}) userGroup.POST(\u0026quot;/login\u0026quot;, func(c *gin.Context) {...}) } shopGroup := r.Group(\u0026quot;/shop\u0026quot;) { shopGroup.GET(\u0026quot;/index\u0026quot;, func(c *gin.Context) {...}) shopGroup.GET(\u0026quot;/cart\u0026quot;, func(c *gin.Context) {...}) shopGroup.POST(\u0026quot;/checkout\u0026quot;, func(c *gin.Context) {...}) } r.Run() } 通常我们将路由分组用在划分业务逻辑或划分API版本时。\n路由分文件 在route中初始化route和切片路由组 在各个文件写路由 在main中把路由函数放入函数路由组 // route中go package route import \u0026quot;github.com/gin-gonic/gin\u0026quot; type Option func(*gin.Engine) var options = []Option{} // 路由函数组 // 注册app的路由配置 func Include(opts ...Option) { options = append(options, opts...) // 路由函数组添加函数 } // 初始化 func Init() *gin.Engine { r := gin.New() for _, function := range options { function(r) // 执行路由函数组中所有函数 } return r } // shop中的文件 package shop import \u0026quot;github.com/gin-gonic/gin\u0026quot; func Routers(e *gin.Engine) {// 路由函数 e.GET(\u0026quot;/post\u0026quot;, func(c *gin.Context) { c.String(200,\u0026quot;psot shop\u0026quot;) }) e.GET(\u0026quot;/comment\u0026quot;, func(c *gin.Context) { c.String(200,\u0026quot;comment shop\u0026quot;) }) } // main.go package main import ( \u0026quot;test/demo1/route\u0026quot; \u0026quot;test/demo1/shop\u0026quot; ) func main() { route.Include(shop.Routers) // 初始化路由 r := route.Init() _ = r.Run() } 路由原理 Gin框架中的路由使用的是httprouter这个库。\n其基本原理就是构造一个路由地址的前缀树。\n参考文章 https://www.liwenzhou.com/posts/Go/Gin_framework/ ","date":"2019-06-30","permalink":"https://daemon365.dev/2019/06/30/gin%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BD%BF%E7%94%A8/","tags":["go","gin"],"title":"Gin框架介绍及使用"},{"content":"包简介 任何包系统设计的目的都是为了简化大型程序的设计和维护工作，通过将一组相关的特性放进一个独立的单元以便于理解和更新，在每个单元更新的同时保持和程序中其它单元的相对独立性。这种模块化的特性允许每个包可以被其它的不同项目共享和重用，在项目范围内、甚至全球范围统一的分发和复用。\n每个包一般都定义了一个不同的名字空间用于它内部的每个标识符的访问。每个名字空间关联到一个特定的包，让我们给类型、函数等选择简短明了的名字，这样可以在使用它们的时候减少和其它部分名字的冲突。\n每个包还通过控制包内名字的可见性和是否导出来实现封装特性。通过限制包成员的可见性并隐藏包API的具体实现，将允许包的维护者在不影响外部包用户的前提下调整包的内部实现。通过限制包内变量的可见性，还可以强制用户通过某些特定函数来访问和更新内部变量，这样可以保证内部变量的一致性和并发时的互斥约束。\n包的导入 每个包是由一个全局唯一的字符串所标识的导入路径定位。出现在import语句中的导入路径也是字符串。\nimport ( \u0026quot;fmt\u0026quot; \u0026quot;math/rand\u0026quot; \u0026quot;encoding/json\u0026quot; \u0026quot;golang.org/x/net/html\u0026quot; \u0026quot;github.com/go-sql-driver/mysql\u0026quot; ) 包声明 在每个Go语言源文件的开头都必须有包声明语句。包声明语句的主要目的是确定当前包被其它包导入时默认的标识符（也称为包名）。\n通常来说，默认的包名就是包导入路径名的最后一段，因此即使两个包的导入路径不同，它们依然可能有一个相同的包名。例如，math/rand包和golang.org/x/exp/rand包的包名都是rand。\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;math/rand\u0026quot; ) // 导入rand包 用rand包里的方法函数等 func main() { fmt.Println(rand.Int63n(10000)) // 9410 } package main import ( \u0026quot;fmt\u0026quot; \u0026quot;golang.org/x/exp/rand\u0026quot; ) func main() { fmt.Println(rand.Int63n(10000)) // 9351 } 导入声明 如果我们想同时导入两个有着名字相同的包，例如math/rand包和golang.org/x/exp/rand包，那么导入声明必须至少为一个同名包指定一个新的包名以避免冲突。这叫做导入包的重命名。\npackage main import ( \u0026quot;fmt\u0026quot; rand1 \u0026quot;math/rand\u0026quot; rand2 \u0026quot;golang.org/x/exp/rand\u0026quot; ) func main() { fmt.Println(rand1.Int63n(10000)) // 9401 fmt.Println(rand2.Int63n(10000)) // 9351 } 包的匿名导入 如果只是导入一个包而并不使用导入的包将会导致一个编译错误。但是有时候我们只是想利用导入包而产生的副作用：它会计算包级变量的初始化表达式和执行导入包的init初始化函数。这时候我们需要抑制“unused import”编译错误，我们可以用下划线_来重命名导入的包。比如序号gorm包是要带入sql驱动\nimport ( \u0026quot;github.com/jinzhu/gorm\u0026quot; _ \u0026quot;github.com/jinzhu/gorm/dialects/mysql\u0026quot; ) func main() { db, err := gorm.Open(\u0026quot;mysql\u0026quot;, \u0026quot;user:password@/dbname?charset=utf8\u0026amp;parseTime=True\u0026amp;loc=Local\u0026quot;) defer db.Close() } 包和命名 当创建一个包，**一般要用短小的包名，但也不能太短导致难以理解。**标准库中最常用的包有bufio、bytes、flag、fmt、http、io、json、os、sort、sync和time等包。\n包名一般采用单数的形式。标准库的bytes、errors和strings使用了复数形式，这是为了避免和预定义的类型冲突，同样还有go/types是为了避免和type关键字冲突。\n工具 Go语言工具箱的具体功能，包括如何下载、格式化、构建、测试和安装Go语言编写的程序。\ngo命令 $ go ... build compile packages and dependencies clean remove object files doc show documentation for package or symbol env print Go environment information fmt run gofmt on package sources get download and install packages and dependencies install compile and install packages and dependencies list list packages run compile and run Go program test test packages version print Go version vet run go tool vet on packages Use \u0026quot;go help [command]\u0026quot; for more information about a command. ... 下载包 使用Go语言工具箱的go命令，不仅可以根据包导入路径找到本地工作区的包，甚至可以从互联网上找到和更新包。\n使用命令go get可以下载一个单一的包或者用\u0026hellip;下载整个子目录里面的每个包。Go语言工具箱的go命令同时计算并下载所依赖的每个包，这也是前一个例子中golang.org/x/net/html自动出现在本地工作区目录的原因。\n构建包 go build命令编译命令行参数指定的每个包。如果包是一个库，则忽略输出结果；这可以用于检测包是可以正确编译的。如果包的名字是main，go build将调用链接器在当前目录创建一个可执行程序；以导入路径的最后一段作为可执行程序的名字。\n包文档 Go语言的编码风格鼓励为每个包提供良好的文档。包中每个导出的成员和包声明前都应该包含目的和用法说明的注释。\nGo语言中的文档注释一般是完整的句子，第一行通常是摘要说明，以被注释者的名字开头。注释中函数的参数或其它的标识符并不需要额外的引号或其它标记注明。例如，下面是fmt.Fprintf的文档注释。\n// Fprintf formats according to a format specifier and writes to w. // It returns the number of bytes written and any write error encountered. func Fprintf(w io.Writer, format string, a ...interface{}) (int, error) 首先是go doc命令，该命令打印其后所指定的实体的声明与文档注释，该实体可能是一个包：\n$ go doc time package time // import \u0026quot;time\u0026quot; Package time provides functionality for measuring and displaying time. const Nanosecond Duration = 1 ... func After(d Duration) \u0026lt;-chan Time func Sleep(d Duration) func Since(t Time) Duration func Now() Time type Duration int64 type Time struct { ... } ...many more... 或者是某个具体的包成员：\n$ go doc time.Since func Since(t Time) Duration Since returns the time elapsed since t. It is shorthand for time.Now().Sub(t). 或者是一个方法：\n$ go doc time.Duration.Seconds func (d Duration) Seconds() float64 Seconds returns the duration as a floating-point number of seconds. 内部包 在Go语言程序中，包是最重要的封装机制。没有导出的标识符只在同一个包内部可以访问，而导出的标识符则是面向全宇宙都是可见的。\nnet/http net/http/internal/chunked net/http/httputil net/url 查询包 go list命令可以查询可用包的信息。其最简单的形式，可以测试包是否在工作区并打印它的导入路径：\n$ go list github.com/go-sql-driver/mysql github.com/go-sql-driver/mysql go list命令还可以获取每个包完整的元信息，而不仅仅只是导入路径，这些元信息可以以不同格式提供给用户。其中-json命令行参数表示用JSON格式打印每个包的元信息。可以用\u0026quot;\u0026hellip;\u0026ldquo;表示匹配相关包的包的导入路径。\n$ go list ...mysql github.com/astaxie/beego/session/mysql github.com/go-sql-driver/mysql 参考文章:\n《go语言圣经》 ","date":"2019-06-29","permalink":"https://daemon365.dev/2019/06/29/goalng%E5%8C%85%E5%92%8C%E5%91%BD%E4%BB%A4%E5%B7%A5%E5%85%B7/","tags":["go"],"title":"goalng包和命令工具"},{"content":"go test go test命令是一个按照一定的约定和组织来测试代码的程序。在包目录内，所有以_test.go为后缀名的源文件在执行go build时不会被构建成包的一部分，它们是go test测试的一部分。\ngo test命令会遍历所有的*_test.go文件中符合上述命名规则的函数，生成一个临时的main包用于调用相应的测试函数，接着构建并运行、报告测试结果，最后清理测试中生成的临时文件。\n在*_test.go文件中有三种类型的函数，单元测试函数、基准测试函数和示例函数。\n类型 格式 作用 测试函数 函数名前缀为Test 测试程序的一些逻辑行为是否正确 基准函数 函数名前缀为Benchmark 测试函数的性能 示例函数 函数名前缀为Example 为文档提供示例文档 测试函数 每个测试函数必须导入testing包。测试函数有如下的签名：\nfunc TestName(t *testing.T) { // ... } 测试函数的名字必须以Test开头，可选的后缀名必须以大写字母开头：\nfunc TestSin(t *testing.T) { /* ... */ } func TestCos(t *testing.T) { /* ... */ } func TestLog(t *testing.T) { /* ... */ } 其中参数t用于报告测试失败和附加的日志信息。testing.T的拥有的方法如下：\nfunc (c *T) Error(args ...interface{}) func (c *T) Errorf(format string, args ...interface{}) func (c *T) Fail() func (c *T) FailNow() func (c *T) Failed() bool func (c *T) Fatal(args ...interface{}) func (c *T) Fatalf(format string, args ...interface{}) func (c *T) Log(args ...interface{}) func (c *T) Logf(format string, args ...interface{}) func (c *T) Name() string func (t *T) Parallel() func (t *T) Run(name string, f func(t *T)) bool func (c *T) Skip(args ...interface{}) func (c *T) SkipNow() func (c *T) Skipf(format string, args ...interface{}) func (c *T) Skipped() bool 测试函数示例 就像细胞是构成我们身体的基本单位，一个软件程序也是由很多单元组件构成的。单元组件可以是函数、结构体、方法和最终用户可能依赖的任意东西。总之我们需要确保这些组件是能够正常运行的。单元测试是一些利用各种方法测试单元组件的程序，它会将结果与预期输出进行比较。\n接下来，我们定义一个split的包，包中定义了一个Split函数，具体实现如下：\n// split/split.go package split import \u0026quot;strings\u0026quot; // split package with a single split function. // Split slices s into all substrings separated by sep and // returns a slice of the substrings between those separators. func Split(s, sep string) (result []string) { i := strings.Index(s, sep) for i \u0026gt; -1 { result = append(result, s[:i]) s = s[i+1:] i = strings.Index(s, sep) } result = append(result, s) return } 在当前目录下，我们创建一个split_test.go的测试文件，并定义一个测试函数如下：\n// split/split_test.go package split import ( \u0026quot;reflect\u0026quot; \u0026quot;testing\u0026quot; ) func TestSplit(t *testing.T) { // 测试函数名必须以Test开头，必须接收一个*testing.T类型参数 got := Split(\u0026quot;a:b:c\u0026quot;, \u0026quot;:\u0026quot;) // 程序输出的结果 want := []string{\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;} // 期望的结果 if !reflect.DeepEqual(want, got) { // 因为slice不能比较直接，借助反射包中的方法比较 t.Errorf(\u0026quot;excepted:%v, got:%v\u0026quot;, want, got) // 测试失败输出错误提示 } } 此时split这个包中的文件如下：\nsplit $ ls -l total 16 -rw-r--r-- 1 zhaohaiyu test 408 4 29 15:50 split.go -rw-r--r-- 1 zhaohaiyu test 466 4 29 16:04 split_test.go 在split包路径下，执行go test命令，可以看到输出结果如下：\nsplit $ go test PASS ok test/split 0.005s 一个测试用例有点单薄，我们再编写一个测试使用多个字符切割字符串的例子，在split_test.go中添加如下测试函数：\nfunc TestMoreSplit(t *testing.T) { got := Split(\u0026quot;abcd\u0026quot;, \u0026quot;bc\u0026quot;) want := []string{\u0026quot;a\u0026quot;, \u0026quot;d\u0026quot;} if !reflect.DeepEqual(want, got) { t.Errorf(\u0026quot;excepted:%v, got:%v\u0026quot;, want, got) } } 再次运行go test命令，输出结果如下：\nsplit $ go test --- FAIL: TestMultiSplit (0.00s) split_test.go:20: excepted:[a d], got:[a cd] FAIL exit status 1 FAIL test/split 0.006s 这一次，我们的测试失败了。我们可以为go test命令添加-v参数，查看测试函数名称和运行时间：\nsplit $ go test -v === RUN TestSplit --- PASS: TestSplit (0.00s) === RUN TestMoreSplit --- FAIL: TestMoreSplit (0.00s) split_test.go:21: excepted:[a d], got:[a cd] FAIL exit status 1 FAIL test/split 0.005s 这一次我们能清楚的看到是TestMoreSplit这个测试没有成功。 还可以在go test命令后添加-run参数，它对应一个正则表达式，只有函数名匹配上的测试函数才会被go test命令执行。\nsplit $ go test -v -run=\u0026quot;More\u0026quot; === RUN TestMoreSplit --- FAIL: TestMoreSplit (0.00s) split_test.go:21: excepted:[a d], got:[a cd] FAIL exit status 1 FAIL test/split 0.006s 现在我们回过头来解决我们程序中的问题。很显然我们最初的split函数并没有考虑到sep为多个字符的情况，我们来修复下这个Bug：\npackage split import \u0026quot;strings\u0026quot; // split package with a single split function. // Split slices s into all substrings separated by sep and // returns a slice of the substrings between those separators. func Split(s, sep string) (result []string) { i := strings.Index(s, sep) for i \u0026gt; -1 { result = append(result, s[:i]) s = s[i+len(sep):] // 这里使用len(sep)获取sep的长度 i = strings.Index(s, sep) } result = append(result, s) return } 这一次我们再来测试一下，我们的程序。注意，当我们修改了我们的代码之后不要仅仅执行那些失败的测试函数，我们应该完整的运行所有的测试，保证不会因为修改代码而引入了新的问题。\nsplit $ go test -v === RUN TestSplit --- PASS: TestSplit (0.00s) === RUN TestMoreSplit --- PASS: TestMoreSplit (0.00s) PASS ok test/split 0.006s 这一次我们的测试都通过了。\n测试组 我们现在还想要测试一下split函数对中文字符串的支持，这个时候我们可以再编写一个TestChineseSplit测试函数，但是我们也可以使用如下更友好的一种方式来添加更多的测试用例。\nfunc TestSplit(t *testing.T) { // 定义一个测试用例类型 type test struct { input string sep string want []string } // 定义一个存储测试用例的切片 tests := []test{ {input: \u0026quot;a:b:c\u0026quot;, sep: \u0026quot;:\u0026quot;, want: []string{\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;}}, {input: \u0026quot;a:b:c\u0026quot;, sep: \u0026quot;,\u0026quot;, want: []string{\u0026quot;a:b:c\u0026quot;}}, {input: \u0026quot;abcd\u0026quot;, sep: \u0026quot;bc\u0026quot;, want: []string{\u0026quot;a\u0026quot;, \u0026quot;d\u0026quot;}}, {input: \u0026quot;沙河有沙又有河\u0026quot;, sep: \u0026quot;沙\u0026quot;, want: []string{\u0026quot;河有\u0026quot;, \u0026quot;又有河\u0026quot;}}, } // 遍历切片，逐一执行测试用例 for _, tc := range tests { got := Split(tc.input, tc.sep) if !reflect.DeepEqual(got, tc.want) { t.Errorf(\u0026quot;excepted:%v, got:%v\u0026quot;, tc.want, got) } } } 我们通过上面的代码把多个测试用例合到一起，再次执行go test命令。\nsplit $ go test -v === RUN TestSplit --- FAIL: TestSplit (0.00s) split_test.go:42: excepted:[河有 又有河], got:[ 河有 又有河] FAIL exit status 1 FAIL test/split 0.006s 我们的测试出现了问题，仔细看打印的测试失败提示信息：excepted:[河有 又有河], got:[ 河有 又有河]，你会发现[ 河有 又有河]中有个不明显的空串，这种情况下十分推荐使用%#v的格式化方式。\n我们修改下测试用例的格式化输出错误提示部分：\nfunc TestSplit(t *testing.T) { ... for _, tc := range tests { got := Split(tc.input, tc.sep) if !reflect.DeepEqual(got, tc.want) { t.Errorf(\u0026quot;excepted:%#v, got:%#v\u0026quot;, tc.want, got) } } } 此时运行go test命令后就能看到比较明显的提示信息了：\nsplit $ go test -v === RUN TestSplit --- FAIL: TestSplit (0.00s) split_test.go:42: excepted:[]string{\u0026quot;河有\u0026quot;, \u0026quot;又有河\u0026quot;}, got:[]string{\u0026quot;\u0026quot;, \u0026quot;河有\u0026quot;, \u0026quot;又有河\u0026quot;} FAIL exit status 1 FAIL test/split 0.006s 子测试 看起来都挺不错的，但是如果测试用例比较多的时候，我们是没办法一眼看出来具体是哪个测试用例失败了。我们可能会想到下面的解决办法：\nfunc TestSplit(t *testing.T) { type test struct { // 定义test结构体 input string sep string want []string } tests := map[string]test{ // 测试用例使用map存储 \u0026quot;simple\u0026quot;: {input: \u0026quot;a:b:c\u0026quot;, sep: \u0026quot;:\u0026quot;, want: []string{\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;}}, \u0026quot;wrong sep\u0026quot;: {input: \u0026quot;a:b:c\u0026quot;, sep: \u0026quot;,\u0026quot;, want: []string{\u0026quot;a:b:c\u0026quot;}}, \u0026quot;more sep\u0026quot;: {input: \u0026quot;abcd\u0026quot;, sep: \u0026quot;bc\u0026quot;, want: []string{\u0026quot;a\u0026quot;, \u0026quot;d\u0026quot;}}, \u0026quot;leading sep\u0026quot;: {input: \u0026quot;沙河有沙又有河\u0026quot;, sep: \u0026quot;沙\u0026quot;, want: []string{\u0026quot;河有\u0026quot;, \u0026quot;又有河\u0026quot;}}, } for name, tc := range tests { got := Split(tc.input, tc.sep) if !reflect.DeepEqual(got, tc.want) { t.Errorf(\u0026quot;name:%s excepted:%#v, got:%#v\u0026quot;, name, tc.want, got) // 将测试用例的name格式化输出 } } } 上面的做法是能够解决问题的。同时Go1.7+中新增了子测试，我们可以按照如下方式使用t.Run执行子测试：\nfunc TestSplit(t *testing.T) { type test struct { // 定义test结构体 input string sep string want []string } tests := map[string]test{ // 测试用例使用map存储 \u0026quot;simple\u0026quot;: {input: \u0026quot;a:b:c\u0026quot;, sep: \u0026quot;:\u0026quot;, want: []string{\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;}}, \u0026quot;wrong sep\u0026quot;: {input: \u0026quot;a:b:c\u0026quot;, sep: \u0026quot;,\u0026quot;, want: []string{\u0026quot;a:b:c\u0026quot;}}, \u0026quot;more sep\u0026quot;: {input: \u0026quot;abcd\u0026quot;, sep: \u0026quot;bc\u0026quot;, want: []string{\u0026quot;a\u0026quot;, \u0026quot;d\u0026quot;}}, \u0026quot;leading sep\u0026quot;: {input: \u0026quot;沙河有沙又有河\u0026quot;, sep: \u0026quot;沙\u0026quot;, want: []string{\u0026quot;河有\u0026quot;, \u0026quot;又有河\u0026quot;}}, } for name, tc := range tests { t.Run(name, func(t *testing.T) { // 使用t.Run()执行子测试 got := Split(tc.input, tc.sep) if !reflect.DeepEqual(got, tc.want) { t.Errorf(\u0026quot;excepted:%#v, got:%#v\u0026quot;, tc.want, got) } }) } } 此时我们再执行go test命令就能够看到更清晰的输出内容了：\nsplit $ go test -v === RUN TestSplit === RUN TestSplit/leading_sep === RUN TestSplit/simple === RUN TestSplit/wrong_sep === RUN TestSplit/more_sep --- FAIL: TestSplit (0.00s) --- FAIL: TestSplit/leading_sep (0.00s) split_test.go:83: excepted:[]string{\u0026quot;河有\u0026quot;, \u0026quot;又有河\u0026quot;}, got:[]string{\u0026quot;\u0026quot;, \u0026quot;河有\u0026quot;, \u0026quot;又有河\u0026quot;} --- PASS: TestSplit/simple (0.00s) --- PASS: TestSplit/wrong_sep (0.00s) --- PASS: TestSplit/more_sep (0.00s) FAIL exit status 1 FAIL test/split 0.006s 这个时候我们要把测试用例中的错误修改回来：\nfunc TestSplit(t *testing.T) { ... tests := map[string]test{ // 测试用例使用map存储 \u0026quot;simple\u0026quot;: {input: \u0026quot;a:b:c\u0026quot;, sep: \u0026quot;:\u0026quot;, want: []string{\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;}}, \u0026quot;wrong sep\u0026quot;: {input: \u0026quot;a:b:c\u0026quot;, sep: \u0026quot;,\u0026quot;, want: []string{\u0026quot;a:b:c\u0026quot;}}, \u0026quot;more sep\u0026quot;: {input: \u0026quot;abcd\u0026quot;, sep: \u0026quot;bc\u0026quot;, want: []string{\u0026quot;a\u0026quot;, \u0026quot;d\u0026quot;}}, \u0026quot;leading sep\u0026quot;: {input: \u0026quot;沙河有沙又有河\u0026quot;, sep: \u0026quot;沙\u0026quot;, want: []string{\u0026quot;\u0026quot;, \u0026quot;河有\u0026quot;, \u0026quot;又有河\u0026quot;}}, } ... } 我们都知道可以通过-run=RegExp来指定运行的测试用例，还可以通过/来指定要运行的子测试用例，例如：go test -v -run=Split/simple只会运行simple对应的子测试用例。\n测试覆盖率 测试覆盖率是你的代码被测试套件覆盖的百分比。通常我们使用的都是语句的覆盖率，也就是在测试中至少被运行一次的代码占总代码的比例。\nGo提供内置功能来检查你的代码覆盖率。我们可以使用go test -cover来查看测试覆盖率。例如：\nsplit $ go test -cover PASS coverage: 100.0% of statements ok test/split 0.005s 从上面的结果可以看到我们的测试用例覆盖了100%的代码。\nGo还提供了一个额外的-coverprofile参数，用来将覆盖率相关的记录信息输出到一个文件。例如：\nsplit $ go test -cover -coverprofile=c.out PASS coverage: 100.0% of statements ok test/split 0.005s 上面的命令会将覆盖率相关的信息输出到当前文件夹下面的c.out文件中，然后我们执行go tool cover -html=c.out，使用cover工具来处理生成的记录信息，该命令会打开本地的浏览器窗口生成一个HTML报告。 上图中每个用绿色标记的语句块表示被覆盖了，而红色的表示没有被覆盖。\n基准测试 基准测试函数格式 基准测试就是在一定的工作负载之下检测程序性能的一种方法。基准测试的基本格式如下：\nfunc BenchmarkName(b *testing.B){ // ... } 基准测试以Benchmark为前缀，需要一个*testing.B类型的参数b，基准测试必须要执行b.N次，这样的测试才有对照性，b.N的值是系统根据实际情况去调整的，从而保证测试的稳定性。testing.B拥有的方法如下：\nfunc (c *B) Error(args ...interface{}) func (c *B) Errorf(format string, args ...interface{}) func (c *B) Fail() func (c *B) FailNow() func (c *B) Failed() bool func (c *B) Fatal(args ...interface{}) func (c *B) Fatalf(format string, args ...interface{}) func (c *B) Log(args ...interface{}) func (c *B) Logf(format string, args ...interface{}) func (c *B) Name() string func (b *B) ReportAllocs() func (b *B) ResetTimer() func (b *B) Run(name string, f func(b *B)) bool func (b *B) RunParallel(body func(*PB)) func (b *B) SetBytes(n int64) func (b *B) SetParallelism(p int) func (c *B) Skip(args ...interface{}) func (c *B) SkipNow() func (c *B) Skipf(format string, args ...interface{}) func (c *B) Skipped() bool func (b *B) StartTimer() func (b *B) StopTimer() 基准测试示例 我们为split包中的Split函数编写基准测试如下：\nfunc BenchmarkSplit(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { Split(\u0026quot;沙河有沙又有河\u0026quot;, \u0026quot;沙\u0026quot;) } } 基准测试并不会默认执行，需要增加-bench参数，所以我们通过执行go test -bench=Split命令执行基准测试，输出结果如下：\nsplit $ go test -bench=Split goos: darwin goarch: amd64 pkg: test/split BenchmarkSplit-8 10000000 203 ns/op PASS ok test/split 2.255s 其中BenchmarkSplit-8表示对Split函数进行基准测试，数字8表示GOMAXPROCS的值，这个对于并发基准测试很重要。10000000和203ns/op表示每次调用Split函数耗时203ns，这个结果是10000000次调用的平均值。\n我们还可以为基准测试添加-benchmem参数，来获得内存分配的统计数据。\nsplit $ go test -bench=Split -benchmem goos: darwin goarch: amd64 pkg: test/split BenchmarkSplit-8 10000000 215 ns/op 112 B/op 3 allocs/op PASS ok test/split 2.394s 其中，112 B/op表示每次操作内存分配了112字节，3 allocs/op则表示每次操作进行了3次内存分配。 我们将我们的Split函数优化如下：\nfunc Split(s, sep string) (result []string) { result = make([]string, 0, strings.Count(s, sep)+1) i := strings.Index(s, sep) for i \u0026gt; -1 { result = append(result, s[:i]) s = s[i+len(sep):] // 这里使用len(sep)获取sep的长度 i = strings.Index(s, sep) } result = append(result, s) return } 这一次我们提前使用make函数将result初始化为一个容量足够大的切片，而不再像之前一样通过调用append函数来追加。我们来看一下这个改进会带来多大的性能提升：\nsplit $ go test -bench=Split -benchmem goos: darwin goarch: amd64 pkg: test/split BenchmarkSplit-8 10000000 127 ns/op 48 B/op 1 allocs/op PASS ok test/split 1.423s 这个使用make函数提前分配内存的改动，减少了2/3的内存分配次数，并且减少了一半的内存分配。\n性能比较函数 上面的基准测试只能得到给定操作的绝对耗时，但是在很多性能问题是发生在两个不同操作之间的相对耗时，比如同一个函数处理1000个元素的耗时与处理1万甚至100万个元素的耗时的差别是多少？再或者对于同一个任务究竟使用哪种算法性能最佳？我们通常需要对两个不同算法的实现使用相同的输入来进行基准比较测试。\n性能比较函数通常是一个带有参数的函数，被多个不同的Benchmark函数传入不同的值来调用。举个例子如下：\nfunc benchmark(b *testing.B, size int){/* ... */} func Benchmark10(b *testing.B){ benchmark(b, 10) } func Benchmark100(b *testing.B){ benchmark(b, 100) } func Benchmark1000(b *testing.B){ benchmark(b, 1000) } 例如我们编写了一个计算斐波那契数列的函数如下：\n// fib.go // Fib 是一个计算第n个斐波那契数的函数 func Fib(n int) int { if n \u0026lt; 2 { return n } return Fib(n-1) + Fib(n-2) } 我们编写的性能比较函数如下：\n// fib_test.go func benchmarkFib(b *testing.B, n int) { for i := 0; i \u0026lt; b.N; i++ { Fib(n) } } func BenchmarkFib1(b *testing.B) { benchmarkFib(b, 1) } func BenchmarkFib2(b *testing.B) { benchmarkFib(b, 2) } func BenchmarkFib3(b *testing.B) { benchmarkFib(b, 3) } func BenchmarkFib10(b *testing.B) { benchmarkFib(b, 10) } func BenchmarkFib20(b *testing.B) { benchmarkFib(b, 20) } func BenchmarkFib40(b *testing.B) { benchmarkFib(b, 40) } 运行基准测试：\nsplit $ go test -bench=. goos: darwin goarch: amd64 pkg: test/fib BenchmarkFib1-8 1000000000 2.03 ns/op BenchmarkFib2-8 300000000 5.39 ns/op BenchmarkFib3-8 200000000 9.71 ns/op BenchmarkFib10-8 5000000 325 ns/op BenchmarkFib20-8 30000 42460 ns/op BenchmarkFib40-8 2 638524980 ns/op PASS ok test/fib 12.944s 这里需要注意的是，默认情况下，每个基准测试至少运行1秒。如果在Benchmark函数返回时没有到1秒，则b.N的值会按1,2,5,10,20,50，…增加，并且函数再次运行。\n最终的BenchmarkFib40只运行了两次，每次运行的平均值只有不到一秒。像这种情况下我们应该可以使用-benchtime标志增加最小基准时间，以产生更准确的结果。例如：\nsplit $ go test -bench=Fib40 -benchtime=20s goos: darwin goarch: amd64 pkg: test/fib BenchmarkFib40-8 50 663205114 ns/op PASS ok test/fib 33.849s 这一次BenchmarkFib40函数运行了50次，结果就会更准确一些了。\n使用性能比较函数做测试的时候一个容易犯的错误就是把b.N作为输入的大小，例如以下两个例子都是错误的示范：\n// 错误示范1 func BenchmarkFibWrong(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { Fib(n) } } // 错误示范2 func BenchmarkFibWrong2(b *testing.B) { Fib(b.N) } 重置时间 b.ResetTimer之前的处理不会放到执行时间里，也不会输出到报告中，所以可以在之前做一些不计划作为测试报告的操作。例如：\nfunc BenchmarkSplit(b *testing.B) { time.Sleep(5 * time.Second) // 假设需要做一些耗时的无关操作 b.ResetTimer() // 重置计时器 for i := 0; i \u0026lt; b.N; i++ { Split(\u0026quot;沙河有沙又有河\u0026quot;, \u0026quot;沙\u0026quot;) } } 并行测试 func (b *B) RunParallel(body func(*PB))会以并行的方式执行给定的基准测试。\nRunParallel会创建出多个goroutine，并将b.N分配给这些goroutine执行， 其中goroutine数量的默认值为GOMAXPROCS。用户如果想要增加非CPU受限（non-CPU-bound）基准测试的并行性， 那么可以在RunParallel之前调用SetParallelism。RunParallel通常会与-cpu标志一同使用。\nfunc BenchmarkSplitParallel(b *testing.B) { // b.SetParallelism(1) // 设置使用的CPU数 b.RunParallel(func(pb *testing.PB) { for pb.Next() { Split(\u0026quot;沙河有沙又有河\u0026quot;, \u0026quot;沙\u0026quot;) } }) } 执行一下基准测试：\nsplit $ go test -bench=. goos: darwin goarch: amd64 pkg: test/split BenchmarkSplit-8 10000000 131 ns/op BenchmarkSplitParallel-8 50000000 36.1 ns/op PASS ok test/split 3.308s 还可以通过在测试命令后添加-cpu参数如go test -bench=. -cpu 1来指定使用的CPU数量。\nSetup与TearDown 测试程序有时需要在测试之前进行额外的设置（setup）或在测试之后进行拆卸（teardown）。\nTestMain 通过在*_test.go文件中定义TestMain函数来可以在测试之前进行额外的设置（setup）或在测试之后进行拆卸（teardown）操作。\n如果测试文件包含函数:func TestMain(m *testing.M)那么生成的测试会先调用 TestMain(m)，然后再运行具体测试。TestMain运行在主goroutine中, 可以在调用m.Run前后做任何设置（setup）和拆卸（teardown）。退出测试的时候应该使用m.Run的返回值作为参数调用os.Exit。\n一个使用TestMain来设置Setup和TearDown的示例如下：\nfunc TestMain(m *testing.M) { fmt.Println(\u0026quot;write setup code here...\u0026quot;) // 测试之前的做一些设置 // 如果 TestMain 使用了 flags，这里应该加上flag.Parse() retCode := m.Run() // 执行测试 fmt.Println(\u0026quot;write teardown code here...\u0026quot;) // 测试之后做一些拆卸工作 os.Exit(retCode) // 退出测试 } 需要注意的是：在调用TestMain时,flag.Parse并没有被调用。所以如果TestMain依赖于command-line标志 (包括 testing 包的标记), 则应该显示的调用flag.Parse。\n子测试的Setup与Teardown 有时候我们可能需要为每个测试集设置Setup与Teardown，也有可能需要为每个子测试设置Setup与Teardown。下面我们定义两个函数工具函数如下：\n// 测试集的Setup与Teardown func setupTestCase(t *testing.T) func(t *testing.T) { t.Log(\u0026quot;如有需要在此执行:测试之前的setup\u0026quot;) return func(t *testing.T) { t.Log(\u0026quot;如有需要在此执行:测试之后的teardown\u0026quot;) } } // 子测试的Setup与Teardown func setupSubTest(t *testing.T) func(t *testing.T) { t.Log(\u0026quot;如有需要在此执行:子测试之前的setup\u0026quot;) return func(t *testing.T) { t.Log(\u0026quot;如有需要在此执行:子测试之后的teardown\u0026quot;) } } 使用方式如下：\nfunc TestSplit(t *testing.T) { type test struct { // 定义test结构体 input string sep string want []string } tests := map[string]test{ // 测试用例使用map存储 \u0026quot;simple\u0026quot;: {input: \u0026quot;a:b:c\u0026quot;, sep: \u0026quot;:\u0026quot;, want: []string{\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;}}, \u0026quot;wrong sep\u0026quot;: {input: \u0026quot;a:b:c\u0026quot;, sep: \u0026quot;,\u0026quot;, want: []string{\u0026quot;a:b:c\u0026quot;}}, \u0026quot;more sep\u0026quot;: {input: \u0026quot;abcd\u0026quot;, sep: \u0026quot;bc\u0026quot;, want: []string{\u0026quot;a\u0026quot;, \u0026quot;d\u0026quot;}}, \u0026quot;leading sep\u0026quot;: {input: \u0026quot;沙河有沙又有河\u0026quot;, sep: \u0026quot;沙\u0026quot;, want: []string{\u0026quot;\u0026quot;, \u0026quot;河有\u0026quot;, \u0026quot;又有河\u0026quot;}}, } teardownTestCase := setupTestCase(t) // 测试之前执行setup操作 defer teardownTestCase(t) // 测试之后执行testdoen操作 for name, tc := range tests { t.Run(name, func(t *testing.T) { // 使用t.Run()执行子测试 teardownSubTest := setupSubTest(t) // 子测试之前执行setup操作 defer teardownSubTest(t) // 测试之后执行testdoen操作 got := Split(tc.input, tc.sep) if !reflect.DeepEqual(got, tc.want) { t.Errorf(\u0026quot;excepted:%#v, got:%#v\u0026quot;, tc.want, got) } }) } } 测试结果如下：\nsplit $ go test -v === RUN TestSplit === RUN TestSplit/simple === RUN TestSplit/wrong_sep === RUN TestSplit/more_sep === RUN TestSplit/leading_sep --- PASS: TestSplit (0.00s) split_test.go:71: 如有需要在此执行:测试之前的setup --- PASS: TestSplit/simple (0.00s) split_test.go:79: 如有需要在此执行:子测试之前的setup split_test.go:81: 如有需要在此执行:子测试之后的teardown --- PASS: TestSplit/wrong_sep (0.00s) split_test.go:79: 如有需要在此执行:子测试之前的setup split_test.go:81: 如有需要在此执行:子测试之后的teardown --- PASS: TestSplit/more_sep (0.00s) split_test.go:79: 如有需要在此执行:子测试之前的setup split_test.go:81: 如有需要在此执行:子测试之后的teardown --- PASS: TestSplit/leading_sep (0.00s) split_test.go:79: 如有需要在此执行:子测试之前的setup split_test.go:81: 如有需要在此执行:子测试之后的teardown split_test.go:73: 如有需要在此执行:测试之后的teardown === RUN ExampleSplit --- PASS: ExampleSplit (0.00s) PASS ok test/split 0.006s 示例函数 示例函数的格式 被go test特殊对待的第三种函数就是示例函数，它们的函数名以Example为前缀。它们既没有参数也没有返回值。标准格式如下：\nfunc ExampleName() { // ... } 示例函数示例 下面的代码是我们为Split函数编写的一个示例函数：\nfunc ExampleSplit() { fmt.Println(split.Split(\u0026quot;a:b:c\u0026quot;, \u0026quot;:\u0026quot;)) fmt.Println(split.Split(\u0026quot;沙河有沙又有河\u0026quot;, \u0026quot;沙\u0026quot;)) // Output: // [a b c] // [ 河有 又有河] } 为你的代码编写示例代码有如下三个用处：\n示例函数能够作为文档直接使用，例如基于web的godoc中能把示例函数与对应的函数或包相关联。\n示例函数只要包含了// Output:也是可以通过go test运行的可执行测试。\nsplit $ go test -run Example PASS ok test/split 0.006s 示例函数提供了可以直接运行的示例代码，可以直接在golang.org的godoc文档服务器上使用Go Playground运行示例代码。下图为strings.ToUpper函数在Playground的示例函数效果。![Go Playground] 参考文章:\n《go语言圣经》 https://www.liwenzhou.com/posts/Go/16_test/ ","date":"2019-06-29","permalink":"https://daemon365.dev/2019/06/29/golang-%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/","tags":["go"],"title":"golang 单元测试"},{"content":"go context标准库 context包在Go1.7版本时加入到标准库中。其设计目标是给Golang提供一个标准接口来给其他任务发送取消信号和传递数据。其具体作用为：\n可以通过context发送取消信号。\n可以指定截止时间（Deadline)，context在截止时间到期后自动发送取消信号。\n可以通过context传输一些数据。\ncontext在Golang中最主要的用途是控制协程任务的取消，但是context除了协程以外也可以用在线程控制等非协程的情况。\n基本概念 context的核心是其定义的Context接口类型。围绕着Context接口类型存在两种角色：\n父任务：创建Context，将Context对象传递给子任务，并且根据需要发送取消信号来结束子任务。\n子任务：使用Context类型对象，当收到父任务发来的取消信号，结束当前任务并退出。\n接下来我们从这两个角色的视角分别看一下Context对象。\ncontext接口 type Context interface { Deadline() (deadline time.Time, ok bool) Done() chan struct{} Err() error Value(key interface{}) interface{} } Deadline方法需要返回当前Context被取消的时间，也就是完成工作的截止时间（deadline）；\nDone方法需要返回一个Channel，这个Channel会在当前工作完成或者上下文被取消之后关闭，多次调用Done方***返回同一个Channel；\nErr方***返回当前Context结束的原因，它只会在Done返回的Channel被关闭时才会返回非空的值；\n如果当前Context被取消就会返回Canceled错误；\n如果当前Context超时就会返回DeadlineExceeded错误；\nValue方***从Context中返回键对应的值，对于同一个上下文来说，多次调用Value 并传入相同的Key会返回相同的结果，该方法仅用于传递跨API和进程间跟请求域的数据；\nBackground()和TODO() Go内置两个函数：Background()和TODO()，这两个函数分别返回一个实现了Context接口的background和todo。我们代码中最开始都是以这两个内置的上下文对象作为最顶层的partent context，衍生出更多的子上下文对象。\nBackground()主要用于main函数、初始化以及测试代码中，作为Context这个树结构的最顶层的Context，也就是根Context。\nTODO()，它目前还不知道具体的使用场景，如果我们不知道该使用什么Context的时候，可以使用这个。\nbackground和todo本质上都是emptyCtx结构体类型，是一个不可取消，没有设置截止时间，没有携带任何值的Context。\nWith函数 WithCancel函数，传递一个父Context作为参数，返回子Context，以及一个取消函数用来取消Context\nWithDeadline函数，和WithCancel差不多，它会多传递一个截止时间参数，意味着到了这个时间点，会自动取消Context，当然我们也可以不等到这个时候，可以提前通过取消函数进行取消\nWithTimeout和WithDeadline基本上一样，这个表示是超时自动取消，是多少时间后自动取消Context的意思\nWithValue函数和取消Context无关，它是为了生成一个绑定了一个键值对数据的Context，这个绑定的数据可以通过Context.Value方法访问到\nContext使用原则 不要把Context放在结构体中，要以参数的方式进行传递\n以Context作为参数的函数方法，应该把Context作为第一个参数，放在第一位]\n给一个函数方法传递Context的时候，不要传递nil，如果不知道传递什么，就使用context.TODO\nContext的Value相关方法应该传递必须的数据，不要什么数据都使用这个传递\npackage main import ( \u0026quot;context\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;time\u0026quot; ) func main() { server1() time.Sleep(time.Second) } func server1() { ctx, cancel := context.WithTimeout(context.Background(),time.Millisecond * 30 )// 30毫秒后过期 defer cancel() ctx = context.WithValue(ctx, \u0026quot;name\u0026quot;, \u0026quot;zhaohaiyu\u0026quot;) for i:=0;i\u0026lt;100;i++{ time.Sleep(time.Millisecond * 10) go server2(ctx) } } func server2(ctx context.Context) { select { case ctx.Done(): fmt.Println(\u0026quot;打断执行\u0026quot;) return default: name,ok := ctx.Value(\u0026quot;name\u0026quot;).(string) if !ok{ fmt.Println(\u0026quot;没有取到\u0026quot;) return } fmt.Println(\u0026quot;name=\u0026quot;,name) } } 请求作用域上下文 在 Go 服务中，每个传入的请求都在其自己的goroutine 中处理。请求处理程序通常启动额外的 goroutine 来访问其他后端，如数据库和 RPC服务。处理请求的 goroutine 通常需要访问特定于请求(request-specific context)的值，例如最终用户的身份、授权令牌和请求的截止日期(deadline)。当一个请求被取消或超时时，处理该请求的所有 goroutine 都应该快速退出(fail fast)，这样系统就可以回收它们正在使用的任何资源。 Go 1.7 引入一个 context 包，它使得跨 API 边界的请求范围元数据、取消信号和截止日期很容易传递给处理请求所涉及的所有 goroutine(显示传递)。\n其他语言: Thread Local Storage(TLS)，XXXContext\ntype Context interface { // Deadline returns the time when work done on behalf of this context // should be canceled. Deadline returns ok==false when no deadline is // set. Successive calls to Deadline return the same results. Deadline() (deadline time.Time, ok bool) // Done returns a channel that's closed when work done on behalf of this // context should be canceled. Done may return nil if this context can // never be canceled. Successive calls to Done return the same value. // The close of the Done channel may happen asynchronously, // after the cancel function returns. // // WithCancel arranges for Done to be closed when cancel is called; // WithDeadline arranges for Done to be closed when the deadline // expires; WithTimeout arranges for Done to be closed when the timeout // elapses. // // Done is provided for use in select statements: // // // Stream generates values with DoSomething and sends them to out // // until DoSomething returns an error or ctx.Done is closed. // func Stream(ctx context.Context, out chan\u0026lt;- Value) error { // for { // v, err := DoSomething(ctx) // if err != nil { // return err // } // select { // case \u0026lt;-ctx.Done(): // return ctx.Err() // case out \u0026lt;- v: // } // } // } // // See https://blog.golang.org/pipelines for more examples of how to use // a Done channel for cancellation. Done() \u0026lt;-chan struct{} // If Done is not yet closed, Err returns nil. // If Done is closed, Err returns a non-nil error explaining why: // Canceled if the context was canceled // or DeadlineExceeded if the context's deadline passed. // After Err returns a non-nil error, successive calls to Err return the same error. Err() error // Value returns the value associated with this context for key, or nil // if no value is associated with key. Successive calls to Value with // the same key returns the same result. // // Use context values only for request-scoped data that transits // processes and API boundaries, not for passing optional parameters to // functions. // // A key identifies a specific value in a Context. Functions that wish // to store values in Context typically allocate a key in a global // variable then use that key as the argument to context.WithValue and // Context.Value. A key can be any type that supports equality; // packages should define keys as an unexported type to avoid // collisions. // // Packages that define a Context key should provide type-safe accessors // for the values stored using that key: // // // Package user defines a User type that's stored in Contexts. // package user // // import \u0026quot;context\u0026quot; // // // User is the type of value stored in the Contexts. // type User struct {...} // // // key is an unexported type for keys defined in this package. // // This prevents collisions with keys defined in other packages. // type key int // // // userKey is the key for user.User values in Contexts. It is // // unexported; clients use user.NewContext and user.FromContext // // instead of using this key directly. // var userKey key // // // NewContext returns a new Context that carries value u. // func NewContext(ctx context.Context, u *User) context.Context { // return context.WithValue(ctx, userKey, u) // } // // // FromContext returns the User value stored in ctx, if any. // func FromContext(ctx context.Context) (*User, bool) { // u, ok := ctx.Value(userKey).(*User) // return u, ok // } Value(key interface{}) interface{} } func IsAdminUser(ctx context.Context) { x := token.GetToken(ctx) userObject := auth.AuthenticateToken(x) return userObject.IsAdmin || userObject.IsRoot() } 如何将 context 集成到 API 中？ 在将 context 集成到 API 中时，要记住的最重要的一点是，它的作用域是请求级别\t的。例如，沿单个数据库查询存在是有意义的，但沿数据库对象存在则没有意义。 目前有两种方法可以将 context 对象集成到 API 中：\nThe first parameter of a function call\n​\t首参数传递 context 对象，比如，参考 net 包 Dialer.DialContext。此函数执行正常的 Dial 操作，但可以通过 context 对象取消函数调用。\nfunc (d *Dialer) DialContext(ctx context.Context,network,address string) (Conn,error)\nOptional config on a request structure 在第一个 request 对象中携带一个可选的 context 对象。例如 net/http 库的 Request.WithContext，通过携带给定的 context 对象，返回一个新的 Request 对象。\nfunc (r *Request) WithContext(ctx context.Context) *Request\n不要在stuct类型中存储上下文 不要在结构题类型中存储上下文；相反，应该显式地将上下文传递给每个需要它的函数。上下文应该是第一个参数，通常命名为ctx:\nfunc DoSomething(ctx context.Context,arg Arg) error { // .. use ctx } 对服务器的传入请求应创建上下文。 使用 context 的一个很好的心智模型是它应该在程序中流动，应该贯穿你的代码。这通常意味着您不希望将其存储在结构体之中。它从一个函数传递到另一个函数，并根据需要进行扩展。理想情况下，每个请求都会创建一个 context 对象，并在请求结束时过期。 不存储上下文的一个例外是，当您需要将它放入一个结构中时，该结构纯粹用作通过通道传递的消息。如下例所示。\ntype message struct { responseChan chan\u0026lt;- int parameter string ctx context.Context } context.WithValue 比如我们新建了一个基于 context.Background() 的 ctx1，携带了一个 map 的数据，map 中包含了 “k1”: “v1” 的一个键值对，ctx1 被两个 goroutine 同时使用作为函数签名传入，如果我们修改了 这个map，会导致另外进行读 context.Value 的 goroutine 和修改 map 的 goroutine，在 map 对象上产生 data race。因此我们要使用 copy-on-write 的思路，解决跨多个 goroutine 使用数据、修改数据的场景。\ncontext.WithValue 内部基于 valueCtx 实现:\ntype valueCtx struct { Context key,val interface{} } 为了实现不断的 WithValue，构建新的 context，内部在查找 key 时候，使用递归方式不断从当前，从父节点寻找匹配的 key，直到 root context(Backgrond 和 TODO Value 函数会返回 nil)。\nfunc (c *valueCtx) Value(key interface{}( interface{} { if c.key == key { return c.val } return c.Context.Value(key) } 调试或跟踪数据在上下文中传递是安全的 context.WithValue 方法允许上下文携带请求范围的数据。这些数据必须是安全的，以便多个 goroutine 同时使用。这里的数据，更多是面向请求的元数据，不应该作为函数的可选参数来使用(比如 context 里面挂了一个sql.Tx 对象，传递到 Dao 层使用)，因为元数据相对函数参数更加是隐含的，面向请求的。而参数是更加显示的。\n同一个 context 对象可以传递给在不同 goroutine 中运行的函数；上下文对于多个 goroutine 同时使用是安全的。对于值类型最容易犯错的地方，在于 context value 应该是 immutable 的，每次重新赋值应该是新的 context，即: context.WithValue(ctx, oldvalue) https://pkg.go.dev/google.golang.org/grpc/metadata Context.Value should inform, not control\nfunc WithValue(parent Context,key val interface{}) Context { if parent == nil { panic(\u0026quot;connot creat context from nil parent\u0026quot;) } if key == nil { panic(\u0026quot;nil key\u0026quot;) } if !reflectlite.Typeof(key).Comparable() { panic(\u0026quot;key is not comparable\u0026quot;) } return \u0026amp;valueCtx{parent,key,val} } type valueCtx struct { Context key,val interface{} } 用于传递作用域的参数的可选api和用于传递作用域的参数的可选api。比如 染色，API 重要性，Trace\n比如我们新建了一个基于 context.Background() 的 ctx1，携带了一个 map 的数据，map 中包含了 “k1”: “v1” 的一个键值对，ctx1 被两个 goroutine 同时使用作为函数签名传入，如果我们修改了 这个map，会导致另外进行读 context.Value 的 goroutine 和修改 map 的 goroutine，在 map 对象上产生 data race。因此我们要使用 copy-on-write 的思路，解决跨多个 goroutine 使用数据、修改数据的场景。\nCOW: 从 ctx1 中获取 map1(可以理解为 v1 版本的 map 数据)。构建一个新的 map 对象 map2，复制所有 map1 数据，同时追加新的数据 “k2”: “v2” 键值对，使用 context.WithValue 创建新的 ctx2，ctx2 会传递到其他的 goroutine 中。这样各自读取的副本都是自己的数据，写行为追加的数据，在 ctx2 中也能完整读取到，同时也不会污染 ctx1 中的数据。\n当上下文被取消时，从它派生的所有上下文也被取消\n当一个 context 被取消时，从它派生的所有 context 也将被取消。WithCancel(ctx) 参数 ctx 认为是 parent ctx，在内部会进行一个传播关系链的关联。Done() 返回 一个 chan，当我们取消某个parent context, 实际上上会递归层层 cancel 掉自己的 child context 的 done chan 从而让整个调用链中所有监听 cancel 的 goroutine退出。\npackage main import ( \u0026quot;context\u0026quot; \u0026quot;fmt\u0026quot; ) func main() { gen := func(ctx context.Context) \u0026lt;-chan int { dst := make(chan int) n := 1 go func(){ for { select { case \u0026lt;- ctx.Done(): return case dst \u0026lt;- n: n++ } } }() } return dst ctx,cancel := context.WithCancel(context.Backgroup()) for n := range gen(ctx) { fmt.Println(n) if n == 5 { break } } 所有阻塞/长操作都应可取消\n如果要实现一个超时控制，通过上面的context 的parent/child 机制，其实我们只需要启动一个定时器，然后在超时的时候，直接将当前的 context 给 cancel 掉，就可以实现监听在当前和下层的额context.Done() 的 goroutine 的退出。\npackage main import ( \u0026quot;context\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;fmt\u0026quot; ) const shortDuration = time.Millisecond func main() { d := time.Now().Add(shortDuration) ctx,cancel := context.WithDeadline(context.Backgroup(),d) defer canel() select { case \u0026lt;-time.After(time.Second): fmt.Println(\u0026quot;overslept\u0026quot;) case \u0026lt;- ctx.Done(): fmt.Println(ctx.Err()) } } ","date":"2019-06-28","permalink":"https://daemon365.dev/2019/06/28/golang-context%E5%8C%85/","tags":["go"],"title":"golang context包"},{"content":"什么是channel channels 是一种类型安全的消息队列，充当两个 goroutine 之间的管道，将通过它同步的进行任意资源的交换。chan 控制 goroutines 交互的能力从而创建了 Go 同步机制。当创建的 chan 没有容量时，称为无缓冲通道。反过来，使用容量创建的 chan 称为缓冲通道。\n要了解通过 chan 交互的 goroutine 的同步行为是什么，我们需要知道通道的类型和状态。根据我们使用的是无缓冲通道还是缓冲通道，场景会有所不同，所以让我们单独讨论每个场景。\n无缓冲管道 make ： ch := make(chan struct{}) 无缓冲 chan 没有容量，因此进行任何交换前需要两个 goroutine 同时准备好。当 goroutine 试图将一个资源发送到一个无缓冲的通道并且没有goroutine 等待接收该资源时，该通道将锁住发送 goroutine 并使其等待。当 goroutine 尝试从无缓冲通道接收，并且没有 goroutine 等待发送资源时，该通道将锁住接收 goroutine 并使其等待。\n无缓冲信道的本质是保证同步。\n无缓冲channel在消息发送时需要接收者就绪。声明无缓冲channel的方式是不指定缓冲大小。以下是一个列子：\npackage main import ( \u0026quot;sync\u0026quot; \u0026quot;time\u0026quot; ) func main() { c := make(chan string) var wg sync.WaitGroup wg.Add(2) go func() { defer wg.Done() c \u0026lt;- `foo` }() go func() { defer wg.Done() time.Sleep(time.Second * 1) println(`Message: `+ \u0026lt;-c) }() wg.Wait() } 第一个协程会在发送消息foo时阻塞，原因是接收者还没有就绪：这个特性在标准文档中描述如下：\n如果缓冲大小设置为0或者不设置，channel为无缓冲类型，通信成功的前提是发送者和接收者都处于就绪状态。\neffective Go文档也有相应的描述：\n无缓冲channel，发送者会阻塞直到接收者接收了发送的值。\n为了更好的理解channel的特性，接下来我们分析channel的内部结构。\npackage main import ( \u0026quot;sync\u0026quot; \u0026quot;time\u0026quot; \u0026quot;fmt\u0026quot; ) func main() { c := make(chan string) var wg sync.WaitGroup wg.Add(2) go func() { defer wg.Done() c \u0026lt;- \u0026quot;foo\u0026quot; }() go func() { defer wg.Done() time.Sleep(time.Second) fmt.Println(\u0026quot;message:\u0026quot; + \u0026lt;- c) }() wg.Wait() } /* foo */ 有缓冲管道 buffered channel 具有容量，因此其行为可能有点不同。当 goroutine 试图将资源发送到缓冲通道，而该通道已满时，该通道将锁住 goroutine并使其等待缓冲区可用。如果通道中有空间，发送可以立即进行，goroutine 可以继续。当goroutine 试图从缓冲通道接收数据，而缓冲通道为空时，该通道将锁住 goroutine 并使其等待资源被发送。\npackage main import ( \u0026quot;sync\u0026quot; \u0026quot;time\u0026quot; \u0026quot;fmt\u0026quot; ) func main() { c := make(chan string,2) var wg sync.WaitGroup wg.Add(2) go func(){ defer wg.Done() c \u0026lt;- \u0026quot;foo\u0026quot; c \u0026lt;- \u0026quot;bar\u0026quot; }() go func(){ defer wg.Done() time.Sleep(time.Second) fmt.Println(\u0026quot;mesage:\u0026quot; + \u0026lt;- c) fmt.Println(\u0026quot;message:\u0026quot; + \u0026lt;- c) }() wg.Wait() } /* mesage:foo message:bar */ 追踪耗时 通过Go工具trace中的synchronization blocking profile来查看测试程序被同步原语阻塞所消耗的时间。接收时的耗时对比：无缓冲channel为9毫秒，缓冲大小为50的channel为1.9毫秒。\n发送时的耗时对比：有缓冲channel将耗时缩小了五倍。\nSend 先于 Receive 发生。 好处: 延迟更小。 代价: 不保证数据到达，越大的 buffer，越小的保障到达。buffer = 1 时，给你延迟一个消息的保障。 参考文章 https://www.it1352.com/807929.html https://www.pengrl.com/p/21027/ ","date":"2019-06-26","permalink":"https://daemon365.dev/2019/06/26/golang-channel/","tags":["go"],"title":"golang channel"},{"content":"goroutine goroutine是Go并行设计的核心。goroutine说到底其实就是线程，但是它比线程更小，十几个goroutine可能体现在底层就是五六个线程，Go语言内部帮你实现了这些goroutine之间的内存共享。执行goroutine只需极少的栈内存(大概是4~5KB)，当然会根据相应的数据伸缩。也正因为如此，可同时运行成千上万个并发任务。goroutine比thread更易用、更高效、更轻便。\ngoroutine是通过Go的runtime管理的一个线程管理器。goroutine通过go关键字实现了，其实就是一个普通的函数。\ngo hello(a, b, c) 通过关键字go就启动了一个goroutine。我们来看一个例子\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;runtime\u0026quot; ) func say(s string) { for i := 0; i \u0026lt; 5; i++ { runtime.Gosched() fmt.Println(s) } } func main() { go say(\u0026quot;world\u0026quot;) //开一个新的Goroutines执行 say(\u0026quot;hello\u0026quot;) //当前Goroutines执行 } // 以上程序执行后将输出： // hello // world // hello // world // hello // world // hello // world // hello 我们可以看到go关键字很方便的就实现了并发编程。 上面的多个goroutine运行在同一个进程里面，共享内存数据，不过设计上我们要遵循：不要通过共享来通信，而要通过通信来共享。\ngoroutine的调度机制 Go runtime的调度器： 在了解Go的运行时的scheduler之前，需要先了解为什么需要它，因为我们可能会想，OS内核不是已经有一个线程scheduler了嘛？ 熟悉POSIX API的人都知道，POSIX的方案在很大程度上是对Unix process进场模型的一个逻辑描述和扩展，两者有很多相似的地方。 Thread有自己的信号掩码，CPU affinity等。但是很多特征对于Go程序来说都是累赘。 尤其是context上下文切换的耗时。另一个原因是Go的垃圾回收需要所有的goroutine停止，使得内存在一个一致的状态。垃圾回收的时间点是不确定的，如果依靠OS自身的scheduler来调度，那么会有大量的线程需要停止工作。\n单独的开发一个GO得调度器，可以是其知道在什么时候内存状态是一致的，也就是说，当开始垃圾回收时，运行时只需要为当时正在CPU核上运行的那个线程等待即可，而不是等待所有的线程。\n用户空间线程和内核空间线程之间的映射关系有：N:1,1:1和M:N N:1是说，多个（N）用户线程始终在一个内核线程上跑，context上下文切换确实很快，但是无法真正的利用多核。 1：1是说，一个用户线程就只在一个内核线程上跑，这时可以利用多核，但是上下文switch很慢。 M:N是说， 多个goroutine在多个内核线程上跑，这个看似可以集齐上面两者的优势，但是无疑增加了调度的难度。\nGo的调度器内部有三个重要的结构：M，P，S M:代表真正的内核OS线程，和POSIX里的thread差不多，真正干活的人 G:代表一个goroutine，它有自己的栈，instruction pointer和其他信息（正在等待的channel等等），用于调度。 P:代表调度的上下文，可以把它看做一个局部的调度器，使go代码在一个线程上跑，它是实现从N:1到N:M映射的关键。\n图中看，有2个物理线程M，每一个M都拥有一个context（P），每一个也都有一个正在运行的goroutine。 P的数量可以通过GOMAXPROCS()来设置，它其实也就代表了真正的并发度，即有多少个goroutine可以同时运行。 图中灰色的那些goroutine并没有运行，而是出于ready的就绪态，正在等待被调度。P维护着这个队列（称之为runqueue）， Go语言里，启动一个goroutine很容易：go function 就行，所以每有一个go语句被执行，runqueue队列就在其末尾加入一个 goroutine，在下一个调度点，就从runqueue中取出（如何决定取哪个goroutine？）一个goroutine执行。\n为何要维护多个上下文P？因为当一个OS线程被阻塞时，P可以转而投奔另一个OS线程！ 图中看到，当一个OS线程M0陷入阻塞时，P转而在OS线程M1上运行。调度器保证有足够的线程来运行所以的context P。\n图中的M1可能是被创建，或者从线程缓存中取出。\n当MO返回时，它必须尝试取得一个context P来运行goroutine，一般情况下，它会从其他的OS线程那里steal偷一个context过来， 如果没有偷到的话，它就把goroutine放在一个global runqueue里，然后自己就去睡大觉了（放入线程缓存里）。Contexts们也会周期性的检查global runqueue，否则global runqueue上的goroutine永远无法执行。\n另一种情况是P所分配的任务G很快就执行完了（分配不均），这就导致了一个上下文P闲着没事儿干而系统却任然忙碌。但是如果global runqueue没有任务G了，那么P就不得不从其他的上下文P那里拿一些G来执行。一般来说，如果上下文P从其他的上下文P那里要偷一个任务的话，一般就‘偷’run queue的一半，这就确保了每个OS线程都能充分的使用。\nchannels goroutine运行在相同的地址空间，因此访问共享内存必须做好同步。那么goroutine之间如何进行数据的通信呢，Go提供了一个很好的通信机制channel。channel可以与Unix shell 中的双向管道做类比：可以通过它发送或者接收值。这些值只能是特定的类型：channel类型。定义一个channel时，也需要定义发送到channel的值的类型。注意，必须使用make 创建channel：\nci := make(chan int) cs := make(chan string) cf := make(chan interface{}) channel通过操作符\u0026lt;-来接收和发送数据\nch \u0026lt;- v // 发送v到channel ch. v := \u0026lt;-ch // 从ch中接收数据，并赋值给v 我们把这些应用到我们的例子中来：\npackage main import \u0026quot;fmt\u0026quot; func sum(a []int, c chan int) { total := 0 for _, v := range a { total += v } c \u0026lt;- total // send total to c } func main() { a := []int{7, 2, 8, -9, 4, 0} c := make(chan int) go sum(a[:len(a)/2], c) go sum(a[len(a)/2:], c) x, y := \u0026lt;-c, \u0026lt;-c // receive from c fmt.Println(x, y, x + y) } 默认情况下，channel接收和发送数据都是阻塞的，除非另一端已经准备好，这样就使得Goroutines同步变的更加的简单，而不需要显式的lock。所谓阻塞，也就是如果读取（value := \u0026lt;-ch）它将会被阻塞，直到有数据接收。其次，任何发送（ch\u0026lt;-5）将会被阻塞，直到数据被读出。无缓冲channel是在多个goroutine之间同步很棒的工具。\nBuffered Channels 上面我们介绍了默认的非缓存类型的channel，不过Go也允许指定channel的缓冲大小，很简单，就是channel可以存储多少元素。ch:= make(chan bool, 4)，创建了可以存储4个元素的bool 型channel。在这个channel 中，前4个元素可以无阻塞的写入。当写入第5个元素时，代码将会阻塞，直到其他goroutine从channel 中读取一些元素，腾出空间。\nch := make(chan type, value) /* value == 0 ! 无缓冲（阻塞） value \u0026gt; 0 ! 缓冲（非阻塞，直到value 个元素） */ 我们看一下下面这个例子，你可以在自己本机测试一下，修改相应的value值\npackage main import \u0026quot;fmt\u0026quot; func main() { c := make(chan int, 2)//修改2为1就报错，修改2为3可以正常运行 c \u0026lt;- 1 c \u0026lt;- 2 fmt.Println(\u0026lt;-c) fmt.Println(\u0026lt;-c) } //修改为1报如下的错误: //fatal error: all goroutines are asleep - deadlock! Range和Close 上面这个例子中，我们需要读取两次c，这样不是很方便，Go考虑到了这一点，所以也可以通过range，像操作slice或者map一样操作缓存类型的channel，请看下面的例子\npackage main import ( \u0026quot;fmt\u0026quot; ) func fibonacci(n int, c chan int) { x, y := 1, 1 for i := 0; i \u0026lt; n; i++ { c \u0026lt;- x x, y = y, x + y } close(c) } func main() { c := make(chan int, 10) go fibonacci(cap(c), c) for i := range c { fmt.Println(i) } } for i := range c能够不断的读取channel里面的数据，直到该channel被显式的关闭。上面代码我们看到可以显式的关闭channel，生产者通过内置函数close关闭channel。关闭channel之后就无法再发送任何数据了，在消费方可以通过语法v, ok := \u0026lt;-ch测试channel是否被关闭。如果ok返回false，那么说明channel已经没有任何数据并且已经被关闭。\n记住应该在生产者的地方关闭channel，而不是消费的地方去关闭它，这样容易引起panic\n另外记住一点的就是channel不像文件之类的，不需要经常去关闭，只有当你确实没有任何发送数据了，或者你想显式的结束range循环之类的\nSelect 我们上面介绍的都是只有一个channel的情况，那么如果存在多个channel的时候，我们该如何操作呢，Go里面提供了一个关键字select，通过select可以监听channel上的数据流动。\nselect默认是阻塞的，只有当监听的channel中有发送或接收可以进行时才会运行，当多个channel都准备好的时候，select是随机的选择一个执行的。\npackage main import \u0026quot;fmt\u0026quot; func fibonacci(c, quit chan int) { x, y := 1, 1 for { select { case c \u0026lt;- x: x, y = y, x + y case \u0026lt;-quit: fmt.Println(\u0026quot;quit\u0026quot;) return } } } func main() { c := make(chan int) quit := make(chan int) go func() { for i := 0; i \u0026lt; 10; i++ { fmt.Println(\u0026lt;-c) } quit \u0026lt;- 0 }() fibonacci(c, quit) } 在select里面还有default语法，select其实就是类似switch的功能，default就是当监听的channel都没有准备好的时候，默认执行的（select不再阻塞等待channel）。\nselect { case i := \u0026lt;-c: // use i default: // 当c阻塞的时候执行这里 } 超时 有时候会出现goroutine阻塞的情况，那么我们如何避免整个程序进入阻塞的情况呢？我们可以利用select来设置超时，通过如下的方式实现：\nfunc main() { c := make(chan int) o := make(chan bool) go func() { for { select { case v := \u0026lt;- c: println(v) case \u0026lt;- time.After(5 * time.Second): println(\u0026quot;timeout\u0026quot;) o \u0026lt;- true break } } }() \u0026lt;- o } runtime goroutine runtime包中有几个处理goroutine的函数：\nGoexit\n退出当前执行的goroutine，但是defer函数还会继续调用\nGosched\n让出当前goroutine的执行权限，调度器安排其他等待的任务运行，并在下次某个时候从该位置恢复执行。\nNumCPU\n返回 CPU 核数量\nNumGoroutine\n返回正在执行和排队的任务总数\nGOMAXPROCS\n用来设置可以并行计算的CPU核数的最大值，并返回之前的值。\n参考文章:\n《go web编程》 https://www.zhihu.com/question/20862617/answer/27964865 ","date":"2019-06-26","permalink":"https://daemon365.dev/2019/06/26/golang%E5%B9%B6%E5%8F%91/","tags":["go"],"title":"golang并发"},{"content":"检测文件是否存在 //存在返回 true，不存在返回 false func fileIfExist(filename string) bool { _, err := os.Stat(filename) if nil != err { fmt.Println(filename, \u0026quot;is not exist!\u0026quot;) return false } if os.IsNotExist(err) { return false } return true } 打开文件 f, err := os.Open(filename) if nil != err { fmt.Println(\u0026quot;open\u0026quot;, filename, \u0026quot;failed!\u0026quot;) return } defer f.Close() 如果文件不存在，就会返回错误，如果存在就以只读的方式打开文件。\n还可以使用 os.OpenFile() 打开文件，达到不存在就新建，存在就清空（os.O_TRUNC）的目的。当然，也可以不清空文件（os.O_APPEND）。\nf, err := os.OpenFile(filename, os.O_RDWR | os.O_CREATE | os.O_TRUNC, 0666) if nil != err { fmt.Println(\u0026quot;create\u0026quot;, filename, \u0026quot;failed!\u0026quot;) return } defer f.Close() 新建文件 f, err := os.Create(filename) if nil != err { fmt.Println(\u0026quot;create\u0026quot;, filename, \u0026quot;failed!\u0026quot;) return } defer f.Close() 注意：如果文件已经存在，那么 os.Create() 会将文件清空。可以使用 os.OpenFile() 新建文件， 参数 flag 为 os.O_CREATE | os.O_EXCL。如果文件已经存在，那么该函数就会返回错误。\nf, err := os.OpenFile(filename, os.O_CREATE | os.O_EXCL, 0666) if nil != err { fmt.Println(\u0026quot;create\u0026quot;, filename, \u0026quot;failed!\u0026quot;) return } defer f.Close() 读取文件 读取全部内容 content := make([]byte, 1024) //需要预先分配空间 f, _ := os.Open(filename) defer f.Close() _, err := f.Read(content) if nil != err { fmt.Println(\u0026quot;read\u0026quot;, filename, \u0026quot;failed!\u0026quot;) return } 读取文件内容可以使用 File 的方法——Read。但是使用该方法时需要预先分配空间，用于存储读取的文件内容。我们当然可以提前获取文件的大小，但是这种方式仍然不如 ioutil.ReadAll() 方便。甚至可以直接使用 ioutil.ReadFile()。\nioutil.ReadAll()：\nf, _ := os.Open(filename) defer f.Close() content, err := ioutil.ReadAll(f) if nil != err { fmt.Println(\u0026quot;read\u0026quot;, filename, \u0026quot;failed!\u0026quot;) return } fmt.Println(string(content)) ioutil.ReadFile()：\ncontent, err := ioutil.ReadFile(filename) if nil != err { fmt.Println(\u0026quot;read\u0026quot;, filename, \u0026quot;failed!\u0026quot;) return } fmt.Println(string(content)) 按行读取 f, _ := os.Open(filename) defer f.Close() scanner := bufio.NewScanner(f) //按行读取 for scanner.Scan() { fmt.Println(scanner.Text()) //输出文件内容 } 写入文件 f, _ := os.OpenFile(filename, os.O_WRONLY | os.O_APPEND, 0666) defer f.Close() _, err = f.WriteString(\u0026quot;target_compile_option\u0026quot;) if nil != err { fmt.Println(err) } 这里使用 os.OpenFile() 以追加的方式打开文件。为什么不使用 os.Open() 打开文件呢？因为 os.Open() 是以只读的方式打开文件，无法向文件写入数据。\n我们也可以使用 ioutil.WriteFile() 写文件。\nwriteContent := \u0026quot;write file test\u0026quot; err = ioutil.WriteFile(filename, []byte(writeContent), os.ModePerm) if nil != err { fmt.Println(\u0026quot;write\u0026quot;, filename, \u0026quot;failed!\u0026quot;) } 注意：使用 ioutil.WriteFile(filename string, data []byte, perm os.FileMode) 向文件中写入时，如果文件存在，文件会先被清空，然后再写入。如果文件不存在，就会以 perm 权限先创建文件，然后再写入。\n关闭文件 直接调用 File 的 Close() 方法。\nf, _ := os.Open(filename) f.Close() 最好使用 defer 关键字执行 Close() 方法，这样能够保证函数退出时文件能被关闭。\n删除文件 err := os.Remove(filename) 删除文件前确保文件没有被其他程序使用。如果在当前程序中该文件已被打开，需要先关闭（Close()）文件。\n","date":"2019-06-23","permalink":"https://daemon365.dev/2019/06/23/go%E8%AF%AD%E8%A8%80%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/","tags":["go"],"title":"go语言文件系统"},{"content":"函数声明 函数声明包括函数名、形式参数列表、返回值列表（可省略）以及函数体。\nfunc function-name(param...) (result...) { body } 形式参数列表描述了函数的参数名以及参数类型。这些参数作为局部变量，其值由参数调用者提供。返回值列表描述了函数返回值的变量名以及类型。如果函数返回一个无名变量或者没有返回值，返回值列表的括号是可以省略的。如果一个函数声明不包括返回值列表，那么函数体执行完毕后，不会返回任何值。\nfunc hypot(x, y float64) float64 { return math.Sqrt(x*x + y*y) } fmt.Println(hypot(3,4)) // \u0026quot;5\u0026quot; 递归 函数可以是递归的，这意味着函数可以直接或间接的调用自身。对许多问题而言，递归是一种强有力的技术，例如处理递归的数据结构。\nfunc a(i int) (res int){ if i == 1 { return i } return i * a(i - 1) } fmt.Println(a(5)) // 120 多返回值 在Go中，一个函数可以返回多个值。\nfunc calculation(a,b int)(add,sub int) { add = a + b sub = a - b } 错误 在Go中有一部分函数总是能成功的运行，对各种可能的输入都做了良好的处理，使得运行时几乎不会失败，除非遇到灾难性的、不可预料的情况，比如运行时的内存溢出。导致这种错误的原因很复杂，难以处理，从错误中恢复的可能性也很低。\npanic是来自被调函数的信号，表示发生了某个已知的bug。一个良好的程序永远不应该发生panic异常。\nvalue, ok := cache.Lookup(key) if !ok { // ...cache[key] does not exist… } 错误处理策略 当一次函数调用返回错误时，调用者有应该选择何时的方式处理错误。根据情况的不同，有很多处理方式.\nresp,err := http.Get(\u0026quot;https://www.google.com\u0026quot;) if err != nil { fmt.Println(err) } 文件结尾错误（EOF） 函数经常会返回多种错误，这对终端用户来说可能会很有趣，但对程序而言，这使得情况变得复杂。很多时候，程序必须根据错误类型，作出不同的响应。让我们考虑这样一个例子：从文件中读取n个字节。如果n等于文件的长度，读取过程的任何错误都表示失败。如果n小于文件的长度，调用者会重复的读取固定大小的数据直到文件结束。这会导致调用者必须分别处理由文件结束引起的各种错误。基于这样的原因，io包保证任何由文件结束引起的读取失败都返回同一个错误——io.EOF，该错误在io包中定义：\npackage io import \u0026quot;errors\u0026quot; // EOF is the error returned by Read when no more input is available. var EOF = errors.New(\u0026quot;EOF\u0026quot;) 调用者只需通过简单的比较，就可以检测出这个错误。下面的例子展示了如何从标准输入中读取字符，以及判断文件结束。\nin := bufio.NewReader(os.Stdin) for { r, _, err := in.ReadRune() if err == io.EOF { break // finished reading } if err != nil { return fmt.Errorf(\u0026quot;read failed:%v\u0026quot;, err) } // ...use r… } 因为文件结束这种错误不需要更多的描述，所以io.EOF有固定的错误信息——“EOF”。对于其他错误，我们可能需要在错误信息中描述错误的类型和数量，这使得我们不能像io.EOF一样采用固定的错误信息。在7.11节中，我们会提出更系统的方法区分某些固定的错误值。\n函数值 在Go中，函数被看作第一类值（first-class values）：函数像其他值一样，拥有类型，可以被赋值给其他变量，传递给函数，从函数返回。对函数值（function value）的调用类似函数调用。\nfunc add(a,b int) (sum int) { sum = a + b } func main() { f = add fmt.Println(sum(1,2)) } 函数类型的零值是nil。调用值为nil的函数值会引起panic错误：\nvar f func(int) int f(3) // 此处f的值为nil, 会引起panic错误 匿名函数 拥有函数名的函数只能在包级语法块中被声明，通过函数字面量（function literal），我们可绕过这一限制，在任何表达式中表示一个函数值。函数字面量的语法和函数声明相似，区别在于func关键字后没有函数名。函数值字面量是一种表达式，它的值被成为匿名函数（anonymous function）。\nfunc squares() func() int { var x int return func() int { x++ return x * x } } func main() { f := squares() fmt.Println(f()) // \u0026quot;1\u0026quot; fmt.Println(f()) // \u0026quot;4\u0026quot; fmt.Println(f()) // \u0026quot;9\u0026quot; fmt.Println(f()) // \u0026quot;16\u0026quot; } 可变参数 参数数量可变的函数称为为可变参数函数。典型的例子就是fmt.Printf和类似函数。Printf首先接收一个的必备参数，之后接收任意个数的后续参数。\n在声明可变参数函数时，需要在参数列表的最后一个参数类型之前加上省略符号“\u0026hellip;”，这表示该函数会接收任意数量的该类型参数。\nfunc main() { fmt.Println(sum(1,2,3,4,5)) // 15 var sli = []int{1,2,3,4,5} fmt.Println(sli)\t// [1 2 3 4 5] fmt.Println(sum(sli...)) // 15 } func sum(values ...int) int { sum := 0 for _, v := range values { sum += v } return sum } Panic异常 Go的类型系统会在编译时捕获很多错误，但有些错误只能在运行时检查，如数组访问越界、空指针引用等。这些运行时错误会引起painc异常。\n一般而言，当panic异常发生时，程序会中断运行，并执行此goroute上的defer函数。\n当某些不应该发生的场景发生时，我们就应该调用panic。\nname := \u0026quot;zhaohaiyu\u0026quot; switch name { case \u0026quot;zhy\u0026quot;: fmt.Println(\u0026quot;zhy\u0026quot;) case \u0026quot;haiyuzhao\u0026quot;: fmt.Println(\u0026quot;haiyuzhao\u0026quot;) default: panic(\u0026quot;没有这个名字\u0026quot;) } 虽然Go的panic机制类似于其他语言的异常，但panic的适用场景有一些不同。由于panic会引起程序的崩溃，因此panic一般用于严重错误，如程序内部的逻辑不一致。\nRecover捕获异常 通常来说，不应该对panic异常做任何处理，但有时，也许我们可以从异常中恢复，至少我们可以在程序崩溃前，做一些操作。\n如果在deferred函数中调用了内置函数recover，并且定义该defer语句的函数发生了panic异常，recover会使程序从panic中恢复，并返回panic value。导致panic异常的函数不会继续运行，但能正常返回。在未发生panic时调用recover，recover会返回nil。\n让我们以语言解析器为例，说明recover的使用场景。考虑到语言解析器的复杂性，即使某个语言解析器目前工作正常，也无法肯定它没有漏洞。因此，当某个异常出现时，我们不会选择让解析器崩溃，而是会将panic异常当作普通的解析错误，并附加额外信息提醒用户报告此错误。\ndefer func () { if p := recover(); p != nil { fmt.Println(p) // 主动抛错 // 可以进行写日志等操作 } }() panic(\u0026quot;主动抛错\u0026quot;) ","date":"2019-06-22","permalink":"https://daemon365.dev/2019/06/22/golang%E5%87%BD%E6%95%B0/","tags":["go"],"title":"golang函数"},{"content":"整型 Go语言同时提供了有符号和无符号类型的整数运算。这里有int8、int16、int32和int64四种截然不同大小的有符号整数类型，分别对应8、16、32、64bit大小的有符号整数，与此对应的是uint8、uint16、uint32和uint64四种无符号整数类型。\nUnicode字符rune类型是和int32等价的类型，通常用于表示一个Unicode码点。这两个名称可以互换使用。同样byte也是uint8类型的等价类型，byte类型一般用于强调数值是一个原始的数据而不是一个小的整数。\n下面是Go语言中关于算术运算、逻辑运算和比较运算的二元运算符，它们按照优先级递减的顺序排列：\n* / % \u0026gt; \u0026amp; \u0026amp;^ + - | ^ == != \u0026gt;= \u0026amp;\u0026amp; || 两个相同的整数类型可以使用下面的二元比较运算符进行比较；比较表达式的结果是布尔类型。\n== // 等于 != // 不等于 \u0026lt; // 小于 \u0026lt;= // 小于等于 \u0026gt; // 大于 \u0026gt;= // 大于等于 浮点型 Go语言提供了两种精度的浮点数，float32和float64。它们的算术规范由IEEE754浮点数国际标准定义，该浮点数规范被所有现代的CPU支持。\nfloat32类型的浮点数可以提供大约6个十进制数的精度，而float64则可以提供约15个十进制数的精度；通常应该优先使用float64类型，因为float32类型的累计计算误差很容易扩散，\nvar f float32 = 212213 fmt.Println(f == f + 1) 复数 Go语言提供了两种精度的复数类型：complex64和complex128，分别对应float32和float64两种浮点数精度。内置的complex函数用于构建复数，内建的real和imag函数分别返回复数的实部和虚部：\nvar x complex128 = complex(1, 2) // 1+2i var y complex128 = complex(3, 4) // 3+4i fmt.Println(x*y) // \u0026quot;(-5+10i)\u0026quot; fmt.Println(real(x*y)) // \u0026quot;-5\u0026quot; fmt.Println(imag(x*y)) // \u0026quot;10\u0026quot; 4.布尔型 一个布尔类型的值只有两种：true和false。if和for语句的条件部分都是布尔类型的值，并且==和\u0026lt;等比较操作也会产生布尔型的值, 布尔值可以和\u0026amp;\u0026amp;（AND）和||（OR）操作符结合\n!true // flase a := 10 a \u0026gt; 1 // true 字符串 一个字符串是一个不可改变的字节序列。字符串可以包含任意的数据，包括byte值0，但是通常是用来包含人类可读的文本。文本字符串通常被解释为采用UTF8编码的Unicode码点（rune）序列\ns := \u0026quot;hello, world\u0026quot; fmt.Println(len(s)) // \u0026quot;12\u0026quot; len() 长度 fmt.Println(s[0], s[7]) // \u0026quot;104 119\u0026quot; ('h' and 'w') 字符串值也可以用字符串面值方式编写，只要将一系列字节序列包含在双引号即可：\n\u0026quot;Hello, 世界\u0026quot; 因为Go语言源文件总是用UTF8编码，并且Go语言的文本字符串也以UTF8编码的方式处理，因此我们可以将Unicode码点也写到字符串面值中。\n在一个双引号包含的字符串面值中，可以用以反斜杠\\开头的转义序列插入任意的数据。下面的换行、回车和制表符等是常见的ASCII控制代码的转义方式：\n\\a 响铃 \\b 退格 \\f 换页 \\n 换行 \\r 回车 \\t 制表符 \\v 垂直制表符 \\' 单引号 (只用在 '\\'' 形式的rune符号面值中) \\\u0026quot; 双引号 (只用在 \u0026quot;...\u0026quot; 形式的字符串面值中) \\\\ 反斜杠 Unicode 因为计算机只能处理数字，如果要处理文本，就必须先把文本转换为数字才能处理。最早的计算机在设计时采用8个比特（bit）作为一个字节（byte）。一个字节能表示的最大的整数就是255（2^8-1=255），而ASCII编码，占用0 - 127用来表示大小写英文字母、数字和一些符号，这个编码表被称为ASCII编码，比如大写字母A的编码是65，小写字母z的编码是122。\n如果要表示中文，显然一个字节是不够的，至少需要两个字节，而且还不能和ASCII编码冲突，所以，中国制定了GB2312编码，用来把中文编进去。\n类似的，日文和韩文等其他语言也有这个问题。为了统一所有文字的编码，Unicode应运而生。Unicode把所有语言都统一到一套编码里，这样就不会再有乱码问题了。\nUnicode通常用两个字节表示一个字符，原有的英文编码从单字节变成双字节，只需要把高字节全部填为0就可以。\nUTF-8 UTF-8（8位元，Universal Character Set/Unicode Transformation Format）是针对Unicode的一种可变长度字符编码。它可以用来表示Unicode标准中的任何字符，而且其编码中的第一个字节仍与ASCII相容，使得原来处理ASCII字符的软件无须或只进行少部份修改后，便可继续使用。因此，它逐渐成为电子邮件、网页及其他存储或传送文字的应用中，优先采用的编码。\n字符串和Byte切片 标准库中有四个包对字符串处理尤为重要：bytes、strings、strconv和unicode包。\nstrings包提供了许多如字符串的查询、替换、比较、截断、拆分和合并等功能。 bytes包也提供了很多类似功能的函数，但是针对和字符串有着相同结构的[]byte类型。因为字符串是只读的，因此逐步构建字符串会导致很多分配和复制。在这种情况下，使用bytes.Buffer类型将会更有效，稍后我们将展示。 strconv包提供了布尔型、整型数、浮点数和对应字符串的相互转换，还提供了双引号转义相关的转换。 unicode包提供了IsDigit、IsLetter、IsUpper和IsLower等类似功能，它们用于给字符分类。每个函数有一个单一的rune类型的参数，然后返回一个布尔值。而像ToUpper和ToLower之类的转换函数将用于rune字符的大小写转换。所有的这些函数都是遵循Unicode标准定义的字母、数字等分类规范。strings包也有类似的函数，它们是ToUpper和ToLower，将原始字符串的每个字符都做相应的转换，然后返回新的字符串。 path和path/filepath包提供了关于文件路径名更一般的函数操作。使用斜杠分隔路径可以在任何操作系统上工作。斜杠本身不应该用于文件名，但是在其他一些领域可能会用于文件名，例如URL路径组件。相比之下，path/filepath包则使用操作系统本身的路径规则，例如POSIX系统使用/foo/bar，而Microsoft Windows使用c:\\foo\\bar等。 当向bytes.Buffer添加任意字符的UTF8编码时，最好使用bytes.Buffer的WriteRune方法，但是WriteByte方法对于写入类似\u0026rsquo;[\u0026lsquo;和\u0026rsquo;]\u0026lsquo;等ASCII字符则会更加有效。 bytes.Buffer类型有着很多实用的功能，我们在第七章讨论接口时将会涉及到，我们将看看如何将它用作一个I/O的输入和输出对象，例如当做Fprintf的io.Writer输出对象，或者当作io.Reader类型的输入源对象。 字符串和数字的转换 除了字符串、字符、字节之间的转换，字符串和数值之间的转换也比较常见。由strconv包提供这类转换功能。\n将一个整数转为字符串，一种方法是用fmt.Sprintf返回一个格式化的字符串；另一个方法是用strconv.Itoa(“整数到ASCII”)：\nx := 123 y := fmt.Sprintf(\u0026quot;%d\u0026quot;, x) fmt.Println(y, strconv.Itoa(x)) // \u0026quot;123 123\u0026quot; FormatInt和FormatUint函数可以用不同的进制来格式化数字：\nfmt.Println(strconv.FormatInt(int64(x), 2)) // \u0026quot;1111011\u0026quot; fmt.Printf函数的%b、%d、%o和%x等参数提供功能往往比strconv包的Format函数方便很多，特别是在需要包含附加额外信息的时候：\ns := fmt.Sprintf(\u0026quot;x=%b\u0026quot;, x) // \u0026quot;x=1111011\u0026quot; 如果要将一个字符串解析为整数，可以使用strconv包的Atoi或ParseInt函数，还有用于解析无符号整数的ParseUint函数：\nx, err := strconv.Atoi(\u0026quot;123\u0026quot;) // x is an int y, err := strconv.ParseInt(\u0026quot;123\u0026quot;, 10, 64) // base 10, up to 64 bits ParseInt函数的第三个参数是用于指定整型数的大小；例如16表示int16，0则表示int。在任何情况下，返回的结果y总是int64类型，你可以通过强制类型转换将它转为更小的整数类型。\n常量 常量表达式的值在编译期计算，而不是在运行期。每种常量的潜在类型都是基础类型：boolean、string、浮点型或整型。常量不可改变,一个常量的声明也可以包含一个类型和一个值，但是如果没有显式指明类型，那么将从右边的表达式推断类型。\nconst 声明:\nconst a = 10\t// 整型 const p = 3.1415926 // 浮点型 const str = \u0026quot;zhaohaiyu\u0026quot;\t// 字符串 const flag = true\t// 布尔型 如果是批量声明的常量，除了第一个外其它的常量右边的初始化表达式都可以省略，如果省略初始化表达式则表示使用前面常量的初始化表达式写法，对应的常量类型也一样的。\nconst ( a = 1 b c = 2 d ) fmt.Println(a, b, c, d) // \u0026quot;1 1 2 2\u0026quot; iota 常量生成器 常量声明可以使用iota常量生成器初始化，它用于生成一组以相似规则初始化的常量，但是不用每行都写一遍初始化表达式。在一个const声明语句中，在第一个声明的常量所在的行，iota将会被置为0，然后在每一个有常量声明的行加一。\nconst ( a = 1 + iota b c d ) fmt.Println(a,b,c,d) // 1,2,3,4 ","date":"2019-06-22","permalink":"https://daemon365.dev/2019/06/22/golang%E5%9F%BA%E7%A1%80%E7%B1%BB%E5%9E%8B/","tags":["go"],"title":"golang基础类型"},{"content":"命名 Go语言中的函数名、变量名、常量名、类型名、语句标号和包名等所有的命名,都遵循一个简单的命名规则：一个名字必须以一个字母（Unicode字母）或下划线开头,后面可以跟任意数量的字母、数字或下划线.大写字母和小写字母是不同的：heapSort和Heapsort是两个不同的名字.\nGo语言的关键字有25个,关键字不能用于自定义名字. 分别为:\nbreak default func interface select case defer go map struct chan else goto package switch const fallthrough if range type continue for import return var 还有大约30多个预定义的名字,这些内部预先定义的名字并不是关键字，你可以在定义中重新使用它们。在一些特殊的场景中重新定义它们也是有意义的，但是也要注意避免过度而引起语义混乱.\n内建常量: true false iota nil 内建类型: int int8 int16 int32 int64 uint uint8 uint16 uint32 uint64 uintptr float32 float64 complex128 complex64 bool byte rune string error 内建函数: make len cap new append copy close delete complex real imag panic recover 在习惯上，Go语言程序员推荐使用 驼峰式 命名，当名字有几个单词组成的时优先使用大小写分隔，而不是优先用下划线分隔。\n声明 Go语言主要有四种类型的声明语句：var、const、type和func，分别对应变量、常量、类型和函数实体对象的声明。\n一个Go语言编写的程序对应一个或多个以.go为文件后缀名的源文件中。每个源文件以包的声明语句开始，说明该源文件是属于哪个包。包声明语句之后是import语句导入依赖的其它包，然后是包一级的类型、变量、常量、函数的声明语句.\npackage main // main包 import \u0026quot;fmt\u0026quot; // 引入fmt内部包 const price = 212.0 // 浮点型常量 func main() { var f = price // f浮点型变量 var c = (f - 32) * 5 / 9 // c浮点型变量 fmt.Printf(\u0026quot;f = %g c = %g\\n\u0026quot;, f, c) // %g为浮点型占位符 fmt.Printf(\u0026quot;f:%T,c:%T\u0026quot;, f, c) // %T为打印类型 结果:f:float64,c:float64 } 变量 var声明语句可以创建一个特定类型的变量，然后给变量附加一个名字，并且设置变量的初始值。变量声明的一般语法如下：\nvar 变量名字 类型 = 表达式\n// 例如 var name string = \u0026quot;zhy\u0026quot; 其中“类型”或“= 表达式”两个部分可以省略其中的一个。如果省略的是类型信息，那么将根据初始化表达式来推导变量的类型信息。如果初始化表达式被省略，那么将用零值初始化该变量。 数值类型变量对应的零值是0，布尔类型变量对应的零值是false，字符串类型对应的零值是空字符串，接口或引用类型（包括slice、指针、map、chan和函数）变量对应的零值是nil。数组或结构体等聚合类型对应的零值是每个元素或字段都是对应该类型的零值。\nvar s string fmt.Println(s) // \u0026quot;\u0026quot; 也可以在一个声明语句中同时声明一组变量，或用一组初始化表达式声明并初始化一组变量。如果省略每个变量的类型，将可以声明多个类型不同的变量（类型由初始化表达式推导）：\nvar i, j, k int // int, int, int var b, f, s = true, 2.3, \u0026quot;four\u0026quot; // bool, float64, string 一组变量也可以通过调用一个函数，由函数返回的多个返回值初始化：\nvar f, err = os.Open(\u0026quot;./project.log\u0026quot;) // 打开文件 f为句柄 err为错误 简短变量声明 在函数内部，有一种称为简短变量声明语句的形式可用于声明和初始化局部变量。它以“名字 := 表达式”形式声明变量，变量的类型根据表达式来自动推导。\nname := \u0026quot;zhy\u0026quot; f,err := os.Open(\u0026quot;./project.log\u0026quot;) 指针 一个变量对应一个保存了变量对应类型值的内存空间。一个指针的值是另一个变量的地址。一个指针对应变量在内存中的存储位置。并不是每一个值都会有一个内存地址，但是对于每一个变量必然有对应的内存地址。通过指针，我们可以直接读或更新对应变量的值，而不需要知道该变量的名字（如果变量有名字的话）。指针的定义为*+类型比如*int等,或者把变量的地址赋值给指针,编辑器会自动推到指针的类型\nvar number *int var a = 100 var b = \u0026amp;a // \u0026amp;为取值符 用*+指针变量出去所指变量的值\nfmt.Println(*p) // 0 fmt.Println(*b) // 100 New初始化 另一个创建变量的方法是调用用内建的new函数。表达式new(T)将创建一个T类型的匿名变量，初始化为T类型的零值，然后返回变量地址，返回的指针类型为*T。\np := new(int) // p, *int 类型, 指向匿名的 int 变量 fmt.Println(*p) // \u0026quot;0\u0026quot; *p = 2 // 设置 int 匿名变量的值为 2 fmt.Println(*p) // \u0026quot;2\u0026quot; 变量的生命周期 变量的生命周期指的是在程序运行期间变量有效存在的时间间隔。对于在包一级声明的变量来说，它们的生命周期和整个程序的运行周期是一致的。而相比之下，局部变量的声明周期则是动态的：**每次从创建一个新变量的声明语句开始，直到该变量不再被引用为止，然后变量的存储空间可能被回收。**函数的参数变量和返回值变量都是局部变量。它们在函数每次被调用的时候创建。\n赋值 直接赋值x = 100 通过指针赋值*p = \u0026ldquo;zhy\u0026rdquo; 结构体赋值stu.name = \u0026ldquo;zhy\u0026rdquo; 数组切片赋值lst[1] = lst[1] + 1 二元运算符x *= 6等同于x = x * 6 自加自减x++x\u0026ndash; 交换赋值x,y = y,x 函数赋值f, err = os.Open(\u0026ldquo;project.log\u0026rdquo;) 类型 一个类型声明语句创建了一个新的类型名称，和现有类型具有相同的底层结构。新命名的类型提供了一个方法，用来分隔不同概念的类型，这样即使它们底层类型相同也是不兼容的。\ntype 类型名字 底层类型 包 Go语言中的包和其他语言的库或模块的概念类似，目的都是为了支持模块化、封装、单独编译和代码重用。一个包的源代码保存在一个或多个以.go为文件后缀名的源文件中，通常一个包所在目录路径的后缀是包的导入路径；\n作用域 一个声明语句将程序中的实体和一个名字关联，比如一个函数或一个变量。声明语句的作用域是指源代码中可以有效使用这个名字的范围。\nfunc f() {} var g = \u0026quot;g\u0026quot; func main() { f := \u0026quot;f\u0026quot; fmt.Println(f) // main函数的作用域 fmt.Println(g) // g包作用域 fmt.Println(h) // pinic报错 没有找到 } 用:=声明变量,只能在函数内使用,在全局使用会报错\n","date":"2019-06-22","permalink":"https://daemon365.dev/2019/06/22/golang%E5%9F%BA%E7%A1%80%E7%BB%93%E6%9E%84/","tags":["go"],"title":"golang基础结构"},{"content":"数组 **数组是一个由固定长度的特定类型元素组成的序列，一个数组可以由零个或多个元素组成。**因为数组的长度是固定的，因此在Go语言中很少直接使用数组。\n数组的每个元素可以通过索引下标来访问，索引下标的范围是从0开始到数组长度减1的位置。内置的len函数将返回数组中元素的个数。\nvar a [3]int // 长度为3的数组 fmt.Println(a[0]) // 打印第一个数据 fmt.Println(a[len(a)-1]) // 打印最后一个数据 for i, v := range a { fmt.Printf(\u0026quot;%d %d\\n\u0026quot;, i, v) // 循环数组 i为索引 v为数据 } 默认情况下，数组的每个元素都被初始化为元素类型对应的零值\n数组的初始化:\nvar a [3]int = [3]int{1, 2, 3} b := [...]int{1, 2, 3} a = [4]int{1, 2, 3, 4} // panic 数据初始化就是定长了 长度不能变化 切片 Slice（切片）代表变长的序列，序列中每个元素都有相同的类型。一个slice类型一般写作[]T，其中T代表slice中元素的类型；slice的语法和数组很像，只是没有固定长度而已。\n一个slice由三个部分构成：指针、长度和容量。指针指向第一个slice元素对应的底层数组元素的地址，要注意的是slice的第一个元素并不一定就是数组的第一个元素。长度对应slice中元素的数目；长度不能超过容量，容量一般是从slice的开始位置到底层数据的结尾位置。内置的len和cap函数分别返回slice的长度和容量。\n多个slice之间可以共享底层的数据，并且引用的数组部分区间可能重叠。\n使用make()函数构造切片\nmake([]T, size, cap) // T:切片类型 size 切片数量 cap 切片容量 append()方法为切片添加元素\nsli := make([]int,0,10) sli = append(sli,1)\t// 添加一个 1 arr := [4]int{6,7,8,9} sli = append(sle,arr...) // 把arr打散并全部添加 添加多个 fmt.Println(sli) // [1 6 7 8 9] 从切片中删除元素\n// 从切片中删除元素 a := []int{30, 31, 32, 33, 34, 35, 36, 37} // 要删除索引为2的元素 a = append(a[:2], a[3:]...) fmt.Println(a) //[30 31 33 34 35 36 37] 切片的扩容策略\nfunc growslice(et *_type, old slice, cap int) slice { newcap := old.cap doublecap := newcap + newcap if cap \u0026gt; doublecap { newcap = cap } else { if old.len \u0026lt; 1024 { newcap = doublecap } else { for 0 \u0026lt; newcap \u0026amp;\u0026amp; newcap \u0026lt; cap { newcap += newcap / 4 } if newcap \u0026lt;= 0 { newcap = cap } } } 在分配内存空间之前需要先确定新的切片容量，Go 语言根据切片的当前容量选择不同的策略进行扩容：\n如果期望容量大于当前容量的两倍就会使用期望容量； 如果当前切片容量小于 1024 就会将容量翻倍； 如果当前切片容量大于 1024 就会每次增加 25% 的容量，直到新容量大于期望容量； MAP 哈希表是一种巧妙并且实用的数据结构。它是一个无序的key/value对的集合，其中所有的key都是不同的，然后通过给定的key可以在常数时间复杂度内检索、更新或删除对应的value。在Go语言中，一个map就是一个哈希表的引用，map类型可以写为map[K]V，其中K和V分别对应key和value。\n初始化: m := make(map[string]int) 赋值初始化 m := map[string]int{ \u0026quot;zhy\u0026quot;: 18, \u0026quot;who\u0026quot;: 30, } // 直接赋值相当于 ages := make(map[string]int) ages[\u0026quot;zhy\u0026quot;] = 18 ages[\u0026quot;who\u0026quot;] = 30 **Map的迭代顺序是不确定的，并且不同的哈希函数实现可能导致不同的遍历顺序。**遍历的顺序是随机的，每一次遍历的顺序都不相同。\n如果要有序:\n// 用sort进项排序 var names []string for name := range ages { names = append(names, name) } sort.Strings(names) for _, name := range names { fmt.Printf(\u0026quot;%s\\t%d\\n\u0026quot;, name, ages[name]) } 结构体 结构体是一种聚合的数据类型，是由零个或多个任意类型的值聚合成的实体。每个值称为结构体的成员。\ntype people struct { name string age uint8 hobby []string address string sex uint8 } var zhy people 结构体赋值\nh := []string{\u0026quot;唱\u0026quot;,\u0026quot;跳\u0026quot;,\u0026quot;RAP\u0026quot;,\u0026quot;篮球\u0026quot;} zhy := people{\u0026quot;zhaohaiyu\u0026quot;,18,h,\u0026quot;地球\u0026quot;,1} 或者\nvar zhy people zhy.name = \u0026quot;zhaohaiyu\u0026quot; zhy.age = 18 zhy.hobby = h zhy.address = \u0026quot;地球\u0026quot; zhy.sex = 1 结构体比较 如果结构体的全部成员都是可以比较的，那么结构体也是可以比较的，那样的话两个结构体将可以使用或!=运算符进行比较。相等比较运算符将比较两个结构体的每个成员，因此下面两个比较的表达式是等价的：\ntype Point struct{ X, Y int } p := Point{1, 2} q := Point{2, 1} fmt.Println(p.X == q.X \u0026amp;\u0026amp; p.Y == q.Y) // \u0026quot;false\u0026quot; fmt.Println(p == q) // \u0026quot;false\u0026quot; 结构体的继承 在java,python,cpp等语言中都有类的继承的概念,go语言中没有类.用结构体的嵌套实现继承.\ntype animal struct { name string age int } type people struct { address string animal animal } var zhy = people{ address: \u0026quot;地球\u0026quot;, animal: animal{ name: \u0026quot;zhaohaiyu\u0026quot;, age: 18, }, } fmt.Println(zhy.animal) // {zhaohaiyu 18} fmt.Println(zhy.animal.name) // zhaohaiyu fmt.Println(zhy.animal.age) // 18 fmt.Println(zhy.address) //地球 JSON JSON(JavaScript Object Notation, JS 对象简谱) 是一种轻量级的数据交换格式。它基于ECMAScript(欧洲计算机协会制定的js规范)的一个子集，采用完全独立于编程语言的文本格式来存储和表示数据。简洁和清晰的层次结构使得 JSON 成为理想的数据交换语言。 易于人阅读和编写，同时也易于机器解析和生成，并有效地提升网络传输效率。\nGo语言对于这些标准格式的编码和解码都有良好的支持，由标准库中的encoding/json包提供支持\nJSON是对JavaScript中各种类型的值——字符串、数字、布尔值和对象——Unicode本文编码。它可以用有效可读的方式表示第三章的基础数据类型和本章的数组、slice、结构体和map等聚合数据类型。\n各类型的json数据 boolean true number -273.15 string \u0026quot;hello world!!!!\u0026quot; array [\u0026quot;i\u0026quot;, \u0026quot;you\u0026quot;, \u0026quot;her\u0026quot;] object {\u0026quot;year\u0026quot;: 2020, \u0026quot;event\u0026quot;:\u0026quot;huawei\u0026quot;,\u0026quot;American virus\u0026quot;,\u0026quot;Trump is crazy\u0026quot;} go语言结构体成员Tag来指定对应的JSON名字。同样，在解码的时候也需要做同样的处理 type People struct { Name string `json:\u0026quot;name\u0026quot;` Age int `json:\u0026quot;age\u0026quot;` PhoneNumber string `json:\u0026quot;phone_number\u0026quot;` } 要将结构体的数据发给游览器前端或者安卓,IOS,APP等进行展示,因为go语言首字母小写只能本包用,在外部包要首字母大写,包括结构体和结构体成员.而且go语言崇尚驼峰体命名,而很多语言崇尚下划线命名.所有我们要用tag把json数据的成员变成首字母小写以及下划线命名. ","date":"2019-06-22","permalink":"https://daemon365.dev/2019/06/22/golang%E5%A4%8D%E6%9D%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","tags":["go"],"title":"golang复杂数据结构"},{"content":"接口的定义 接口类型是对其它类型行为的抽象和概括；因为接口类型不会和特定的实现细节绑定在一起，通过这种抽象的方式我们可以让我们的函数更加灵活和更具有适应能力。\n很多面向对象的语言都有相似的接口概念，但Go语言中接口类型的独特之处在于它是满足隐式实现的。也就是说，我们没有必要对于给定的具体类型定义所有满足的接口类型；简单地拥有一些必需的方法就足够了。这种设计可以让你创建一个新的接口类型满足已经存在的具体类型却不会去改变这些类型的定义；当我们使用的类型来自于不受我们控制的包时这种设计尤其有用。\n接口（interface）定义了一个对象的行为规范，只定义规范不实现，由具体的对象来实现规范的细节。接口类型是一种抽象的类型。它不会暴露出它所代表的对象的内部值的结构和这个对象支持的基础操作的集合；它们只会展示出它们自己的方法。也就是说当你有看到一个接口类型的值时，你不知道它是什么，唯一知道的就是可以通过它的方法来做什么。\npackage main import \u0026quot;fmt\u0026quot; type canSay interface{ Say() } type dog struct { name string } type cat struct { name string } func (d dog) Say() { fmt.Println(d.name,\u0026quot;say\u0026quot;) } func main() { var tom2 canSay tom := dog{name: \u0026quot;汤姆\u0026quot;} tom2 = tom tom2.Say() // 汤姆 say mi := cat{name: \u0026quot;猫咪\u0026quot;} tom2 = mi // 报错 因为cat没有实现接口规定的say方法 } 接口值 接口值，由两个部分组成，一个具体的类型和那个类型的值。它们被称为接口的动态类型和动态值。在我们的概念模型中，一些提供每个类型信息的值被称为类型描述符，比如类型的名称和方法。在一个接口值中，类型部分代表与之相关类型的描述符。\npackage main import ( \u0026quot;bytes\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;io\u0026quot; \u0026quot;os\u0026quot; ) func main() { var w io.Writer fmt.Printf(\u0026quot;类型:%T,值:%v\\n\u0026quot;,w,w) // 类型:,值: w = os.Stdout fmt.Printf(\u0026quot;类型:%T,值:%v\\n\u0026quot;,w,w) // 类型:*os.File,值:\u0026amp;{0xc00007e280} w = new(bytes.Buffer) fmt.Printf(\u0026quot;类型:%T,值:%v\\n\u0026quot;,w,w) // 类型:*bytes.Buffer,值: w = nil fmt.Printf(\u0026quot;类型:%T,值:%v\\n\u0026quot;,w,w) // 类型: 类型: } 一个包含nil指针的接口不是nil接口\n类型断言 类型断言是一个使用在接口值上的操作。语法上它看起来像x.(T)被称为断言类型，这里x表示一个接口的类型和T表示一个类型。一个类型断言检查它操作对象的动态类型是否和断言的类型匹配。\nx.(T)T表示类型\npackage main import ( \u0026quot;bytes\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;io\u0026quot; \u0026quot;os\u0026quot; ) func main() { var w io.Writer w = os.Stdout f := w.(*os.File) fmt.Printf(\u0026quot;类型%T,值:%v\\n\u0026quot;,f,f) // 类型*os.File,值:\u0026amp;{0xc00014a280} c := w.(*bytes.Buffer) fmt.Printf(\u0026quot;类型%T,值:%v\\n\u0026quot;,c,c) // panic } 判断是什么类型:\npackage main import ( \u0026quot;fmt\u0026quot; ) func judgeType(x interface{}) { switch v := x.(type) { case string: fmt.Printf(\u0026quot;is string:%v\\n\u0026quot;, v) case int: fmt.Printf(\u0026quot;is int:%v\\n\u0026quot;, v) case bool: fmt.Printf(\u0026quot;is bool:%v\\n\u0026quot;, v) default: fmt.Println(\u0026quot;donot know \u0026quot;) } } func main() { judgeType(1) // is int:1 judgeType(true) // is bool:true judgeType(\u0026quot;true\u0026quot;) // is string:true judgeType(1.22) // donot know } 空接口 空接口的定义 空接口是没有定义任何方法的接口。因此任何类型都实现了空接口。空接口类型的变量可以存储任意类型的变量。\npackage main import \u0026quot;fmt\u0026quot; func main() { var test interface{} t1 := 1 test = t1 fmt.Printf(\u0026quot;类型:%T 值:%v\\n\u0026quot;,test,test) // 类型:int 值:1 t2 := \u0026quot;zhaohaiyu\u0026quot; test = t2 fmt.Printf(\u0026quot;类型:%T 值:%v\\n\u0026quot;,test,test) // 类型:string 值:zhaohaiyu t3 := false test = t3 fmt.Printf(\u0026quot;类型:%T 值:%v\\n\u0026quot;,test,test) // 类型:bool 值:false t4 := 3.14 test = t4 fmt.Printf(\u0026quot;类型:%T 值:%v\\n\u0026quot;,test,test) // 类型:float64 值:3.14 } 空接口的应用 空接口作为函数的参数 func show(test interface{}) { fmt.Printf(\u0026quot;类型:%T 值:%v\\n\u0026quot;,test,test) } 空接口作为map的值 var studentInfo = make(map[string]interface{}) studentInfo[\u0026quot;name\u0026quot;] = \u0026quot;赵海宇\u0026quot; studentInfo[\u0026quot;age\u0026quot;] = 18 fmt.Println(studentInfo) // map[age:18 name:赵海宇] ","date":"2019-06-22","permalink":"https://daemon365.dev/2019/06/22/golang%E6%8E%A5%E5%8F%A3/","tags":["go"],"title":"golang接口"},{"content":"方法声明 在函数声明时，在其名字之前放上一个变量，即是一个方法。这个附加的参数会将该函数附加到这种类型上，即相当于为这种类型定义了一个独占的方法。\npackage main import \u0026quot;fmt\u0026quot; type People struct { name string age uint8 } func (p People) SayHello() { fmt.Println(p.name, \u0026quot;: hello world\u0026quot;) p.age = 20 } func main() { p := People{name: \u0026quot;zhaohaiyu\u0026quot;, age: 18} p.SayHello() // zhaohaiyu : hello world fmt.Println(p.age)\t//18 } 基于指针对象的方法 当调用一个函数时，会对其每一个参数值进行拷贝，如果一个函数需要更新一个变量，或者函数的其中一个参数实在太大我们希望能够避免进行这种默认的拷贝，这种情况下我们就需要用到指针了。\npackage main import \u0026quot;fmt\u0026quot; type People struct { name string age uint8 } func (p *People) SayHello() { fmt.Println(p.name, \u0026quot;: hello world\u0026quot;) p.age = 20 } func main() { p := People{name: \u0026quot;zhaohaiyu\u0026quot;, age: 18} p.SayHello() // zhaohaiyu : hello world fmt.Println(p.age)\t// 20 } 调用时p为person的结构体对象,SayHello是People结构体指针的方法,在go中可以直接调用,亦可以(\u0026amp;p).SayHello()\nNil也是一个合法的接收器类型\npackage main import \u0026quot;fmt\u0026quot; type MySlice []int func (m *MySlice) sum() int { var num int for _, i := range *m { num += i } return num } func main() { m := MySlice{1,2,3,4,5} fmt.Println(m.sum()) // 15 m = nil fmt.Println(m.sum()) // 0 } 封装 一个对象的变量或者方法如果对调用方是不可见的话，一般就被定义为“封装”。封装有时候也被叫做信息隐藏，同时也是面向对象编程最关键的一个方面。\nGo语言只有一种控制可见性的手段：大写首字母的标识符会从定义它们的包中被导出，小写字母的则不会。这种限制包内成员的方式同样适用于struct或者一个类型的方法。因而如果我们想要封装一个对象，我们必须将其定义为一个struct。\n这也就是前面的小节中IntSet被定义为struct类型的原因，尽管它只有一个字段：\ntype IntSet struct { words []uint64 } 当然，我们也可以把IntSet定义为一个slice类型，尽管这样我们就需要把代码中所有方法里用到的s.words用*s替换掉了：\ntype IntSet []uint64 ","date":"2019-06-22","permalink":"https://daemon365.dev/2019/06/22/golang%E6%96%B9%E6%B3%95/","tags":["go"],"title":"golang方法"},{"content":"时间类型 time.Time类型表示时间。\nfunc demo() { now := time.Now() //获取当前时间 fmt.Printf(\u0026quot;Now:%v\\n\u0026quot;, now) // Now:2020-08-19 21:53:31.1633023 +0800 CST m=+0.003989401 year := now.Year() //年 month := now.Month() //月 day := now.Day() //日 hour := now.Hour() //小时 minute := now.Minute() //分钟 second := now.Second() //秒 fmt.Printf(\u0026quot;%d-%02d-%02d %02d:%02d:%02d\\n\u0026quot;, year, month, day, hour, minute, second) // 2020-08-19 21:53:31 } 时间戳 func stamp() { now := time.Now() //获取当前时间 timestamp1 := now.Unix() //时间戳 timestamp2 := now.UnixNano() //纳秒时间戳 fmt.Printf(\u0026quot;秒时间戳:%v\\n\u0026quot;, timestamp1) // 秒时间戳:1597845356 fmt.Printf(\u0026quot;纳秒时间戳:%v\\n\u0026quot;, timestamp2) // 纳秒时间戳:1597845356562315400 } 使用time.Unix()函数可以将时间戳转为时间格式。\nfunc demo2(timestamp int64) { timeObj := time.Unix(1462032000, 0) //将时间戳转为时间格式 fmt.Println(timeObj) // 2016-05-01 00:00:00 +0800 CST } 时间格式化 时间类型有一个自带的方法Format进行格式化，需要注意的是Go语言中格式化时间模板不是常见的Y-m-d H:M:S而是使用Go的诞生时间2006年1月2号15点04分\nfunc demo4() { now := time.Now() fmt.Println(now.Format(\u0026quot;2006-01-02 15:04:05.000 Mon Jan\u0026quot;)) // 2020-08-19 22:02:46.296 Wed Aug fmt.Println(now.Format(\u0026quot;2006-01-02 03:04:05.000 PM Mon Jan\u0026quot;)) // 2020-08-19 10:02:46.296 PM Wed Aug fmt.Println(now.Format(\u0026quot;2006*01*02\u0026quot;)) // 2020*08*19 } 解析字符串格式的时间\n// 加载时区 loc, err := time.LoadLocation(\u0026quot;Asia/Shanghai\u0026quot;) if err != nil { fmt.Println(err) return } // 解析字符串时间 timeObj, err := time.ParseInLocation(\u0026quot;2006/01/02 15:04:05\u0026quot;, \u0026quot;2016/04/30 22:00:00\u0026quot;, loc) if err != nil { fmt.Println(err) return } fmt.Println(timeObj) // 2016-04-30 22:00:00 +0800 CST fmt.Println(timeObj.Unix()) // 1462024800 时间操作 func (t Time) Add(d Duration) Time加时间 func (t Time) Sub(u Time) Duration减时间 func (t Time) Before(u Time) bool在u之前 func (t Time) After(u Time) bool在u之后 package main import ( \u0026quot;fmt\u0026quot; \u0026quot;time\u0026quot; ) func formatDemo() { now := time.Now() fmt.Println(now.Format(\u0026quot;2006-01-02 15:04:05.000 Mon Jan\u0026quot;)) // 2020-08-19 22:02:46.296 Wed Aug fmt.Println(now.Format(\u0026quot;2006-01-02 03:04:05.000 PM Mon Jan\u0026quot;)) // 2020-08-19 10:02:46.296 PM Wed Aug fmt.Println(now.Format(\u0026quot;2006*01*02\u0026quot;)) // 2020*08*19 } func main() { // 加载时区 loc, err := time.LoadLocation(\u0026quot;Asia/Shanghai\u0026quot;) if err != nil { fmt.Println(err) return } // 解析字符串时间 timeObj, err := time.ParseInLocation(\u0026quot;2006/01/02 15:04:05\u0026quot;, \u0026quot;2016/04/30 22:00:00\u0026quot;, loc) if err != nil { fmt.Println(err) return } fmt.Println(timeObj) // 2016-04-30 22:00:00 +0800 CST now := time.Now() a := now.Add(time.Hour) fmt.Println(a) // 2020-08-19 23:15:30.0153059 +0800 CST m=+3600.002023801 s := now.Sub(timeObj) fmt.Println(s) // 37728h15m30.0153059s fmt.Println(now.Before(timeObj)) // false fmt.Println(now.After(timeObj)) // true } 定时器 使用time.Tick(时间间隔)来设置定时器，定时器的本质上是一个channel\nfunc tickDemo() { ticker := time.Tick(time.Second) //定义一个1秒间隔的定时器 for i := range ticker { fmt.Println(i) //每秒都会打印时间 } } ","date":"2019-06-21","permalink":"https://daemon365.dev/2019/06/21/golang-time%E5%8C%85/","tags":["go"],"title":"golang time包"},{"content":"error定义 数据结构 go语言error是一普通的值，实现方式为简单一个接口。\n// The error built-in interface type is the conventional interface for // representing an error condition, with the nil value representing no error. type error interface { Error() string } 创建error\n使用errors.New()\n// New returns an error that formats as the given text. // Each call to New returns a distinct error value even if the text is identical. func New(text string) error { return \u0026amp;errorString{text} } // errorString is a trivial implementation of error. type errorString struct { s string } func (e *errorString) Error() string { return e.s } 返回的是errorString结构体 实现了error接口的Error()方法\n使用fmt.Errorf（）创建\n创建方式为把字符串拼接起来，然后调用errors.New().\n基础库中的自定义的error bufio中的错误：\nErrTooLong = errors.New(\u0026quot;bufio.Scanner: token too long\u0026quot;) ErrNegativeAdvance = errors.New(\u0026quot;bufio.Scanner: SplitFunc returns negative advance count\u0026quot;) ErrAdvanceTooFar = errors.New(\u0026quot;bufio.Scanner: SplitFunc returns advance count beyond input\u0026quot;) ErrBadReadCount = errors.New(\u0026quot;bufio.Scanner: Read returned impossible count\u0026quot;) error的比较 package main import ( \u0026quot;errors\u0026quot; \u0026quot;fmt\u0026quot; ) type errorString struct { s string } func new(s string) error { return \u0026amp;errorString{s: s} } func (e *errorString) Error() string { return e.s } func main() { error1 := errors.New(\u0026quot;test\u0026quot;) error2 := new(\u0026quot;test\u0026quot;) fmt.Println(error1 == error2) // false } // 比较结构体 package main import ( \u0026quot;fmt\u0026quot; ) type errorString struct { s string } func new(s string) error { return \u0026amp;errorString{s: s} } func (e *errorString) Error() string { return e.s } func main() { error1 := new(\u0026quot;test\u0026quot;) fmt.Println(error1 == new(\u0026quot;test\u0026quot;)) // false } package main import ( \u0026quot;fmt\u0026quot; ) type errorString struct { s string } func new(s string) error { return errorString{s: s} } func (e errorString) Error() string { return e.s } func main() { error1 := new(\u0026quot;test\u0026quot;) fmt.Println(error1 == new(\u0026quot;test\u0026quot;)) // true } error对比为对比对比实现interface的结构体类型 和结构体本身\nError or Exception 处理错误的演进 C 单返回值，一般通过传递指针作为入参，返回值为 int 表示成功还是失败。 C++ 引入了 exception，但是无法知道被调用方会抛出什么异常。 Java 引入了 checked exception，方法的所有者必须申明，调用者必须处理。在启动时抛出大量的异常是司空见惯的事情，并在它们的调用堆栈中尽职地记录下来。Java 异常不再是异常，而是变得司空见惯了。它们从良性到灾难性都有使用，异常的严重性由函数的调用者来区分 go Go 的处理异常逻辑是不引入 exception，支持多参数返回，所以你很容易的在函数签名中带上实现了 error interface 的对象，交由调用者来判定。\n如果一个函数返回了 value, error，你不能对这个 value 做任何假设，必须先判定 error。唯一可以忽略 error 的是，如果你连 value 也不关心。\nGo 中有 panic 的机制，如果你认为和其他语言的 exception 一样，那你就错了。当我们抛出异常的时候，相当于你把 exception 扔给了调用者来处理。比如，你在 C++ 中，把 string 转为 int，如果转换失败，会抛出异常。或者在 java 中转换 string 为 date 失败时，会抛出异常。\nGo panic 意味着 fatal error(就是挂了)。不能假设调用者来解决 panic，意味着代码不能继续运行。\n使用多个返回值和一个简单的约定，Go 解决了让程序员知道什么时候出了问题，并为真正的异常情况保留了 panic。\n代码对比 package main import \u0026quot;fmt\u0026quot; func Positive(x int) bool { return x \u0026gt;= 0 } func Check(x int) { if Positive(x) { fmt.Println(\u0026quot;正数\u0026quot;) } else { fmt.Println(\u0026quot;负数\u0026quot;) } } func main() { Check(-1) // 负数 Check(0) // 正数 bug Check(1) // 正数 } package main import \u0026quot;fmt\u0026quot; func Positive(x int) (bool, bool) { if x == 0 { return false, false } return x \u0026gt;= 0, true } func Check(x int) { t, ok := Positive(x) if !ok { fmt.Println(\u0026quot;零\u0026quot;) return } if t { fmt.Println(\u0026quot;正数\u0026quot;) } else { fmt.Println(\u0026quot;负数\u0026quot;) } } func main() { Check(-1) // 负数 Check(0) // 零 Check(1) // 正数 } package main import ( \u0026quot;errors\u0026quot; \u0026quot;fmt\u0026quot; ) func Positive(x int) (bool, error) { if x == 0 { return false, errors.New(\u0026quot;为零\u0026quot;) } return x \u0026gt;= 0, nil } func Check(x int) { t, err := Positive(x) if err != nil { fmt.Println(err) return } if t { fmt.Println(\u0026quot;正数\u0026quot;) } else { fmt.Println(\u0026quot;负数\u0026quot;) } } func main() { Check(-1) // 负数 Check(0) // 为零 Check(1) // 正数 } error使用 对于真正意外的情况，那些表示不可恢复的程序错误，例如索引越界、不可恢复的环境问题、栈溢出，我们才使用 panic。对于其他的错误情况，我们应该是期望使用 error 来进行判定。\n简单。\n考虑失败，而不是成功。\n没有隐藏的控制流。\n完全交给你来控制 error。\nError are values。\nSentinel Error 预定义的特定错误，我们叫为 sentinel error，这个名字来源于计算机编程中使用一个特定值来表示不可能进行进一步处理的做法。所以对于 Go，我们使用特定的值来表示错误。\nif err == ErrSomething { … } 类似的 io.EOF，更底层的 syscall.ENOENT。\n使用 sentinel 值是最不灵活的错误处理策略，因为调用方必须使用 == 将结果与预先声明的值进行比较。当您想要提供更多的上下文时，这就出现了一个问题，因为返回一个不同的错误将破坏相等性检查。\n甚至是一些有意义的 fmt.Errorf 携带一些上下文，也会破坏调用者的 == ，调用者将被迫查看 error.Error() 方法的输出，以查看它是否与特定的字符串匹配。\n不依赖检查 error.Error 的输出。 不应该依赖检测 error.Error 的输出，Error 方法存在于 error 接口主要用于方便程序员使用，但不是程序(编写测试可能会依赖这个返回)。这个输出的字符串用于记录日志、输出到 stdout 等。\nSentinel errors 成为你 API 公共部分。 如果您的公共函数或方法返回一个特定值的错误，那么该值必须是公共的，当然要有文档记录，这会增加 API 的表面积。\n如果 API 定义了一个返回特定错误的 interface，则该接口的所有实现都将被限制为仅返回该错误，即使它们可以提供更具描述性的错误。\n比如 io.Reader。像 io.Copy 这类函数需要 reader 的实现者比如返回 io.EOF 来告诉调用者没有更多数据了，但这又不是错误。\nSentinel errors 在两个包之间创建了依赖。 sentinel errors 最糟糕的问题是它们在两个包之间创建了源代码依赖关系。例如，检查错误是否等于 io.EOF，您的代码必须导入 io 包。这个特定的例子听起来并不那么糟糕，因为它非常常见，但是想象一下，当项目中的许多包导出错误值时，存在耦合，项目中的其他包必须导入这些错误值才能检查特定的错误条件(in the form of an import loop)。\n结论: 尽可能避免 sentinel errors。 我的建议是避免在编写的代码中使用 sentinel errors。在标准库中有一些使用它们的情况，但这不是一个您应该模仿的模式。\n错误类型 Error type 是实现了 error 接口的自定义类型。例如 MyError 类型记录了文件和行号以展示发生了什么。\ntype Myerror struct { line int file string s string } func (e *Myerror) Error() string { return e.s } func new(file string, line int, s string) error { return \u0026amp;Myerror{line: line, file: file, s: s} } 因为 MyError 是一个 type，调用者可以使用断言转换成这个类型，来获取更多的上下文信息。\nerr := new(\u0026quot;main.go\u0026quot;, 23, \u0026quot;test error\u0026quot;) switch err := err.(type) { case nil: fmt.Println(\u0026quot;err is nil\u0026quot;) case *Myerror: fmt.Println(\u0026quot;type is *Myerror err line :\u0026quot;, err.line) default: fmt.Println(\u0026quot;None of them\u0026quot;) } // 结果:type is *Myerror err line : 23 与错误值相比，错误类型的一大改进是它们能够包装底层错误以提供更多上下文。\n一个不错的例子就是 os.PathError 他提供了底层执行了什么操作、那个路径出了什么问题。\n调用者要使用类型断言和类型 switch，就要让自定义的 error 变为 public。这种模型会导致和调用者产生强耦合，从而导致 API 变得脆弱。\n结论是尽量避免使用 error types，虽然错误类型比 sentinel errors 更好，因为它们可以捕获关于出错的更多上下文，但是 error types 共享 error values 许多相同的问题。\n因此，我的建议是避免错误类型，或者至少避免将它们作为公共 API 的一部分。\n非透明的error 在我看来，这是最灵活的错误处理策略，因为它要求代码和调用者之间的耦合最少。\n我将这种风格称为不透明错误处理，因为虽然您知道发生了错误，但您没有能力看到错误的内部。作为调用者，关于操作的结果，您所知道的就是它起作用了，或者没有起作用(成功还是失败)。\n这就是不透明错误处理的全部功能–只需返回错误而不假设其内容\npackage main import \u0026quot;os\u0026quot; func test() error { f, err := os.Open(\u0026quot;filename.txt\u0026quot;) if err != nil { return err } // use f } 为行为而不是类型断言错误 在少数情况下，这种二分错误处理方法是不够的。例如，与进程外的世界进行交互(如网络活动)，需要调用方调查错误的性质，以确定重试该操作是否合理。在这种情况下，我们可以断言错误实现了特定的行为，而不是断言错误是特定的类型或值。考虑这个例子：\n// 封装内部 type temporary interface { Temporary() bool } func IsTemporary(err error) bool { te, ok := err.(temporary) return ok \u0026amp;\u0026amp; te.Temporary() } // net包的error type Error interface { error Timeout() bool // Is the error a timeout? Temporary() bool // Is the error temporary? } // 错误处理 if nerr, ok := err.(net.Error); ok \u0026amp;\u0026amp; nerr.Temporary() { // 处理 return } if err != nil { } Handling Error 无错误的正常流程代码，将成为一条直线，而不是缩进的代码。\nf,err := os.Open(\u0026quot;file\u0026quot;) if err != nil { // 处理错误 return } // 逻辑 f,err = os.Open(\u0026quot;file2\u0026quot;) if err != nil { // 处理错误 return } // 逻辑 通过消除错误消除错误处理\n// 改进前 func AutoRusquest() (err error) { err = Anto() if err != nil { return } return } // 改进后 func AutoRusquest() (err error) { return Anto() } func main() { err := AutoRusquest() if err != nil { // log } } // 统计行数 func Countlines(r io.Reader) (int, error) { var ( br = bufio.NewReader(r) lines int err error ) for { _, err = br.ReadString('\\n') lines++ if err != nil { break } } if err != io.EOF { return 0, err } return lines, nil } // 改进后 func Countlines(r io.Reader) (int, error) { sr := bufio.NewScanner(r) lines := 0 for sr.Scan() { lines ++ } return lines,sr.Err() } Wrap errors 传统error的问题 还记得之前我们 auth 的代码吧，如果 Auto 返回错误，则 Aut0Request 会将错误返回给调用方，调用者可能也会这样做，依此类推。在程序的顶部，程序的主体将把错误打印到屏幕或日志文件中，打印出来的只是：没有这样的文件或目录。\n没有生成错误的 file:line 信息。没有导致错误的调用堆栈的堆栈跟踪。这段代码的作者将被迫进行长时间的代码分割，以发现是哪个代码路径触发了文件未找到错误。\nfunc AutoRusquest() (err error) { err = Anto() if err != nil { err = fmt.Errorf(\u0026quot;auto failed:%v\u0026quot;,err) return } return } 但是正如我们前面看到的，这种模式与 sentinel errors 或 type assertions 的使用不兼容，因为将错误值转换为字符串，将其与另一个字符串合并，然后将其转换回 fmt.Errorf 破坏了原始错误，导致等值判定失败。\n你应该只处理一次错误。处理错误意味着检查错误值，并做出单个决策。\nfunc WriteAll(w io.Writer, buf []byte) { w.Write(buf) } 我们经常发现类似的代码，在错误处理中，带了两个任务: 记录日志并且再次返回错误。\nfunc WriteAll(w io.Writer, buf []byte) error { _, err := w.Write(buf) if err != nil { log.Panicln(\u0026quot;write buf failed:\u0026quot;, err) return err } return nil } 在这个例子中，如果在 w.Write 过程中发生了一个错误，那么一行代码将被写入日志文件中，记录错误发生的文件和行，并且错误也会返回给调用者，调用者可能会记录并返回它，一直返回到程序的顶部。\nfunc WriteConfig(w *io.Writer,config *Config) { buf, err := json.Marshal(conf) if err != nil { log.Printf(\u0026quot;could not marshal config: %V\u0026quot;, err) return err } if err := Writeall(w, buf); err != nil { log.Printf(\u0026quot;could not write config: %v\u0026quot;, err) return err } } func main() { err := Writeconfig(f, \u0026amp;conf) fmt.Println(err) } /* unable to write: io.EOF could not write config: io.EOF */ Go 中的错误处理契约规定，在出现错误的情况下，不能对其他返回值的内容做出任何假设。由于 JSON 序列化失败，buf 的内容是未知的，可能它不包含任何内容，但更糟糕的是，它可能包含一个半写的 JSON 片段。\n由于程序员在检查并记录错误后忘记 return，损坏的缓冲区将被传递给 WriteAll，这可能会成功，因此配置文件将被错误地写入。但是，该函数返回的结果是正确的。\n栈处理错误 日志记录与错误无关且对调试没有帮助的信息应被视为噪音，应予以质疑。记录的原因是因为某些东西失败了，而日志包含了答案。\n错误要被日志记录。\n应用程序处理错误，保证100%完整性。\n之后不再报告当前错误。\n包:github.com/pkg/errors\nfunc main() { _, err := Readconfig() if err != nil { fmt.Println(err) os.Exit(1) } } func Readfile(path string) ([]byte, error) { f, err := os.Open(path) if err != nil { return nil, errors.Wrap(err, \u0026quot;open failed\u0026quot;) } defer f.Close() buf, err := ioutil.ReadAll(f) if err != nil { return nil, errors.Wrap(err, \u0026quot;read failed\u0026quot;) } return buf, nil } func Readconfig() ([]byte, error) { home := os.Getenv(\u0026quot;HOME\u0026quot;) config, err := Readfile(filepath.Join(home, \u0026quot;settings.xml\u0026quot;)) return config, errors.WithMessage(err, \u0026quot;could not read config\u0026quot;) } /* could not read config: open failed: open /Users/zhaohaiyu/settings.xml: no such file or directory exit status 1 */ func main() { _, err := Readconfig() if err != nil { fmt.Printf(\u0026quot;original error: %T -\u0026gt; %v\\n\u0026quot;, errors.Cause(err), errors.Cause(err)) fmt.Printf(\u0026quot;stack trace: \\n%+v\\n\u0026quot;, err) os.Exit(1) } } /* original error: *os.PathError -\u0026gt; open /Users/zhaohaiyu/settings.xml: no such file or directory stack trace: open /Users/zhaohaiyu/settings.xml: no such file or directory open failed main.Readfile /Users/zhaohaiyu/code/test/main.go:35 main.Readconfig /Users/zhaohaiyu/code/test/main.go:51 main.main /Users/zhaohaiyu/code/test/main.go:22 runtime.main /usr/local/Cellar/go/1.15.3/libexec/src/runtime/proc.go:204 runtime.goexit /usr/local/Cellar/go/1.15.3/libexec/src/runtime/asm_amd64.s:1374 could not read config exit status 1 */ 通过使用 pkg/errors 包，您可以向错误值添加上下文，这种方式既可以由人也可以由机器检查。\nerrors.Wrap(err, \u0026quot;read failed\u0026quot;) wrap errors使用 在你的应用代码中，使用 errors.New 或者 errros.Errorf 返回错误。 func parseargs(args []string) error { if len(args) \u0026lt; 3 { return errors.Errorf(\u0026quot;not enough arguments, expected at Least\u0026quot;) } // ... return nil } 如果调用其他的函数，通常简单的直接返回。 if err != nil { return err } 如果和其他库进行协作，考虑使用 errors.Wrap 或者 errors.Wrapf 保存堆栈信息。同样适用于和标准库协作的时候。 f, err := os.Open(file) if err != nil { return errors.Wrapf(err, \u0026quot;open %s failed\u0026quot;,file) } 直接返回错误，而不是每个错误产生的地方到处打日志。\n在程序的顶部或者是工作的 goroutine 顶部(请求入口)，使用 %+v 把堆栈详情记录。\nfunc main() { err := app.Run() if err != nil { fmt.Printf(\u0026quot;FATAL:%+v\\n\u0026quot;, err) os.Exit(1) } } 使用 errors.Cause 获取 root error，再进行和 sentinel error 判定。 总结: Packages that are reusable across many projects only return root error values.（选择 wrap error 是只有 applications 可以选择应用的策略。具有最高可重用性的包只能返回根错误值。此机制与 Go 标准库中使用的相同(kit 库的 sql.ErrNoRows)。）\nIf the error is not going to be handled, wrap and return up the call stack.（这是关于函数/方法调用返回的每个错误的基本问题。如果函数/方法不打算处理错误，那么用足够的上下文 wrap errors 并将其返回到调用堆栈中。例如，额外的上下文可以是使用的输入参数或失败的查询语句。确定您记录的上下文是足够多还是太多的一个好方法是检查日志并验证它们在开发期间是否为您工作。）\nOnce an error is handled, it is not allowed to be passed up the call stack any longer.（ 一旦确定函数/方法将处理错误，错误就不再是错误。如果函数/方法仍然需要发出返回，则它不能返回错误值。它应该只返回零(比如降级处理中，你返回了降级数据，然后需要 return nil)。）\nGo1.13 error 1 函数在调用栈中添加信息向上传递错误，例如对错误发生时发生的情况的简要描述。\nif err != nil { return fmt.Errorf(\u0026quot;decompress %v:%v\u0026quot;, name, err) } 使用创建新错误 fmt.Errorf 丢弃原始错误中除文本外的所有内容。正如我们在上面的QueryError 中看到的那样，我们有时可能需要定义一个包含底层错误的新错误类型，并将其保存以供代码检查。这里是 QueryError：\ntype QueryError struct { Query string Err error } 程序可以查看 QueryError \\p值以根据底层错误做出决策。\nif e, ok := err.(*Queryerror); ok \u0026amp;\u0026amp; e.Err == ErrPermission { //query failed because of a permission problem } go1.13为 errors 和 fmt 标准库包引入了新特性，以简化处理包含其他错误的错误。其中最重要的是: 包含另一个错误的 error 可以实现返回底层错误的 Unwrap 方法。如果 e1.Unwrap() 返回 e2，那么我们说 e1 包装 e2，您可以展开 e1 以获得 e2。\n按照此约定，我们可以为上面的 QueryError 类型指定一个 Unwrap 方法，该方法返回其包含的错误:\nfunc (e *Queryerror) Unwrap() error { return e.Err } go1.13 errors 包包含两个用于检查错误的新函数：Is 和 As。\n// Similar to: // if err = Errnotfound {...} if errors.Is(err, Errnotfound) { // something wasnt found } // Similar to // if e, ok := err.(*Queryerror); ok {...} var e *Queryerror // Note: *Queryerror is the type of the error if errorsAs(err, \u0026amp;e) { // err is a *Queryerror, and e is set to the errors value } Wrapping errors with %w 如前所述，使用 fmt.Errorf 向错误添加附加信息。\nif err != nil { return fmt.Errorf(\u0026quot;decompress %v:%v\u0026quot;, name, err) } 在 Go 1.13中 fmt.Errorf 支持新的 %w 谓词。\nif err != nil { return fmt.Errorf(\u0026quot;decompress %v:%w\u0026quot;, name, err) } 用 %w 包装错误可用于 errors.Is 以及 errors.As\nerr := fmt.Errorf(\u0026quot;access denied: % W\u0026quot;, Errpermission) if errors.Is(err, Errpermission) { // ... } Go2介绍 https://go.googlesource.com/proposal/+/master/design/29934-error-values.md\n原文地址 https://www.zhaohaiyu.com/post/go/go-error/ 参考文章 https://u.geekbang.org/subject/go https://lailin.xyz/post/go-training-03.html ","date":"2019-06-20","permalink":"https://daemon365.dev/2019/06/20/golang-error%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86/","tags":["go"],"title":"golang error错误处理"},{"content":"fmt fmt包实现了类似C语言printf和scanf的格式化I/O。主要分为向外输出内容和获取输入内容两大部分。\n向外输出 标准库fmt提供了以下几种输出相关函数。\nPrint Print系列函数会将内容输出到系统的标准输出，区别在于Print函数直接输出内容，Printf函数支持格式化输出字符串，Println函数会在输出内容的结尾添加一个换行符。\nfunc Print(a ...interface{}) (n int, err error) func Printf(format string, a ...interface{}) (n int, err error) func Println(a ...interface{}) (n int, err error) Fprint Fprint系列函数会将内容输出到一个io.Writer接口类型的变量w中，我们通常用这个函数往文件中写入内容。\nfunc Fprint(w io.Writer, a ...interface{}) (n int, err error) func Fprintf(w io.Writer, format string, a ...interface{}) (n int, err error) func Fprintln(w io.Writer, a ...interface{}) (n int, err error) Sprint Sprint系列函数会把传入的数据生成并返回一个字符串。\nfunc Sprint(a ...interface{}) string func Sprintf(format string, a ...interface{}) string func Sprintln(a ...interface{}) string Errorf Errorf函数根据format参数生成格式化字符串并返回一个包含该字符串的错误。\nfunc Errorf(format string, a ...interface{}) error 格式化占位符 *printf系列函数都支持format格式化参数，在这里我们按照占位符将被替换的变量类型划分，方便查询和记忆。\n通用占位符 占位符 说明 %v 值的默认格式表示 %+v 类似%v，但输出结构体时会添加字段名 %#v 值的Go语法表示 %T 打印值的类型 %% 百分号 布尔型 占位符 说明 %t true或false 整型 占位符 说明 %b 表示为二进制 %c 该值对应的unicode码值 %d 表示为十进制 %o 表示为八进制 %x 表示为十六进制，使用a-f %X 表示为十六进制，使用A-F %U 表示为Unicode格式：U+1234，等价于”U+%04X” %q 该值对应的单引号括起来的go语法字符字面值，必要时会采用安全的转义表示 浮点数与复数 占位符 说明 %b 无小数部分、二进制指数的科学计数法，如-123456p-78 %e 科学计数法，如-1234.456e+78 %E 科学计数法，如-1234.456E+78 %f 有小数部分但无指数部分，如123.456 %F 等价于%f %g 根据实际情况采用%e或%f格式（以获得更简洁、准确的输出） %G 根据实际情况采用%E或%F格式（以获得更简洁、准确的输出） 字符串和[]byte 占位符 说明 %s 直接输出字符串或者[]byte %q 该值对应的双引号括起来的go语法字符串字面值，必要时会采用安全的转义表示 %x 每个字节用两字符十六进制数表示（使用a-f） %X 每个字节用两字符十六进制数表示（使用A-F） 指针 占位符 说明 %p 表示为十六进制，并加上前导的0x 宽度标识符 宽度通过一个紧跟在百分号后面的十进制数指定，如果未指定宽度，则表示值时除必需之外不作填充。精度通过（可选的）宽度后跟点号后跟的十进制数指定。如果未指定精度，会使用默认精度；如果点号后没有跟数字，表示精度为0。举例如下：\n占位符 说明 %f 默认宽度，默认精度 %9f 宽度9，默认精度 %.2f 默认宽度，精度2 %9.2f 宽度9，精度2 %9.f 宽度9，精度0 其他falg 占位符 说明 ’+’ 总是输出数值的正负号；对%q（%+q）会生成全部是ASCII字符的输出（通过转义）； ’ ‘ 对数值，正数前加空格而负数前加负号；对字符串采用%x或%X时（% x或% X）会给各打印的字节之间加空格 ’-’ 在输出右边填充空白而不是默认的左边（即从默认的右对齐切换为左对齐）； ’#’ 八进制数前加0（%#o），十六进制数前加0x（%#x）或0X（%#X），指针去掉前面的0x（%#p）对%q（%#q），对%U（%#U）会输出空格和单引号括起来的go字面值； ‘0’ 使用0而不是空格填充，对于数值类型会把填充的0放在正负号后面； 获取输入 Go语言fmt包下有fmt.Scan、fmt.Scanf、fmt.Scanln三个函数，可以在程序运行过程中从标准输入获取用户的输入。\nfmt.Scan 函数定签名如下：\nfunc Scan(a ...interface{}) (n int, err error) Scan从标准输入扫描文本，读取由空白符分隔的值保存到传递给本函数的参数中，换行符视为空白符。 本函数返回成功扫描的数据个数和遇到的任何错误。如果读取的数据个数比提供的参数少，会返回一个错误报告原因。 具体代码示例如下：\nfunc main() { var ( name string age int married bool ) fmt.Scan(\u0026amp;name, \u0026amp;age, \u0026amp;married) fmt.Printf(\u0026quot;扫描结果 name:%s age:%d married:%t \\n\u0026quot;, name, age, married) } 将上面的代码编译后在终端执行，在终端依次输入小王子、28和false使用空格分隔。\n$ ./scan_demo 小王子 28 false 扫描结果 name:小王子 age:28 married:false fmt.Scan从标准输入中扫描用户输入的数据，将以空白符分隔的数据分别存入指定的参数。\nfmt.Scanf 函数签名如下：\nfunc Scanf(format string, a ...interface{}) (n int, err error) Scanf从标准输入扫描文本，根据format参数指定的格式去读取由空白符分隔的值保存到传递给本函数的参数中。 本函数返回成功扫描的数据个数和遇到的任何错误。 代码示例如下：\nfunc main() { var ( name string age int married bool ) fmt.Scanf(\u0026quot;1:%s 2:%d 3:%t\u0026quot;, \u0026amp;name, \u0026amp;age, \u0026amp;married) fmt.Printf(\u0026quot;扫描结果 name:%s age:%d married:%t \\n\u0026quot;, name, age, married) } 将上面的代码编译后在终端执行，在终端按照指定的格式依次输入小王子、28和false。\n$ ./scan_demo 1:zhy 2:18 3:false 扫描结果 name:zhy age:18 married:false fmt.Scanf不同于fmt.Scan简单的以空格作为输入数据的分隔符，fmt.Scanf为输入数据指定了具体的输入内容格式，只有按照格式输入数据才会被扫描并存入对应变量。\n例如，我们还是按照上个示例中以空格分隔的方式输入，fmt.Scanf就不能正确扫描到输入的数据。\n$ ./scan_demo zhy 18 false 扫描结果 zhy: age:0 married:false fmt.Scanln 函数签名如下：\nfunc Scanln(a ...interface{}) (n int, err error) Scanln类似Scan，它在遇到换行时才停止扫描。最后一个数据后面必须有换行或者到达结束位置。 本函数返回成功扫描的数据个数和遇到的任何错误。 具体代码示例如下：\nfunc main() { var ( name string age int married bool ) fmt.Scanln(\u0026amp;name, \u0026amp;age, \u0026amp;married) fmt.Printf(\u0026quot;扫描结果 name:%s age:%d married:%t \\n\u0026quot;, name, age, married) } $ ./scan_demo zhy 18 false 扫描结果 name:zhy age:28 married:false fmt.Scanln遇到回车就结束扫描了，这个比较常用。\nbufio.NewReader 有时候我们想完整获取输入的内容，而输入的内容可能包含空格，这种情况下可以使用bufio包来实现。示例代码如下：\nfunc bufioDemo() { reader := bufio.NewReader(os.Stdin) // 从标准输入生成读对象 fmt.Print(\u0026quot;请输入内容：\u0026quot;) text, _ := reader.ReadString('\\n') // 读到换行 text = strings.TrimSpace(text) fmt.Printf(\u0026quot;%#v\\n\u0026quot;, text) } Fscan系列 这几个函数功能分别类似于fmt.Scan、fmt.Scanf、fmt.Scanln三个函数，只不过它们不是从标准输入中读取数据而是从io.Reader中读取数据。\nfunc Fscan(r io.Reader, a ...interface{}) (n int, err error) func Fscanln(r io.Reader, a ...interface{}) (n int, err error) func Fscanf(r io.Reader, format string, a ...interface{}) (n int, err error) Sscan系列 这几个函数功能分别类似于fmt.Scan、fmt.Scanf、fmt.Scanln三个函数，只不过它们不是从标准输入中读取数据而是从指定字符串中读取数据。\nfunc Sscan(str string, a ...interface{}) (n int, err error) func Sscanln(str string, a ...interface{}) (n int, err error) func Sscanf(str string, format string, a ...interface{}) (n int, err error) 参考文章:https://www.liwenzhou.com/posts/Go/go_fmt/\n","date":"2019-06-20","permalink":"https://daemon365.dev/2019/06/20/golang-fmt%E5%8C%85/","tags":["go"],"title":"golang fmt包"}]